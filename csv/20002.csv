"LDA模型是一种完全的概率主题生成模型，可以利用有效的概率算法来训练和使用模型，但是LDA模型在训练过程中并没有考虑文档之间的链接对主题生成的影响，而在本文提出的RTM模型中，就加入对文档之间链接的计算，计算过程中使用EM算法来对潜在变量进行计算，因为无法准确计算，所以采用变分分布算法。最后我们对没有训练的数据进行预测，分别为根据文档预测链接和根据链接预测文档，在试验结果中可以看到在数据集中两个RTM模型的变型都表现良好。 关键词 :LDA模型，RTM模型，链接，变分分布 Copyright © 2017 by authors and Hans Publishers Inc. This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/"
"近年来，关于文本过滤的研究都集中在过滤模型方面，基本上是引入和改进机器学习领域的相关成果 [ 1 ] 。隐含狄利克雷分布 [ 2 ] 是近年来这方面发展起来的一种重要的离散数据集合的建模方法，首先，LDA模型是完全的概率生成模型 [ 3 ] ，因此具有丰富的内在结构，并且可以利用成熟有效的概率算法来训练和使用模型，再者，LDA模型参数空间的规模是K*N (k是隐含主题的数量，N是此表中词的数量)，与文档的数量无关，使得LDA更适合在大规模语料库上构造文本表示模型。但是LDA模型只考虑文本本身的内容，而忽略了节点之间的联系。"
"这个关系主体模型(RTM)是在统计和机器学习的研究基础上提出的，它是一个可以提供节点属性以及其网络结构的潜在空间模型，一些对网络结构的潜在空间模型已经被提出过 [ 4 ] ，但是，这些模型只单单计算数据，并没有考虑节点属性；解释网络的链接结构或者是节点属性的模型的建立过程大都是通过降维 [ 5 ] 和获取节点属性 [ 6 ] 来建立，都趋向于研究它们中的一个或者它们之外的其他属性 [ 7 ] ，包含一些主题模型 [ 8 ] ，RTM模型综合考虑了节点属性和他们之间的链接结构，这样可以通过其中一个预测到另一个。 对一篇文档来说，每一个节点信息就是它包含的关键词，RTM模型研究的是文档关键词和文档之间的链接的关系。除了可以通过链接预测词语和词语预测链接，还可以对没有经过训练的文档数据进行预测。RTM模型是一种新的针对文档和文档之间链接的概率生成模型 [ 9 ] ，关于生成模型 [ 10 ] ，在之前研究中，通常把链接当作相互独立的单元，与本文提出的研究最相近的是Nallapati和Mei的研究 [ 11 ] ，他们试图扩展混合成员模型 [ 12 ] ，他们假设可交换，在模型中允许使用主题来解释链接并且使用其他词语来解释另外一些词语，但是这样影响了使用词语信息来预测链接信息，与此不同的是，在RTM模型中强制使用主题来解释全部词语和链接。RTM是一种新的概率生成模型，它可以被用来分析链接集，比如网页引用、网页链接、社交网络等，我们在实验中证实了它在分析这些数据的时候是适用的，与之前的模型相比在效果上有了明显的提升。"
"RTM假设一组被观察到的文档 以及他们之间的双链接 是由以下方法产生的，见图1。 1.对于每一个文档d： a.抽取主题比例 b.对于每一个词语 i.抽取主题分配 ii.抽取单词 图1. 大量文件的图形模型 2.对于每一对文档 ： a.抽取二元连接指示器 完整的模式是很难说明的。因为其中包含了所有从文档中观察的词语，以及他们之间的每个可能的链接变量。函数 是两个文件之间的链接分布。这个函数依赖于生成他们的词语 ，这里我们探讨两种可能性。 第一，我们考虑： (1) 其中 ， 表示函数是S状的曲线。这个链接函数对每一对二元变量作隐藏协变量回归建模。它由 参数化、由 拦截。协变量是由 构造，捕捉两个文件隐含主题之间的相似性。 第二：我们考虑： (2) 使用和 相同的协变量，但是由一个指数函数代替，这个函数返回的概率呈指数增长，可以被看作是Blei提出的建模方法 [ 13 ] 的一个近似变体。 下面我们对这两种情况进行分析。 3.2计算过程 图2表示了文档间的关系。 变量y表示两个文件是否有联系。 通过上面的 函数可以认为，潜在特性的期望函数是回应 这个公式，由监督LDA模型 [ 14 ] 可确保用于生成文档内容的相同潜在主题分配，另外还负责生成他们的链接结构。  我们的计算过程参考变分 [ 15 ] 推理过程，在变分推理中使用EM (期望最大化)算法来做参数估计，最大期望(EM)算法是在概率模型 [ 16 ] 中寻找参数极大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测到的潜在变量，那么就是计算观测变量的潜在变量的后验分布，但是准确的后验分布是难 图2. 文档关系图 以计算的，所以只能使用变分分布。在变分分布中，首先假设存在一个由参数索引的潜在变量的分布函数，这些参数接近真实的后验，然后使用相对熵测量(见Jordan等，1999) [ 17 ] 。经过分解后的 是一组狄利克雷参数，每一个对应一个文档；而 是一组多项参数，对应每个文档中每个词语。 (3) 这个推理过程只对观测到的链接建模，即 ，这样做的原因有两个： 首先，当文档d1和d2之间的链接被观测到就修改 ，否则 ，但是这种方法不能证明当 时，d1和d2之间没有链接，此时，对待这些链接作为潜在变量是更符合真实的，例如，在大型的社交网络如Facebook中，两个人之间没有联系并不一定意味着他们不是朋友，他们可能是真正的朋友，只是谁都不知道在彼此的存在而以 [ 18 ] 。 第二，隐藏未被观测到的链接可以降低计算成本，因为计算的复杂性与观察到的链接的数量成正比。 我们选择等式一时，由于等式难以计算，对它进行一次近似 [ 19 ] ，得到等式4于是乎目标转化为计算等式4。 (4) (5) 其中 以及 ，当 是目标函数时，这个式子可以被准确的计算为： (6) 使用坐标上升方法来优化变分参数 和 可得到： 的计算取决于方程式5或者6中对于 的选择。 可以通过对 元素取对数被计算。 是 ，而 是双伽马函数。 的更新和LDA模型中变分参数一样。即 。  我们通过对每个参数计算其极大似然估计的方法来对模型进行调整，主要是多项主题向量 和链接函数参数 、 ，但是我们发现直接计算是比较困难的，所以我们转向求近似值，采用变分EM，在优化式子4的变分分布和模型参数之间迭代。 因为包含 的式子4中术语和传统LDA中的一样，所以估算主题向量可以通过相同的方法，即对称的狄利克雷来估算 。 但是不能在没有经过观察的情况下直接优化链路概率函数的参数，而是通过使用分等级的正则化任意参数化，这种正则化是先假设网络中存在一些潜在的负面意见，然后将其纳入参数估计，负观测的频率由 控制。当使用公式1的逻辑，我们使用基于梯度的优化 [ 20 ] 来估计参数 和 。使用公式5中使用的近似，ELBO的相关梯度是 ， 。 当使用等式2的指数函数时，可以分析出参数 和 。 其中， 。 我们的最终目标是要对新的数据进行预测，分为两个类别的预测：从文字预测链接的和从链接预测文字。 在链接预测中，给定一个新的文档及其中的词语。我们需要从这个文档到其他文件的链接。这需要计算一个关于后验的期望，这个后验我们是无法计算的。 通过之前介绍的推理算法，根据变分分布的优化方法，我们得到了使用训练文本集 [ 21 ] 中的词语和链接以及测试文件中的词语来进行计算的方法，使用近似值 来替换上面提到的后验，那么预测是值就约等于： (7) 在词语预测中，我们仅仅基于链接预测一篇未知文档中的词语，与链路预测一样， 无法计算，使用和上面相同的技术，使用变分分布来近似这个后验，就产生了预测等式： 通过文件和链接，我们的模型能够根据词语预测链接，以及根据链接预测词语，或者两者混合。"
"我们使用一组数据集来做实验，完成分词处理的词语(删除停顿词和常用词)，由定向链接转化为不定向链接，删除没有直接链接的文件 [ 22 ] ，Cora数据 [ 23 ] 包含使用论文搜素引擎搜索出来的摘要，以及文件间相互引用的链接 [ 2 ] 。 评估 RTM模型对未经训练的数据定义了概率分布，根据第三部分所描述的，从数据中推断潜在变量，我们需要知道的是在预测未经训练的数据时这个模型的效果有多好，我们研究上面所说的RTM的两种变体： 图3. 链接预测结果对比图 图4. 词语预测结果对比图 利用等式1使用逻辑链接的逻辑RTM01和利用等式二使用指数链接的指数RTM02，通过两种备选方案来比较这两种模型，首先是基准模型，在这个模型中词语和链接是相互独立的，使用多项分布对词语建模，使用伯努利对链接进行建模；第二种是回归LDA，首次拟合LDA模型对文件处理，然后对观测到的链接进行逻辑回归，并输入每对文件的潜在链接，而不是进行降维和回归，该方法首先进行无监督降维，然后回归潜在空间和潜在的连结结构之间的关系。所有的模型都进行了训练， 的值取5.0。 通过对上面所说模型关于词语预测和链接预测的计算，我们将数据集分成四份，对于每一份数据对应一种模型，我们提出两个预测查询：给定一个未知文档中的词语，他们之间的链接是怎样的；给定文档中的链接，它包含的词语有哪些。其次，预测查询是针对未经过观测训练的文件，在训练测试文档时，它们之间的链接，在图3中显示了链接预测的结果，图4显示的是词语预测的结果。 在预测链接上，RTM的两个变体比另外两个模型在所有数据集上表现都要好，Cora数据集是例证，指数RTM比基准模型提高了6%，比回归LDA提高了5%；逻辑RTM比基准模型提高了将近5%，比回归LDA提高了4%。在词语预测上，RTM的两个变体又一次的比其他模型都要好，这是因为RTM模型使用链接信息去影响词语的预测分布，LDA回归模型和预测和基准模型相似。"
