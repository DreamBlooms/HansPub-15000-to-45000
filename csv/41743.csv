"事件检测与分类任务，包含两个步骤的子任务：识别事件触发词和将其分类为正确的事件类型。在这项任务中首要关键的就是触发词的识别，利用基于神经网络的模型来识别句子中的触发词是这些年的主流方法。然而，当涉及到由语义结构不清和语义相近的字符和词组组成的句子时，识别事件的触发词变得有些困难。本文提出一个融合字与词信息，再通过原型网络来精确事件分类的模型：输入融合字与词的信息的嵌入信息，将各个组成的嵌入信息投影到一个高维的特征空间中，对于每个维度类型的样本信息提取他们的均值作为聚类中心即原型，使用欧几里得距离作为距离度量，训练使得测试样本到自己类别原型的距离越近越好，到其他类别原型的距离越远越好，更精确地识别出句子所包含的触发词，分辨出事件类型。"
"事件检测(Event detection)的目标是检测文本中是否含有事件并对其进行分类。这项工作处理事件检测的任务，其目标是检测预定义事件的发生并对它们进行分类。例如，考虑下面这句话“在巴格达，美国坦克向巴勒斯坦饭店开枪时，一名摄影师丧生。”理想的事件检测系统应该识别两个事件，攻击事件和死亡事件(假设死亡和攻击都在预定义的事件集中)。 在事件检测阶段，触发字无法识别的问题会严重影响事件检测系统的性能。因为在中文中有一些词的含义是处于单字和单词中不一样的，触发器可能是一个单词或者一个单词的特定部分或包含多个单词。在这种情况下，基于单词的方法无法正确定位触发器，从而成为任务的严重限制。有一些基于特征的方法 [ 1 ] [ 2 ] [ 3 ] 被提出来缓解了这个问题，但是这些方法极大程度上依赖于人工标记。针对这一问题，相关人员提出了Nugget Proposal Networks [ 4 ]，该网络通过直接提出以每个字符为中心的整个触发器块来解释字触发器不匹配问题，而不用考虑字边界。然而，该网络的机制将候选触发器的范围限制在一个固定大小的窗口内，这是有些刻板的，并且存在触发器重复的问题。 该文章提出一种用于进行小样本分类任务的原型网络(Prototypical Network)，其认为存在一种嵌入的表示方式，使得每一种类别对应的特征点都聚集在一个单独的原型表示附近。该文章为小样本和零样本设置建立了原型网络基于这一想法，作者采用了一种类似于聚类的方式，实现小样本分类任务。首先，作者采用一种简单的四级神经网络结构将输入的信息映射到高维的特征空间(度量空间)，由于引入了ReLU激活函数因此该映射是非线性的。然后对于每一个类别，取其高维特征向量的均值作为原型(Prototype，可以理解为聚类中心)。最后计算测试样本对应的高维特征向量与各个原型之间的距离，并利用softmax将其转化为概率值，预测该样本的类别。但该文章对一些更核心的问题没有给出解答，比如输入部分采用简单的神经网络结构如何保证提取到的特征信息足够抽象等。 在这篇文章中，我们提出将每个句子字符级的嵌入表示和词组级的嵌入表示结合到一起，输入到原型网络中进行分类。将句子的字符级的嵌入表示和词组级的嵌入表示结合到一起，能够更加抽象细腻表达每个字符元素的特征，从而更准确识别出每个句子中决定事件类型的触发词。接着输入到原型网络中进行进一步的触发词识别和分类，能弥补原型网络原有的训练上的不足，抽象了原型网络的输入，更深层次地识别出事件分类的触发词。 在基于小样本分类的原型网络中，我们将字符信息与词组信息合并，并分析原型网络模型中使用的距离计算函数。原型网络与聚类联系起来 [ 5 ]，证明计算距离时用布雷格曼散度(如平方欧几里德距离)，使用类方法作为原型是合理的。根据以往经验可以确定的是，距离的选择对聚类的准确度十分重要，因为用欧几里德距离进行衡量标准比常用的余弦相似性更为有效和精确。在几项测试中，我们实现了最先进的技能。将字层面与词层面的表示与原型网络小样本分类结合到一起，改进了触发词识别与事件分类的效果。"
"事件检测任务要求检测注释数据中提到的特定类型的事件。之前工作中最常用的基准数据集是ACE 2005语料库。该语料库包括8种事件，33种子事件。我们忽略每一个事件和子事件之间的层次结构，把这个数据集视为33种独立的事件类型。事件检测是自然语言处理的重要课题之一。已经为这项任务提出了许多方法。现有的ACE事件任务方法几乎都遵循监督范式。我们进一步将它们分为基于特征的方法和基于表示的方法。在基于特征的方法中，已经使用了一组不同的策略来将分类线索转换成特征向量。Ahn [ 6 ] 使用词汇特征(如全词)、句法特征(如依存特征)和外部知识特征(WordNet)来提取事件。受每个话语一个意义的假设的启发， [ 7 ] 等学者将来自相关文献的全局证据与用于事件提取的局部决策相结合。为了从文本中获取更多的线索， [ 8 ] [ 9 ] [ 10 ] 等学者提出了ACE事件任务的跨事件和跨实体推理。 [ 11 ] 等学者提出了一个联合模型来捕捉触发器和参数的组合特征。 [ 12 ] 等学者提出了一种全局推理方法，利用潜在的局部和全局信息进行事件检测。近年来，基于表征的方法主导了研究。在这个范例中，候选事件提及由嵌入表示，嵌入通常被馈送到神经网络中。 [ 13 ] 和 [ 14 ] 是这一范式的第一个工作。他们的模型基于卷积神经网络。为了模拟触发器和参数的依赖性， [ 15 ] 等学者提出了一种基于递归神经网络的联合事件提取方法。 [ 16 ] 等学者提出通过监督注意机制对事件检测中的论点信息进行编码。  距离测度学习算法在机器学习中非常重要，此算法目的在于学习一种距离测度d(x i , x j )来表征x i 与x j 之间的某种相似度。其本质是通过对样本进行线性或非线性变换获取另一种更有类别区分度的表示形式，更好地反映了样本的空间特性，在变换过程中保持了样本空间的流形。NCA算法就是一种简单有效的距离测度学习算法。该算法随机选择近邻，通过优化留一法(Leave-one-out, LOO)的交叉检验结果来求得马式距离中的变换矩阵。"
"本文将触发词识别与事件分类视为一项序列标记任务。对于每个字符，模型应该识别它是否是一个触发器的一部分，并将该触发器正确地分类为一个特定的事件类型。我们的模型架构主要包括以下三个部分： (1) 分层表示学习，它揭示字符级、单词级和意义级嵌入向量。 (2) 触发感知特征提取器，通过树形结构LSTM模型自动提取不同层次的语义特征。 (3) 将结合不同层次的特征非线性映射到嵌入空间，计算各特征集群之间的距离，求出均值作为类的原型，然后在测试集中对嵌入的查询点进行分类。  给定输入序列 S = { c 1 , c 2 , ⋯ , c N } ，其中 c i 表示序列中的第i个字符。使用Skip-Gram模型SkipGram方法 [ 17 ]，在字符级，每个字符将被表示为 x c 向量： x i c = e ( c i ) (1) 在单词级，输入序列S也是 S = { w 1 , w 2 , ⋯ , w M } ，其中单个词 w i 表示序列中的第i个词语。在本文中，我们将使用b和e两个索引来表示一个单词的开头和结尾。在单词级中表示为： x b , e w = e ( w b , e ) (2)  触发感知特征提取器是我们模型的核心组件。经过训练，提取器的输出是输入句子的隐藏状态向量h。传统LSTM是递归神经网络(RNN)的扩展，在RNN上增加了门(gates)来控制信息。按传统的来说，LSTM有以下基本门：输入门i、输出门o和遗忘门f。它们共同控制哪些信息要被保留、遗忘和输出。所有三个门都配有相应的权重矩阵W。当前窗口状态c记录了所有流到当前时间的历史信息。因此，基于字符级别的LSTM函数是： [ i i c o i c f i c c ˜ i c ] = [ σ σ σ tanh ] ( W c T [ x i c h i − 1 c ] + b c ) (3) c i c = f i c ⊙ c i − 1 c + i i c ⊙ c ˜ i c (4) h i c = o i c ⊙ tanh ( c i c ) (5) 其中 h i c 是隐藏状态向量。 本文使用的触发词感知网格LSTM是LSTM和lattice LSTM的扩展。假定字符和单词有K个含义，第i个字符 c i 的第j个含义的表示为 s j c i 。 1) 整合字符的多义信息 使用附加的LSTMCell整合该字符的所有含义，因此多义字符 c i 的cellgate计算是： [ i j C i f j C i c ˜ j C i ] = [ σ σ tanh ] ( W c T [ s j c i h c i − 1 ] + b c ) (6) c j c j = f j c i ⊙ c c i − 1 + i j c i ⊙ c ˜ j c i (7) 其中 c j c i 是第i个字符的第j个含义的cell状态， c c i − 1 是第i −1个字符的最终cell状态。为了获得字符的cell状态，使用了一个附加的字符含义门： g j c i = σ ( W T [ x i c c j c i ] + b ) (8) 然后所有的含义需要被动态整合到一个临时的cell状态，如下所示。其中 α j c i 是归一化后的字符含义门： c ∗ c i = ∑ j K α j c i ⊙ c j c i (9) α j c i = exp ( g j c i ) ∑ k K exp ( g k c i ) (10) 2) 整合词的多义信息 式(9)通过合并字符所有的含义信息，得到了临时的cell状态 c ∗ c i ，但是，还需要考虑词语级(word level)的信息， s j W b , e 表示词 W b , e 的第j个含义的表示。和字符类似，使用LSTMCell计算每个单词的cell state： [ i j w b , c f j w b , c c j w b , c ] = [ σ σ tanh ] ( W c T [ s j w b , c h c b − 1 ] + b c ) (11) c j w b , c = f j w b , c ⊙ c c b − 1 + i j w b , c ⊙ c ˜ j w b , c (12) 和式(8)~(10)类似，整合所有senses的cells信息，得到词的cell state： g j w b , c = σ ( W T [ x b c c j w b , e ] + b ) (13) c w b , e = ∑ j K α j w b , e ⊙ c j w b , e (14) α j w b , e = exp ( g j w b , e ) ∑ k K exp ( g k w b , e ) (15) 3) 合并字符信息和词信息 对于字符 c i ，临时的cell状态 c ∗ c i 包含了所有sense的信息。通过式(14)也可以计算出所有以索引i结尾的词的cell状态，记为 { c W b , i | b ∈ [ 1 , i ] , w b , i ∈ D } (D表示词典)。为了保证对应的信息可以流到 c i 最终的cell 状态，使用一个额外的门 g b , i m 整合字符cells和词cells： g b , i m = σ ( W l T [ x i c c b , i w ] + b l ) (16) 字符 c i 最终的cell状态计算如下，其中 α W b , i 和 α C i 分别是word gate和character gate归一化后的值： c c i = ∑ b ∈ { b ′ | w b ′ , i d ∈ D } α w b , i ⊙ c w b , i + α c i ⊙ c ∗ c i (17) 因此，最终的 c c i 可以动态地表示多义的字符和词。如式(5)所示，得到的 c c i 再过一个输出门，得到输出，也就是原型网络的输入。  该网络由四个卷积块构成，每个卷积块包含一个64通道的3 * 3的卷积层，一个批规范化层，一个ReLU激活层和一个2 * 2的最大池化层。 距离度量方式本文选用的是Bregman散度中的平方欧氏距离，这是因为Bregman散度具备一种优良的性质，对于一系列特征点，特征空间中与所有点之间的平均距离最近的点是该系列特征点的均值。正是由于该原因，我们可以直接利用每个类别特征向量的均值作为该类别对应的原型。采用平方欧氏距离作为距离度量方式其效果要优于余弦距离，我们认为利用Bergman散度作为距离度量方式可以将小样本的分类问题转化为一种线性分类模型，而必要的非线性信息包含在嵌入的神经网络中了。 对于训练样本与原型之间的距离d取负数，并利用softmax函数转化为概率值p，对概率值取负对数作为损失。目标是使样本与正确类别原型之间的距离越来越近，与其他类别原型之间的距离越来越远。 本文采用一种称为Episode的小批量梯度下降训练方法，对于每一个批次随机选择 N C 个类别的样本，对于每个类别选择 N S 个样本作为支持训练集(Support Set)，用于计算原型。对于每个类别，除去作为支持训练集的样本，剩余的样本中选取 N Q 个样本作为测试点，用于计算与原型之间的距离，并得到损失，更新网络的参数。作者通过实验发现，用于训练的样本类别数量应该多于测试样本的类别数量，而每个类别训练样本的数量应该与测试样本的数量相同，这样能够改善小样本的分类效果。原作者认为是由于训练中采用更多类别的样本使得训练任务更加困难，减弱了过拟合问题，提高了模型的泛化能力。"
"本文在两个真实数据集上进行了一系列实验：ACE2005中文数据集(ACE 2005)和TAC KBP 2017事件金块检测评估数据集(KBP 2017)。为了更好的比较，我们使用了与以前作品相同的数据分割 [ 18 ] [ 19 ] [ 20 ]。具体来说，ACE2005数据集包含697篇文章，其中569篇用于培训，64篇用于验证，其余64篇用于测试。对于KBP2017中文数据集(LDC2017E55)，我们遵循与 [ 19 ] 等学者相同的设置，分别使用506/20/167文档作为训练/开发/测试集。  评估标准选择标准微平均精度(Standard micro-averaged Precision)、召回率(Recall)和F1分数。对于ACE2005，计算方法与 [ 1 ] 相同。  我们通过在验证数据集上进行网格搜索来调整模型的参数。Adam [ 18 ] 利用学习率衰减作为优化器。文字和感官的嵌入大小都是50。为了避免过拟合，系统中使用了rate机制 [ 13 ]，dropout rate设置为0.5。我们通过提前停止使用验证数据集上的F1结果来选择最佳模型。由于影响有限，我们遵循其他超参数的经验设置。  在这一部分，我们将我们的模型与以前最先进的方法进行比较。之前提出过的最先进的模型有以下几种。 DMCNN：提出了一种动态多池卷积神经网络，它根据事件触发器和参数使用动态多池层来保留更多关键信息。 HBTNGMA：提出了一个具有门控多级注意机制的层次化和偏向性标注网络，以集中整合句子级和文档级信息。 NPN：提出了一种通过自动学习触发器内部组成结构来解决触发器不匹配问题的综合模型。 从表1结果中，我们可以观察到： (1) 对于ACE2005和KBP2017，我们的方法显著优于其他提出的模型，在两个数据集上取得了最好的结果。这表明添加两种级别的表示信息可以更加精确触发词的识别。此外，由于加入了多重级别的信息，高精度识别出触发词，此模型能够更加精确识别出事件类型。 (2) 通过原型网络的聚类将所有候选单词的快捷路径与当前字符联系起来，该模型可以有效地利用字符和单词信息，从而缓解触发词不匹配的问题。此外，使用欧几里德距离大大提高了余弦距离的性能。这种效应对于原型网络更为明显，在原型网络中，因为余弦距离与布雷格曼散度无关，将类原型计算为表示支持点的平均值更自然地适用于欧几里德距离。 Table 1 模型 ACE2005 KBP2017 P R F1 P R F1 字符级 DMCNN 60.10 61.60 60.90 53.67 49.92 51.73 HBTNGMA 41.67 59.29 48.94 40.52 46.76 43.41 词语级 DMCNN 66.60 63.60 65.10 60.43 51.64 55.69 HBTNGMA 54.29 62.82 58.25 46.92 53.57 50.02 字词 混合表征 NPN 70.63 64.74 67.56 58.03 59.91 58.96 我们的方法 68.12 75.46 71.60 66.71 59.85 63.09 表1. 在数据集ACE2005上与数据集KBP2017上，基于字符级提取的DMCNN与HBTNGMA的训练结果，基于词语级的DMCNN与HBTNGMA的训练结果，以及字词混合联合训练学习的NPN与本文方法的结果对比"
"我们提出了字符词组特征与原型网络融合训练事件分类的模型来解决提高触发词识别精确度和明确事件类型分类的问题。字符级与词组级的表示学习可以同时解决触发词不匹配和多义词触发的问题，通过分层表示学习和触发感知特征提取器，再加入原型网络改进事件分类，能够有效地利用多层次信息并学习深层语义特征。在两个真实数据集上的实验表明，该模型能够有效地解决这两个问题，并产生比各种神经网络模型更好的效果。在未来的工作中，我们将在更多语种上进行实验，努力实验出更好的效果。"
