"高维数据已成为现代大数据分析中的热点研究领域。变量选择是一种被广泛用于高维数据分析问题的方法。文献中已出现大量高维变量选择方法，为研究其中有影响的几种方法的适用范围和利弊，本文考虑了lasso、自适应lasso等变量选择方法来研究logistic回归模型中的变量选择问题。首先，通过随机模拟实验研究，分别在低维和高维的情况下比较不同变量选择方法的预测和变量选择效果。然后，在实际数据集中做进一步地实证比较研究。研究结果表明：在同等条件下，自适应lasso在模型预测和可解释性方面均比lasso更具优势。 关键词 :高维数据，变量选择，Logistic回归模型 Copyright © 2019 by author(s) and Hans Publishers Inc. This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/"
"随着科学技术的发展，很多研究领域中的大规模的高维数据越来越多。高维数据分析逐渐成为现代大数据分析中的一个重要的研究课题。对大规模高维数据分析的现实需求推动了统计思维、数据分析方法及理论研究的发展。 在机器学习领域，降维或变量选择技术被广泛运用于高维数据的分析问题中。降维或变量选择技术的一个关键思想是“稀疏原理”，即假设真实模型仅由预测变量的一个稀疏子集来决定。在统计学习过程中，当真实模型存在稀疏表示时，变量选择不仅能确保拟合模型的预测精度，也能提高模型的可解释性。现有文献中提出了很多变量选择的标准。传统的变量选择方法可追溯到 C p 、AIC、BIC等，但它们都存在计算开销会随着数据维度的增加而增加的问题。目前，常用变量选择方法主要有子集选择、系数压缩(或正则化)等方法。子集选择同样存在计算开销很大的问题，而且子集选择是一个离散的过程，这导致模型拟合变异性较高。系数压缩或正则化方法是一个连续的变量选择过程，即可同时进行系数估计和变量选择。它对系数施加一定程度的惩罚，使系数被压缩甚至压缩至0。虽然正则化方法得到的估计是有偏差的，但却能在很大程度上改进模型的预测效果，即牺牲较小的偏差而在很大程度上降低方差，在提高预测精度的同时到达变量选择的目的，实现模型预测精度和可解释性的平衡 [ 1 ] 。在正则化方法中，具有里程碑意义lasso (Tibshirani, 1996) [ 2 ] 是一种基于“ l 1 ”惩罚的正则化方法。在此基础上，又发展出了adaptive lasso (Hui ZOU, 2006) [ 3 ] 等不同形式的变量选择方法。 本文首先介绍logistic回归模型、lasso、自适应lasso的理论研究，然后通过随机模拟实验，在低维数据和高维数据的情况下，从系数估计效果、模型的可解释性、分类正确率等方面对不同的变量选择方法进行比较，并进一步在实际数据集进行比较。"
"本文主要考虑分类问题中的一个重要的模型：logistic回归模型。给定数据 ( x i , y i ) , i = 1 , 2 , ⋯ , n ，其中， x i ∈ ℝ p 是预测变量， y i ∈ { 0 , 1 } 是服从二项分布的响应变量，logistic回归模型的定义为 P ( y i = 1 | x i ) = p i ( β ) = exp ( x i T β ) 1 + exp ( x i T β ) (1) 其中， β ∈ ℝ p 是 p × 1 的系数向量。通常用极大似然估计法求解未知系数向量 β ，即求解(2)式的优化问题。 β ^ M L E = arg max β l ( β ) = arg max β ∑ i = 1 n [ y i log p i ( β ) + ( 1 − y i ) log { 1 − p i ( β ) } ] (2) 从分析上来说，由于(2)不存在闭合解，因此用牛顿迭代法来求解(2)式。牛顿迭代法的迭代过程公式如下： β ^ ( t + 1 ) = β ^ ( t ) − { ∑ i = 1 n   w i ( β ^ ( t ) ) x i x i T } − 1 ∂ l ( β ^ ( t ) ) ∂ β (3) 其中， w i ( β ) = p i ( β ) { 1 − p i ( β ) } ， l ( β ) 为(2)式中的损失函数。若(3)收敛，迭代过程终止。  Lasso是一种可以同时进行变量选择和系数估计的正则化方法(Tibshirani, 1996) [ 3 ] 。Logistic回归模型的lasso估计可定义为 β ^ l a s s o = arg min β ∑ i = 1 n [ − y i ( x i T β ) + log ( 1 + e x i T β ) ] + λ ∑ j = 1 p | β j | (4) 其中， λ 是非负的正则参数。(4)式中的第二项也称为“ l 1 ”惩罚项。当 λ 逐渐增大时，lasso方法可连续地将系数向0压缩，当 λ 足够大时，一些系数可以被压缩为0，从而达到变量选择的目的。从(4)显然可见，lasso方法对所有系数所施加的惩罚都是相同的，因此在lasso方法下，系数的值越大，估计结果的偏差越大，而且lasso估计不具有神谕性(oracle properties)，神谕性指正确选择真实模型中的变量的概率收敛到1，且非零系数的估计是渐近正态的。ZOU (2006)提出了自适应lasso (adaptive lasso)，其关键思想是在“ l 1 ”惩罚项中，对不同的系数 β j 使用不同的自适应权重。Adaptive lasso (以下简称alasso)估计定义为 β ^ a l a s s o = arg min β ∑ i = 1 n [ − y i ( x i T β ) + log ( 1 + e x i T β ) ] + λ ∑ j = 1 p   ω j | β j | (5) 其中， ω = ( ω 1 , ω 2 , ⋯ , ω P ) T 为已知的基于数据集得到的权重向量。不同于lasso，自适应lasso估计具有神谕性。Logistic回归模型中，通常用极大似然估计来设计自适应权重，即 ω j = ( | ( β ^ M L E ) j | ) − γ ， γ 是一个大于0的常数，该方法记为APLR。然而，在高维数据中，由(2)式定义的极大似然估计可能无法求解，此时基于极大似然估计的自适应权重的变量选择方法就不再适用于高维数据的情况。为解决alasso在高维数据中面临的问题，有学者用lasso估计来设计自适应权重(Bielza, C., Robles, V., & Larrañaga, P. (2011)) [ 4 ] ，即 ω j = ( | ( β ^ l a s s o ) j | ) − γ ，该方法记为LAPLR。又有学者提出了基于预测变量的相关系数来设计自适应权重的方法 [ 5 ] ，即 ω j = ( | ( β ^ c b ) j | ) − γ ，其中 β ^ c b 可通过求解(6)式的优化问题得到，该方法记为CAPLR。 β ^ c b = arg min β ∑ i = 1 n [ − y i ( x i T β ) + log ( 1 + e x i T β ) ] + λ ∑ i = 1 p − 1 ∑ j > i { ( β i − β j ) 2 1 − ρ i j + ( β i + β j ) 2 1 + ρ i j } (6) ρ i j 为第i，j个预测变量之间的相关系数，注意 ρ i j ≠ 1 。"
"本文通过随机模拟实验来比较2.2中提到的lasso及基于不同权重设计方案的自适应lasso在logistic回归模型中的变量选择和预测效果。我们主要通过系数的均方误差 M S E ( β ^ ) ，变量选择的特异性 S p e c i f i c i t y ( β ^ , β ) 和敏感性 S e n s i t i v i t y ( β ^ , β ) 、分类正确率率这几个指标来进行比较 [ 6 ] 。它们的定义如(7)，(8)，(9)所示。 M S E ( β ^ ) = 1 B ∑ b = 1 B ‖ β ^ b − β ‖ 2 (7) S e n s i t i v i t y ( β ^ , β ) = # { ( b , j ) : β ^ b j ≠ 0 , β b j ≠ 0 } # { ( b , j ) : β b j ≠ 0 } (8) S p e c i f i c i t y ( β ^ , β ) = # { ( b , j ) : β ^ b j = 0 , β b j = 0 } # { ( b , j ) : β b j = 0 } (9) 其中，B表示随机模拟的次数，“#”表示计数。从(8)式和(9)式可以看出，敏感性指标反映的是真实模型中系数非零的变量的选择情况，敏感性越接近于1，说明变量选择方法越能识别模型的非零系数，若等于1，则说明真实模型的非零参数能完全被识别出来。特异性指标的值越接近于1，说明变量选择的方法能正确识别真实模型中的零系数变量，模型的可解释性越好，若其值等于1，则说明真实模型的零参数能完全被识别出来。两个指标的值越接近于1，变量选择的效果越好。  首先，我们先研究低维情况下的变量选择问题。本文模拟实验的参数设置如下： n = 500 , p = 20 ，预测变量的稀疏水平 s = 5 ，系数真值为 β 0 = ( 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , ⋯ , 0 ) T ， β 0 的前s个系数为1，其余为0。预测变量之间的自相关系数 ρ = 0.5 ，均值向量 μ = 1 p T 。设计矩阵 X n × p 中每一个观测分别从以下几个多元分布中独立生成：1) 多元正态分布： X ~ N ( μ , Σ ) ， Σ i j = ρ | i − j | (下同)，记为G；2) 多元t分布： X ~ t d ( μ , Σ ) ， d = 1 , 3 , 10 为自由度，对应数据集分别记为T1，T3，T10。然后按照模型(1)生成响应变量，随机模拟实验次数为 B = 100 。 由表1可以看出，在不同的数据分布中，基于不同权重的自适应lasso的预测效果及变量选择结果均优于lasso。从模型预测的角度来说，自适应lasso的系数估计的MSE均小于lasso的MSE，且自适应lasso方法的分类正确率均大于lasso方法。从变量选择的角度来说，四种方法的敏感性均为1，表明四种变量选择方法均能正确识别出真实模型中的非零系数(或重要变量)，但在四个数据集中，lasso的特异性均低于自适应lasso方法，说明lasso方法更容易将系数为零0的变量选入模型中，模型的可解释性没有自适应lasso的好，这也反映了自适应lasso方法的神谕性。值得注意的是，基于不同权重的自适应lasso方法适用于不同数据分布。正态分布中，APLR效果最好，在T1和T3中，LAPLR最优，而在T10中则是CAPLR最好。  本节研究高维数据变量选择的问题，即 p > n 。由于极大似然估计不适用于高维数据的情形，因此在一部分我们只比较lasso、LAPLR、CAPLR的变量选择和预测效果。 Table 1 数据 方法 MSE 分类正确率 敏感性 特异性 G lasso 0.5828 0.9445 1.00 0.8480 LAPLR 0.3284 0.9424 1.00 0.9400 APLR 0.2380 0.9479 1.00 0.9940 CAPLR 0.2741 0.9444 1.00 0.9380 T1 lasso 0.9292 0.9651 1.00 0.4600 LAPLR 0.6200 0.9696 1.00 0.9360 APLR 0.8197 0.9695 1.00 0.9307 CAPLR 0.3544 0.9691 1.00 0.9393 T3 lasso 0.4263 0.9463 1.00 0.6707 LAPLR 0.2170 0.9465 1.00 0.8953 APLR 0.2560 0.9476 1.00 0.8587 CAPLR 0.2960 0.9476 1.00 0.8507 T10 lasso 4.3922 0.9240 1.00 0.2813 LAPLR 3.7008 0.9271 1.00 0.4760 APLR 3.3473 0.9289 1.00 0.4247 CAPLR 2.9012 0.9324 1.00 0.5027 表1. 低维情形下的变量选择及预测结果 随机模型试验的参数设置如下： n = 500 , p = 1000 , s = 5 ， β 0 的前五个元素等于1，其余为0。其余参数与低维情形相同。 由表2可知，在高维数据中，两种自适应lasso方法的预测及变量选择效果均优于lasso方法。从模型预测的角度来说，两种自适应lasso系数估计的均方误差MSE更小，分类正确率更高。从变量选择的角度来说，四种方法的敏感性都等于1，表明这两种方法均均能正确识别真实模型中的重要变量，而自LAPLR、CAPLR的特异性在四个数据分布中都等于1，表明自适应lasso能完全识别出真实模型，具有很好的神谕性，模型的可解释性要优于lasso。 Table 2 数据 方法 MSE 分类正确率 敏感性 特异性 G lasso 2.1328 0.8431 1.00 0.9589 LAPLR 1.0352 0.8765 1.00 1.00 CAPLR 1.6061 0.8637 1.00 0.9878 T1 lasso 1.5561 0.8908 1.00 0.9756 LAPLR 0.7094 0.9013 1.00 1.00 CAPLR 0.8462 0.9020 1.00 1.00 T3 lasso 1.1132 0.8057 1.00 0.9664 LAPLR 0.6816 0.8058 1.00 1.00 CAPLR 0.4923 0.8100 1.00 1.00 T10 lasso 0.8466 0.8531 1.00 0.9687 LAPLR 0.2975 0.8627 1.00 1.00 CAPLR 0.2101 0.8600 1.00 1.00 表2. 高维情形下的变量选择及预测结果"
"我们用数据集“Pima Indians Diabetes”对上述几种方法的预测和变量选择效果进行比较。Pima Indians Diabetes数据集包含392个医疗记录，每个记录包含8个医学检测指标以及糖尿病检测结果。数据集的变量描述如表3所示，“diabetes”为响应变量，取值为 { 0 , 1 } ，其余变量为预测变量。 Table 3 变量 含义 pregnant 怀孕次数 glucose 血糖浓度 pressure 舒张压 triceps 三头肌皮肤皱褶厚度 insulin 血清胰岛素浓度 mass 体重 pedigree 糖尿病谱系功能 age 年龄 diabetes 糖尿病检测结果 表3. 皮马印第安人糖尿病数据集信息 *注：Pima Indians Diabetes来源于UCI机器学习数据库。 在实证分析中，本文按80%、20%的比例将Pima Indians Diabetes数据集划分为训练集，测试集。在训练集中，我们使用10重交叉验证，选择出最优的模型，然后在测试集中用该模型进行预测。由表4可知，lasso、APLR、CAPLR选出的变量个数均为6个，而APLR选出了5个变量。四种方法共同选出的变量有“pregnant”、“glucose”、“triceps”、“mass”、“pedigree”，从预测角度来说，自适应lasso方法的预测准确率均优于lasso方法，其中LAPLR的预测准确率最高，为79.21%。 Table 4 lasso LAPLR APLR CAPLR 系 数 估 计 结 果 截距项 −8.5568 −9.6380 −9.0407 −9.9288 pregnant 0.0706 0.1053 0.1593 0.1031 glucose 0.0339 0.0370 0.0369 0.03780 pressure 0 0 0 0 triceps 0.0223 0.0224 0.0252 0.0261 insulin 0 0 0 0 mass 0.0385 0.0514 0.0492 0.0513 pedigree 0.7819 1.1265 1.2209 1.1167 age 0.0285 0.0261 0 0.0290 选择的变量个数 6 6 5 6 分类正确率 74.36% 78.21% 79.49% 76.92% 表4. 四种方法预测及变量选择结果 * 选择的变量个数不包含截距项。"
"本文通过随机模拟实验及实际数据分别比较了lasso、APLR、LAPLR、CAPLR这四种变量选择方法的预测及变量选择效果。随机模拟实验的结果表明：在低维和高维的数据中，APLR、LAPLR、CAPLR的预测及变量选择能力均优于lasso，但是APLR、LAPLR、CAPLR方法之间并不存在绝对最优的方法，在不同的数据集中，这三种方法各有优势。实证分析结果与随机模拟的结果基本一致，即：相较与lasso而言，三种自适应lasso方法的预测准确率更高，APLR方法得到的模型更稀疏，可解释性更好。值得强调的是，不同的变量选择方法适用于不同的数据集，在实际分析中，要对多种方法进行综合比较，从而选择出最适合的方法。"
