"股票市场的情况是一个国家在经济上发展水平的重要参考，在分析股票市场数据时往往需要处理高维度的变量数据。直接分析这些高维度的变量数据是一件困难的工作，因此在处理这些数据时会使用降低变量维度的模拟方法来减少分析的难度。奇异值分解、因子分析和主成分回归是三种最常见的被考虑用来降低变量维度的模拟方法。为了比较这三种方法的模拟效果，本文中使用理论推导和证明的方法，得到三种方法在一定条件下有相同模拟效果的结果。于是可以得出在一定的条件下这三种降低变量维度的模拟方法具有一致性的结论。 关键词 :降维，奇异值分解，因子分解，主成分回归，模拟 Copyright © 2019 by authors and Hans Publishers Inc. This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/"
"股票市场中包含各种各样的时间序列数据，例如股票收盘价、股票价格指数等。在处理多个时间序列的数据时，需要分析的变量维度有时会很高，这给分析和处理带来了一定的难度。尽管每个时间序列都有各自的变化特点，但它们之间有一定的相关性。利用这种相关性，就能够减少变量的数量，从而减少分析的难度和成本。 本文对奇异值分解 [ 1 ] 、因子分析 [ 2 ] [ 3 ] [ 4 ] 和主成分回归 [ 5 ] 三种常见的降维方法进行讨论。其中，奇异值分解方法通过提取的奇异值来构造模拟矩阵；因子分析通过提取因子来构造模拟矩阵；主成分回归通过提取主成分和线性回归的方法来构造模拟矩阵。最后通过理论推导和证明的方式说明在一定的条件下，三种方法得到的模拟结果是一致的。"
"对于时间序列，这里有一定的条件限制。第一点，时间序列为平稳时间序列；第二点，时间序列的期望值为0。本文中以行业价格指数作为例子，不再对假设条件做过多的讨论。设 { X j ( t ) , t ∈ ℕ } 是第j ( j = 1 , 2 , ⋯ , r ) 个行业股票指数每日收益率的时间序列，其中 每 日 收 益 率 = ( 当 日 收 盘 价 − 昨 日 收 盘 价 ) / 昨 日 收 盘 价 为了简化操作，将时间序列 { X j ( t ) , t ∈ ℕ } 简化为随机变量 X j 产生的多个独立同分布的样本。设向量 x j = [ x 1 j , x 2 j , ⋯ , x n j ] T 为第j个行业的时间序列对应的数值。那么对于所有的r个行业，有数据矩阵(观测值矩阵) A ( n × r ) = [ x 1 , x 2 , ⋯ , x r ] (1) 对应的样本协方差矩阵为 S ( r × r ) = [ s 11 2 ⋯ s 1 r 2 ⋮ ⋱ ⋮ s r 1 2 ⋯ s r r 2 ] 假设 n > r ， 1 n ∑ i = 1 n x i j = 0 ( j = 1 , 2 , ⋯ , r ) 且 rank ( A ) = r 。  对式(1)定义的矩阵A，由奇异值分解定理，存在正交矩阵 U ( n × n ) = [ u 1 , u 2 , ⋯ , u n ] (2) V ( r × r ) = [ v 1 , v 1 , ⋯ , v r ] (3) 和矩阵 M ( n × r ) = [ μ 1 μ 2 ⋱ μ r 0 ( r × ( n − r ) ) ] T (4) 使 A = U M V T 其中 μ i ( i = 1 , 2 , ⋯ , r ) 为A的奇异值(默认 μ 1 ≥ μ 2 ≥ ⋯ ≥ μ r ≥ 0 ，下文不再提及)。则 B SVD = ∑ i = 1 s μ i u i v i T (5) 是矩阵A的一个秩为 s ( s < r ) 的同阶模拟矩阵，均方误差 M S E SVD = 1 ( n − 1 ) r t r [ ( A − B ) ( A − B ) T ] = 1 ( n − 1 ) r ∑ i = s + 1 p μ i 2  假设 X j 由 s ( s < r ) 个公共因子 F i 组成，即 X 1 = l 11 F 1 + l 12 F 2 + ⋯ + l 1 s F s + ε 1 X 2 = l 21 F 1 + l 22 F 2 + ⋯ + l 2 s F s + ε 2                                       ⋮ X r = l r 1 F 1 + l r 2 F 2 + ⋯ + l r s F s + ε r 或者写为矩阵形式 X ( r × 1 ) = L ( r × s ) F ( s × 1 ) + ε ( r × 1 ) 设 ( λ ^ k , e ^ k ) , k = 1 , 2 , ⋯ , r 为样本协方差矩阵S的特征值–特征向量对(默认 e ^ k 为单位向量且 λ ^ 1 ≥ λ ^ 2 ≥ ⋯ ≥ λ ^ r ≥ 0 ，下文不再提及)，则 载荷矩阵L的估计值为 L ^ ( r × s ) = [ λ ^ 1 e ^ 1 λ ^ 2 e ^ 2 ⋯ λ ^ s e ^ s ] = { l ^ i j } r × s 矩阵A的近似估计 B FA = A S − 1 L ^ L ^ T (6) 均方误差 M S E FA = 1 ( n − 1 ) r t r [ ( A − B ) ( A − B ) T ] = 1 r ∑ i = 1 r ψ ^ i 其中 ψ ^ i = s i i 2 − l ^ i 1 2 − ⋯ − l ^ i s 2 = Var ( ε ^ i ) 。 (这里载荷矩阵的估计使用的是主成分法，因子得分使用的是回归法。)  对于 X j ( j = 1 , 2 , ⋯ , p ) ，通过样本协方差矩阵S提取其前s个主成分的估计值 Y ^ k = [ y ^ k 1 , y ^ k 2 , ⋯ , y ^ k n ] T = A e ^ k ,     k = 1 , 2 , ⋯ , s 其中 ( λ ^ k , e ^ k ) 为S的特征值-特征向量对，于是有回归函数 x j = β j 1 Y ^ 1 + β j 2 Y ^ 2 + ⋯ + β j s Y ^ s + ε j ,     j = 1 , 2 , ⋯ , r (这里令常数项为0。) 由多元线性回归结果为： β ^ j = [ β j 1 , β j 2 , ⋯ , β j s ] T = ( Y ^ T Y ^ ) − 1 Y ^ T x j 其中 Y ^ ( n × s ) = [ Y ^ 1 Y ^ 2 ⋯ Y ^ s ] 于是矩阵A有近似估计 B PCR = Y ^ ( Y ^ T Y ^ ) − 1 Y ^ T A (7) 均方误差 M S E PCR = 1 ( n − 1 ) r t r [ ( A − B ) ( A − B ) T ]"
"引理3.1 设A是 n × r ( n > r ) 阶实矩阵， s < r = rank ( A ) ，并且有奇异值分解 U M V T ，具体形式见式(2) (3) (4)，则 B ∗ = ∑ i = 1 s μ i u i v i T 是A的秩-s最小二乘逼近，使得在所有秩小于等于s的 n × r 阶矩阵B中，平方误差和 t r [ ( A − B ) ( A − B ) T ] 最小，且最小值为 ∑ i = s + 1 r μ i 2 (见文献 [ 6 ] )。 # 引理3.2 设A是 n × r ( n > r ) 阶实矩阵， r = rank ( A ) ，并且有奇异值分解 U M V T ，具体形式见式(2) (3) (4)，则 A T A v i = μ i 2 v i ,     i = 1 , 2 , ⋯ , r 即 A T A 有特征值-特征向量对 ( μ i 2 , v i ) (见文献 [ 6 ] )。 # 利用之前的三个模型结果和引理3.1、引理3.2，可以证明下面的定理。 定理3.1 对于在2.2、2.3和2.4中三种使用同阶的低维度矩阵 B ( rank ( B ) = s ) 来模拟原数据矩阵 A ( rank ( A ) = r , r > s ) 的方法中(见式(5) (6) (7)，且矩阵A满足2.1.中的假设条件)，并且都使用样本协方 差矩阵S进行操作时，三种方法的模拟结果相同，即模拟矩阵 B FA = B PCR = B SVD = A V s V s T 其中 V s ( r × s ) = [ v 1 , v 2 , ⋯ , v s ] = [ e ^ 1 , e ^ 2 , ⋯ , e ^ s ] 且均方误差 M S E SVD = M S E FA = M S E PCR = 1 r ∑ i = s + 1 r λ ^ i = 1 ( n − 1 ) r ∑ i = s + 1 r μ i 2 达到最小值。 ( ( λ ^ i , e ^ i ) 为矩阵S的特征值-特征向量对， v i 的定义见式(3)， μ i 的定义见式(4)。) # 证明：根据 1 n ∑ i = 1 n x i j = 0 ( j = 1 , 2 , ⋯ , r ) 有 1 n − 1 A T A = S (8) 于是由引理3.2有 λ ^ i = μ i 2 n − 1 ,     i = 1 , 2 , ⋯ , r (9) v i = e ^ i ,     i = 1 , 2 , ⋯ , r (10) (使(10)式成立有时需要做一定的调整，这里我们不多做考虑。) 令 Λ ^ ( s × s ) = [ λ ^ 1 0 ⋯ 0 0 λ ^ 2 ⋯ 0 ⋮ ⋮ ⋱ ⋮ 0 0 ⋯ λ ^ s ] 对于特征值分解方法，有 B SVD = ∑ i = 1 s μ i u i v ′ i = U M V T V s V s T = A V s V s T (11) 对于因子分解方法，由特征值和特征向量的定义，有 S V s = V s Λ ^ (12) 于是 B FA = A S − 1 L ^ L ^ T = A S − 1 V s Λ ^ 1 / 2 ( V s Λ ^ 1 / 2 ) T = A S − 1 V s Λ ^ V s T = A V s V s T (13) 对于主成分回归方法，有 Y ^ = A V s 再使用式(8)，(12)得到 B PCR = Y ^ ( Y ^ T Y ^ ) − 1 Y ^ T A = A V s ( V s T S V s ) − 1 V s T S = A V s V s T (14) 综合式(11)，(13)和(14)得到 B FA = B PCR = B SVD = A V s V s T 于是 M S E SVD = M S E FA = M S E PCR 最后，根据引理3.1和式(9)得到均方误差 M S E SVD = M S E FA = M S E PCR = 1 p ∑ i = s + 1 p λ ^ i = 1 ( n − 1 ) p ∑ i = s + 1 p μ i 2 达到最小值。 #"
"当由多个时间序列构成的数据矩阵满足对应时间序列的期望为零，且特征值和特征向量均由对应的样本协方差矩阵提取时，奇异值分解、因子分析和主成分回归构造的降维模拟方法具有一致性(这里的一致性仅限于上文提到的构造方法)。其中，模拟矩阵的结果仅依赖于所提取的特征向量(或奇异值分解的其中一个正交矩阵)，模拟矩阵均方误差的结果由所提取的特征值(或奇异值)完全决定。"
