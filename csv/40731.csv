"高维数据的变量选择一直是统计学领域的热门研究方向。本文研究SICA罚估计在线性模型变量选择中的应用，结合LLA (Local linear approximation)和坐标下降算法给出一种有效的迭代算法，并提出BIC准则选择正则化参数。实际数据的分析表明，与其他变量选择方法相比较，SICA方法在参数估计精度和变量选择方面具有较好的表现。"
"在统计建模的过程中，我们总是希望筛选出对响应变量影响较强的变量，剔除没有影响或影响较弱的变量。但面对一些高维数据，往往存在维数灾难(curse of dimension)的现象，即数据的维数要远远多于样本量的大小。这对传统的模型选择方法提出了巨大的挑战。 早在1970年，Hoerl和Kennard提出岭回归 [ 1 ]，是对最小二乘估计的改进。岭回归通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。但岭回归不具有变量选择的功能，近年来，统计学家门提出过一系列变量选择的方法 [ 2 ] [ 3 ] [ 4 ]。1996年，受NG (Nonnegative Garrot) [ 5 ] [ 6 ] 方法的启发，Robert Tibshirani首次提出LASSO方法 [ 7 ]。该方法是岭回归的一种特殊形式。它通过构造一个惩罚函数来压缩一些回归系数，即强制系数绝对值之和小于某个固定值；同时设定一些回归系数为零，以此达到变量选择的目的。但LASSO估计是有偏估计，2001年，Fan和Li [ 8 ] 提出SCAD方法，并理论上证明了SCAD罚估计具有Oracle性质。此后，2010年，Zhang提出MCP估计 [ 9 ]，也是一种近似无偏估计。 2009年，Lv and Fan提出了一种新的罚函数估计方法——SICA罚 [ 10 ] 用于变量选择和参数估计。本文针对线性模型，研究基于SICA罚的变量选择，结合LLA和坐标下降提出一种迭代算法，并提出BIC准则选择正则化参数。最后通过对实际数据分析，与其他方法进行比较。"
"考虑线性模型 y = X β + ε (1) 其中，y为 n × 1 的响应变量；X为 n × p 设计阵； β = ( β 1 , β 2 , ⋯ , β p ) ，为 p × 1 维未知参数向量； ε 为 n × 1 独立同分布随机误差向量，均值为0，方差为 δ 2 。 基于模型(1)的SICA惩罚最小二乘定义如下： Q ( n ) ( β ) = 1 2 n ‖ y − X β ‖ 2 + ∑ j = 1 p p λ , τ ( | β j | ) (2) 其中 p λ , τ ( | β j | ) = λ ( τ + 1 ) | β j | | β j | + τ (3) 为SICA惩罚函数， λ , τ 为正则化参数。 λ 控制罚函数的惩罚力度， τ 控制罚函数的凹凸度。当取 λ = 1 , τ 分别取0.01，0.05，0.1，绘制SICA罚函数图像如图1所示。由图可见 τ 的取值越小，SICA罚越趋近于L0罚。 图1. SICA罚函数 对上述(2)式求极小值，得到SICA惩罚最小二乘估计 β ^ = arg min β Q n ( β ) 。记 β ⌢ = { β ⌢ j ; j = 1 , ⋯ , p } 。极小化 Q n ( β ) 的结果使一些系数为0，从而实现了保留对响应变量影响较大的变量，去除对响应变量影响较小的变量。所以我们可以通过对 Q n ( β ) 实施极小化同时达到系数估计和变量选择的效果。"
"在本文中使用局部一次逼近(LLA)方法来求解模型。对于固定正则化参数 λ ，LLA方法采用局部线性近似的方法估计 ∑ j = 1 p P α ( | β j | ) 。对于给定的初值 β 0 = ( β 1 0 , ⋯ , β p 0 ) T ，LLA将 ∑ j = 1 p p α ( | β j | ) 进行一阶泰勒展开： ∑ j = 1 p [ p λ , τ ( | β j 0 | ) + p ′ λ , τ ( | β j 0 | ( | β j | − | β j 0 | ) ) ] (4) 将上式带入SICA罚函数得到： 1 2 ‖ y − X β ‖ 2 + ∑ j = 1 p [ p λ , τ ( | β j ( 0 ) | ) + p ′ λ , τ ( | β j ( 0 ) | ( | β j | − | β j 0 | ) ) ] (5) 其中 p λ , τ ( | β j | ) = λ ( τ + 1 ) | β j | | β j | + τ , j = 1 , 2 , ⋯ , p 。上式其实是一种加权LASSO模型，然后采用坐标下降算法求最小值即得到各参数的值，模型得以求解。  使用SICA方法时要确定参数 λ 和 τ 。对于参数 τ 可以直接取0.01，相当于SCAD中取a = 3.7 [ 11 ]。所以我们只需要固定 τ = 0.01 ，选取适当的参数 λ 即可。AIC准则 [ 12 ]，GCV准则 [ 13 ] 是常用的参数选择方法，但其不具有一致性，易发生过拟合的现象。BIC准则没有这一缺点，所以本文提出基于BIC准则的 的选取： BIC λ = log ‖ y − X β λ ‖ 2 2 n − k + d f λ n log ( n ) (6) 其中 d f λ 为广义自由度： d f λ = t r { X ( X ′ X + n ∑ λ ) − 1 X } 其中 ∑ λ = d i a g ( p ′ λ ( | β ⌢ λ 1 | ) / | β ⌢ λ 1 | , ⋯ , p ′ λ ( | β ⌢ λ k | ) / | β ⌢ λ k | ) ，k为模型中非零参数个数，n为样本数量，取 λ ⌢ = arg min λ ( BIC λ ) 。"
"实际数据分析 本文使用Python的Keras库中的数据集Boston_House，该数据集包含美国人口普查局收集的美国马萨诸塞州波士顿住房价格的有关信息，共506个样本，13个变量。通过多种方法对房价与各自变量之间建立回归模型。对数据的具体描述如下： X 1 代表犯罪率；X 2 代表住宅用地所占比例；X 3 代表非零售地区所占比例；X 4 代表是否在河边(0代表不是，1代表是)；X 5 代表一氧化氮浓度；X 6 代表平均每居民房数；X 7 代表建筑年龄；X 8 代表与市中心的距离；X 9 代表公路可达指数；X 10 代表物业税率；X 11 代表城镇师生比例；X 12 代表黑人比例；X 13 代表低收入人口所占比例。 首先将数据以7:3的比例划分为训练集与测试集。分别采用经典最小二乘法，岭回归，LASSO回归，SICA方法对测试集进行回归建模，将所得模型运用于测试集做预测。从变量选择的效果与预测准确性来比较不同方法的优劣，准确性以MSE作为判断标准。 MSE称为均方误差(Mean Square Error)，是真实值与预测值的差值的平方然后求平均数，计算公式如下： MSE = 1 n ∑ i = 1 n ( y i − y ⌢ i ) 2 (7) 其中 y i 表示真实值， y ⌢ i 表示预测值。 SICA方法首先要确定参数 λ 的值，事先给定 λ 一个取值范围，查看不同 λ 下BIC的值，取使得BIC最小的 λ 为参数。BIC随着参数 λ 变化而变化的结果如图2所示： 图2. BIC变化曲线 可以看出BIC曲线呈现V型，说明已经找到使得BIC最小的 λ 值。模型拟合的系数和MSE如表1，表2所示： Table 1 最小二乘 岭回归 LASSO 逐步回归 SICA MSE 45.0775 61.1200 96.5622 36.8327 44.9028 表1. 不同方法的MSE Table 2 最小二乘 岭回归 LASSO 逐步回归 SICA X 1 0.4007 0.7135 0 0 0 X 2 2.6494 1.7956 0.8958 0.1033 2.5856 X 3 −2.2908 −1.2861 −0.3943 −0.2543 −2.4047 X 4 1.3066 1.1989 0.1809 3.6142 1.3564 X 5 2.2435 0.2506 0 0 0 X 6 5.7128 1.1778 0 0 6.4960 X 7 2.0795 0.3829 0 0 2.2103 X 8 −10.0440 −6.9809 −3.4737 −1.8660 −9.8784 X 9 −1.2864 0.7831 0 0.0334 0 X 1 0 −0.9701 −1.6955 0 −0.0073 0 X 1 1 0.1385 0.0590 0 0 0 X 1 2 10.2976 1.1828 2.6773 0.0099 9.8597 X 1 3 −8.7597 −4.3576 −9.4379 −0.7102 −8.6776 表2. 不同方法的变量系数 SICA方法剔除了5个变量，并且所得模型预测结果的MSE较小。这说明相对于最小二乘回归和岭回归，SICA方法准确的剔除了对因变量影响较小的因素，保留了影响较大的因素。LASSO方法虽然剔除了更多的变量，但是MSE却是最大的，说明剔除了应该保留的变量，变量选择的能力上不如SICA方法。逐步回归方法剔除了5个变量，MSE也小于SICA方法，说明对于这份数据，逐步回归的变量选择与模型预测优于SICA方法。综上，SICA方法无论是变量的选择还是模型的预测精度都优于大部分的传统方法。"
"本文讨论了SICA方法在线性模型的变量选择和参数估计中的应用。通过对实际数据的分析，可以发现相对于大部分的传统方法，本文提出的SCIA方法有更强的变量选择能力，对参数的估计具有更高的精度。"
