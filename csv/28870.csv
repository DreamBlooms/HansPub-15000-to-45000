"Nesterov动量法可以很好地改进梯度下降方向，但是其所有参数都具有相同的学习率，并且学习率需要人为设定。Adadelta算法可以自适应学习率，并且每维参数具有独立的学习率。因此，本文首先基于Adadelta算法推导出每一维的学习率公式，其次将其带入Nesterov动量法中，得到了Ada_Nesterov动量法。为了验证提出的Ada_Nesterov动量法，本文设计了两个实验。实验结果表明：动量参数0.5时，Ada_Nesterov动量法在VggNet_16神经网络架构上，基于CIFAR_100数据集的验证准确率最高，损失最小，收敛速度最快。即Ada_Nesterov动量法改进了Nesterov动量法，具有自适应学习率。 关键词 :Nesterov动量法，Adadelta算法，Ada_Nesterov动量法，自适应学习率 Copyright © 2019 by author(s) and Hans Publishers Inc. This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/"
"优化算法是深度学习的一个重要组成部分，近年来深度学习能够取得瞩目成绩的一个原因为其优化算法的改进。在求解深度学习网络时，优化算法按照反向求导梯度的阶数可以分为一阶方法(梯度下降法 [ 1 ] 、随机梯度下降法 [ 2 ] 、动量法 [ 3 ] 等)和二阶近似法(牛顿法 [ 4 ] [ 5 ] 、共轭梯度法 [ 6 ] [ 7 ] 和存储受限的BFGS(L-BFGS)算法等)。但是由于二阶近似方法计算量很大，对于高维度的数据的优化效果不好，所以本文主要研究一阶方法。 深度学习中一阶梯度优化算法的改进均为以梯度下降法为基础，对其梯度下降方向或者学习率进行改进。Nesterov动量法可以很好地改进梯度下降方向，但是其所有参数都具有相同的学习率，并且学习率需要人为设定。Adadelta算法具有自适应学习率的功能，但是在训练后期存在梯度下降方向不准确的现象。因此，本文结合Nesterov动量法的梯度下降策略和Adadelta算法的自适应学习率策略，提出了具有自适应学习率，且梯度下降方向准确的Ada_Nesterov动量法。"
"深度学习优化算法求解损失函数最小化的问题，其基础的算法为梯度下降法(Gradient Descent, GD) [ 1 ] 。在深度学习领域，梯度下降法又被称为批量梯度下降法(BGD)。在批量梯度下降法中，学习率可以理解为梯度下降法的步长，梯度可以理解为梯度下降法的下降方向，学习率决定以多大的步长更新参数。批量梯度下降法在每次迭代的时候，通过计算数据集上所有数据来完成一次迭代，该算法可以得到较准确的局部最优解或者全局最优解，但是随着数据集的增多，算法速度会越来越低。同时，该算法不能在训练过程中添加数据。  为了解决批量梯度下降法的求解速度问题，Bottou [ 2 ] 于1998年提出了随机梯度下降法。与批量梯度下降法不同的是随机梯度下降法首先把数据集分成m个小批量样本，然后通过计算他们的梯度均值，计算梯度无偏估计，最后进行参数迭代。当m = 1时，称为在线梯度下降法；当m > 1时，称为小批量随机梯度下降法。 随机梯度下降法的优点为收敛速度快，其计算时间不取决于数据集的大小。对于大容量的数据集，往往在没有全部计算完数据之前，其收敛精度已经达到规定的范围。随机梯度下降法的缺点为需要人为调参。由于在随机梯度下降法随机采样m个训练样本，引入了梯度估计的噪声源(在极点处不消失)，因此随机梯度下降法不能像批量梯度下降法固定学习率的数值，而是要根据实验结果人为的调整学习率的大小。  随机梯度下降法虽然比梯度下降法的收敛速度提高，但是在接近最优点时有时很难通过陡谷，即在一个维度上的表面弯曲程度远大于其他维度的区域 [ 3 ] 。为了提高随机梯度下降法的收敛速度Polyak等人 [ 8 ] 提出了动量法。 与随机梯度下降法不同的是动量法在速度向量上乘以了一个超参数α ϵ [0,1)。和学习率一样，超参数也需要不断调整。对于梯度点处具有相同方向的维度时，超参数增大；相反，超参数减小。从而，通过减少摇摆，提高收敛速度 [ 9 ] 。  动量法梯度下降改进了梯度的方向，但是该梯度方向具有一定的盲目性，不能预测以后的梯度方向是上升还是下降。受Nesterov [ 10 ] [ 11 ] 影响，Sutskever等人 [ 12 ] 提出了动量算法的一个变种——Nesterov动量法。Nesterov动量法在动量法的梯度计算之前，对参θ进行了校正，其更新规则如(公式(1)、公式(2))所示： θ ← θ + Δ θ (1) Δ θ = ∂ θ − η ∇ θ ( 1 m ∑ i = 1 m L ( f ( x ( i ) ; θ ) , y ( i ) ) ) (2) 实验表明在凸批量梯度的情况下，Nesterov动量法将额外误差收敛率从O(1/k) (k步后)改进到(1/k 2 ) [ 12 ] 。但是，在随机梯度的情况下，Nesterov动量法没有改进收敛率。  由于损失通常敏感于参数空间的某些方向，动量法可以改变梯度下降的方向，但是却引入了超参数。基于Delta-bar-delta算法 [ 13 ] 中自动适应模型参数各自学习率的方法，Duchi等人 [ 14 ] 提出了Adagrad算法。该算法基于一种很简单的思想：具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降，其更新规则如公式(3)~公式(6)所示： 计算梯度： g ← ∇ θ ( 1 m ∑ i = 1 m L ( f ( x ( i ) ; θ ) , y ( i ) ) ) (3) 累积平方梯度： r ← r + g ⊙ g (4) 通过逐元素地应用除和求平方根，计算θ更新： Δ θ ← − ε ξ + r ⊙ g (5) 应用更新： θ ← θ + Δ θ (6) Adagrad算法具有良好的理论依据，但是由于其在分母中累加了梯度的平方根，在训练过程中累加会持续增长，导致学习率变小。在学习率无限小时，Adagrad算法将无法获得额外的信息。Adagrad算法在某些模型 [ 15 ] [ 16 ] 上可以得到很好的收敛，但不是全部。为了解决批量梯度下降法的求解速度问题，Bottou [ 2 ] 于1998年提出了随机梯度下降法。与批量梯度下降法不同的是随机梯度下降法首先把数据集分成m个小批量样本，然后通过计算他们的梯度均值，计算梯度无偏估计，最后进行参数迭代(公式(3))。当m = 1时，称为在线梯度下降法；当m > 1时，称为小批量随机梯度下降法。 随机梯度下降法的优点为收敛速度快，其计算时间不取决于数据集的大小。对于大容量的数据集，往往在没有全部计算完数据之前，其收敛精度已经达到规定的范围。随机梯度下降法的缺点为需要人为调参。由于在随机梯度下降法随机采样m个训练样本，引入了梯度估计的噪声源(在极点处不消失)，因此随机梯度下降法不能像批量梯度下降法固定学习率的数值，而是要根据实验结果人为的调整学习率的大小。  Adadelta算法 [ 17 ] 是Adagrad算法的扩展，较Adagrad算法的改进有以下几点：a) 将梯度的平方递归地表示成所有历史梯度平方的均值，而不是计算所有的梯度的平方(公式(7))； E [ g 2 ] t = γ E [ g 2 ] t − 1 + ( 1 − γ ) g t 2 (7) b) 为了解决非凸优化问题，Adadelta算法更新规则中设计参数具有相同的假设单位，首次定义了另一个指数衰减均值，不是梯度平方，而是参数平方的更新(公式(8))； E [ Δ θ 2 ] t = γ E [ Δ θ 2 ] t − 1 + ( 1 − γ ) Δ θ t 2 (8) 改进后的Adadelta算法的更新规格为： Δ θ t = − R M S [ Δ θ ] t − 1 R M S [ Δ g ] t ⋅ g t (9) Adadelta算法只需要小的计算量，并且一直是一阶梯度计算。此外，Adadelta算法不需要调节学习率。在模型训练初期和中期，Adadelta算法具有很好的收敛速度，但是在训练后期，Adadelta算法会反复在局部最小值附近抖动，很难脱离局部最小值吸引盆。这时如果换Adadelta算法为Nesterov动量法，就会发现模型测试准确率将提高5%左右，说明Adadelta算法在局部最小值附近时，梯度下降方向不准确。 此外，为了使Adam算法和AdaGrad算法有更快的收敛速度，张慧 [ 18 ] 在Adam算法和AdaGrad算法中引入了动量的思想，提出了带动量的Adam算法和Adarad算法：AMM算法。"
"Ada_Nesterov动量法在Nesterov动量法的基础上添加了Adadelta算法的自适应学习率策略。Adadelta算法在Adagrad算法基础上做出来两个方面的改进，首先，为解决Adagrad算法在训练后期学习率非常小的问题，将梯度的平方递归地表示成所有历史梯度平方的均值，而不是计算所有的梯度的平方(公式(8))，得到公式(10)： Δ θ t = − η R M S [ Δ g ] t ⋅ g t (10) 其次，AdaDelta算法通过计算指数衰减的累积梯度平方和(公式(8))，解决了SGD算法、Adagrad算法、动量法等算法在参数更新时单位不一致的问题，得到梯度下降的公式(9)。比较公式(10)和公式(9)，得出每一维的学习率为指数衰减均值的均方根(公式(11))： η = R M S [ Δ θ ] t − 1 (11) 把公式(11)带入Nesterov动量法的梯度下降公式(2)，得Ada_Nesterov动量法的梯度下降公式(12)： Δ θ = ρ [ Δ θ ] t − 1 − R M S [ Δ θ ] t − 1 ∇ θ ( 1 m ∑ i = 1 m L ( f ( x ( i ) ; θ ) , y ( i ) ) ) (12) Ada_Nesterov动量法的具体更新规则如算法1所示： 步骤1：设置动量因子ρ，常数项，衰减系数α；初始化假设函数变量θ，初始化梯度下降的初始期望的平方为零。 步骤2：在设定的循环次数内，计算指数衰减均值，并对其求平方根，更新假设函数变量θ，并计算下一步梯度下降参数。 步骤3：判断循环是否达到终止条件，如果未达到终止条件，则循环步骤2，如果达到了终止条件，则算法终止。"
"本文对Ada_Nesterov动量法进行了两个实验，实验一构建了一个简单的卷积神经网络的网络训练，以确定Ada_Nesterov动量法的最优动量因子。实验二构建了一个神经网络，并训练了CIFAR-100数据集，比较Nesterov动量法，Adadelta算法和Ada_Nesterov动量法的性能。  为了确定Ada_Nesterov动量法的最优动量因子，本文构建了一个具有两个卷积层，两个全连接层的简单卷积神经网络。此神经网络卷积层的卷积核大小为5 × 5，第二个卷积成后进行dropout操作，激活函数为ReLU [ 19 ] 。训练和测试数据集为MNIST数据集。试验平台为OS X EI Capitan系统，Intel Core i5主板，8G内存，tensorflow为1.2.1 CPU版本。 Ada_Nesterov动量法的常数项为0.00001，衰减系数α为0.001。在经过1个epoch后，不同的动量因子，得到的测试正确率和损失结果如表1所示： Table 1 动量因子 正确率 损失 0.1 94% 0.2187 0.2 95% 0.1898 0.3 95% 0.1849 0.4 96% 0.1502 0.5 97% 0.1378 0.6 96% 0.1748 0.7 95% 0.1833 0.8 88% 0.4458 0.9 10% 2.3045 表1. Ada_Nesterov动量法在不同的动量因子下的测试结果 由表1可以看出，在影响因子0.5时，Ada_Nesterov动量法的正确率最高(97%)，损失最小(0.1378)；在影响因子0.9时，Ada_Nesterov动量法的正确率最低(10%)，损失最大(2.3.45)。 也可以看到随着动量因子在0.5附近其正确率和损失大小比较稳定，在动量因子减小或者增大时，正确率均会降低，损失均增加。同时，动量因子减小正确率缓慢降低，损失缓慢增加；但是，动量因子增大正确率急剧降低，损失急剧增加。 综上所述，动量因子在0.5时，Ada_Nesterov动量法的正确率最高，损失最小。本文在进行实验二时，对动量因子0.5的Ada_Nesterov动量法进行验证。  为了验证训练了Ada_Nesterov动量法，Nesterov动量法和Adadelta算法的性能，本文基于VggNet-16网络架构，训练了CIFAR-100数据集。三种算法均设置64的批处理量，训练6000步。图1和图2分别为三种算法的验证正确率曲线和验证损失曲线。 图1. 三种优化算法在CIFAR-100数据集上的验证准确率 由图1可知，Ada_Nesterov动量法的验证准确率曲线基本上处于Adadelta算法和Nesterov动量法之上，即Ada_Nesterov动量法的验证准确率整体上高于Adadelta算法和Nesterov动量法。在训练6000步时，三种算法的准确率均为最高，Ada_Nesterov动量法，Adadelta算法和Nesterov动量法的验证准确率分别为：82.8%，78.1%和68.8%。 图2. 三种优化算法在CIFAR-100数据集上的验证损失 由图2可知，Ada_Nesterov动量法的验证损失曲线基本上处于Adadelta算法和Nesterov动量法之下，即Ada_Nesterov动量法的验证损失整体上高于Adadelta算法和Nesterov动量法。在训练6000步时，三种算法的损失均为最低，Ada_Nesterov动量法，Adadelta算法和Nesterov动量法的验证损失分别为：0.72、0.94和0.91。 本文对训练得到的各神经网络模型，基于CIFAR-100测试数据集进行了测试，测试结果如表2所示。由表2可知，基于Nesterov动量法、Adadelta算法和Ada_Nesterov动量法训练得到的模型的测试正确率分别为：78.5%、75.2%和61.7%；测试损失分别为：1.13、1.58和2.17。 此外，从三种优化算法的运行时间分析，运行6000时，Ada_Nesterov动量法，Adadelta算法和Nesterov动量法耗时分别约为16小时，19小时和10小时。可见，Adadelta算法复杂度最高，Nesterov动量法复杂度最低，Ada_Nesterov动量法复杂度介于Adadelta算法和Nesterov动量法之间。 Table 2 算法 测试正确率 测试损失 Nesterov动量法 61.7% 2.17 Adadelta算法 75.2% 1.58 Ada_Nesterov动量法 78.5% 1.13 表2. 各算法测试正确率和测试损失 综上所述，动量因子0.5的Ada_Nesterov动量法可以给每一维数据一个自适应学习率，算法复杂度适中，具有验证准确率高，损失小，收敛速度快，是一个综合性能较好的优化算法。"
"Nesterov动量法可以很好的改变动量法上梯度的下降方向，但是Nesterov动量法的每一维度都具有相同的学习率，本文借助Adadelta算法的自适应学习率的策略，提出了自适应学习率的Ada_Nesterov动量法。为了验证Ada_Nesterov动量法的性能，本文设计了两个实验。实验一为确定Ada_Nesterov动量法的最优的动量因子ρ。我们分别在动量因子0.1~0.9的情况下，基于MNIST数据集训练了一个简单的卷积神经网络。对神经网络模型进行测试的结果表明：动量因子0.5时Ada_Nesterov动量法具有最高的测试正确率和损失。实验二为Ada_Nesterov动量法(动量因子0.5)与Adadelta算法和Nesterov动量法的性能比较。我们在VggNet_16神经网络架构上，基于CIFAR_100数据集，分别对三种算法进行了训练，实验结果表明Ada_Nesterov动量法的准确率最高，损失最小，收敛速度最快。即Ada_Nesterov动量法改进了Nesterov动量法的自适应学习率。"
