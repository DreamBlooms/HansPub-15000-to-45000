"数据驱动的医疗保健(data driven healthcare)作为可用的大型医疗数据的使用，以提供最好的和最个性化的护理，正成为医疗行业革命成功的主要趋势之一。电子病历是推动这一数据驱动的医疗革命成功的主要载体。本文运用深度学习的方法，基于词嵌入模型实现对电子病历信息的表示，利用长短期记忆(LSTM)网络模型特性，实现对电子病历信息时间上的无规律性和疾病信息长期依赖挑战的解决，实现对疾病风险的预测，通过与卷积神经网络(CNN)模型进行比对，实验结果显示本文方法的有效性。 关键词 :深度学习，电子病历，词嵌入，长短期记忆网络 Copyright © 2020 by author(s) and Hans Publishers Inc. This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/"
"全球医疗保健系统正在迅速采用电子病历(EHR)，它是在任何护理交付环境 [ 1 ] 中由一个或多个相遇产生的纵向患者健康信息的系统集合(例如，诊断、药物、实验室测试、程序等)。这将极大地增加有效的电子临床数据。因此，数据驱动的医疗保健作为可用的大型医疗数据的使用，以提供最好的和最个性化的护理，正成为医疗行业革命成功的主要趋势之一 [ 2 ] [ 3 ] [ 4 ]。由于患者EHR是进行数据驱动的医疗研究的主要载体，了解EHR中包含的信息并对其信息有效提取是至关重要的。"
"近年来，病历中表型的提取和表示的现有方法包括：基于向量的表示。这种方法针对每个病人构建了一个向量，它的维度等于在HER中出现的不同事件的数量，每个维度上的值是在特定时间段内相应医疗事件的汇总统计(例如，总和、平均值、最大值、最小值等)。使用基于向量的表示，每个表型通常被假定为这些原始医疗事件的线性组合，组合系数是通过一些优化过程获得的，这种表示的局限性在于它忽略了这些事件之间的时间关系。Ho等人 [ 5 ] 提出了一种基于非负张量因子分解的EHR张量表型提取方法。该方法探索了不同医学实体之间的相互作用；其局限性是，他们仍然没有考虑事件–时间关系。基于序列的表示，该方法根据每个事件的时间戳为每个患者构建EHR序列，然后，运用频繁模式挖掘方法将时间模式识别为表型，一个问题在于，患者EHR之间的高度可变性，这种方法通常会返回大量的模式(也称为“模式爆炸”现象)。基于时间矩阵的表示 [ 6 ]，这种方法表示患者EHR作为时间矩阵，其中一维对应于时间，另一维对应于医疗事件。Zhou等人 [ 7 ] 提出了将相似时态趋势的医疗事件分组在一起的表型分型方法。然而，他们没有考虑不同事件之间的时间关系。Wang等人 [ 8 ] 提出了一种卷积矩阵分解方法，用于检测患者EHR矩阵上的移位不变模式，但不能确定最优模式长度，需要枚举所有可能的值。 为了克服上述电子病历信息的稀疏性、时间上无规律性和疾病信息的长期依赖性挑战，本文将基于自然语言处理 [ 9 ] 中的词嵌入模型将EHR中离散的元素嵌入到连续向量空间中，将相同类型的向量合并为单个向量，把向量经过特定池化类型处理表示为一次入院，以这种方式，可变大小的输入被嵌入到连续分布的向量空间中，作为LSTM的输入特征，通过LSTM对时序信息的有效处理，实现对疾病风险的预测。因为嵌入是从数据中学习的，所以模型不依赖于人工特征工程。实验结果表明，该模型具有有效性。"
"用“词”来指一个大的上下文中的离散元素(例如，文档中的一个词或一次入院的一种诊断)，输入到许多机器学习模型中通常被表示为固定长度的特征向量。对于文本，通常使用词袋模型。词W用一个独 热向量 V w ∈ R | V | 表示，其中 V w = [ v w 1 , ⋯ , v w | V | ] ， | V | 是词典中词的数量： V w = [ 0 , ⋯ , 0 , 1 , 0 , ⋯ , 0 ] (如果 w = i ，则 v w i = 1 ，表示w是词典中的第i个词，否则 v w i = 0 )。在“词袋”的表示法下，一个句子 w 0 , ⋯ , w n 的向量是其词向量的总和： u = v w 0 + v w 1 + ⋯ + v w n 。然而，“词袋法”无法捕获单词的顺序和语义。  一种强有力的词袋替代方法是将单词嵌入到M维的向量空间中，其中 M ≪ | V | 。每一个词都映射到一个唯一的向量，该向量是矩阵 E ∈ R M × | V | | 中的一列。词嵌入有几个好处：首先，维数大大减少，不依赖于新词的出现。第二，一个单词的语义是以分布式的方式表示的，也就是说，有多个元素对单词的意思进行编码。第三，使用诸如加法和矩阵乘法等现有代数工具，连续向量的操作要容易得多，这在最近的工作中得到了证明。例如，两个词之间的相似性只是两个向量之间的余弦。更重要的是，嵌入矩阵E可以从数据中学习。 学习嵌入矩阵E的方法有很多种，最流行的方法可能是连续的词袋模型。对于词序列中的一个词w i ，模型使用w i 周围的词预测w i 。输入上下文大小为C时， w i − C , ⋯ , w i − 1 , w i + 1 , ⋯ , w i + C 被称为w i 的上下文词。利用嵌入矩阵E将所有上下文词嵌入向量中，然后求平均值得到平均向量h h = E w i − C + ⋯ + E w i − 1 + E w i + 1 + ⋯ + E w i + C 2 C (1) 其中E t 是矩阵E的第t列。然后模型生成输出 a = E ¯ h ，其中 E ¯ ∈ R | V | × M ，中心词w i 预测使用softmax函数 P ( w i | w i − C , ⋯ , w i − 1 , w i + 1 , ⋯ , w i + C ) = softmax ( a ) (2) 参数E和 E ¯ 通过最小化损失函数来学习 L = 1 T ∑ i = 1 T log P ( w i | w i − C , ⋯ , w i − 1 , w i + 1 , ⋯ , w i + C ) (3) 通过随机梯度下降的反向传播。  长短时记忆网络 [ 10 ] 是一种有效解决梯度消失问题的循环神经网络(RNN) [ 11 ]。LSTM的中心是一个线性自循环记忆细胞，允许记忆通过长序列流动。记忆细胞被封闭来调节信息流向或来自细胞的信息。LSTM在许多应用中都是成功的，例如机器翻译、手写识别和语音识别。 一个LSTM单元在时间t有一个状态 c t ∈ R K 的记忆细胞，通过记忆细胞的信息流被3个门控制：输入门，遗忘门和输出门。输入门 流入到细胞的输入，遗忘门 f t ∈ R K 控制记忆细胞的遗忘且输出门 o t ∈ R K 调节来自记忆细胞的输出流。在技术细节处理之前，我们用 σ 表示矢量逐元素的sigmoid函数， ∗ 表示两个矢量的乘积。 三个门都是sigmoid单元，它将门的每个元素的值设置为0到1之间： i t = σ ( W i x t + U i h t − 1 + b i ) (4) f t = σ ( W f x t + U f h t − 1 + b f ) (5) o t = σ ( W o x t + U o h t − 1 + b 0 ) (6) 其中 W { i , f , o } , U { i , f , o } , b { i , f , o } 是参数门控制通过的信息量：当值为1时满值，当值为0时完全阻塞。在每一个时间步长t，通过输入 x t ∈ R M 第一次计算输入特征，先前的隐藏层 h t − 1 ∈ R K 通过一个挤压tanh函数： g t = tanh ( W c x t + U c h t − 1 + b c ) (7) 通过部分遗忘先前的记忆细胞和调节输入特征来更新记忆细胞，如下所示 c t = f t ∗ c t − 1 + i t ∗ g t (8) 记忆细胞序列是相加的，因此梯度也通过链式法则以线性方式更新。这有效地防止梯度消失或爆炸。记忆细胞在通过可学习的遗忘门f t 记忆过去的经验中起着至关重要的作用。相反， f t → 1 ，所有的过去记忆被保存，且随着新的输入新的记忆不断更新。如果 f t → 0 ，只有新的经验被更新，系统变得无记忆。 最后，根据记忆c t 计算隐藏的输出状态h t ，由输出门o t 控制如下： h t = o t ∗ tanh ( c t ) (9) 注意，由于系统动态是确定性的，h t 是所有先前输入的一个函数： h t = LSTM ( x 1 : t ) 然后，输出状态用来产生输出。  模型参数个数为 M × | V | + M × K + K × K + K × D ，由以下部分组成： LSTM层的参数： 对于入院嵌入，使用嵌入矩阵 A ∈ R M × | V | 。 输入门： ， U i ∈ R K × K 和 b i ∈ R K × 1 。 输出门： W o ∈ R M × | V | ， U o ∈ R K × K 和 b o ∈ R K × 1 。 遗忘门： W f ∈ R M × | V | ， 和 b f ∈ R K × 1 。 记忆细胞： W i ∈ R M × | V | ， U i ∈ R K × K 和 b i ∈ R K × 1 。"
"本节我们具体描述对EHR信息的处理，利用LSTM对序列中的长期依赖性进行建模的能力，解决了可变大小的离散输入和无规律时间的挑战，实现对疾病风险的预测。  一次入院包含多个诊断。诊断、手术和药物是使用编码方案编码的。我们观察到NLP和EHR之间有相当大的相似性，EHR类似于句子，所以基于NLP中的词嵌入模型，我们的方法是将入院嵌入到向量中。一次入院是一个可变大小的代码集(诊断或干预)。设D是诊断代码集。这个集合索引从1到D。用 A ∈ R M × | D | 表示一次诊断嵌入矩阵。设 A j 是第j列的元素， A i j 是矩阵A的第i行第j列元素。每次入院t 包含h次诊断： d 1 , d 2 , ⋯ , d h ∈ { 1 , 2 , ⋯ , | D | } 。代码首先被嵌入到向量中，将相同类型的向量合并为单个向 图1. 编码嵌入 量，方式如图1所示。诊断与干预代码的嵌入向量是 A d 1 , ⋯ , A d k 。然后，我们汇集所有当前诊断向量得到 x t ∈ R M ，但是得到的向量仍是稀疏的，我们通过引入池化，进一步处理电子病历的稀疏问题。最后，多次入院嵌入是一个矩阵，对其进行随机初始化，然后通过训练预测任务来学习。  设 x t i 是向量 x t 的第i个元素，入院采用最大池化、合并池化或平均池化，合并方式如下： 最大池化admssion(max adm)。按元素池化如下： x t i = max ( A i d 1 , A i d 2 , ⋯ , A i d h ) i = 1 , 2 , ⋯ , M 。这类似于选择性注意诊断和干预之间的影响最大的因素。它也类似于通常的编码实践，即选择一个诊断作为入院的主要原因。 归一化和池(sum adm)。患有多种疾病(多并发症)的患者比单病症患者更容易发生危险。我们提出了下面的归一化和池： x t i = A i d 1 + A i d 2 + ⋯ + A i d h | A i d 1 + A i d 2 + ⋯ + A i d h | 。归一化降低了大量的诊断和干预的影响。 平均池化admission(mean adm.)。在缺乏主要条件的情况下，平均池可能是一个合理的选择： x t = A d 1 + A d 2 + ⋯ + A d h h  深度学习推动基于电子病历特征提取的疾病诊断预测 [ 12 ] [ 13 ] [ 14 ]，风险预测 [ 15 ] [ 16 ] [ 17 ] 不断发展。本文疾病风险预测模型中，我们使用基本的LSTM单元，其行为由三个门控机制控制：输入门、输出门和遗忘门。基于门值记忆单元在时间t累计从输入vt输入的有用的信息，并将该信息存储在其内部状态。最终从LSTM输出Z(p)是病人P的向量表示。一个全连接softmax层被用于产生预测概率，数学表达如下： y ^ p = softmax ( W y z ( p ) + b y ) (10) 设 θ 为LSTM模型中所有参数的集合，预测概率向量 y ^ p 也可用模型后验分布 P ( y p | X p ; θ ) 表示，其中 y p 是真值。利用真值 y p 与预测概率 y ^ p 之间的交叉熵来计算损失，因此，风险预测的目标函数是交叉熵的平均值： L ( θ ) = − 1 | P | ∑ p = 1 | P | ( y p T log ( y ^ p ) + ( 1 − y p ) T log ( 1 − y ^ p ) ) (11)"
"电子病历信息经过词嵌入模型处理，将离散的元素嵌入到连续向量空间中，将相同类型的向量合并为单个向量，然后经过上述特定类型的池化操作将其表示为一次入院。 本文针对疾病风险预测的研究，因此主要考察模型在慢性病上的性能。实验选取基线为，传统机器学习方法随机森林(RF)和深度学习方法的卷积神经网络(CNN)，通过不同模型性能评估方法与本文提出的基于深度学习方法长短期记忆网络(LSTM)形成比对。实验数据选取现有的心脏疾病真实医疗数据集。实验设置医疗代码总数为 | c | = 1016 ，数据集按0.75:0.10:0.15的比例随机划分为训练集、验证集和测试集。将基于NLP中词嵌入模型处理的数据作为上述疾病风险预测模型的输入，通过上述三种模型进行疾病风险的预测，通过对不同模型的性能评估数据进行比对，分析本文提出基于LSTM模型的优势。 在深度学习模型中，因为引入了参数矩阵，可能会导致过拟合。因此，引入L2范数和Dropout = 0.5来防止过拟合。L2范数正则化，也称为“权重衰减”，用于防止权重参数出现极值。引入一个常数λ来控制正则化的大小。Dropout是DNN的一种正则化方法。训练期间，使用预先定义的概率1 − p (Dropout率)删除单元，其余部分像往常一样通过反向传播进行训练。这就阻止了各单元之间的相互适应，从而防止过拟合。  实验在windows10操作系统，python3.7.3，基于pytorch框架，运用能够处理长时间依赖性的LSTM模型，参数设置如下表1所示和参数设置如下表2所示的普通卷积神经网络(CNN)模型上运行。 Table 1 Bitch_size 32 Visit_size 256 L2_reg 0.01 Hidden_size 128 Droupout_rate 0.05 Cuda_id 0 Log_eps 1e-8 Use_gpu False 表1. 预测模型LSTM上的参数设置 其中，LSTM采用Hidden_size即隐层为128。 Table 2 参数名称 参数值 参数名称 参数值 Bitch_size 32 N_filter 16 L2_reg 0.01 Fiter_hs [2,3,4,5] Droupout_rate 0.05 Cuda_id 0 Log_eps 1e-8 Use_gpu False 表2. 预测模型CNN上的参数设置 其中，对于CNN采用N_filter即滤波器的数量为16，Filter_hs即滤波的大小采用大小不等的2，3，4，5进行特征提取。  对模型运行N_epoch = 10即迭代10次如表3所示。 Table 3 n_epoch mean_cost test_cnn_loss mean_cost test_lstm_loss 0 0.700642 0.664165 0.697237 0.694940 1 0.691573 0.659424 0.693990 0.688363 2 0.686926 0.659424 0.687902 0.688363 3 0.671121 0.659425 0.685048 0.671891 4 0.656447 0.659425 0.683142 0.659692 5 0.655964 0.659957 0.682925 0.649857 6 0.620882 0.659957 0.676765 0.641257 7 0.614401 0.657919 0.674915 0.641257 8 0.567315 0.643577 0.662062 0.627468 9 0.583820 0.648483 0.663815 0.608677 表3. 实验结果 表3所示，mean_cost分别为CNN模型和LSTM模型的训练集的平均损失，通过交叉熵函数得到CNN和LSTM模型的测试损失如上表所示，选取两模型最小损失值进行比对，LSTM模型明显优于CNN；此外，CNN模型随着epoch的增大，mean_cost减小但测试损失并未明显减小，而LSTM模型表现良好。 图2. CCN和LSTM模型的损失图像 图2显示了CNN和LSTM模型的损失变化图像。从图2中，我们看到，LSTM模型的损失相比较于CNN模型在前一阶段下降的更快，在后一阶段相比较于自身前一阶段虽然有减小，但是下降的速度还是明细优于CNN模型，是表3数据的可视化显示，进一步说明本文方法的优势。 表4呈现三种模型使用精确度、F1分数和AUC作为模型评估指标的结果。我们可以观察到，与两种基线相比，所提出的方法在所有度量值方面都取得了最佳性能。 Table 4 模型 性能评估指标 Accuracy F1分数 AUC RF 0.9137 0.8444 0.8755 CNN 0.8712 0.9260 0.8994 LSTM 0.9339 0.8827 0.9034 表4. 三种模型的性能评估"
"本文通过对电子病历的处理，实现了入院信息的表示，解决了电子病历时间不规律性，通过LSTM模型对时序信息的处理特征实现了对电子病历信息的长时间依赖性的处理，通过与基线模型，传统机器学习方法随机森林和深度学习方法CNN进行实验比对，结果显示了本文方法的有效性。 未来，我们将继续针对疾病风险预测进行进一步的研究，努力克服LSTM在处理长久未来方面的缺陷，更好的实现对疾病风险的预测。"
