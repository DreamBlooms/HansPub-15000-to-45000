"大数据时代，高维数据的变量选择是现代统计的研究热点问题之一。MCP正则化方法是常用的变量选取方法，但MCP正则化方法的优劣取决于能否选取出最优的正则化参数。本文在BIC准则的基础上，提出适用于MCP正则化参数选择的MBIC准则。通过数据模拟及实际应用表明，MCP方法在MBIC准则下能够以更高的概率选择正确的模型，MBIC准则明显优于其它参数选择方法。 关键词 :变量选择，MBIC，MCP，正则化参数，高维数据 Copyright © 2019 by author(s) and Hans Publishers Inc. This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/"
"随着大数据时代的来临，庞大的数据资源吸引了越来越多领域的关注。各行各业都希望通过数据处理与挖掘发现数据隐含的信息，为相关决策提供现实依据。特别在分析建模中，为了全面而准确地反应信息的特征及其内在规律，常常引入多个指标，进而形成高维数据。然而并不是高维数据中的所有信息都是有效的，过多的变量反而会导致模型复杂度提升，以至于模型拟合效果和预测精度的降低。因此，如何从海量的高维数据中提取有用特征是一个亟待解决的问题。变量选择就是这样一种从大量信息中提取相关变量从而建立稀疏稳健模型的技术。 传统的变量选择方法如最佳子集选择或逐步向前向后回归，需要结合AIC [ 1 ]，BIC [ 2 ] 等准则。但在高维数据下，容易出现难以克服的NP-Hard问题。为了克服传统方法的缺陷，统计学家们提出了众多基于惩罚函数的变量选择方法 [ 3 ]，如：Lasso估计(Least Absolute Shrinkage and Selection Operator) [ 4 ]，SCAD估计(Smoothly Clipped Absolute Deviation) [ 5 ]，MCP估计(Minimax Concave Penalty) [ 6 ] 等。MCP由Zhang等提出用于高维数据的变量选择，MCP估计满足变量选择的Oracle性质，即一致地选择出正确的模型，且参数的估计满足渐进正态性。在实际应用中，MCP方法优劣取决于能否选择合适的正则化参数λ，正则化参数越小，模型复杂程度越高；正则化参数越大，模型精确程度越低。因此，如何选择合适的正则化参数λ是一个至关重要的问题。常见的正则化参数选择方法有交叉验证(CV)，广义交叉验证(GCV) [ 4 ] 以及AIC，BIC [ 7 ] 等信息准则。Wang et al. [ 8 ] 考虑到GCV方法的过拟合性，提出了参数选择的BIC准则，并从理论上证明了模型选择的一致性。SCAD等常借助BIC准则选择正则化参数 [ 9 ]，但该准则对于MCP估计未必能选出最优的模型。 鉴于上述原因，本文通过对BIC准则进行改进，提出一种更适合于MCP正则化参数选择的修正BIC准则(MBIC)。通过数据模拟，比较MBIC准则与BIC准则在MCP方法中的效果。最后，讨论不同方法在实际数据中的应用，分析了1986和1987年赛季美国职业棒球大联盟的棒球运动员收入数据，探究与美国棒球运动员收入相关的影响因素。"
"考虑线性模型 y i = x i T β + ε i ,     i = 0 , 1 , ⋯ , n 其中 y i 是第 个响应变量， x i 是 p × 1 阶的协变量， ε i 是均值为0，方差为 σ 2 的i.i.d的随机误差项。为了同时进行变量选择和参数估计，常采用很多基于罚函数的稀疏正则化方法，其一般框架为 L ( β ; λ ) = ‖ y − X β λ ‖ 2 2 + n ∑ j = 1 p p ( | β j | ; λ ) (1) 其中 p ( | β j | ; λ ) 表示惩罚函数。  2010年，CUN-Hui Zhang提出MCP [ 6 ]，MCP是一种非凸罚函数，在 [ 0 , ∞ ) 的定义为 p ( β j ; λ ) = λ ∫ 0 β j ( 1 − x γ λ ) + d x 其一阶导数为 p λ , γ ( β j ) = { λ − β j γ ,       if   0 < β j < γ λ 0 ,                       if   β j > γ λ (2) 其中 λ ≥ 0 和 γ > 1 为正则化参数。 结合MCP罚函数的一阶导数的形式，可以看出MCP从0到 γ λ 惩罚力度呈线性下降趋势，当 β j > γ λ 是惩罚力度为0，即不惩罚。MCP罚函数满足近似连续性，稀疏性和无偏性。  Lasso方法对参数的L1范数进行惩罚，Lasso的惩罚项为 p λ ( | β j | ) = λ | β j | ，估计形式为 min β ‖ y − X β λ ‖ 2 ,   s .t .   ‖ β ‖ 1 ≤ t 其中 ‖ β ‖ 1 = ∑ i = 1 p | β i | ，上式也等价于 β ^ = arg min β { 1 n ‖ y − X β λ ‖ 2 2 + λ ‖ β ‖ 1 } 但是，Lasso对较大系数的估计是有偏估计，并且Lasso估计也不满足变量选择的Oracle性质。2001年Fan and Li提出SCAD方法 [ 5 ]，同时证明了其满足变量选择的Oracle性质。与Lasso相比，SCAD是无偏估计，因而受到广泛关注。SCAD罚函数的惩罚项为 p λ ( | β j | ) = { λ | β j | ,                                                         | β j | ≤ λ − | β j | 2 − 2 α λ | β j | + λ 2 2 ( α − 1 ) ,     λ ≤ | β j | ≤ α γ ( α + 1 ) 2 λ 2 ,                                             | β j | > α γ (3) 其中 λ ≥ 0 和 α > 1 为正则化参数，在实际应用中常取 α = 3.7 。"
"在实际应用中，正则化模型(1)的优劣与正则化参数 λ 取值密切相关，不同的参数 λ 会导致不同的惩罚力度，进而影响最终的模型。因此，参数λ的选择至关重要。常见的选择参数 λ 的方法有CV，GCV和各种信息准则，如AIC及BIC等。 针对LASSO估计，Zou H. et al. [ 8 ] 给出了估计的自由度，并提出了适用于Lasso估计的BIC准则，定义如下 BIC ( λ ) = ‖ y − X β λ ‖ 2 2 n σ 2 + log ( n ) n d f ^ λ (4) 其中 d f ^ λ = ∑ i = 1 n c o v ( x i β λ − y i ) / σ 2 ， σ 2 = V a r ( ε ) 。 此外，Wang et al. [ 7 ] 证明了GCV方法易出现模型选择的过拟合现象，针对SCAD估计提出了BIC准则。BIC准则是在有限的模型集合中的模型选择准则，BIC准则认为具有最小BIC值的模型是模型集合中最优良的模型。BIC具体定义如下 BIC ( λ ) = log ‖ y − X β λ ‖ 2 2 n + d f ^ λ n log ( n ) (5) 其中 d f ^ λ 为广义自由度， d f ^ λ = t r { X ( X ′ X + n ∑ λ ) − 1 X } 其中 ∑ λ = d i a g ( p ′ λ ( | β ^ λ 1 | ) / | β ^ λ 1 | , ⋯ , p ′ λ ( | β ^ λ d | ) / | β ^ λ d | ) 。 但在实际应用中，MCP估计在BIC准则下选择了较为复杂的模型，故本文提出MBIC准则，定义如下 MBIC ( λ , α ) = log ( ‖ y − X β λ , α ‖ 2 2 n − p 0 ) + p 0 n log ( n ) (6) 其中， p 0 表示非零变量个数。"
"小节通过模拟实验比较LASSO，SCAD，MCP变量选择方法的性能。 考虑线性模型 y = X T β + σ ε 进行随机模拟，从而产生数据x和y。在模拟实验中， n = 200 ， ε ~ N ( 0 , 1 ) ， σ = 2 ，变量个数p分别取8，12，20，且 β = ( 3 , 1.5 , 0 , 0 , 2 , 0 , 0 , 0 , ⋯ ) 1 × p T ， x i , x j 之间的相关系数为 c o r ( j 1 , j 2 ) = 0.5 | j 1 − j 2 | 。 算法上，Lasso估计，SCAD估计和MCP估计均采用坐标下降算法 [ 10 ]。MCP估计分别利用BIC准则(5)，MBIC准则(6)选择正则化参数，SCAD估计采用BIC准则(5)，而LASSO估计采用BIC准则(4)选择正则化参数。所有模拟实验重复进行100次，模拟结果如表1所示。 为比较Lasso、SCAD、MCP估计精确性，给出模型误差公式 M E ( μ ^ ) = ( β ^ − β ) T E ( X X T ) ( β ^ − β ) 其中，“MME”表示100次重复实验中模型误差ME的中位数；“SD”表示100次重复实验中模型误差ME的标准差；“C”表示100次重复实验中非零系数被正确估计为非零个数的均值；“IC”表示100次重复实验中零系数被错误估计为非零个数的均值；“Underfit”表示欠拟合，即在100次模拟实验中将非零系数错误估计为零的比例；“Correctfit”表示正确拟合，即在100次模拟实验中将非零系数正确估计为非零的比例；“Overfit”表示过拟合，即100次模拟实验中选择了3个重要变量并且包含了非零系数的比例。 从表1可以看出，在BIC正则化参数选择方法下，MCP估计和SCAD估计方法在变量选择和模型误差方面优于LASSO方法。在模型误差中，所有变量选择的方法均能减小模型误差，而MCP方法在MBIC准则下具有最小的模型误差，而且能够以更高概率选择真实模型。综上，在MBIC正则化参数选择方法下，MCP估计在变量选择能力和模型误差方面均最优。 Table 1 p 准则 MME SD C IC Underfit Correctfit Overfit 8 Lasso_BIC 0.0979 0.0692 3 1.6200 0 0.0800 0.9200 SCAD_BIC 0.0584 0.0645 3 0.3300 0 0.7500 0.2500 MCP_BIC 0.0482 0.0489 3 0.3700 0 0.6900 0.3100 MCP_MBIC 0.0429 0.0546 3 0.0400 0 0.9600 0.0400 12 Lasso_BIC 0.1276 0.0911 3 2.3600 0 0.0650 0.9350 SCAD_BIC 0.0664 0.0581 3 0.9300 0 0.5000 0.5000 MCP_BIC 0.0726 0.0570 3 0.7050 0 0.5450 0.4550 MCP_MBIC 0.0530 0.0594 3 0.0650 0 0.9400 0.0600 20 Lasso_BIC 0.1394 0.0885 3 3.5300 0 0.0000 1.0000 SCAD_BIC 0.0753 0.0688 3 1.9500 0 0.3300 0.6700 MCP_BIC 0.0840 0.7210 3 1.4100 0 0.3900 0.6100 MCP_MBIC 0.0446 0.0690 3 0.1000 0 0.9300 0.0700 表1. 模拟结果  本文利用来自R语言的ISLR包中的数据集Hitters，该数据集包含20个变量，322次样本，描述关于1986和1987赛季的棒球大联盟中的棒球运动员收入的相关信息。数据集各个变量描述如下： X1：1986年击球的次数；X2：1986年的点击次数；X3：1986年的本垒打数量；X4：1986年的运行次数；X5：1986年击败的次数；X6：1986年的散步次数；X7：联赛的年份；X：职业生涯中击球的次数；X9：职业生涯中的点击次数；X10：职业生涯中的本垒打数量；X11：职业生涯中的跑步次数；X12：职业生涯中击球的次数；X13：职业生涯中的散步次数；X14：表示1986年底的球员联赛A级和N级的因素；X15：表示1986年底的分裂E和W等级的因素；X16：1986年的罢工数量；X17：1986年的助攻数量；X18：1986年的错误数量；X19：表示1987年初的球员联赛A级和N级的因素；Y：1987年开业日的年薪数(千美元)。 假设棒球运动员在各个赛季的表现与棒球运动员收入呈线性关系，即有如下线性模型 y i = ∑ j = 1 20     x i j β j + ε i ,     i = 0 , ⋯ , n 其中 y i 表示第i个运动员的收入， x i j 是他的第j个变量， ε i 是均值为0，方差为 σ 2 的i.i.d的随机误差项。 利用最小二乘估计(OLS)、Lasso、SCAD和MCP估计分析该数据。变量选择结果如表2所示。从表2可以看出，无罚的最小二乘估计(OLS)选择了所有的变量，Lasso选择了15个变量，SCAD选择了6个变量，基于BIC准则的MCP估计选择了8个协变量，而基于MBIC准则的MCP估计选择了7个协变量，选择了相对稀疏的模型。从参数估计的结果看，基于MBIC的MCP估计的结果更接近于最小二乘估计值。 Table 2 变量 OLS LASSO_BIC SCAD_BIC MCP_BIC MCP_MBIC X 1 −852.8903 33.4617 0 0 0 X 2 963.6420 66.0012 101.7722 137.8352 130.4466 X 3 72.9293 14.9911 0 0 0 X 4 −177.1966 56.9189 0 0 0 X 5 −82.1408 48.1183 0 0 0 X 6 337.3167 67.8294 36.5964 124.7608 119.1098 X 7 27.9380 2.2523 0 0 0 X 8 −742.3870 43.0421 0 29.8314 97.8530 X 9 183.5095 64.3198 190.8192 151.0980 119.9879 X 10 −20.0722 52.7284 20.9577 128.7815 113.1012 X 11 794.4982 66.7017 0 111.6680 0 X 12 416.9650 67.5518 0 0 0 X 13 −377.9468 20.5463 0 0 0 X 14 131.7961 77.7381 69.4525 108.7831 107.0552 X 15 70.7693 0 0 0 0 X 16 −30.3442 0 0 0 0 X 17 −54.4371 0 0 0 0 X 18 100.5867 58.8511 54.1784 73.6822 84.1708 X 19 29.6243 0 0 0 0 表2. 不同方法下的参数估计"
"本文讨论了MCP方法在变量选择和参数估计的应用，提出了更适合MCP估计的MBIC准则。数据模拟以及实际数据分析中都表明在MBIC准则下MCP估计的结果更优于BIC准则估计结果。"
