"离散化算法将连续属性的取值范围划分为很多个小的区间,每个区间都对应着自己的离散化符号，合理的离散化能够更准确的表达信息。本课题研究并实现了一种基于信息熵的离散化算法，通过赋予断点信息熵来度量断点的重要性从而对集合S进行划分。首先计算连续的属性的候选断点属性集，其次从候选断点集合中选取一个使信息熵最小的断点加入到断点集合中，该断点把集合S分成了两个部分，之后对于每一个子集合确定断点直到对于集合S的划分足够表达不同信息，满足最小区分长度准则完成。本文最后用实验验证了此算法的正确性和有效性，并对多组数据进行了测试和比较。 关键词 :信息熵，离散化，断点 Copyright © 2019 by author(s) and Hans Publishers Inc. This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/"
"离散化是数据挖掘中非常重要的一步，它属于数据预处理阶段，是对连续属性数据进行的分析处理。具体离散化算法会把连续属性的数据取值范围划分为很多个小区间，划分的原则分为有监督和无监督，有监督的分为等宽度，等频率 [ 1 ] 等，而无监督就是指没有指定要求的情况下的根据具体算法具体分析。而基于信息熵的离散化算法即是其中一种无监督的离散化方法，它通过判断断点信息熵的大小来划分实例集，达到离散化的目的。"
"信息是个很抽象的概念，起初人们对信息的多少无法说清楚，直到香农在1948年信息论中提出的信息熵 [ 2 ] 这一概念才使的量化度量信息的问题得以解决。在信息论中，信息熵被视为接收到的每条信息种包含的信息量的平均值，也被称为信源熵、平均自信息量。 用X表示随机变量，随机变量的取值为( x 1 , x 2 , ⋯ , x n )， p ( x i ) 表示事件x i 发生的概率，且有 ∑ p ( x i ) = 1 。定义事件x i 的信息量是它发生概率对数的负数，记为I(x i )，即： I ( x i ) = − log ( p ( x i ) ) 。 而H(X)为随机变量X的平均信息量，即X的信息熵。 H ( X ) = E [ − log P ( x i ) ] = − ∑ i = 1 N P ( x i ) log P (xi) 其中 P ( x i ) 表示事件x i 发生的先验概率，且 ∑ P ( x i ) = 1 。 另外，公式中的log的底和信息熵的单位有关。被广泛应用的是以2为底，它的单位是bit。当使用以e为底，单位为Nat。"
"在智能化信息处理中，经常遇到的技术难点主要是离散化问题和数据的不完整性问题。在应用粗糙集理论来处理信息决策表时，需要满足的是决策表中的值应该是离散的。不管是连续属性还是离散化的数据，都是需要进行离散化，连续属性在处理前必须进行离散化处理；对于离散化的数据，可以使用离散化方法将本来也就是离散的数据合并或者抽象得到更好的离散值。离散化算法应该满足： 1) 离散化后的信息系统的空间维数应该尽量少，即离散化后信息系统剩余的属性个数尽量少。 2) 离散化后的信息丢失尽量少。 1993年，Fayyad和Irani于提出最小信息熵离散化方法 [ 3 ]。离散的门限边界值由候选区间的类信息熵来选择。子集S 1 和子集S 2 的信息熵为Ent(S 1 )和Ent(S 2 )，则由断点T关于属性A的类信息熵可以表示为 E ( A , T ; S ) = | S 1 | / | S | ⋅ E n t ( S 1 ) + | S 2 | / | S | ⋅ E n t (S2) |S|表示集合中元素个数。 对于给定的属性A，将使得E(A;T;S)值最小的点作为离散化的划分点，并将其记为Tmin。划分点Tmin将区间划分为两个部分，样本集合也被分为两个集合S 1 和S 2 。在分别计算S 1 和S 2 的所有划分点的时候，假设T 1 和T 2 分别为S 1 和S 2 的最好的划分点，而它们所对应的类信息熵分别为E(A,T 1 ,S 1 )和E(A,T 2 ,S 2 )；如果E(A,T 1 ,S 1 ) > E(A,T 2 ,S 2 )，就继续对S 1 进行划分，反之则是S 2 。也就是说，哪个大就划分哪个。然后递归调用他们，直到满足递归停止条件。停止条件是利用最小区分长度准则作为判断递归离散化是否停止，就是说集合S中的递归的划分区间，当满足下列条件时停止， G a i n ( A , T ; S ) < log 2 ( N − 1 ) / N + Δ ( A , T ; S ) / N 其中：N是集合S中的样本数量， G a i n ( A , T ; S ) = E n t ( S ) − E n t ( A , T , S ) ， Δ ( A , T ; S ) = log 2 ( 3 k − 2 ) − [ k ⋅ E n t ( S ) − k 1 ⋅ E n t ( S 2 ) − k 2 ⋅ E n t ( S 2 ) ] 其中k 1 和k 2 分别是集合S 1 和S 2 中的类别数量，即k = |S|，k 1 = |S 1 |，k 2 = |S 2 |。"
"基于信息熵的离散化算法有很多，比如文献 [ 4 ] [ 5 ] 等。本文采取的基于最小信息熵的离散化算法的主要步骤为：第一，计算一个连续属性的候选断点属性集。第二，从第一步得到的候选断点属性集中选取一个使信息熵的值最小的点，此断点把实例集合划分为两个部分，将此断点加入到断点集合中。第三，确定每个实例集合的断点，划分实例集合S直到它满足递归停止条件则退出程序，完成离散化工作。该算法是一个比较基础的算法，它每次仅从单个条件属性的候选断点中选择断点 [ 6 ]，得到的最终结果断点数目较多，但时间复杂度较低。但本文中是以针对该算法采用多组数据进行比较，实验结果表明此算法是有效的。 具体算法描述表示如下： 步骤1：计算连续属性的候选断点属性集。 步骤2：从第一步得到的候选断点属性集中选取一个使信息熵的值最小的点，此断点把实例集合划分为两个部分，将此断点加入到断点集合中。信息熵计算方法如下：对于一个给定的实例的集合S，一个属性A，一个分类的边界T（把S分为S 1 和S 2 两个部分），那么由分类边界T确定的分类的信息熵为： E ( A , T ; S ) = | S 1 | / | S | ⋅ E n t ( S 1 ) + | S 2 | / | S | ⋅ E n t (S2) 信息熵计算公式为 E n t ( S ) = − ∑ j = 1 k P j log (Pj) 步骤3：对每个实例集合都调用步骤2中的方法进行划分，确定断点。递归停止条件为： G a i n ( A , T ; S ) < log 2 ( N − 1 ) / N + Δ ( A , T ; S ) / N 具体算法伪代码如下："
"测试一个较为简单的例子，条件属性：4个；决策属性：1个；记录条数：75条；属性名称：A，B，C，D，E；属性类型：float，float，float，float，integer。离散化前后结果如图1~2所示： 图1. 离散化前 图2. 离散化后 测试结果表明，对于float类型的属性进行离散化后得到了离散化后的属性符号，并且该例中本是离散属性的Ｅ不需要再离散化。由于数据量小，离散化时间几乎为0。  本次测试主要测试了75~4000条数据，具体运行时间折线图如图3所示。 图3. 运行时间比较图 从图上可以看出，随着数据量的增大，离散化时间也在增大。但增长又不是线性的，这是因为离散化时间不但和记录的条数有关，还和断点的个数、需要离散化属性的个数以及属性的类型等有关。"
"本文研究并实现了基于最小信息熵的离散化算法，它可以从这些含有连续属性的数据集中取得好的数据样本，得到简洁且有效的规则，如此可以方便数据在后续挖掘中的处理，并帮助企业及用户更有效的挖掘需要的数据。"
