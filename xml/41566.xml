<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114086</article-id><article-id pub-id-type="publisher-id">CSA-41566</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_18958828.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于Alphapose骨骼点和GRU的摔倒检测技术
  Fall Detection Technology Based on Alphapose Bone Key Points and GRU Neural Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>金泳</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>孙</surname><given-names>琛琛</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>庆涛</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>述煌</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>金宇</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>徐</surname><given-names>婉瀛</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>天津理工大学，天津</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>840</fpage><lpage>848</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   基于现有的一些人体摔倒检测模型实现复杂，适用性差等缺点，本文提出了一种新的更加简便，适用性更强的人体摔倒检测模型。该模型是一种基于GRU神经网络和人体骨骼关键点的人体摔倒检测模型。该模型中，先通过Alphapose对每一帧图像进行人体骨骼关键点识别与检测，然后将得到的骨骼关键点数据进行归一化数据处理，再分组输入到GRU神经网络中进行时序特征提取，最后将GRU模型中隐含层的输出向量输入到全连接层进行处理并得出检测结果。本文使用的是UR Fall Detection Dataset热舒夫大学摔倒数据集进行测试实验，并与多种检测模型的实验性能进行了横向对比。实验结果表明本文的模型在多场景，多视角和多种摔倒姿势等情况下较其他模型都有较高的检测精度，且实现难度较其他模型而言要低。 Based on the shortcomings of some existing human fall detection models, such as complex implementation and poor applicability, this paper proposes a new simpler and more applicable human fall detection model. This model is a human fall detection model based on GRU neural network and key points of human bones. In this model, the key points of human bones are identified and detected through Alphapose for each frame of image, Then the obtained bone key point data is normalized data processing, and then grouped and input into the GRU neural network for time series feature extraction, finally, the output vector of the hidden layer in the GRU model is input to the fully connected layer for processing and the detection result is obtained. This article uses the UR Fall Detection Dataset Rzeszow University fall data set for test experiments, and horizontal comparison with the experimental performance of a variety of detection models. Experimental results show that the model in this paper has higher detection accuracy than other models in multiple scenes, multiple perspectives, and multiple falling postures, and the difficulty of implementation is lower than other models. 
  
 
</p></abstract><kwd-group><kwd>摔倒检测，骨骼关键点，GRU神经网络, Fall Detection</kwd><kwd> Bone Key Points</kwd><kwd> GRU Neural Network</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>基于现有的一些人体摔倒检测模型实现复杂，适用性差等缺点，本文提出了一种新的更加简便，适用性更强的人体摔倒检测模型。该模型是一种基于GRU神经网络和人体骨骼关键点的人体摔倒检测模型。该模型中，先通过Alphapose对每一帧图像进行人体骨骼关键点识别与检测，然后将得到的骨骼关键点数据进行归一化数据处理，再分组输入到GRU神经网络中进行时序特征提取，最后将GRU模型中隐含层的输出向量输入到全连接层进行处理并得出检测结果。本文使用的是UR Fall Detection Dataset热舒夫大学摔倒数据集进行测试实验，并与多种检测模型的实验性能进行了横向对比。实验结果表明本文的模型在多场景，多视角和多种摔倒姿势等情况下较其他模型都有较高的检测精度，且实现难度较其他模型而言要低。</p></sec><sec id="s2"><title>关键词</title><p>摔倒检测，骨骼关键点，GRU神经网络</p></sec><sec id="s3"><title>Fall Detection Technology Based on Alphapose Bone Key Points and GRU Neural Network</title><p>Jinyong Li, Chenchen Sun, Qingtao Chen, Shuhuang Liu, Jinyu Li, Wanying Xu</p><p>Tianjin University of Technology, Tianjin</p><p><img src="//html.hanspub.org/file/7-1542093x4_hanspub.png" /></p><p>Received: Mar. 14<sup>th</sup>, 2021; accepted: Apr. 8<sup>th</sup>, 2021; published: Apr. 15<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/7-1542093x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Based on the shortcomings of some existing human fall detection models, such as complex implementation and poor applicability, this paper proposes a new simpler and more applicable human fall detection model. This model is a human fall detection model based on GRU neural network and key points of human bones. In this model, the key points of human bones are identified and detected through Alphapose for each frame of image, Then the obtained bone key point data is normalized data processing, and then grouped and input into the GRU neural network for time series feature extraction, finally, the output vector of the hidden layer in the GRU model is input to the fully connected layer for processing and the detection result is obtained. This article uses the UR Fall Detection Dataset Rzeszow University fall data set for test experiments, and horizontal comparison with the experimental performance of a variety of detection models. Experimental results show that the model in this paper has higher detection accuracy than other models in multiple scenes, multiple perspectives, and multiple falling postures, and the difficulty of implementation is lower than other models.</p><p>Keywords:Fall Detection, Bone Key Points, GRU Neural Network</p><disp-formula id="hanspub.41566-formula27"><graphic xlink:href="//html.hanspub.org/file/7-1542093x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/7-1542093x7_hanspub.png" /> <img src="//html.hanspub.org/file/7-1542093x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>我国老龄化现象不断加剧，据国家统计局公布数据显示 [<xref ref-type="bibr" rid="hanspub.41566-ref1">1</xref>]，2019年末全国大陆人口140005万人，我国60周岁及以上人口2538万人，占总人口的18.1%。其中65周岁及以上人口17604万人，占总人口的12.6%。由于老年人身体机能下降，摔倒后容易引起严重后果，而当前对于老人的服务资源不能满足社会要求，独生子女外出工作，摔倒的老人往往错过最佳救治时间。当老年人独自在家发生摔倒时，及时的检测能尽可能保障老人的安全。</p><p>目前，人体摔倒检测主要有基于穿戴式传感器的摔倒检测和基于计算机视觉的摔倒检测。基于穿戴式传感器的摔倒检测通常将设备放在人的身体某个部位上，由外部传感设备采集数据，提取相关数据后利用机器学习模型进行分组，检测的准确性十分依赖于提取特征值的好坏，对于设备的要求较高 [<xref ref-type="bibr" rid="hanspub.41566-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.41566-ref3">3</xref>]。基于计算机视觉的摔倒检测是将摄像头放置在用户日常活动范围的固定位置，对获取的图像进行处理，通过机器学习、深度学习等方法实现对监控视频进行摔倒检测 [<xref ref-type="bibr" rid="hanspub.41566-ref4">4</xref>]。杨碧瑶 [<xref ref-type="bibr" rid="hanspub.41566-ref5">5</xref>] 使用PCANet提取图像特征，依次通过两个SVM分类器来判断是否摔倒。徐世文 [<xref ref-type="bibr" rid="hanspub.41566-ref6">6</xref>] 采用CenterNet目标检测网络，利用建立的红外图像摔倒数据集进行网络的训练与测试。Wen-Nung Lie [<xref ref-type="bibr" rid="hanspub.41566-ref7">7</xref>] 采用卷积神经网络(CNN)为每个输入帧提取2D骨架，然后使用LSTM来对摔倒进行预测。</p><p>当前两种主流的摔倒检测方法都存在一些问题。基于穿戴式传感器的摔倒检测设备成本要求高，检测精度低，用户需要随身佩戴传感器设备导致生活不便。基于计算机视觉的摔倒检测法存在场景适应性差，不同摔倒姿势误检率高等问题 [<xref ref-type="bibr" rid="hanspub.41566-ref8">8</xref>]。针对以上问题，本文提出一种基于人体骨骼关键点的摔倒检测模型。通过使用Alphapose实现对人体骨骼关键点特征的提取，将得到关键点坐标归一化后根据时间序列输入GRU神经网络进行分类，实现人体摔倒检测，避免了场景中环境的改变导致图像特征提取效果差，并在公开数据集上进行测试，表明本模型拥有良好的性能。</p></sec><sec id="s6"><title>2. 系统结构</title><p>检测算法与GRU神经网络模型结构如图1所示，首先将视频分割成帧，帧图像输入Alphapose得到每帧图片人体的17个坐标点(x, y)。这17个关键点包括人的头部，上半身，下半身几乎所有关节的关键点，可以有效的代表人体。人体在运动时，身体骨骼点x,y的位置存在明显变化，将得到的坐标点进行归一化处理。检测多帧图像后可以得到(x, y)的序列，当序列长度满足GRU设置的size后，将序列输入GRU神经网络提取特征，每当有一个新的帧加入后删除原序列的最旧坐标值，保持坐标序列为设置的窗口长度。最后将特征信息通过全连接层进行分类，根据输出结果判断是否摔倒。</p><p>图1. 摔倒检测流程图</p></sec><sec id="s7"><title>3. 摔倒检测方法</title><sec id="s7_1"><title>3.1. Alphapose骨骼点检测模型</title><p>Alphapose是由上海交通大学卢策吾团队提出的一种自顶向下的姿态估计技术 [<xref ref-type="bibr" rid="hanspub.41566-ref9">9</xref>]，通过区域姿态估计(Regional Multi-Person Pose Estimation, RMPE)框架来实现姿态估计，能有效解决目前骨骼模型检测框定位错误和检测框冗余问题。</p><p>RMPE框架主要包括Symmetric Spatial Trans-former Network (SSTN)，Parametric Pose Non-Maximum-Suppression (P-NMS)和Pose-Guided Proposals Generator (PGPG)三个模块。对称空间变换网络SSTN模块先进行姿态估计，将估计结果映射到原图，以此来调整原本的识别框，它能使目标检测的框变得更精准。姿态非极大值抑制器P-NMS通过姿态距离和空间距离消除冗余的检测框，当姿态距离和空间距离的权重和低于某一阈值时删除检测框，它能有效的消除冗余的检测框。姿态引导样本生成器PGPG通过Kmeans算法得到原子姿势，根据密集采样生成偏移量产生增强的训练建议，增加了样本的数量，它使样本数量增多，模型的精准度得到提高。通过这三个模块Alphapose有效的提升了骨骼检测的性能。Alphapose骨骼点检测模型效果如图2。</p><p>卢策吾团队在COCO数据集上与Openpose，Mask-RCNN进行骨骼检测模型对比试验，Alphapose在准确率和平均速度上均有优势，是目前精确度最高的骨骼检测模型。本文摔倒检测模型采用Alphpose得到每帧图像的17个骨骼关键点坐标(x, y)。</p></sec><sec id="s7_2"><title>3.2. GRU网络模型</title><p>GRU网络是LSTM网络的一种变体，它是一种特殊的RNN (循环卷积网络)类型。摔倒动作并不是瞬时发生而是由一连串的状态组成的，与传统的卷积神经网络相比，RNN能记录并使用先前的状态信息。</p><p>传统的RNN结构如图3所示，是一种能将以往的信息利用的网络模型，但是这种模型存在许多的</p><p>图2. Alphapose骨骼点检测模型</p><p>图3. RNN结构图</p><p>弊端。当预测所需的信息距离比较远时，因为结果可以拆分成中间梯度的乘积，如果中间权重小于1，经过多次自乘后，梯度会出现指数级缩小，这称为梯度消失，与梯度消失相对，如果中间权重大于1，经过多次自乘后会出现梯度爆炸。LSTM (长短期记忆网络)可以解决这一问题，其模型的核心思想是“细胞状态”，它通过叫做“门”的结构来通过或去除信息到细胞状态。LSTM有三种“门”，在第一步通过遗忘门来决定忘记的信息，第二步通过输入门来确定细胞存放的新信息，第三通过输出门来更新旧细胞的状态并作为结果输出。结构如图4。</p><p>图4. LSTM结构图</p><p>GRU作为LSTM的一种变体，他只有更新门和重置门，具体结构如图5所示。</p><p>图5. GRU细胞结构</p><p>更新门相当于将LSTM的输入门和遗忘门融合，并加入少许改动，控制前一细胞状态信息带入当前状态的程度。重置门控制前一状态有多少信息写入到候选集上。计算过程为公式(1)到公式(4)。</p><p>z t = σ ( W z ⋅ [ h t − 1 , x t ] ) (1)</p><p>r t = σ ( W r ⋅ [ h t − 1 , x t ] ) (2)</p><p>h t ˜ = tanh ( W ⋅ [ r t ∗ h t − 1 , x t ] ) (3)</p><p>h t = ( 1 − z t ) ∗ h t − 1 + z t ∗ h t ˜ (4)</p><p>GRU能够利用之前的信息，由于GRU的结构比LSTM更简单，在相同数据集的情况下GRU拥有明显的速度优势。本文摔倒模型将得到的骨骼坐标点进行处理后，按照时间的顺序输入并得到摔倒结果分类。</p></sec><sec id="s7_3"><title>3.3. 人体摔倒检测模型</title><p>本文基于Alphapose和GRU神经网络提出了摔倒检测模型。Alphapose骨骼关键点检测是目前已知技术中准确度和速度最高的模型，GRU神经网络与其他处理时间序列中包含的信息拥有优势的模型LSTM和RNN相比在摔倒检测中拥有较高的准确率，实验将说明这一点。因为GRU拥有比LSTM更简单的细胞结构，所以神经网络的处理速度更快 [<xref ref-type="bibr" rid="hanspub.41566-ref10">10</xref>]。本文首先使用Alphapose对运动的每一帧图像进行人体骨骼点提取得到17个(x,y)坐标，对得到的坐标进行处理后将骨骼点坐标输入GRU神经网络提取特征，每有新的一帧图像加入序列，旧的一帧图像被抛弃，保证序列中始终存储最新的一系列帧图像。将GRU提取的隐层特征经过全连接层分为两类：摔倒，未摔倒，来实现人体摔倒检测判断。如公式(5)所示</p><p>y = f ( h t ) = { 1 ,       摔 倒 0 ,     未 摔 倒 (5)</p></sec></sec><sec id="s8"><title>4. 数据集与数据处理</title><p>UR Fall Detection Dataset热舒夫大学摔倒数据集包含30个跌倒与40个日常生活活动摔倒的视频与加速度等数据。通过Alphapose对视频进行处理，它通过将视频流转换成若干帧图像，再对每一帧图像进行骨骼点坐标识别，得到坐标json文件。</p><p>由于每个图像中人体在图像中的位置不同，摔倒检测只需要关注人体的动作，同时增强模型通用性，所以需要对数据进行归一化。确定每一个图像坐标的X_MIN，X_MAX，Y_MIN，Y_MAX，通过下列等式对数据进行归一化处理。计算过程为公式(6) (7)。</p><p>x = x − x min x max − x min (6)</p><p>y = y − y min y max − y min (7)</p><p>GRU数据样本通过滑动窗口法获取。首先设置窗口大小s，如图x所示，当前帧为n，其标签L(n)为该样本标签，其中Z(n)包含17个骨骼点(x, y)的坐标，即 Z ( n ) = { ( x 1 , y 1 ) , ( x 2 , y 2 ) , ⋯ , ( x 17 , y 17 ) } ，窗口不断向右滑动来获取新样本，当有新数据添加进窗口时，删除窗口最左侧数据帧。据此来获取测试数据样本。</p></sec><sec id="s9"><title>5. 实验</title><p>本文实验环境为：Win10操作系统，编程环境为Anaconda下Python3.6，开发工具为Pycharm 2020.1，处理器为Intel(R) Core(TM) i7-8750H CPU @2.20 Ghz 2.21 GHz，内存为16 G，显卡为GTX1050，搭建GRU网络使用Keras2.2.4。</p><sec id="s9_1"><title>5.1. 评价标准</title><p>实验结果的样本主要被分为四类 [<xref ref-type="bibr" rid="hanspub.41566-ref11">11</xref>]，其中真正例TP(True Position)表示摔倒样本被正确分类的样本；假正例FP(False Position)表示未摔倒样本被模型错误分类为摔倒样本；假反例FN(False Negative)表示摔倒样本被错误分类为未摔倒样本；真反例(True Negative)表示未摔倒样本被正确分类为未摔倒样本。</p><p>本文选取摔倒检测常用的准确率(Accuracy)和漏警率(MA)表示，准确率表示模型分类正确的样本占所有样本的比重，可以评价一个模型的整体性能，如公式(8)所示。漏警率(MA)表示将摔倒模型错分为非摔倒样本占实际摔倒样本总数的比重，因为错把摔倒样本当作未摔倒样本造成比把未摔倒样本错分为摔倒样本的后果要严重的多，所以要关注模型的漏警率，如公式(9)所示，其中P表示正例的样本数量，N表示反例的样本数量，TP，TN，FN，P，N通过实验统计获取。</p><p>Accuracy = TP + TN P + N (8)</p><p>MA = FN TP + FN (9)</p></sec><sec id="s9_2"><title>5.2. 实验结果分析</title><p>因为目前循环卷积神经网络包括传统意义上的RNN (循环卷积网络)，LSTM (长短时记忆)，GRU (LSTM的变体)以及其他类型的循环卷积神经网络，本文主要选取RNN，LSTM，GRU三种来进行横向对比实验，利用控制变量法实验中除去神经网络的搭建的其他步骤均保持一致。实验数据集选取热舒夫大学摔倒数据集UR Fall Detection Dataset。</p><p>在训练过程中，选取摔倒过程和躺下的状态为摔倒训练样本，其他行为(坐下，起立，行走)为正常训练样本，窗口大小为40帧进行训练。本文采用交叉验证法进行实验，通过随机均匀选取70个视频中14个视频作为训练集进行循环训练达到训练预期，使用剩余数据作为训练样本对模型进行评估，其中摔倒样本包括24个，未摔倒样本包括32个，共划分为5组P1，P2，P3，P4，P5。实验样本的分类结果如表1~表3。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> RNN experimental sample classification result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >P1</th><th align="center" valign="middle" >P2</th><th align="center" valign="middle" >P3</th><th align="center" valign="middle" >P4</th><th align="center" valign="middle" >P5</th><th align="center" valign="middle" >average</th></tr></thead><tr><td align="center" valign="middle" >TP</td><td align="center" valign="middle" >22</td><td align="center" valign="middle" >19</td><td align="center" valign="middle" >18</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >22</td><td align="center" valign="middle" >21</td></tr><tr><td align="center" valign="middle" >FP</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >3</td></tr><tr><td align="center" valign="middle" >FN</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >3</td></tr><tr><td align="center" valign="middle" >TN</td><td align="center" valign="middle" >27</td><td align="center" valign="middle" >31</td><td align="center" valign="middle" >31</td><td align="center" valign="middle" >26</td><td align="center" valign="middle" >30</td><td align="center" valign="middle" >29</td></tr></tbody></table></table-wrap><p>表1. RNN实验样本分类结果</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> LSTM experimental sample classification result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >P1</th><th align="center" valign="middle" >P2</th><th align="center" valign="middle" >P3</th><th align="center" valign="middle" >P4</th><th align="center" valign="middle" >P5</th><th align="center" valign="middle" >average</th></tr></thead><tr><td align="center" valign="middle" >TP</td><td align="center" valign="middle" >21</td><td align="center" valign="middle" >21</td><td align="center" valign="middle" >22</td><td align="center" valign="middle" >21</td><td align="center" valign="middle" >23</td><td align="center" valign="middle" >21.6</td></tr><tr><td align="center" valign="middle" >FP</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1.6</td></tr><tr><td align="center" valign="middle" >FN</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2.4</td></tr><tr><td align="center" valign="middle" >TN</td><td align="center" valign="middle" >30</td><td align="center" valign="middle" >30</td><td align="center" valign="middle" >30</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >30</td><td align="center" valign="middle" >30.4</td></tr></tbody></table></table-wrap><p>表2. LSTM实验样本分类结果</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> GRU experimental sample classification result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >P1</th><th align="center" valign="middle" >P2</th><th align="center" valign="middle" >P3</th><th align="center" valign="middle" >P4</th><th align="center" valign="middle" >P5</th><th align="center" valign="middle" >average</th></tr></thead><tr><td align="center" valign="middle" >TP</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >21</td><td align="center" valign="middle" >21</td><td align="center" valign="middle" >22</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >22.4</td></tr><tr><td align="center" valign="middle" >FP</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0.8</td></tr><tr><td align="center" valign="middle" >FN</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1.6</td></tr><tr><td align="center" valign="middle" >TN</td><td align="center" valign="middle" >30</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >30</td><td align="center" valign="middle" >31.2</td></tr></tbody></table></table-wrap><p>表3. GRU实验样本分类结果</p><p>对比3个不同模型的TP，FP，FN，TN可以发现GRU不管在摔倒样本检测还是在未摔倒样本检测的准确率高于其他模型。使用3个模型average的数值来计算不同模型的Accuracy和MA，结果如表4所示。从表中可以看出，本文提出的GRU摔倒检测模型在准确率和漏警率上明显由于RNN和LSTM模型。</p><p>为了进一步得到更直观的比较，图6，图7分别为不同实验样本得到的3个模型准确率Accuracy和漏警率MA对比折线图。从图中可以看出，GRU模型在不同的实验样本中均表现出优于其他模型的性能。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Comparison results of different model</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >RNN</th><th align="center" valign="middle" >LSTM</th><th align="center" valign="middle" >GRU</th></tr></thead><tr><td align="center" valign="middle" >Accuracy</td><td align="center" valign="middle" >89.3%</td><td align="center" valign="middle" >92.9%</td><td align="center" valign="middle" >95.7%</td></tr><tr><td align="center" valign="middle" >MA</td><td align="center" valign="middle" >12.5%</td><td align="center" valign="middle" >10.0%</td><td align="center" valign="middle" >6.6%</td></tr></tbody></table></table-wrap><p>表4. 不同模型对比结果</p><p>本文GRU模型通过与RNN，LSTM模型进行对比得到分类为TP，FP，FN，TN的实验样本，得到的实验数据包括利用交叉验证法的五组测试样本和平均值。实验数据表明GRU相比于RNN准确率提高6.4%，相比于LSTM准确率提高2.8%。为了尽量避免把摔倒的样本分类为未摔倒的样本，MA越小性能越有优势。在3种模型中，GRU拥有最低的漏警率MA。</p><p>图6. Accuracy对比折线图</p><p>图7. MA对比折线图</p></sec></sec><sec id="s10"><title>6. 总结</title><p>本文比较了主流的两种摔倒检测方法，针对可穿戴式的设备成本要求高，检测精度低和当前计算机视觉环境适应性差，误检率高等问题，提出了一种基于Alphapose和GRU的摔倒检测模型，该模型通过Alphapose检测人体骨骼关键点，通过对关键点坐标进行归一化处理后，利用滑动窗口法将时序信息输入GRU网络提取特征实现摔倒检测分类与判断。通过UR Fall Detection Dataset热舒夫大学摔倒数据集实验表明本模型拥有较高的精度。</p></sec><sec id="s11"><title>基金项目</title><p>大学生创新创业训练计划项目(202010060160)。</p></sec><sec id="s12"><title>文章引用</title><p>李金泳,孙琛琛,陈庆涛,刘述煌,李金宇,徐婉瀛. 基于Alphapose骨骼点和GRU的摔倒检测技术Fall Detection Technology Based on Alphapose Bone Key Points and GRU Neural Network[J]. 计算机科学与应用, 2021, 11(04): 840-848. https://doi.org/10.12677/CSA.2021.114086</p></sec><sec id="s13"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41566-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">国家统计局. 中华人民共和国2019年国民经济和社会发展统计公报[EB/OL] 
http://www.stats.gov.cn/tjsj/zxfb/202002/t20200228_1728913.html, 2020-02-28.</mixed-citation></ref><ref id="hanspub.41566-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">王亚玲. 基于NB-IoT技术的可穿戴式老人摔倒监测系统的设计与开发[D]: [硕士学位论文]. 南昌: 江西师范大学, 2019.</mixed-citation></ref><ref id="hanspub.41566-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">贺佚南. 基于可穿戴传感器的摔倒检测研究[D]: [硕士学位论文]. 广州: 华南理工大学, 2018.</mixed-citation></ref><ref id="hanspub.41566-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Miao Yu, Liyun Gong, and Stefanos Kollias.(2017).Computer vision based fall detection by a convolutional neural network. Proceedings of the 19th ACM International Conference on Multimodal Interaction,416-420.  
doi: &lt;br&gt;https://doi.org/10.1145/3136755.3136802</mixed-citation></ref><ref id="hanspub.41566-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">杨碧瑶. 基于计算机视觉的独居老人摔倒检测方法研究[D]: [硕士学位论文]. 西安: 陕西科技大学, 2020.</mixed-citation></ref><ref id="hanspub.41566-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">徐世文. 基于红外图像特征的人体摔倒检测方法[D]: [硕士学位论文]. 绵阳: 西南科技大学, 2020.</mixed-citation></ref><ref id="hanspub.41566-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">W. Lie, A. T. Le and G. Lin.(2018).Human fall-down event detection based on 2D skeletons and deep learning approach.2018 International Workshop on Advanced Image Technology (IWAIT), Chiang Mai, Thailand, 2018, pp. 1-4,  
doi: 10.1109/IWAIT.2018.8369778.</mixed-citation></ref><ref id="hanspub.41566-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">陈永彬, 何汉武, 王国桢, 王桂棠. 基于机器视觉的老年人摔倒检测系统[J]. 自动化与信息工程, 2019, 40(5): 37-41.</mixed-citation></ref><ref id="hanspub.41566-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Fang, H., Xie, S., Tai, Y. and Lu, C. (2017) RMPE: Regional Mul-ti-Person Pose Estimation. 2017 IEEE International Conference on Computer Vision (ICCV), Venice, 22-29 October 2017, 2353-2362.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.256</mixed-citation></ref><ref id="hanspub.41566-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Fu, R., Zhang, Z. and Li, L. (2017) Using LSTM and GRU Neural Network Methods for Traffic Flow Prediction. 2016 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC), Wuhan, 11-13 November 2016, 324-328. &lt;br&gt;https://doi.org/10.1109/YAC.2016.7804912</mixed-citation></ref><ref id="hanspub.41566-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Noury, N., et al. (2007) Fall Detection-Principles and Methods. 2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, Lyon, 22-26 Au-gust 2007, 1663-1666.  
&lt;br&gt;https://doi.org/10.1109/IEMBS.2007.4352627</mixed-citation></ref></ref-list></back></article>