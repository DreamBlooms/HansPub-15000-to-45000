<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2019.107136</article-id><article-id pub-id-type="publisher-id">CSA-36517</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20190700000_31644680.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于实体描述和关系图卷积神经网络的实体分类研究
  Research on Entity Classification Based on Entity Descriptions with Relational-Graph Convolutional Networks
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郑</surname><given-names>小柏</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>崔</surname><given-names>岩</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>兴林</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>何</surname><given-names>雅芳</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>五邑大学智能制造学部，广东 江门</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff3"><addr-line>五邑大学智能制造学部，广东 江门；珠海四维时代网络科技有限公司，广东 珠海</addr-line></aff><pub-date pub-type="epub"><day>15</day><month>07</month><year>2020</year></pub-date><volume>10</volume><issue>07</issue><fpage>1319</fpage><lpage>1326</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   随着大数据时代的来临，人工智能快速发展，知识图谱已经在垂直搜索和智能问答等领域中发挥着重要的作用。但是，即使是全世界上最大的知识图谱也仍然不完整，所以知识推理一直是知识图谱的研究热点之一。本文提出了一个融合实体描述和关系图卷积神经网络(R-GCN)的模型(DR-GCN)，并将其应用于知识推理中的标准任务：实体分类(Entity Classification)，即缺失实体属性的恢复。R-GCN是根据图卷积神经网络(GCN)，专门针对实际知识图谱的高度多元关系数据特征而开发的一类图卷积神经网络。本文研究的DR-GCN模型，该模型充分利用了知识图谱中的关系类型、关系方向、实体自循环、实体描述等信息进行实体分类。本文对DR-GCN模型进行了彻底的评估，并与已建立的基线进行了比较。实验表明，DR-GCN模型的实验结果比现有的基线有所提高，在AIFB和BGS数据集上，DR-GCN模型的准确率分别比基线中准确率最高的G-GAT和RDF2Vec模型的96.19%和87.24%高0.24%和0.35%，验证了改进后的模型效果更佳。 With the advent of the era of big data and the rapid development of artificial intelligence, knowledge map has played an important role in vertical search, intelligent Q &amp; A and other fields. However, even the largest knowledge map in the world is still incomplete, so knowledge reasoning has always been one of the research hot spots of knowledge graph. In this paper, we propose a model (DR-GCN) of neural network which integrates entity description and relation graph convolution (R-GCN), and apply it to the standard task of knowledge reasoning: entity classification, that is, the restoration of missing entity attributes. R-GCN is a kind of graph convolution neural network (GCN), which is specially developed according to the characteristics of highly multivariate relational data of practical knowledge graph. The DR-GCN model in this paper makes full use of the relationship type, relationship direction, entity self cycle, entity description and other information in the knowledge graph for entity classification. In this paper, the DR-GCN model is thoroughly evaluated and compared with the established baseline. Experiments show that the experimental results of DR-GCN model are better than that of the existing baseline. On the AIFB dataset, the accuracy of the DR-GCN model is 0.24% higher than the 96.19% of the G-GAT model with the highest accuracy in the base-line. On the BGS dataset, the accuracy of the DR-GCN model is 0.35% higher than the 87.24% of the RDF2Vec model with the highest accuracy in the baseline, which proves that the improved model is more effective. 
  
 
</p></abstract><kwd-group><kwd>实体分类，知识图谱，实体描述，关系图卷积神经网络, Entity Classification</kwd><kwd> Knowledge Graph</kwd><kwd> Entity Descriptions</kwd><kwd> Relation-Graph Convolutional Networks</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于实体描述和关系图卷积神经网络的实体分类研究<sup> </sup></title><p>郑小柏<sup>1</sup>，崔 岩<sup>1,2</sup>，刘兴林<sup>1</sup>，何雅芳<sup>1</sup></p><p><sup>1</sup>五邑大学智能制造学部，广东 江门</p><p><sup>2</sup>珠海四维时代网络科技有限公司，广东 珠海</p><p>收稿日期：2020年6月24日；录用日期：2020年7月8日；发布日期：2020年7月15日</p><disp-formula id="hanspub.36517-formula28"><graphic xlink:href="//html.hanspub.org/file/3-1541817x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>随着大数据时代的来临，人工智能快速发展，知识图谱已经在垂直搜索和智能问答等领域中发挥着重要的作用。但是，即使是全世界上最大的知识图谱也仍然不完整，所以知识推理一直是知识图谱的研究热点之一。本文提出了一个融合实体描述和关系图卷积神经网络(R-GCN)的模型(DR-GCN)，并将其应用于知识推理中的标准任务：实体分类(Entity Classification)，即缺失实体属性的恢复。R-GCN是根据图卷积神经网络(GCN)，专门针对实际知识图谱的高度多元关系数据特征而开发的一类图卷积神经网络。本文研究的DR-GCN模型，该模型充分利用了知识图谱中的关系类型、关系方向、实体自循环、实体描述等信息进行实体分类。本文对DR-GCN模型进行了彻底的评估，并与已建立的基线进行了比较。实验表明，DR-GCN模型的实验结果比现有的基线有所提高，在AIFB和BGS数据集上，DR-GCN模型的准确率分别比基线中准确率最高的G-GAT和RDF2Vec模型的96.19%和87.24%高0.24%和0.35%，验证了改进后的模型效果更佳。</p><p>关键词 :实体分类，知识图谱，实体描述，关系图卷积神经网络</p><disp-formula id="hanspub.36517-formula29"><graphic xlink:href="//html.hanspub.org/file/3-1541817x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-1541817x7_hanspub.png" /> <img src="//html.hanspub.org/file/3-1541817x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>随着计算机科学的迅猛发展，大数据时代来临，互联网数据爆炸式的增长，这些数据中蕴含着大量有价值的信息，如何对其进行提取和表达引起了大量学者的关注。知识图谱作为一个能对其进行有效的表达和组织的工具应运而生。知识图谱这一概念，由2012年Google公司所提出，用于Google浏览器的智能搜索引擎。自此以后，知识图谱得到了众多学者的关注和青睐，在许多领域得到了广泛的应用。然而，知识图谱大多都是由人工或半自动的方法构建，导致了知识图谱的不完整。据2014年统计，世界上最大的知识库之一freebase，存在着71%的人没有确切的出生日期，75%的人没有国籍信息 [<xref ref-type="bibr" rid="hanspub.36517-ref1">1</xref>]。知识图谱的不完整制约着其下游应用的发展，如何对现有的知识图谱进行补全，扩大其规模成为人们迫切需要解决的问题之一，因此知识推理成为了人们关注和研究的重点之一。</p><p>Schlichtkrull等人 [<xref ref-type="bibr" rid="hanspub.36517-ref2">2</xref>] 提出了关系图卷积神经网络模型(R-GCN)从微观上建模星形结构以建模知识图谱。R-GCN网络将目标实体与知识图谱中的邻居实体进行卷积学习，输入当前实体的相邻关系信息，包括关系类型、关系的方向以及实体自循环的信息，输出目标实体的隐性特征向量表示，然后通过节点损失函数进行实体分类。该模型充分地利用了知识图谱的关系信息，却忽略了实体的简明描述中蕴含了丰富的语义信息。本文提出了结合实体描述和关系图卷积神经网络的模型(DR-GCN)，并将其应用于知识推理的实体分类任务。实验结果表明，该模型有效地提高了实体分类的准确度，验证了该方法的有效性。</p></sec><sec id="s4"><title>2. 相关研究</title><p>知识推理是指利用知识图谱中已有的知识，通过某些方法，推理出新的知识或识别知识图谱中错误的知识 [<xref ref-type="bibr" rid="hanspub.36517-ref3">3</xref>]。即已知知识图谱三元组中的任意两个元素，采用某种方法，推理出三元组中所缺失的另外一个元素或识别出另外一个元素的正确与否。例如，事实三元组(美国，首都，？)，由这个三元组我们可以知道美国和首都这两个元素，不难推断出三元组的另外一个元素为华盛顿。知识推理根据推理任务的类型可分为：链路预测、实体预测、关系预测、属性预测、实体分类等。本文研究的主要内容为实体分类。</p><p>知识推理根据其推理方法的不同可分为基于规则的推理、基于分布式表示的推理、基于神经网络的推理以及混合推理 [<xref ref-type="bibr" rid="hanspub.36517-ref4">4</xref>]。传统的基于规则的推理方法，虽然它推理的准确率非常高，但它需要知道全面的推理规则和本体约束，而在现实中很少能达成这一条件。基于神经网络的推理方法具有很强的学习能力、推理能力和泛化能力，既能从大量的文本语料中学习其特性，解决知识图谱中数据规模庞大的问题，也能对知识图谱中的事实三元组进行直接建模，解决了知识推理中的计算难度大的问题，还能通过巧妙的设计神经网络，使其在某种程度上模拟人脑进行知识推理。</p><p>基于神经网络的知识推理的研究最早始于Socher等人 [<xref ref-type="bibr" rid="hanspub.36517-ref5">5</xref>] 提出的一种神经张量网络模型(NTN)，为了能够有效了提取各个实体之间存在的潜在关系，该模型合理的使用了双线性张量层取代了常用的标准线性神经网络层。随后出现了各种各样的知识推理神经网络模型。例如，对于前人的知识推理研究一直没有用到的实体描述信息，Xie等人 [<xref ref-type="bibr" rid="hanspub.36517-ref6">6</xref>] 提出了一个利用CBOW或卷积神经网络模型对知识图谱中的实体进行学习表示的基于实体描述的表示学习模型(DKRL)，DKRL模型能够有效的提取知识图谱中实体的描述信息，利用实体描述进行实体和关系预测任务。受多候选项排名问题的启发，Shi等人 [<xref ref-type="bibr" rid="hanspub.36517-ref7">7</xref>] 提出了一个投影嵌入的共享变量的神经网络模型(ProjE)，该模型通过使用低维图嵌入学习知识图谱中的实体和关系的联合嵌入，将实体预测视为排序问题，在其输出结果中取最高得分候选项作为实体预测的结果。Tay等人 [<xref ref-type="bibr" rid="hanspub.36517-ref8">8</xref>] 认为多任务的交叉学习表示能有有效提高非离散属性预测的准确性，提出了多任务神经网络模型(MT-KGNN)，该模型能够有效的利用两个任务中的实体、关系和属性等信息进行非离散属性预测。Shi等人 [<xref ref-type="bibr" rid="hanspub.36517-ref9">9</xref>] 提出了一个开放世界知识图谱补全的ConMask模型，该模型能够学习实体名称和实体描述等信息，将知识图谱外部的实体无缝的连接有现有的知识图谱内部，从而实现知识图谱的动态补全。Lukovnikov等人 [<xref ref-type="bibr" rid="hanspub.36517-ref10">10</xref>] 认为对知识图谱中的主谓语进行排序学习有利于匹配给定问题的相似事情，提出了一个整体神经匹配模型(HNM)，有效的提高了端到端智能问答的效率。Shen等人 [<xref ref-type="bibr" rid="hanspub.36517-ref11">11</xref>] 提出了隐形推理网模型(IRN)，该模型通过巧妙的设计神经网络，使其在某种程度上模拟人脑进行知识推理。Graves等人 [<xref ref-type="bibr" rid="hanspub.36517-ref12">12</xref>] 提出了一个DNC模型，该模型通过合理的设计和使用辅助神经单元，模拟人脑中对记忆的增、改、删过程。Schlichtkrull等人 [<xref ref-type="bibr" rid="hanspub.36517-ref2">2</xref>] 从知识图谱的内部结构角度出发，提出了关系图卷积神经网络模型(R-GCN)，该模型充分的利用了知识图谱中的关系信息进行知识推理，其中包括了关系类型、关系方向、实体自循环等信息。Dan等人 [<xref ref-type="bibr" rid="hanspub.36517-ref13">13</xref>] 从关系图卷积神经网络的基础上出发，加入了内部关系图注意力机制，提出了关系图注意力卷积神经网络模型(R-GAT)，该模型进一步的提升了实验的准确率。</p></sec><sec id="s5"><title>3. 模型设计与分析</title><sec id="s5_1"><title>3.1. DR-GCN模型</title><p>GCNs模型由Duvenaud等人 [<xref ref-type="bibr" rid="hanspub.36517-ref14">14</xref>] 和Kipf等人 [<xref ref-type="bibr" rid="hanspub.36517-ref15">15</xref>] 所提出，我们的模型作为GCN模型的一个扩展，是根据知识图谱的内部结构所设计的一类型神经网络。GCN通过知识图谱的局部邻域关系信息进行学习表示，从而扩展成完整的知识图谱。这一类型的神经网络模型，如GCN，可以视为是Gilmer等人 [<xref ref-type="bibr" rid="hanspub.36517-ref16">16</xref>] 提出的简单可微消息传递框架的特殊情况。</p><p>h i ( l + 1 ) = σ ( ∑ m ∈ M i g m ( h i ( l ) , h j ( l ) ) ) (1)</p><p>其中， h i ( l ) ∈ R d ( l ) 是神经网络第l层节点 V i 的隐藏状态， d ( l ) 是表示该层的维数， g m ( ⋅ ， ⋅ ) 表示函数中传入的消息被累计并通过 σ ( ⋅ ) 元素激活函数传递，例如 R e L U ( ⋅ ) = max ( 0 ， ⋅ ) 。 M i 表示节点 V i 的传入消息集，通常选择与传入边集相同。 g m ( ⋅ ， ⋅ ) 函数通常被选为(消息指定的)类似神经网络的函数，或者简单地选为Kipf等人 [<xref ref-type="bibr" rid="hanspub.36517-ref15">15</xref>] 提出的带权矩阵W的线性变换 g m ( h i , h j ) = W h j 。</p><p>这种类型的转换在知识图谱局部结构化邻域信息的学习特征表示方面非常出色，并且给Duvenaud等人 [<xref ref-type="bibr" rid="hanspub.36517-ref14">14</xref>] 提出的图形分类和Kipf等人 [<xref ref-type="bibr" rid="hanspub.36517-ref15">15</xref>] 提出的基于图形的半监督学习等带来了显著的提高，在图形领域作出了重大的贡献。2018年，Schlichtkrull等人 [<xref ref-type="bibr" rid="hanspub.36517-ref2">2</xref>] 根据知识图谱局部结构化邻域关系信息，对此进一步改进，提出了关系图卷积神经网络模型，并将其模型传播方式定义为：</p><p>h i ( l + 1 ) = σ ( ∑ r ∈ R ∑ j ∈ N i r 1 c i , r W r ( l ) h j ( l ) + W 0 ( l ) h i ( l ) ) (2)</p><p>其中 N i r 表示在关系 r ∈ R 下节点i的邻域索引集，r是可以预先学习或选择的问题特定的标准化常数，例如 c i , r = | N i r | 。</p><p>受这些模型结构的启发，我们引入了实体描述这一信息，我们把该模型称为DR-GCN模型，并定义了以下简单的传播模型，用于计算知识图谱局部结构中实体 V i 的前向传递更新：</p><p>h i ( l + 1 ) = σ ( ∑ r ∈ R ∑ j ∈ N i r ( 1 c i , r W r ( l ) h j ( l ) + E d r , j i ) + W 0 ( l ) h i ( l ) ) (3)</p><p>其中 E d r , j i 表示在节点i的邻域索引集，关系 r ∈ R 下第j个节点的实体描述。</p><p>神经网络层的更新通过公式(3)对知识图谱中的每个节点进行并行计算。在实践中，式(3)可以通过使用稀疏矩阵相乘的方法实现。该DR-GCN模型利用了知识图谱局部结构化邻域的关系信息和实体描述，图1描述了DR-GCN模型中计算单个节点更新的过程。</p><p>如图1所示，收集来自目标节点(红色)相邻的节点(蓝色)的激活，然后分别对其中的关系类型、关系方向、实体自循环和实体描述进行类型转换，转换的结果(绿色)再以归一化的方式累加总和并通过激活函数(ReLU)传递更新。其中，每个节点的更新和参数的共享可以在整个图中进行并行计算。</p></sec><sec id="s5_2"><title>3.2. 正则化</title><p>我们注意到，将式(3)应用到大规模知识图谱的时候，出现参数数量随着知识图谱中关系数量的增长而呈指数型增长的问题。</p><p>为了解决这个问题，我们效仿了Schlichtkrull等人 [<xref ref-type="bibr" rid="hanspub.36517-ref2">2</xref>] 的做法，采用了偏置分解的方法来调整DR-GCN的权重。通过偏置分解，每个<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541817x28_hanspub.png" xlink:type="simple"/></inline-formula>定义如下：</p><p>W r ( l ) = ∑ b = 1 B a r b ( l ) V b ( l ) (4)</p><p>即 W r ( l ) 通过偏置分解为 V b ( l ) ∈ R d ( l ) &#215; d ( l + 1 ) 与系数 a r b ( l ) 的线性组合。其中， a r b ( l ) 为基础系数， V b ( l ) 为基础矩阵。偏置分解式(4)可以看作是不同关系类型之间有效权重共享的一种形式，偏置分解有效的减少了大规模关系数据中所需要学习的参数数量。</p><p>图1. DR-GCN模型中单个图节点/实体(红色)的更新过程</p></sec><sec id="s5_3"><title>3.3. 实体分类</title><p>通过堆叠(3)式的神经网络层，在输出层上使用softmax( )激活函数，组成DR-GCN模型用于实体分类实验。在DR-GCN模型中，我们将所有标记节点上的交叉熵损失将至最低：</p><p>L = − ∑ i ∈ Y ∑ k = 1 K t i k ln h i k ( L ) (5)</p><p>其中，Y是具有标签的节点索引集，是第i个标签节点的网络输出的第k个条目，表示其各种的基本事实标签。在实践中，我们使用梯度下降技术训练模型。我们的实体分类模型示意图如图2所示。</p><p>图2. 用每个节点损失函数描述实体分类的DR-GCN模型</p></sec></sec><sec id="s6"><title>4. 实验结果分析</title><sec id="s6_1"><title>4.1. 数据集</title><p>我们将用Ristoski等人 [<xref ref-type="bibr" rid="hanspub.36517-ref17">17</xref>] 提出的AIFB、MUTAG和BGS三个数据集对基线中的各个模型进行评估。在每个数据集中，实体分类的目标可以表示为节点的一组实体属性。数据集的各种指标书记统计见表1。关于数据集更详细的描述，可以参考Ristoski等人 [<xref ref-type="bibr" rid="hanspub.36517-ref17">17</xref>] 的文献。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Number of entities, relations, edges and classes along with the number of labeled entities for each of the dataset</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Dataset</th><th align="center" valign="middle" >AIFB</th><th align="center" valign="middle" >MUTAG</th><th align="center" valign="middle" >BGS</th></tr></thead><tr><td align="center" valign="middle" >Entities</td><td align="center" valign="middle" >8,285</td><td align="center" valign="middle" >23,644</td><td align="center" valign="middle" >333,845</td></tr><tr><td align="center" valign="middle" >Relations</td><td align="center" valign="middle" >45</td><td align="center" valign="middle" >23</td><td align="center" valign="middle" >103</td></tr><tr><td align="center" valign="middle" >Edges</td><td align="center" valign="middle" >29,043</td><td align="center" valign="middle" >74,277</td><td align="center" valign="middle" >916,199</td></tr><tr><td align="center" valign="middle" >Labeled</td><td align="center" valign="middle" >176</td><td align="center" valign="middle" >340</td><td align="center" valign="middle" >146</td></tr><tr><td align="center" valign="middle" >Classes</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >2</td></tr></tbody></table></table-wrap><p>表1. 每个数据集中的实体、关系、边和标记的实体以及类的数量</p></sec><sec id="s6_2"><title>4.2. 实验设置</title><p>作为我们实验的基线，我们比较了来自Ristoski等人 [<xref ref-type="bibr" rid="hanspub.36517-ref18">18</xref>] 提出的RDF2Vec嵌入、Vries等人 [<xref ref-type="bibr" rid="hanspub.36517-ref19">19</xref>] 提出的Weisfeilaer-Lehman 内核模型WL和Schlichtkrull等人 [<xref ref-type="bibr" rid="hanspub.36517-ref2">2</xref>] 提出的关系图卷积神经网络模型R-GCN，以及Dan等人 [<xref ref-type="bibr" rid="hanspub.36517-ref13">13</xref>] 提出的关系图注意力卷积神经网络模型R-GAT。RDF2Vec提取标记图上的行走，然后使用Mikolov等人 [<xref ref-type="bibr" rid="hanspub.36517-ref20">20</xref>] 提出的Skipgram模型进行处理，生成实体嵌入，用于后续的分类。R-GCN模型通过对知识图谱中的局部结构化邻域关系信息进行卷积学习，输入目标实体的相邻关系信息，其中包括关系类型、关系方向和实体自循环的信息，在输出层上使用softmax( )激活函数进行实体分类。R-GAT在关系图卷积神经网络的基础上出发，加入了内部关系图注意力机制。</p><p>对于数据集AIFB、MUTAG和BGS的训练，我们选择为具有16个隐层的DR-GCN模型，学习率设置为0.01，归一化常数选择为 c i , r = | N i r | ，训练迭代次数为50次。所有实体分类实验均在16 GB内存的CUP上运行。</p></sec><sec id="s6_3"><title>4.3. 实验结果</title><p>AIFB、MUTAG和BGS三个数据集在WL、RDF2Vec、R-GCN和R-GAT，以及DR-GCN等模型下的实体分类结果如表2所示：</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of classification accuracy of various model entitie</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Model</th><th align="center" valign="middle" >AIFB</th><th align="center" valign="middle" >MUTAG</th><th align="center" valign="middle" >BGS</th></tr></thead><tr><td align="center" valign="middle" >WL</td><td align="center" valign="middle" >80.55</td><td align="center" valign="middle" >80.88</td><td align="center" valign="middle" >86.20</td></tr><tr><td align="center" valign="middle" >RDF2Vec</td><td align="center" valign="middle" >88.88</td><td align="center" valign="middle" >67.20</td><td align="center" valign="middle" >87.24</td></tr><tr><td align="center" valign="middle" >R-GCN</td><td align="center" valign="middle" >95.83</td><td align="center" valign="middle" >73.23</td><td align="center" valign="middle" >83.10</td></tr><tr><td align="center" valign="middle" >R-GAT</td><td align="center" valign="middle" >96.19</td><td align="center" valign="middle" >74.38</td><td align="center" valign="middle" >85.35</td></tr><tr><td align="center" valign="middle" >DR-GCN</td><td align="center" valign="middle" >96.43</td><td align="center" valign="middle" >75.93</td><td align="center" valign="middle" >87.59</td></tr></tbody></table></table-wrap><p>表2. 各模型实体分类准确率对比</p><p>在表2中，我们评估了在AIFB、MUTAG和BGS上的DR-GCN模型，实验结果显示，在AIFB数据集上，实体分类准确率由高到低的模型为：1) DR-GCN，2) R-GAT，3) R-GCN，4) RDF2Vec，5) WL。我们的DR-GCN模型的实体分类准确率比R-GAT模型的实体分类准确率还要高0.24%。在BGS数据集上，实体分类准确率由高到低的模型为：1) DR-GCN，2) RDF2Vec，3) WL，4) R-GAT，5) R-GCN。我们的DR-GCN模型的实体分类准确率比RDF2Vec模型的实体分类准确率还要高0.35%。在MUTAG数据集上，实体分类准确率由高到低的模型为：1) WL，2) DR-GCN，3) R-GAT，4) R-GCN，5) RDF2Vec。在MUTAG数据集上，准确率最高是WL模型，但是，我们注意到，DR-GCN模型仍比同是关系图卷积神经网络类型的R-GCN和R-GAT模型的准确率分别要高出2.70%和1.55%。正如Schlichtkrull等人 [<xref ref-type="bibr" rid="hanspub.36517-ref2">2</xref>] 在他们的文献中所提到的，理解MUTAG数据集的本质与其特性，是一系列的关系图卷积神经网络类R-GCN、R-GAT和DR-GCN模型在MUTAG数据集上准确性比较一般的原因。若想对MUTAG数据集的本质特性有进一步详细的了解，可以参考Schlichtkrull等人 [<xref ref-type="bibr" rid="hanspub.36517-ref2">2</xref>] 和Ristoski等人 [<xref ref-type="bibr" rid="hanspub.36517-ref17">17</xref>] 的文献。</p></sec></sec><sec id="s7"><title>5. 结束语</title><p>本文探索了利用基于实体描述和关系图卷积神经网络的DR-GCN模型进行知识推理的实体分类任务。与现有的R-GCN模型不同的是，DR-GCN模型加入了实体描述这一信息。通过多组的实验，实验结果表明DR-GCN模型的方法比其他的四种基线在AIFB和BGS两个数据集上取得了更高的准确率，验证了改进后的模型效果更佳。下一步将利用DR-GCN模型作为编码器和ComplEx模型作为解码器形成一个自动编码器，这更适合对知识图谱中存在的大量非对称关系的建模，并对其进行链路预测任务检验模型的有效性。</p></sec><sec id="s8"><title>基金项目</title><p>本广东省科技厅项目(2016A070708002)；广东省教育厅研究生教育创新计划项(2016SFKC_42, YJS-SFKC-14-05, YJS-PYJD-17-03)资助；教育部“云数融合、科教创新”基金项目(2017B02101)；江门市创新科研团队引进和资助项目(人工智能三维数字化技术在大型场景(地理空间)方向的应用研究)；江门市基础与理论科学研究类科技计划项目( 2017JC01021)资助。</p></sec><sec id="s9"><title>文章引用</title><p>郑小柏,崔 岩,刘兴林,何雅芳. 基于实体描述和关系图卷积神经网络的实体分类研究Research on Entity Classification Based on Entity Descriptions with Relational-Graph Convolutional Networks[J]. 计算机科学与应用, 2020, 10(07): 1319-1326. https://doi.org/10.12677/CSA.2019.107136</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.36517-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Dong, X., Gabrilovich, E., Heitz, G., et al. (2014) Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion. 20th ACM SIGKDD International Conference on Knowledge Discovery and Date Mining, New York, 24-27 August 2014, 601-610. &lt;br&gt;https://doi.org/10.1145/2623330.2623623</mixed-citation></ref><ref id="hanspub.36517-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Schlichtkrull, M., Kipf, T.N., Bloem, P., Van Den Berg, R., Titov, I. and Welling, M. (2018) Modeling Relational Data with Graph Convolutional Networks. Eu-ropean Semantic Web Conference, Heraklion, 3-7 June 2018, 593-607.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-93417-4_38</mixed-citation></ref><ref id="hanspub.36517-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">丁建辉, 贾维嘉. 知识图谱补全算法综述[J]. 信息通信技术, 2018, 27(1): 57-62.</mixed-citation></ref><ref id="hanspub.36517-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">官赛萍, 靳小龙, 贾岩涛. 面向知识图谱的知识推理研究进展[J]. 软件学报, 2018, 29(10): 74-102.</mixed-citation></ref><ref id="hanspub.36517-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Socher, R., Chen, D., Manning, C.D., et al. (2013) Reasoning with Neural Tensor Networks for Knowledge Base Completion. In: Proceedings of Advances in Neural Information Processing Systems, Curran Associ-ates Inc., Red Hook, 926-934.</mixed-citation></ref><ref id="hanspub.36517-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Xie, R.B., Liu, Z., Jia, J., et al. (2016) Representation Learning of Knowledge Graphs with Entity Descriptions. In: Proceedings of the 30th AAAI Conference on Artificial Intelligence, AAAI, Menlo Park, 2659-2665.</mixed-citation></ref><ref id="hanspub.36517-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Shi, B. and Weninger, T. (2017) ProjE: Embedding Projection for Knowledge Graph Completion. Proceedings of the 31st AAAI Conference on Artificial Intelligence, Menlo Park, 1236-1242.</mixed-citation></ref><ref id="hanspub.36517-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Tay, Y., Tuan, L.A., Phan, M.C., et al. (2017) Multi-Task Neural Network for Non-Discrete Attribute Prediction in Knowledge Graphs. Pro-ceedings of the 2017 ACM on Conference on Information and Knowledge Management, Singapore, 6-10 November 2017, 1029-1038. &lt;br&gt;https://doi.org/10.1145/3132847.3132937</mixed-citation></ref><ref id="hanspub.36517-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Shi, B. and Weninger, T. (2017) Open-World Knowledge Graph Completion. Computer Science, Louisiana, 2017, 22-31.</mixed-citation></ref><ref id="hanspub.36517-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Lukovnikov, D., Fischer, A., Lehmann, J., et al. (2017) Neural Network-Based Question Answering over Knowledge Graphs on Word and Character Level. In-ternational World Wide Web Conference Committee, Perth, 591-597.  
&lt;br&gt;https://doi.org/10.1145/3038912.3052675</mixed-citation></ref><ref id="hanspub.36517-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Shen, Y., Huang, P.S., Chang, M.W., et al. (2017) Modeling Largescale Structured Relationships with Shared Memory for Knowledge Base Completion. Proceedings of the 2nd Workshop on Representation Learning for NLP, Vancouver, August 2017, 57-68. &lt;br&gt;https://doi.org/10.18653/v1/W17-2608</mixed-citation></ref><ref id="hanspub.36517-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Graves, A., Wayne, G., Reynolds, M., et al. (2016) Hybrid Computing Using a Neural Network with Dynamic External Memory. Nature, 538, 471-476. &lt;br&gt;https://doi.org/10.1038/nature20101</mixed-citation></ref><ref id="hanspub.36517-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Busbridge, D., Sherburn, D., Cavallo, P. and Hammerla, N.Y. (2019) Relational Graph Attention Networks. Springer, Berlin, 311-317.</mixed-citation></ref><ref id="hanspub.36517-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Duvenaud, D.K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A. and Adams, R.P. (2015) Convolutional Networks on Graphs for Learning Molecular Fingerprints. Proceedings of Advances in Neural Information Processing Systems, Montreal, 7-12 December 2015, 113-119.</mixed-citation></ref><ref id="hanspub.36517-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Kipf, T.N. and Welling, M. (2017) Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017, Toulon, 24-26 April 2017, 51-57.</mixed-citation></ref><ref id="hanspub.36517-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O. and Dahl, G.E. (2017) Neural Message Passing for Quantum Chemistry. Proceedings of the 34th International Confer-ence on Machine Learning, Sydney, 33-39.</mixed-citation></ref><ref id="hanspub.36517-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Ristoski, P., de Vries, G.K.D. and Paulheim, H. (2016) A Collection of Benchmark Datasets for Systematic Evaluations of Machine Learning on the Semantic Web. In: International Semantic Web Conference, Springer, Berlin, 186-194. &lt;br&gt;https://doi.org/10.1007/978-3-319-46547-0_20</mixed-citation></ref><ref id="hanspub.36517-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Ristoski, P. and Paulheim, H. (2016) Rdf2vec: Rdf Graph Embeddings for Data Mining. In: International Semantic Web Conference, Springer, Berlin, 498-514. &lt;br&gt;https://doi.org/10.1007/978-3-319-46523-4_30</mixed-citation></ref><ref id="hanspub.36517-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">de Vries, G.K.D. and de Rooij, S. (2015) Substructure Counting Graph Kernels for Machine Learning from Rdf Data. Web Semantics: Science, Services and Agents on the World Wide Web, 35, 71-84.  
&lt;br&gt;https://doi.org/10.1016/j.websem.2015.08.002</mixed-citation></ref><ref id="hanspub.36517-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J. (2013) Distributed Representations of Words and Phrases and Their Compositionality. NIPS 2013, Lake Tahoe, 5-8 December 2013, 333-339.</mixed-citation></ref></ref-list></back></article>