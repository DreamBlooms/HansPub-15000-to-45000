<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.1012252</article-id><article-id pub-id-type="publisher-id">CSA-39518</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20201200000_48123814.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于Attention-Bi-LSTM的微博评论情感分析研究
  Attention-Bi-LSTM Based Analysis of Weibo Comments
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>彬</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>蒋</surname><given-names>鸿玲</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>吴</surname><given-names>槟</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>中国科学院信息工程研究所，信息安全国家重点实验室，北京;中国科学院大学，网络空间安全学院，北京</addr-line></aff><aff id="aff2"><addr-line>北京信息科技大学信息管理学院，北京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>12</month><year>2020</year></pub-date><volume>10</volume><issue>12</issue><fpage>2380</fpage><lpage>2387</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   短文本情感分析，在舆情监控和商业上有很多重要应用。以微博评论文本为研究对象，通过对微博评论文本进行分词、去除停用词，并使用Word2vec进行词向量训练得到词向量，并在Bi-LSTM中引入Attention机制，对Bi-LSTM双向处理后的结果进行加权进行输出。实验结果表明，Attention-Bi-LSTM与Bi-LSTM相比能有效识别出情感语句中重要的语义，提高预测的准确度。 Sentiment analysis has many important applications in public opinion monitoring and business. In this paper, microblog comment text is taken as the research object. Word segmentation is carried out on microblog comment text, stopping words are removed, and Word2vec is used for word vector training to obtain the word vector. Attention mechanism is introduced in Bi-LSTM, and the results of Bi-LSTM two-way processing are weighted and output. Experiments show that Attention-Bi-LSTM can effectively identify the important semantics of emotional statements and improve the accuracy of prediction compared with Bi-LSTM. 
  
 
</p></abstract><kwd-group><kwd>情感分析，Word2vec，Attention，Bi-LSTM, Emotion Analysis</kwd><kwd> Word2vec</kwd><kwd> Attention</kwd><kwd> Bi-LSTM</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>短文本情感分析，在舆情监控和商业上有很多重要应用。以微博评论文本为研究对象，通过对微博评论文本进行分词、去除停用词，并使用Word2vec进行词向量训练得到词向量，并在Bi-LSTM中引入Attention机制，对Bi-LSTM双向处理后的结果进行加权进行输出。实验结果表明，Attention-Bi-LSTM与Bi-LSTM相比能有效识别出情感语句中重要的语义，提高预测的准确度。</p></sec><sec id="s2"><title>关键词</title><p>情感分析，Word2vec，Attention，Bi-LSTM</p></sec><sec id="s3"><title>Attention-Bi-LSTM Based Analysis of Weibo Comments</title><p>Bin Wang<sup>1</sup>, Hongling Jiang<sup>1*</sup>, Bin Wu<sup>2,3</sup></p><p><sup>1</sup>School of Information Management, Beijing Information Science &amp; Technology University, Beijing</p><p><sup>2</sup>State Key Laboratory of Information, Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing</p><p><sup>3</sup>School of Cyber Security, University of Chinese Academy of Sciences, Beijing</p><p><img src="//html.hanspub.org/file/27-1541857x4_hanspub.png" /></p><p>Received: Nov. 28<sup>th</sup>, 2020; accepted: Dec. 23<sup>rd</sup>, 2020; published: Dec. 30<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/27-1541857x6_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Sentiment analysis has many important applications in public opinion monitoring and business. In this paper, microblog comment text is taken as the research object. Word segmentation is carried out on microblog comment text, stopping words are removed, and Word2vec is used for word vector training to obtain the word vector. Attention mechanism is introduced in Bi-LSTM, and the results of Bi-LSTM two-way processing are weighted and output. Experiments show that Attention-Bi-LSTM can effectively identify the important semantics of emotional statements and improve the accuracy of prediction compared with Bi-LSTM.</p><p>Keywords:Emotion Analysis, Word2vec, Attention, Bi-LSTM</p><disp-formula id="hanspub.39518-formula29"><graphic xlink:href="//html.hanspub.org/file/27-1541857x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/27-1541857x8_hanspub.png" /> <img src="//html.hanspub.org/file/27-1541857x9_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>微博是具有巨大用户群体的社交网络平台，用户在微博上发表自己的观点，情绪，记录自己的生活，大量用户的情感趋向对于政府的决策起到很重要的作用。情感分析分为有监督学习和无监督学习。有监督学习需要手动提取特征，如传统的决策树、随机森林等，无监督学习无需手动提取文本特征，借助词典方式或者对句子语法分析进行提取情感信息。</p><p>彭丹蕾 [<xref ref-type="bibr" rid="hanspub.39518-ref1">1</xref>] 在商品评论情感分析中使用机器学习的SVM方法和深度学习LSTM方法进行对比分析，发现LSTM能够更好的提取词向量中隐藏的情感信息，达到更好的效果。Subarno Pal [<xref ref-type="bibr" rid="hanspub.39518-ref2">2</xref>] 等人在电影评论数据集中使用LSTM，并使用LSTM彼此堆叠和双向LSTM进行对比研究，研究发现双向LSTM在此数据集中准确度更高。Luong等人 [<xref ref-type="bibr" rid="hanspub.39518-ref3">3</xref>] 提出局部注意力机制，对一个窗口范围内的词进行分布式表示，对固定大小的窗口范围内所有隐状态进行权重计算。本文使用word2vec在已有微博情感语料库中进行词向量训练，同时去除了部分低频词。为了加强词语关联性，提升情感分析准确率，在Bi-LSTM中引入Attention机制应用在已有微博情感评论数据集进行情感分析，实验表明引入Attention机制的Bi-LSTM要优于基准Bi-LSTM算法。</p></sec><sec id="s6"><title>2. 算法描述</title><p>本文使用Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification (基于注意力的双向长短期记忆网络关系分类网络)算法，以下简称ATT-Bi-LSTM。这是一种改进的RNN算法模型，模型结构如图1所示。与传统RNN模型相比，此模型的双层LSTM能解决了循环神经网络中的梯度消失问题。LSTM具有门机制，从而能够控制每一个LSTM单元保留的历史信息的程度以及记忆当前输入的信息，保留重要特征，丢弃不重要的特征 [<xref ref-type="bibr" rid="hanspub.39518-ref1">1</xref>]。并且引入了Attention机制，解决传统中文文本分类对于长序列编码解码精度下降的问题，针对时序获取关键信息。</p><sec id="s6_1"><title>2.1. Word2vec模型</title><p>Word2vec是Tomas Mikolov通过NNLM模型的改进而研究出来的工具 [<xref ref-type="bibr" rid="hanspub.39518-ref4">4</xref>]。Word2vec模型如图2所示，将每个词映射到低维空间，并且其中意思相近的词会被映射到向量空间中相近的位置，解决了表征语义信息、数据稀疏、维度灾难问题。Word2vec主要分为CBOW (Continuous Bag of Words)和Skip Gram两种模式，模型采用的方法也类似，均是根据上下文语句推断当前词发生的概率。本文主要运用Skip Gram进行词向量训练，因为此训练模式在大型语料中效果更好。</p><p>图1. Attention Bi-LSTM结构</p></sec><sec id="s6_2"><title>2.2. LSTM模型</title><p>LSTM模型解决了RNN模型存在的梯度消失和梯度爆炸的问题，在网络加入了输入门、输出门、遗忘门，可以学习长期依赖信息。结构如图3所示 [<xref ref-type="bibr" rid="hanspub.39518-ref5">5</xref>]，x<sub>t</sub>为输入序列；i<sub>t</sub>为输入门；o<sub>t</sub>为输出门；c<sub>t</sub>为激活向量；h<sub>t</sub>表示单元输出激活函数。</p><p>图2. Word2vec模型</p><p>遗忘门用来控制遗忘上一层细胞状态的内容，根据上一序列的h<sub>t</sub><sub>−1</sub>和本序列的X<sub>t</sub><sub>−1</sub>为输入，通过sigmoid激活函数，计算得出需要遗忘的信息f<sub>t</sub>；输入门用来计算保存到单元终得信息i<sub>t</sub>和当前添加到单元中的新信息 c ˜ t ，输出门用来计算输出的信息o<sub>t</sub>，再乘单元状态通过函数tanh的值，得到输出h<sub>t</sub>。</p><p>图3. LSTM模型结构图</p><p>根据LSTM结构图得到公式(1)~(6)：</p><p>f t = σ ( W x f ∗ x t + W h f ∗ h ( t − 1 ) + W c f ∗ c ( t − 1 ) + b f ) (1)</p><p>i t = σ ( W x i ∗ x t + W h i ∗ h ( t − 1 ) + W c i ∗ c ( t − 1 ) + b i ) (2)</p><p>c ˜ t = tanh ( W x c ∗ x t + W h c ∗ h ( t − 1 ) + b c ) (3)</p><p>c t = f t ∗ c ( t − 1 ) + i t ∗ c ˜ t (4)</p><p>o t = σ ( W o ∗ x t + W h o ∗ h ( t − 1 ) − W c o ∗ c ( t − 1 ) + b o ) (5)</p><p>h t = o t ∗ tanh ( c t ) (6)</p><p>其中σ为sigmoid激活函数，tanh为tanh激活函数，w为权重矩阵，b为偏置项，h<sub>t</sub>表示单元输出激活函数。</p><p>LSTM可以对任意长度文本序列进行处理，有效解决了文本不定长问题，并且使当前单元得到本单元之前所有单元的信息，缺点是无法获得本单元之后单元的信息 [<xref ref-type="bibr" rid="hanspub.39518-ref6">6</xref>]。考虑到网络能否同时学习前向和后向数据，提出了Bi-LSTM。</p></sec><sec id="s6_3"><title>2.3. Bi-LSTM模型</title><p>Bi-LSTM将正向和逆向LSTM结合起来，一个负责前向信息传递，一个负责后向信息传递，两个LSTM信息组合，传入输出层，解决了汉语的双向语义依赖。</p></sec><sec id="s6_4"><title>2.4. 注意力机制层</title><p>Attention层将LSTM层输出的向量集合表示为 H : [ h 1 , h 2 , ⋯ , h t ] 。其Attention层得到的权重矩阵由下面的方式得到公式7~9：</p><p>M = tanh ( H ) (7)</p><p>α = softmax ( w t M ) (8)</p><p>r = H α t (9)</p><p>其中， H ∈ R d w &#215; t ，d<sup>w</sup>为词向量的维度，w<sup>t</sup>是一个训练学习得到的参数向量的转置。最终用以分类的句子将表示公式如下：</p><p>h * = tanh ( r ) (10)</p><p>序列输入时，随着序列的不断增长，原始根据时间步的方式的表现越来越差，这是由于原始的这种时间步模型设计的结构有缺陷，即所有的上下文输入信息都被限制到固定长度，整个模型的能力都同样受到限制。引入Attention机制的基本思想是：打破了传统编码器-解码器结构在编解码时都依赖于内部一个固定长度向量的限制。通过保留LSTM编码器对输入序列的中间输出结果，然后训练一个模型来对这些输入进行选择性的学习并且在模型输出时将输出序列与之进行关联。即生成一个权重向量，通过与这个权重向量相乘，使每一次迭代中的词汇级的特征合并为句子级的特征。</p></sec></sec><sec id="s7"><title>3. 微博情感评论情感分析流程</title><sec id="s7_1"><title>3.1. 分析流程</title><p>针对微博情感评论，实验主要包括四步：利用微博情感评论构成语料库、对语料库进行预处理如：分词、去停用词、训练词向量，将训练数据和测试数据传入Attention-Bi-LSTM模型，最后得出模型评价。流程如下图4：</p><p>图4. 分析流程</p></sec><sec id="s7_2"><title>3.2. 数据准备</title><p>微博是具有巨大用户群体的社交网络平台，本文选用已分类标注好的微博情感数据集评论进行研究，数据集有119,987条数据，分为正向59,993条和负向59,994条，分别用1和0进行标注，1为正向情感，0为负向情感。部分评论如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Partial comment dat</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >情感类型</th><th align="center" valign="middle" >评论信息</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >亲爱哒给我做的牛排，我给它起名叫小丑牛排，哈哈哈[鼓掌][心][心]</td></tr><tr><td align="center" valign="middle" >0</td><td align="center" valign="middle" >怎么回来后随便买点东西就几千块了，感觉好不习惯啊[抓狂]</td></tr></tbody></table></table-wrap><p>表1. 部分评论数据</p></sec><sec id="s7_3"><title>3.3. 数据预处理</title><p>将全部的正向评论和负向评论通过jieba分词进行数据分词、并进行去除停用词后储存合并为一个txt文件得到词集A，部分结果如表2所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Some results of word set </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >情感类型</th><th align="center" valign="middle" >评论信息</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >亲爱/做/牛排/起名叫/小丑/牛排/哈哈哈/鼓掌/心/心</td></tr><tr><td align="center" valign="middle" >0</td><td align="center" valign="middle" >回来/后/随便/买/东西/几千块/感觉/好/不/习惯/抓狂</td></tr></tbody></table></table-wrap><p>表2. 词集A部分结果</p><p>通过Python工具包gensim对词集A进行Word2vec词向量训练，得到词集A中所有词的词向量。</p></sec><sec id="s7_4"><title>3.4. 词向量表示</title><p>将预处理完的微博情感评论文本数据传入Word2vec中获得文本的词向量表示，并将生成的此索引通过词嵌入层转换成ATT-Bi-LSTM的输入。</p></sec><sec id="s7_5"><title>3.5. 情感分析结果生成</title><p>最后加入分类器。将实验数据分成两类：正向(1)，负向(0)，并计算评估指标。</p></sec></sec><sec id="s8"><title>4. 实验结果及分析</title><sec id="s8_1"><title>4.1. 实验环境</title><p>本实验环境如下：Windows10操作系统；CPU Intel CORE i7-6500U；内存大小为8个G，GPU为AMD Radeon R7 M360，开发环境为Tensorflow1.7，开发工具为Pycharm [<xref ref-type="bibr" rid="hanspub.39518-ref3">3</xref>]。</p></sec><sec id="s8_2"><title>4.2. 模型参数</title><p>实验参数对于训练结果有着很大影响，本文通过固定参数的方法对ATT-Bi-LSTM参数进行调整，最终词嵌入层大小为200，批尺寸(batch-size)为128，epoch为8，丢弃率(Dropout)为0.5，L2正则值为0.2，隐藏节点数为128。如表3所示。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Parameters of experiment</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >参数</th><th align="center" valign="middle" >值</th></tr></thead><tr><td align="center" valign="middle" >词向量大小</td><td align="center" valign="middle" >200</td></tr><tr><td align="center" valign="middle" >词向量训练模型</td><td align="center" valign="middle" >Skip-gram</td></tr><tr><td align="center" valign="middle" >批尺寸(batch-size)</td><td align="center" valign="middle" >128</td></tr><tr><td align="center" valign="middle" >Epoch</td><td align="center" valign="middle" >8</td></tr><tr><td align="center" valign="middle" >优化器(optimizer)</td><td align="center" valign="middle" >Adam</td></tr><tr><td align="center" valign="middle" >丢弃率(Dropout)</td><td align="center" valign="middle" >0.5</td></tr><tr><td align="center" valign="middle" >L2正则</td><td align="center" valign="middle" >0.2</td></tr><tr><td align="center" valign="middle" >学习率</td><td align="center" valign="middle" >0.001</td></tr><tr><td align="center" valign="middle" >训练集：测试集</td><td align="center" valign="middle" >8:2</td></tr><tr><td align="center" valign="middle" >隐藏层节点数</td><td align="center" valign="middle" >128</td></tr></tbody></table></table-wrap><p>表3. 实验参数</p></sec><sec id="s8_3"><title>4.3. 评价指标</title><p>本文使用精准率(Precision)，和召回率(Recall)，F值(F-Measure)对模型进行评价 [<xref ref-type="bibr" rid="hanspub.39518-ref7">7</xref>]，精准率是分类正向情感正确的示例在实际分类正向情感中的比例，反映了模型对样本的正确识别能力；召回率是模型所有检测出来分类正确的数据占总数据集的比例；F值是精准率和召回率的调和均值，当β = 1时，即为F1值。公式如下。</p><p>P = TP TP + FP (11)</p><p>R = TP TP + FN (12)</p><p>F = 2 ∗ P ∗ R P + R (13)</p><p>其中，TP为“真正向”，FP为“假正向”，TN为“真负向”，FN为“假负向”。如表4所示。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Confusion matri</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Positive</th><th align="center" valign="middle" >Negative</th></tr></thead><tr><td align="center" valign="middle" >True</td><td align="center" valign="middle" >True Positive(TP)</td><td align="center" valign="middle" >True Negative(TN)</td></tr><tr><td align="center" valign="middle" >False</td><td align="center" valign="middle" >False Positive(FP)</td><td align="center" valign="middle" >False Negative(FN)</td></tr></tbody></table></table-wrap><p>表4. 混淆矩阵</p></sec><sec id="s8_4"><title>4.4. 实验结果及分析</title><p>本文通过对比不同epoch下，精准率、召回率和F值，如图5所示。发现在Epoch为30时精准率与召回率、F值明显下降。Epoch为8时模型效果为最好。</p><p>图5. 不同Epoch结果比较</p></sec><sec id="s8_5"><title>4.5. 算法对比</title><p>本文算法与Bi-LSTM的对比在相同数据集和词向量上进行实验对比情况如表5所示。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Experimental results of Micro Blog sentiment datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >精准率%</th><th align="center" valign="middle" >召回率%</th><th align="center" valign="middle" >F1值%</th></tr></thead><tr><td align="center" valign="middle" >Bi-LSTM</td><td align="center" valign="middle" >91.11</td><td align="center" valign="middle" >89.13</td><td align="center" valign="middle" >90.11</td></tr><tr><td align="center" valign="middle" >ATT-Bi-LSTM</td><td align="center" valign="middle" >95.65</td><td align="center" valign="middle" >93.62</td><td align="center" valign="middle" >94.62</td></tr></tbody></table></table-wrap><p>表5. 微博情感数据集实验结果</p><p>通过对比发现引入Attention机制能提高对于局部信息的提取，模型的精准率、召回率、F1值分别提升了4.54%、4.49%、4.51%。</p></sec></sec><sec id="s9"><title>5. 结束语</title><p>本文在微博评论情感分类中对Bi-LSTM引入attention机制，发现引入attention机制对情感分类的效果要优于Bi-LSTM，但训练时间比Bi-LSTM时间长。后续对词典无用低频词进行删除，完善词典的准确度，进一步提高情感分类的效率。</p></sec><sec id="s10"><title>基金项目</title><p>北京市教委科技计划项目(KM202011232022)；北京信息科技大学2019年度“实培计划”项目；国家自然科学基金(U1936119，61941116)；国家重点研发计划课题(Grant No. 2019QY(Y)0602)。</p></sec><sec id="s11"><title>文章引用</title><p>王 彬,蒋鸿玲,吴 槟. 基于Attention-Bi-LSTM的微博评论情感分析研究Attention-Bi-LSTM Based Analysis of Weibo Comments[J]. 计算机科学与应用, 2020, 10(12): 2380-2387. https://doi.org/10.12677/CSA.2020.1012252</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.39518-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">彭丹蕾, 谷利泽, 孙斌. 基于SVM和LSTM两种模型的商品评论情感分析研究[J]. 软件, 2019, 40(1): 41-45.</mixed-citation></ref><ref id="hanspub.39518-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Pal, S., Ghosh, S. and Nag, A. (2018) Sentiment Analysis in the Light of LSTM Recurrent Neural Net-works. International Journal of Synthetic Emotions, 9, 33-39. &lt;br&gt;https://doi.org/10.4018/IJSE.2018010103</mixed-citation></ref><ref id="hanspub.39518-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Lu-ong, M.T., Pham, H. and Manning, C.D. (2015) Effective Approaches to Attention-Based Neural Machine Translation. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, September 2015, 1412-1421. &lt;br&gt;https://doi.org/10.18653/v1/D15-1166</mixed-citation></ref><ref id="hanspub.39518-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Zhou, P., Shi, W., Tian, J., et al. (2016) Attention-Based Bidirectional Long Short-Term Memory Networks for Relation, Classification. Meeting of the Association for Computa-tional Linguistics. &lt;br&gt;https://doi.org/10.18653/v1/P16-2034</mixed-citation></ref><ref id="hanspub.39518-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Understanding LSTM Networks. (2015) &lt;br&gt;https://colah.github.io/posts/2015-08-Understanding-LSTMs</mixed-citation></ref><ref id="hanspub.39518-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">杨煜, 张炜. TensorFlow平台上基于LSTM神经网络的人体动作分类[J]. 智能计算机与应用, 2017, 7(5): 41-45.</mixed-citation></ref><ref id="hanspub.39518-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">赵容梅, 熊熙, 琚生根, 等. 基于混合神经网络的中文隐式情感分析[J]. 四川大学学报(自然科学版), 2020, 57(2): 264-270.</mixed-citation></ref></ref-list></back></article>