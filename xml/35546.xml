<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.105095</article-id><article-id pub-id-type="publisher-id">CSA-35546</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200500000_12966477.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于知识蒸馏的实时动作预测方法研究
  Action Prediction Research Based on Knowledge Distillation
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>祥</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>中国海洋大学，山东 青岛</addr-line></aff><pub-date pub-type="epub"><day>29</day><month>04</month><year>2020</year></pub-date><volume>10</volume><issue>05</issue><fpage>927</fpage><lpage>934</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   动作预测是一类特殊的动作识别问题，不同于针对完整动作的传统动作识别，动作预测旨在动作尚未完成时尽可能早地识别动作所属的类别，以便对该动作可能造成的影响进行分析，从而实现事故预警、智能陪护、犯罪预警等目标。本文针对实时动作预测问题提出一种应用知识蒸馏技术的多阶段LSTM实时动作预测方法。本文中的动作预测模型为两阶段的LSTM模型，在第一阶段利用全局特征对动作进行分析，第二阶段利用全局特征与动作特征对动作进行分析。为提高动作预测模型的性能，本文利用知识蒸馏技术并设计新型的损失函数提高动作预测模型的性能。UT-Interaction数据集、JHMDB-21数据集以及UCF-101数据集的实验结果表明本文所提出的动作预测方法不但具有良好的动作预测能力，而且能够满足实际应用中的实时性要求。 Action recognition is a hot topic in the domain of computer vision, and it’s widely applied in human-computer interaction, studio entertainment, automatic drive, intelligent video surveillance, and intelligent medical care. Action prediction is a special class of action recognition. Different from conventional action recognition which aims at recognizing complete actions, the purpose of action prediction is to distinguish an action before it’s fully executed so that some objectives, such as accident early warning and crime prevention, can be achieved by analyzing the possible impact of the action. In order to solve the problem of real-time action prediction, this paper develops a multi-stage LSTM architecture that leverages knowledge distillation technique. The context-aware fea-ture and action-aware feature are exploited for action modeling. The proposed multi-stage LSTM architecture is composed of two stages. In the first stage it focuses on the global, context-aware information. The second stage then combines these context-aware features with action-aware ones. In order to improve the performance of proposed method in the early stage, the knowledge distillation technique is exploited for transferring the knowledge from teacher model to student model. A novel loss function is designed for the whole action prediction architecture and the performance is improved with the novel loss function. Experimental results on the UT-Interaction dataset, JHMDB-21 dataset and the UCF-101 dataset show that the proposed methods not only improve the accuracy of action prediction but also have the ability of real-time running. 
  
 
</p></abstract><kwd-group><kwd>动作识别，动作预测，跌倒预测，知识蒸馏, Action Recognition</kwd><kwd> Action Prediction</kwd><kwd> Fall Prediction</kwd><kwd> Knowledge Distillation</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于知识蒸馏的实时动作预测方法研究<sup> </sup></title><p>王祥</p><p>中国海洋大学，山东 青岛</p><p>收稿日期：2020年4月23日；录用日期：2020年5月7日；发布日期：2020年5月14日</p><disp-formula id="hanspub.35546-formula22"><graphic xlink:href="//html.hanspub.org/file/12-1541764x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>动作预测是一类特殊的动作识别问题，不同于针对完整动作的传统动作识别，动作预测旨在动作尚未完成时尽可能早地识别动作所属的类别，以便对该动作可能造成的影响进行分析，从而实现事故预警、智能陪护、犯罪预警等目标。本文针对实时动作预测问题提出一种应用知识蒸馏技术的多阶段LSTM实时动作预测方法。本文中的动作预测模型为两阶段的LSTM模型，在第一阶段利用全局特征对动作进行分析，第二阶段利用全局特征与动作特征对动作进行分析。为提高动作预测模型的性能，本文利用知识蒸馏技术并设计新型的损失函数提高动作预测模型的性能。UT-Interaction数据集、JHMDB-21数据集以及UCF-101数据集的实验结果表明本文所提出的动作预测方法不但具有良好的动作预测能力，而且能够满足实际应用中的实时性要求。</p><p>关键词 :动作识别，动作预测，跌倒预测，知识蒸馏</p><disp-formula id="hanspub.35546-formula23"><graphic xlink:href="//html.hanspub.org/file/12-1541764x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/12-1541764x7_hanspub.png" /> <img src="//html.hanspub.org/file/12-1541764x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>随着人工智能技术的快速发展，作为计算机视觉领域热门研究方向的动作预测技术在自动驾驶、智能视频监控、人机交互、智能医疗看护等多个领域具有十分广阔的应用前景。</p><p>动作预测是指实时对输入的视频序列进行分析处理，从而在该视频中所包含的动作尚未执行完之前尽可能早地对其动作类别进行识别。因此动作预测是对正在进行中的动作进行识别，属于一类特殊的动作识别技术。传统动作识别技术是对视频序列中已经完成的动作进行识别，而动作预测技术则是对视频序列中正在发生的动作进行识别。因此，动作预测技术与传统动作识别技术的不同之处在于视频序列中动作的完整性。在实际场景中，某些动作在动作早期在外观上具有相似性，例如“拥抱”和“握手”这两个动作在动作开始时都存在手臂前伸的举动，动作外观的相似性导致从部分视频序列中分析提取的特征是相似的，两个动作特征向量之间的距离较小，使预测模型无法有效对上述两个动作进行识别，增加了预测问题的难度。在对视频序列的观测结束之前，预测算法无法获取动作执行完毕所需要的时间，不能够确定动作的完成程度，无法通过动作持续时间的不同对动作进行识别。因此，从已经观测的部分视频序列中所提取的动作特征往往既不能提供用于识别这些动作的关键信息，也不能直接用于获取完整动作的时序结构。因此，与动作识别技术相比，动作预测技术具有关键动作特征信息缺乏和完整时序动作结构未知这两个特点，因此动作预测技术与动作识别技术相比更加具有挑战性。</p><p>为解决动作预测中动作早期可利用关键信息较少的难点，本文设计了基于知识蒸馏的多阶段LSTM动作预测模型，第一阶段考虑全局特征，第二阶段综合考虑全局特征与动作特征，充分利用上述两种特征对动作进行预测。为增加可用的关键信息，利用知识蒸馏技术将知识迁移到动作预测模型中，从而增强动作预测模型的性能。为充分发挥所设计架构的性能，设计了合适的损失函数以提高模型的预测性能。</p></sec><sec id="s4"><title>2. 相关技术</title><sec id="s4_1"><title>2.1. 动作预测技术</title><p>与非实时动作预测技术不同，实时动作预测技术需要在任意时刻给出动作所属的标签，因此实时动作预测任务比非实时动作预测任务难度更高。在实时动作预测方法中，LSTM模型能够输出任意时刻的动作概率，因此通常作为实时动作预测领域中的基础模型。Ma等人 [<xref ref-type="bibr" rid="hanspub.35546-ref1">1</xref>] 利用LSTM模型对时序动作进行建模，该模型利用卷积神经网络对每一帧图像进行特征提取，所提取的特征经过一个全连接层被送入到LSTM网络中，并输出该时刻图像中内容属于各个动作类别的概率。该方法中为了使所提出的方法能够更好的胜任动作预测任务，设计了一种名为Ranking的损失函数，将判断为正确类别的概率随时间的增长而增长这一先验知识引入到损失函数中，利用所设计的损失函数，提高了LSTM模型的预测性能。Aliakbarian等人 [<xref ref-type="bibr" rid="hanspub.35546-ref2">2</xref>] 同样采用LSTM模型对动作进行建模，在他们的方法中利用两阶段的LSTM模型对动作进行建模分析，第一阶段使用环境特征对动作进行建模，第二阶段使用CAM动作特征以及第一阶段的输出对动作建模。在他们的方法中同样设计了一种新型损失函数，该损失函数能够提高LSTM模型在动作初期的识别能力。</p></sec><sec id="s4_2"><title>2.2. 知识蒸馏</title><p>知识蒸馏(Knowledge Distillation)是将复杂模型(Teacher)中的知识迁移到简单模型(Student)中。其中，Teacher模型具有强大的能力而Student模型则更为紧凑。通过知识蒸馏，希望Student尽可能逼近抑或是超过Teacher，从而用更少的复杂度的模型获得更好的预测效果。知识蒸馏概念最早由Hinton [<xref ref-type="bibr" rid="hanspub.35546-ref3">3</xref>] 提出，通过引入Teacher的软目标以诱导学生网络的训练。Romero等 [<xref ref-type="bibr" rid="hanspub.35546-ref4">4</xref>] 提出最小化Teacher模型与Student模型输出之间的均方误差来达到知识迁移的目的。Yim等 [<xref ref-type="bibr" rid="hanspub.35546-ref5">5</xref>] 借助于Gram矩阵损失函数实现知识蒸馏，提高模型图像识别的能力，教师模型由32层网络组成，学生模型由12层网络组组成，利用Gram矩阵实现知识的迁移，将知识从教师模型迁移到学生模型中。Li等 [<xref ref-type="bibr" rid="hanspub.35546-ref6">6</xref>] 人证明了最下化Gram矩阵损失函数等价于最小化MMD损失函数 [<xref ref-type="bibr" rid="hanspub.35546-ref7">7</xref>]。在本文中同样借鉴知识蒸馏的思想，将从完整性视频中学习到的知识迁移到动作预测模型中。</p><p>图1. 实时动作预测网络架构图</p></sec></sec><sec id="s5"><title>3. 基于知识蒸馏的动作预测模型</title><sec id="s5_1"><title>3.1. 网络架构</title><p>基于知识蒸馏的多阶段LSTM动作预测网络架构如图1所示，与现有的模型 [<xref ref-type="bibr" rid="hanspub.35546-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.35546-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.35546-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.35546-ref11">11</xref>] 相比，本文方法无需将视频预先进行分割。该模型对动作的建模分为两阶段。第一阶段，关注动作所处的环境特征，将环境感知网络所提取的全局特征依次输入到模型中，计算关注环境因素的隐藏层特征。第二阶段，将第一阶段计算的环境相关的隐藏层特征与动作特征相融合，综合考虑全局特征与动作相关特征，利用上述两种特征推测动作的类别。</p><p>在每一阶段，网络都由一个教师(Teacher)–学生(Student)学习模块组成，该学习模块用于将知识从教师网络迁移到学生网络，从而增强学生网络的性能。其中，学生网络由一层标准的长短期记忆单元(LSTM) [<xref ref-type="bibr" rid="hanspub.35546-ref12">12</xref>] 组成，对执行到不同程度的动作进行实时预测，尽可能早的预测动作所属的类别。教师网络由一层双向长短期记忆单元(Bi-LSTM) [<xref ref-type="bibr" rid="hanspub.35546-ref13">13</xref>] 组成。</p></sec><sec id="s5_2"><title>3.2. 特征提取</title><p>在本文中，分别基于RGB图像数据以及人体骨骼点坐标数据提取全局特征以及动作特征，并且利用上述两种特征对动作进行建模。下面分别对两种特征的提取进行详细介绍。</p><p>图2. 全局特征提取网络</p><sec id="s5_2_1"><title>3.2.1. 全局特征提取</title><p>如图2所示，本文中采用在ImageNet数据集上经过预训练的VGG-16网络 [<xref ref-type="bibr" rid="hanspub.35546-ref14">14</xref>] 进行全局特征提取。该网络由五个卷积模块、五个池化层以及三个全连接层组成，各层的具体参数如图2所示。本文全局特征提取网络在VGG-16网络的基础上将VGG-16网络的fc8全连接层的神经元个数改为具体数据集中的动作类别数，以fc7全连接层的输出作为全局特征，由图2可知，全局特征的维度为4096。</p></sec><sec id="s5_2_2"><title>3.2.2. 动作特征提取</title><p>在本文中，采用OpenPose [<xref ref-type="bibr" rid="hanspub.35546-ref15">15</xref>] 提取人体骨骼点坐标。基于骨骼数据的动作特征提取网络如图3所示。首先利用OpenPose对每一帧中的人物进行骨骼点坐标提取，然后将每一帧表示为一个三维张量，张量的维度为T &#215; N &#215; D，其中第一个维度代表帧数(本文中T = 1)，第二个维度代表骨骼数(本文中N = 18)，第三个维度代表骨骼点的坐标维度(本文中D = 2)。动作特征提取分为三个阶段，第一阶段为点特征提取，第二阶段为全局特征提取，第三阶段为时序依赖特征提取。在第一阶段，输入张量首先经过两次1 &#215; 1 (conv1, conv2)卷积操作，会强迫网络学习单个骨骼点坐标之间的相互关系，进而得到点级别的特征表示。随后，将特征图按参数(0, 2, 1)进行转置操作，将各个骨骼点从第二个维度转换到第三个维度。在第二阶段，使网络学习到各个骨骼点之间的相互关系，得到动作的全局特征。在第三阶段，将第二阶段输出的特征图展成一个256维的向量，将该向量输入到LSTM单元中，得到动作在时间维度上的依赖关系。最后LSTM隐藏层的输出经过一个全连接层完成最终的动作识别任务。网络训练完成后，将LSTM单元隐藏层的输出作为动作特征，其中LSTM隐藏层的维度为256。</p><p>图3. 动作特征提取网络</p></sec></sec><sec id="s5_3"><title>3.3. 损失函数</title><p>在动作预测领域，一个精巧设计的损失函数可以提高动作预测模型的性能。借鉴 [<xref ref-type="bibr" rid="hanspub.35546-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.35546-ref11">11</xref>] 中的损失函数设计思想，本文针对所提出的包含知识蒸馏的多阶段LSTM动作预测模型设计了一个高效的损失函数，如公式(1)所示。</p><p>L ( y , y ^ ) = L C ( y , y ^ ) + L T S ( S , T ) (1)</p><p>本文中的动作预测模型分为两个阶段，模型每个阶段的损失函数都如公式(1)所示，但每个阶段损失函数的参数不同。公式(1)中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/12-1541764x13_hanspub.png" xlink:type="simple"/></inline-formula>表示模型的预测损失，y表示样本的标签， y ^ 表示模型的预测值， L T S 表示教师网络与学生网络的知识蒸馏损失，S表示学生网络计算的各个时刻的特征，T教师网络计算的各个时刻的特征。下面对 L C 以及 L T S 做具体的介绍。</p><p>L C ( y , y ^ ) = − 1 N ∑ k = 1 N ∑ t = 1 T [ y t ( k ) log ( y ^ t ( k ) ) + t ( 1 − y t ( k ) ) T log ( 1 − y ^ t ( k ) ) ] (2)</p><p>公式(2)中， y t ( k ) 表示在t时刻动作的标签，例如 y t ( k ) = 1 表示样本属于类别k， y t ( k ) = 0 表示样本不属于类别k。 y ^ t ( k ) 表示模型对样本的预测标签，N表示动作类别综述，T表示输入序列的长度(帧数)。</p><p>L T S ( S , T ) = α L M S E ( S , T ) = α ‖ S − T ‖ F 2 (3)</p><p>公式(3)为知识蒸馏损失函数，该损失函数用于指导学生网络的学习，使学生网络输出的隐藏层特征尽可能逼近或等于教师网络输出的隐藏层特征。公式3中， S 为学生网络在所有时刻输出的隐藏层特征， T 为教师网络在所有时刻输出的隐藏层特征， α 表示该损失函数的影响因子。</p></sec></sec><sec id="s6"><title>4. 实验结果</title><p>在本节中，给出在UT-Interaction数据集、JHMDB-21数据集以及UCF-101数据集上本文提出方法与现有相关方法的比较结果，采用折线图以及表格的方式对观测比率为0.1、0.2、0.3、0.4、0.5、0.6、0.7、0.8、0.9、1.0时各方法的识别准确率进行展示。</p><sec id="s6_1"><title>4.1. JHMDB-21数据集</title><p>在JHMDB-21数据集上，本文方法与DP-SVM [<xref ref-type="bibr" rid="hanspub.35546-ref16">16</xref>]、S-SVM [<xref ref-type="bibr" rid="hanspub.35546-ref16">16</xref>]、Where/What [<xref ref-type="bibr" rid="hanspub.35546-ref17">17</xref>]、MS [<xref ref-type="bibr" rid="hanspub.35546-ref2">2</xref>] 等方法的比较结果如图4所示。由图4可知，本文所提出方法的识别准确率在所有观测比率上都远远优于所比较的方法。</p><p>图4. JHMDB-21数据集实验对比结果</p></sec><sec id="s6_2"><title>4.2. UT-Interaction数据集</title><p>在UT-Interaction数据集上，本文方法与IBoW [<xref ref-type="bibr" rid="hanspub.35546-ref16">16</xref>]、DBoW [<xref ref-type="bibr" rid="hanspub.35546-ref16">16</xref>]、SC [<xref ref-type="bibr" rid="hanspub.35546-ref18">18</xref>]、MSSC [<xref ref-type="bibr" rid="hanspub.35546-ref18">18</xref>]、MMAPM [<xref ref-type="bibr" rid="hanspub.35546-ref9">9</xref>] 的比较结果如图5所示。由图5可知，本文方法在所有的观测比率上均超过所比较的方法。</p></sec><sec id="s6_3"><title>4.3. UCF-101数据集</title><p>UCF-101数据集是十分具有挑战性的数据集，在UCF-101数据集上，本文方法与IBoW [<xref ref-type="bibr" rid="hanspub.35546-ref16">16</xref>]、DBoW [<xref ref-type="bibr" rid="hanspub.35546-ref16">16</xref>]、DeepSCN [<xref ref-type="bibr" rid="hanspub.35546-ref8">8</xref>]、Mem-LSTM [<xref ref-type="bibr" rid="hanspub.35546-ref10">10</xref>]、MTSSVM [<xref ref-type="bibr" rid="hanspub.35546-ref9">9</xref>]、MSSC [<xref ref-type="bibr" rid="hanspub.35546-ref18">18</xref>]、MS [<xref ref-type="bibr" rid="hanspub.35546-ref2">2</xref>]、PTSL [<xref ref-type="bibr" rid="hanspub.35546-ref11">11</xref>] 的比较结果如图6所示。由图6可知，除PTSL方法外，本文方法在所有观测比率上皆超过其他几种方法。</p><p>图5. UT-Interaction数据集实验对比结果</p><p>图6. UT-Interaction数据集实验对比结果</p></sec></sec><sec id="s7"><title>5. 总结与展望</title><p>本文针对实时动作预测问题提出一种应用知识蒸馏技术的多阶段LSTM实时动作预测方法。本文中的动作预测模型为两阶段的LSTM模型，在第一阶段利用全局特征对动作进行分析，第二阶段利用全局特征与动作特征对动作进行分析。为提高动作预测模型的性能，本文利用知识蒸馏技术将教师模型从完整视频序列中所学到的知识迁移到学生模型中，提高学生模型的性能。为更好地发挥本文所设计模型的性能，本文针对所设计的模型架构设计了适用于本文模型架构的损失函数，利用该损失函数可以使模型得到更好的训练效果。实验结果表明本文所提出的动作预测方法不但具有良好的动作预测能力，而且能够满足实际应用中的实时性要求。</p><p>虽然本文在实时动作预测方面提出了较为不错的方法，但在对实时动作预测的研究工作中仍然存在一些不足之处有待进一步完善，本文中的动作特征借助于人体骨骼点数据，利用OpenPose从图像中提取人体骨骼点数据，但在某些情况下图像中的人物是被遮挡的以及图像中人物较小，因此在上述情况下无法提取人体动作特征，从而导致方法的准确性降低。因此可以在本文工作的基础上，从图像中提取动作特征，然后将两种动作特征融合组成新的动作特征，从而解决无法提取骨骼点坐标问题。</p></sec><sec id="s8"><title>文章引用</title><p>王 祥. 基于知识蒸馏的实时动作预测方法研究Action Prediction Research Based on Knowledge Distillation[J]. 计算机科学与应用, 2020, 10(05): 927-934. https://doi.org/10.12677/CSA.2020.105095</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.35546-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Ma, S., Sigal, L. and Sclaroff, S. (2016) Learning Activity Progression in LSTMs for Activity Detection and Early De-tection. IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 1942-1950.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.214</mixed-citation></ref><ref id="hanspub.35546-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Aliakbarian, M.S., Saleh, F.S., Salzmann, M., Fernando, B., Peters-son, L. and Andersson, L. (2017) Encouraging LSTMs to Anticipate Actions Very Early. Proc. IEEE International Con-ference on Computer Vision, Venice, 22-29 October 2017, 280-289. &lt;br&gt;https://doi.org/10.1109/ICCV.2017.39</mixed-citation></ref><ref id="hanspub.35546-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Hinton, G., Vinyals, O. and Jeff, D. (2015) Distilling the Knowledge in a Neural Network. NIPS 2014 Deep Learning Workshop, Montreal, 8-13 December 2014, 1546-1552.</mixed-citation></ref><ref id="hanspub.35546-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Adriana, R., Gatta, C. and Bengio, Y. (2015) FitNets: Hints for Thin Deep Nets. 3rd International Conference on Learning Rep-resentations, San Diego, 7-9 May 2015, 1-13.</mixed-citation></ref><ref id="hanspub.35546-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Yim, J. (2017) A Gift from Knowledge Distillation: Fast Optimiza-tion, Network Minimization and Transfer Learning. IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 4133-4141.</mixed-citation></ref><ref id="hanspub.35546-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Li, Y. and Wang, N. (2017) Demystifying Neural Style Transfer. Pro-ceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, Melbourne, 19-25 August 2017, 2230-2236.</mixed-citation></ref><ref id="hanspub.35546-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Gretton, A., et al. (2012) A Kernel Two-Sample Test. Journal of Machine Learning Research, 13, 723-773.</mixed-citation></ref><ref id="hanspub.35546-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Kong, Y., Tao, Z. and Fu, Y. (2017) Deep Sequential Context Networks for Action Prediction. IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 3662-3670.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.390</mixed-citation></ref><ref id="hanspub.35546-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Kong, Y., Kit, D. and Fu, Y. (2014) A Discriminative Model with Multiple Temporal Scales for Action Prediction. 13th European Conference, Zurich, 6-12 September 2014, 596-611. &lt;br&gt;https://doi.org/10.1007/978-3-319-10602-1_39</mixed-citation></ref><ref id="hanspub.35546-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Kong, Y., Gao, S., Sun, B. and Fu, Y. (2018) Action Predic-tion from Videos via Memorizing Hard-to-Predict Samples. Proc. 32nd AAAI Conference on Artificial Intelligence, New Orleans, 2-7 February 2018, 7000-7007.</mixed-citation></ref><ref id="hanspub.35546-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Wang, X., Hu, J., Lai, J., Zhang, J. and Zheng, W. (2019) Progressive Teacher-Student Learning for Early Action Prediction. IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, 16-21 June 2019, 3556-3565.</mixed-citation></ref><ref id="hanspub.35546-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Hochreiter, S. and Schmidhuber, J. (1997) Long Short-Term Memory. Neural Computation, 9, 1-32.  
&lt;br&gt;https://doi.org/10.1162/neco.1997.9.1.1</mixed-citation></ref><ref id="hanspub.35546-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Graves, A. and Schmidhuber, J. (2005) Framewise Phoneme Classi-fication with Bidirectional LSTM Networks. IEEE International Joint Conference on Neural Networks, Montreal, 31 Ju-ly-4 August 2005, 846-853.</mixed-citation></ref><ref id="hanspub.35546-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan, K. and Zisserman, A. (2014) Very Deep Convolutional Networks for Large-Scale Image Recognition. 3rd International Conference on Learning Representations, San Diego, 7-9 May 2015, 1-14.</mixed-citation></ref><ref id="hanspub.35546-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Cao, Z., Simon, T., Wei, S.E. and Sheikh, Y. (2017) Real-Time Multi-Person 2D Pose Estimation Using Part Affinity Fields. 30th IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2016, 1302-1310.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.143</mixed-citation></ref><ref id="hanspub.35546-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Ryoo, M.S. (2011) Human Activity Prediction: Early Recognition of Ongoing Activities from Streaming Videos. 13th International Conference on Computer Vision, Barcelona, 6-13 No-vember 2011, 1036-1043.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2011.6126349</mixed-citation></ref><ref id="hanspub.35546-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Soomro, K., Idrees, H. and Shah, M. (2016) Predicting the Where and What of Actors and Actions through Online Action Localization. The IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 2648-2657. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.290</mixed-citation></ref><ref id="hanspub.35546-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Cao, Y., Barrett, D., Barbu, A., Narayanaswamy, S., Yu, H., Michaux, A., Lin, Y., Dickinson, S., Siskind, J.M. and Wang, S. (2013) Recognize Human Activities from Partially Ob-served Videos. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Vol. 1, 2658-2665. &lt;br&gt;https://doi.org/10.1109/CVPR.2013.343</mixed-citation></ref></ref-list></back></article>