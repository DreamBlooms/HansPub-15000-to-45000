<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AIRR</journal-id><journal-title-group><journal-title>Artificial Intelligence and Robotics Research</journal-title></journal-title-group><issn pub-type="epub">2326-3415</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AIRR.2018.74020</article-id><article-id pub-id-type="publisher-id">AIRR-27389</article-id><article-categories><subj-group subj-group-type="heading"><subject>AIRR20180400000_62955638.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject><subject> 工程技术</subject></subj-group></article-categories><title-group><article-title>
 
 
  无人机视觉下的输电塔线关键部件实时定位识别
  UAV-Vision Based Real-Time Power Line Components Identification
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>安</surname><given-names>站东</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>黄</surname><given-names>锋</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>阳煤集团，山西 阳泉</addr-line></aff><aff id="aff3"><addr-line>中国能源建设集团，陕西省电力设计院有限公司，陕西 西安</addr-line></aff><aff id="aff2"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>30</day><month>09</month><year>2018</year></pub-date><volume>07</volume><issue>04</issue><fpage>171</fpage><lpage>177</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   传统的高压输电线路巡检由专职检修人员负责，其劳动强度大、危险性高且效率较低。目前，基于无人机视觉的高压塔线系统巡检技术由于其成本低、安全性高、不受空间限制及航程远等优势在电力行业中受到了越来越多的关注和应用。无人机电力塔线系统巡检的首要任务是对输电塔线及其关键部件进行识别定位。目前，机器视觉中常用的目标检测算法以浅层结构模型为主(支持向量机、回归算法模型、传统BP神经元网络等)。而浅层机器学习模型受限于其本身的特征学习能力限制，在复杂背景中的识别效果较为有限。深度学习虽然具有较强的特征学习分类能力，但绝大多数深度学习中需要大量的卷积和微分计算，计算强度大耗时长。目前常用的基于深度学习的目标检测算法如RCNN等虽然在识别准确率上高于浅层算法，但识别过程需要的计算时间长，难以满足实时检测的要求。本文使用了最新的YOLO3深度学习目标检测算法模型对输电塔线系统中的关键部件进行识别取得了良好的效果，同时，由于YOLO3内只使用单个卷积神经元网络核心对视频图像中的目标检测对象进行识别，其效率更高速度更快。通过测试发现，基于YOLO3模型的输电塔线系统关键部件识别平均时间在0.36 ms，完全达到了实时检测的要求。 Traditional power line inspections are mostly conducted manually with experienced workers. It is low efficient, labor intensive, and high-risk. Therefore, UAV vision based techniques have shown a growing interest in high-voltage power line inspection. The fundamental task for power line in-spection is the detection and classification of power components on the power transmission infra-structures. So far, classical machine learning based target recognition and detection algorithms, such as Support Vector Machine (SVM), Regression and other shallow structured machine learning models are difficult to achieve high accuracy. Deep learning models, convolutional neural network (CNN) for example, perform much better in target recognition and become the most preferred al-gorithm in many multi-target recognition scenarios. However, most CNN based target detection methods (RCNN, Faster RCNN, etc.) are computationally expensive. It is difficult to achieve real time detection in UAV-Video. YOLO3 (You Only Look Once V3) is recently developed CNN based target detection model, it has been proved as one of the top target recognition methods in terms of speed and accuracy. In this work, we deployed YOLO3 in power components recognition. The experiments show that the proposed method can be used to identify and locate power components within an average speed of 36 ms which fully achieves the speed requirement for real-time inspection. 
 
</p></abstract><kwd-group><kwd>无人机，深度学习，电力线路部件识别, UAV-Vision</kwd><kwd> Deep Learning</kwd><kwd> Power Components Recognition</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>无人机视觉下的输电塔线关键部件实时定位 识别<sup> </sup></title><p>安站东<sup>1</sup>，黄锋<sup>2</sup></p><p><sup>1</sup>阳煤集团，山西 阳泉</p><p><sup>2</sup>中国能源建设集团，陕西省电力设计院有限公司，陕西 西安</p><p><img src="//html.hanspub.org/file/4-2610135x1_hanspub.png" /></p><p>收稿日期：2018年10月12日；录用日期：2018年10月25日；发布日期：2018年11月1日</p><disp-formula id="hanspub.27389-formula21"><graphic xlink:href="//html.hanspub.org/file/4-2610135x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>传统的高压输电线路巡检由专职检修人员负责，其劳动强度大、危险性高且效率较低。目前，基于无人机视觉的高压塔线系统巡检技术由于其成本低、安全性高、不受空间限制及航程远等优势在电力行业中受到了越来越多的关注和应用。无人机电力塔线系统巡检的首要任务是对输电塔线及其关键部件进行识别定位。目前，机器视觉中常用的目标检测算法以浅层结构模型为主(支持向量机、回归算法模型、传统BP神经元网络等)。而浅层机器学习模型受限于其本身的特征学习能力限制，在复杂背景中的识别效果较为有限。深度学习虽然具有较强的特征学习分类能力，但绝大多数深度学习中需要大量的卷积和微分计算，计算强度大耗时长。目前常用的基于深度学习的目标检测算法如RCNN等虽然在识别准确率上高于浅层算法，但识别过程需要的计算时间长，难以满足实时检测的要求。本文使用了最新的YOLO3深度学习目标检测算法模型对输电塔线系统中的关键部件进行识别取得了良好的效果，同时，由于YOLO3内只使用单个卷积神经元网络核心对视频图像中的目标检测对象进行识别，其效率更高速度更快。通过测试发现，基于YOLO3模型的输电塔线系统关键部件识别平均时间在0.36 ms，完全达到了实时检测的要求。</p><p>关键词 :无人机，深度学习，电力线路部件识别</p><disp-formula id="hanspub.27389-formula22"><graphic xlink:href="//html.hanspub.org/file/4-2610135x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2018 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/4-2610135x7_hanspub.png" /> <img src="//html.hanspub.org/file/4-2610135x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>现行的电力输电塔线系统的巡检主要是由检修人员人工作业完成，劳动强度大、耗时长、危险性较高且效率较低，而无人机电力系统巡检是一种以机器视觉为主的非接触式电力塔线系统检测技术，其主要优势是人力资源成本低、作业危险性小、易于操作、续航时间长、灵活高效等。因此，以无人机视觉技术为主的检测方法目前受到了越来越多的关注和应用。但目前，无人机主要还是负责对飞行路径上的塔线进行拍摄，将存储的图像和视频信息交由专业人员进行识别统计最终得到巡检报告，这种方法虽然提高了效率和安全性，但是还不是完全自主的智能化检测方法。因此，将机器视觉和人工智能技术应用在输电塔线系统无人机检测中，使无人机能够实时的对电力线路的目标部件进行有效识别定位将会极大的提高巡检的智能化水平和工作效率。</p><p>传统机器视觉中的识别定位方法中，基于浅层机器学习算法如SIFT (Scale-Invariant Feature Transform)识别方法 [<xref ref-type="bibr" rid="hanspub.27389-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.27389-ref2">2</xref>]、结合HOG (Histogram of Oriented Gradients)和SVM (Support Vector Machine)等方法 [<xref ref-type="bibr" rid="hanspub.27389-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.27389-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.27389-ref5">5</xref>] 主要基于图像中的边缘或纹理特征，这部分算法在背景环境相对简单，被测物边缘清晰的图像中取得了良好的效果 [<xref ref-type="bibr" rid="hanspub.27389-ref6">6</xref>]。在图像场景识别中，T. Yu和R. Wang提出了利用图像匹配进行场景解析的算法，并在街道视图下取得了良好的效果 [<xref ref-type="bibr" rid="hanspub.27389-ref7">7</xref>]。但无人机所拍摄的电力塔线图像中含有大量变化的环境背景信息，塔线特征在环境背景中对比度低，目前很多算法在这一场景下的识别准确率较低。基于塔线部件轮廓骨架的分割算法 [<xref ref-type="bibr" rid="hanspub.27389-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.27389-ref9">9</xref>] 对阈值参数的依赖较大，在变化的图像背景下，图像分割的鲁棒性难以保证，准确率低。无人机所拍摄的电力系统图像常存在大量不同背景，不同时间段拍摄同一目标时，由于光照不同，图像背景也不尽相同，这种情况下，对参数依赖较大的分割算法很难满足要求。</p><p>近年来，深度学习 [<xref ref-type="bibr" rid="hanspub.27389-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.27389-ref11">11</xref>]，特别是卷积神经元网络模型在图像识别上取得了非常显著的效果，其主要原因在于相对浅层学习模型，具有大量隐含层的深度神经网络在数据的特征学习能力强，其所获得的图像特征更加丰富和全面。基于深度学习的图像检测模型中，以2014年Ross提出的基于区域卷积神经元网络模型(RCNN)最为知名 [<xref ref-type="bibr" rid="hanspub.27389-ref12">12</xref>]。该模型首先对图像进行分割，生成大量候选区域，然后对每个候选区使用卷积神经元网络进行特征提取，最后对符合训练特征的区域内的目标进行回归计算，匹配出最佳位置。在此深度学习检测模型上，又衍生了一系列基于区域提议技术(Region Proposal)的图像目标检测模型，如Fast-RCNN，Faster-RCNN等 [<xref ref-type="bibr" rid="hanspub.27389-ref13">13</xref>]，这些模型通过改进区域提议方法，提高了识别的速度。但总体来说，由于这些模型对硬件运算能力要求非常高，运算耗时较长，因此识别速度慢是其主要缺点 [<xref ref-type="bibr" rid="hanspub.27389-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.27389-ref14">14</xref>] [<xref ref-type="bibr" rid="hanspub.27389-ref15">15</xref>]。因此，为了达到无人机实时检测的目标，本文采用了一种基于YOLO的深度学习图像目标检测算法，其优点在于只使用单个卷积神经元网络模型对图像中不同的目标类型和位置进行预测，大大降低了运算复杂度，运算速度快，实时检测效果较好 [<xref ref-type="bibr" rid="hanspub.27389-ref16">16</xref>]。</p></sec><sec id="s4"><title>2. YOLO深度学习模型</title><p>目标识别主要包括目标位置的确定和目标类别的判别。早期的目标检测与识别方法将目标识别任务分为目标区域预测和目标类别预测等多个流程。YOLO则将目标区域预测和目标类别预测整合于单个神经网络模型中，在较高准确率的情况下快速的完成目标检测与识别，更加适合现场应用环境。YOLO将目标检测任务看作目标区域预测和目标类别预测的回归问题，用整张图像训练模型，直接在图像的多个位置回归出目标的边框和所属的类别，这样可以更好的区分目标和背景区域，实现端到端的目标检测和识别。</p>YOLO-V3深度学习模型<p>华盛顿大学的Joseph Redmon和Ali Farhadi提出的YOLO V3是继YOLO V1和YOLO V2之后的最新版本。2016年，Joseph Redmon等人首次提出了YOLO方法。YOLO V1训练和检测均在一个单独网络中进行，使用全图作为环境信息，背景错误较少且泛化能力强；2017年，Joseph Redmon等人提出了升级版的YOLO V2，相比于YOLO V1提高了物体定位精准性和召回率；2018年提出的最新的YOLO V3模型比之前的模型更加复杂，可以通过改变模型结构的大小来权衡速度与精度。YOLO V3的分类预测抛弃了softmax分类器，对每个边框采用单独的逻辑分类器。YOLO V3的特征提取模型是一个混合的模型，它使用了YOLO V2，Darknet-19以及ResNet，这个模型使用了很多有良好表现的3*3和1*1的卷积层，也在后边增加了一些Shortcut Connection结构，输出层使用卷积层替代全连接层，最终该模型一共有53个卷积层。</p><p>YOLO V3的卷积神经网络(CNN)模型在实现上使用GPU进行并行计算，计算效率明显优于采用单CPU计算，边框回归使目标定位的精确性进一步提升。YOLO V3的训练分为三个步骤：① 将输入图像分成S &#215; S个网格，每个网格负责检测“落入”该网格的物体；② 对每个网格用CNN提取特征，形成高维特征向量，本文采用的CNN是训练好的ImageNet网络；③ 利用边框和提取出的特征对CNN进行调优，调优依据标准的反向传播算法进行，从特征层开始向后调整各层权重；④ 以特征层输出的高维特征向量和目标类别标签为输入，训练对目标边框和目标类别进行精细回归的回归器。本文主要研究了利用YOLO V3方法对电力部件进行识别。</p></sec><sec id="s5"><title>3. 基于YOLO V3方法的电力部件识别定位</title><p>YOLO V3在基本保证准确度的前提下，速度大幅提升，背景误检率低，通用性强。因此本文以YOLO V3识别方法为主，提取电力部件的识别特征并进行目标识别验证。</p>电力部件检测识别过程<p>YOLO V3方法中的目标检测识别过程如图1所示。其主要实现步骤为：① 分割图像，边界框的预测和分类。YOLO V3将输入图像分成S &#215; S个格子，每个格子在三个不同的尺度预测若干个边界框及其置信度，以及若干物体属于某种类别的概率信息。边界框信息为物体的中心位置相对于格子位置的偏移量及宽度和高度。置信度反映是否包含物体以及包含物体情况下位置的准确性。YOLO V3对每个边界框通过逻辑回归预测一个物体的得分，如果预测的这个边界框与真实的边框值大部分重合且比其他所有预测的要好，那么这个值就为1。如果重合度没有达到一个阈值(YOLO V3中设定的阈值是0.5)，那么这个预测的边界框将会被忽略，也就是会显示成没有损失值。② CNN提取网格中的特征。CNN对整个图像进行系列卷积运算，得到特征图。取出特征图上每个边框内的特征形成高维特征向量。③ 图像中物体的识别分类。每个网格预测多个边界框，根据计算得到的分类误差，置信度以及类别概率等判别物体。</p><p>图1. 检测识别过程</p></sec><sec id="s6"><title>4. 实验结果与分析</title><p>无人机拍摄影像具有分辨率较高、包含目标较小的特点，拍摄影像的角度具有多样性和随机性。本次实验平台为Linux系统，训练使用的硬件为NVIDIA GeForce GTX 1070 Ti显卡。本文识别四类电力部件——输电塔、防震锤、绝缘子和均压环。</p><sec id="s6_1"><title>4.1. 基于无人机航拍视频的YOLO V3训练和测试图像数据集</title><p>本次实验数据集来源于无人机巡检图像。训练集共有2000张图像，其中输电塔、防震锤、绝缘子和均压环各500张。测试集共1000张图像，输电塔、防震锤、绝缘子和均压环各250张。为了提高准确率，训练集中每张图像都清晰展现电力部件。图2随机展示了数据集中的八张图像。本次实验用正确率和召回率来评判识别的准确性，其中正确率为目标类别标记正确的外围框个数除以所有标记出的外围框个数；召回率为目标类别标记正确的外围框个数除以所有该类别目标的外围框个数。</p><p>图2. 数据集图像样本</p></sec><sec id="s6_2"><title>4.2. 软件界面</title><p>本工作实现的软件界面基于QT模块化用户UI设计，后台使用C++语言实现，软件通过对无人机视频实时抓取，将视频采样并通过YOLO V3模型库接口输送给识别算法库进行目标检测运算(图3)。</p><p>图3. 软件界面</p></sec><sec id="s6_3"><title>4.3. 实验结果</title><sec id="s6_3_1"><title>4.3.1. YOLO V3模型输电塔线系统识别结果图示</title><p>YOLO V3模型输电塔线系统识别结果图示如图4。</p><p>图4. 部分模型输电塔线系统识别结果</p></sec><sec id="s6_3_2"><title>4.3.2. 实验统计信息</title><p>我们使用YOLO3模型对测试集图像进行测试，结果如下表(表1)所示，针对塔线系统的四个关键部分的识别正确率较高，对不同目标的平均运算速度为32.9 ms。同时，我们对比了几种不同常用模型的目标识别速度(以帧每秒FPS单位为标准)，结果如表2所示，可以看到，YOLO V3模型的处理速度要远远好于常用模型，30.4帧每秒的速度完全达到了对视频进行实时目标识别定位的要求。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Performance on test dataset and the average processing spee</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Performance</th><th align="center" valign="middle" >输电塔</th><th align="center" valign="middle" >防震锤</th><th align="center" valign="middle" >绝缘子</th><th align="center" valign="middle" >均压环</th><th align="center" valign="middle" >平均值</th></tr></thead><tr><td align="center" valign="middle" >正确率(Precision)</td><td align="center" valign="middle" >89.7%</td><td align="center" valign="middle" >92.4%</td><td align="center" valign="middle" >90.2%</td><td align="center" valign="middle" >88.3%</td><td align="center" valign="middle" >90.2%</td></tr><tr><td align="center" valign="middle" >召回率(Recall)</td><td align="center" valign="middle" >87.5%</td><td align="center" valign="middle" >88.6%</td><td align="center" valign="middle" >84.5%</td><td align="center" valign="middle" >80.3%</td><td align="center" valign="middle" >85.2%</td></tr><tr><td align="center" valign="middle" >每种目标平均识别速度</td><td align="center" valign="middle" >33.1 ms</td><td align="center" valign="middle" >31.4 ms</td><td align="center" valign="middle" >32.7 ms</td><td align="center" valign="middle" >34.3 ms</td><td align="center" valign="middle" >32.9 ms</td></tr></tbody></table></table-wrap><p>表1. 测试集上识别结果及</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of processing speed of the most used target detection models on the same datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型(Model)</th><th align="center" valign="middle" >DPM</th><th align="center" valign="middle" >Fast RCNN</th><th align="center" valign="middle" >Faster RCNN</th><th align="center" valign="middle" >Yolo V3</th></tr></thead><tr><td align="center" valign="middle" >平均处理速度(帧每秒FPS)</td><td align="center" valign="middle" >0.61</td><td align="center" valign="middle" >0.55</td><td align="center" valign="middle" >7.5</td><td align="center" valign="middle" >30.4</td></tr></tbody></table></table-wrap><p>表2. 在本塔线系统视频图像测试数据集上常用目标检测算法的平均处理速度对比</p></sec></sec></sec><sec id="s7"><title>5. 结论</title><p>本文验证了基于YOLO V3的深度学习目标检测模型对电力塔线系统关键部件的识别和定位的可行性。实验表明，YOLO深度学习目标检测模型的识别效果显著，准确率和召回率都达到理想水平，特别需要指出的是，YOLO模型的目标检测速度要远远好于常用的几种模型，平均速度是30.4帧每秒(FPS)，完全可以达到无人机实时监测的要求。</p></sec><sec id="s8"><title>文章引用</title><p>安站东,黄 锋. 无人机视觉下的输电塔线关键部件实时定位识别 UAV-Vision Based Real-Time Power Line Components Identification[J]. 人工智能与机器人研究, 2018, 07(04): 171-177. https://doi.org/10.12677/AIRR.2018.74020</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.27389-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Mortensen, E.N., Deng, H. and Shapiro, L. (2005) A SIFT Descriptor with Global Context. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Diego, 20-25 June 2005, 184-190.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2005.45</mixed-citation></ref><ref id="hanspub.27389-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">周丽芬. 基于SIFT的图像匹配算法[J]. 计算机与现代化, 2014(7): 63-67.</mixed-citation></ref><ref id="hanspub.27389-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">孙锐, 陈军, 高隽. 基于显著性检测与HOG-NMF特征的快速行人检测方法[J]. 电子与信息学报, 2013, 35(8): 1921-1926.</mixed-citation></ref><ref id="hanspub.27389-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">黄茜, 顾杰峰, 杨文亮. 基于梯度向量直方图的行人检测[J]. 科学技术与工程, 2009, 9(13): 3646-3651.</mixed-citation></ref><ref id="hanspub.27389-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Zeng, C., Ma, H. and Ming, A. (2010) Fast Human Detection Using mi-sVM and a Cascade of HOG-LBP Features. IEEE International Conference on Image Processing, Hong Kong, 26-29 September 2010, 3845-3848.  
&lt;br&gt;https://doi.org/10.1109/ICIP.2010.5654100</mixed-citation></ref><ref id="hanspub.27389-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">徐渊, 许晓亮, 李才年, 姜梅, 张建国. 结合svm分类器与hog特征提取的行人检测[J]. 计算机工程, 2016, 42(1), 56-60.</mixed-citation></ref><ref id="hanspub.27389-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Yu, T. and Wang, R. (2016) Scene Parsing Using Graph Matching on Street-View Data. Computer Vision and Image Understanding, 145, 70-80. &lt;br&gt;https://doi.org/10.1016/j.cviu.2016.01.004</mixed-citation></ref><ref id="hanspub.27389-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">金立军, 胡娟, 闫书佳. 基于图像的高压输电线间隔棒故障诊断方法[J]. 高电压技术, 2013, 39(5), 1040-1045.</mixed-citation></ref><ref id="hanspub.27389-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">曹婧. 航拍输电线路图像中绝缘子部件的提取[D]: [硕士学位论文]. 大连: 大连海事大学, 2012.</mixed-citation></ref><ref id="hanspub.27389-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Lecun, Y., Bengio, Y. and Hinton, G. (2015) Deep Learning. Nature, 521, 436. &lt;br&gt;https://doi.org/10.1038/nature14539</mixed-citation></ref><ref id="hanspub.27389-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">卢宏涛, 张秦川. 深度卷积神经网络在计算机视觉中的应用研究综述[J]. 数据采集与处理, 2016, 31(1), 1-17.</mixed-citation></ref><ref id="hanspub.27389-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Girshick, R., Donahue, J., Darrell, T. and Malik, J. (2016) Region-Based Convolutional Networks for Accurate Object Detection and Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 142-158.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2015.2437384</mixed-citation></ref><ref id="hanspub.27389-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Girshick, R. (2015) Fast R-CNN. IEEE International Conference on Com-puter Vision, Santiago, 7-13 December 2015, 1440-1448. &lt;br&gt;https://doi.org/10.1109/ICCV.2015.169</mixed-citation></ref><ref id="hanspub.27389-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Sempau. J., Wilderman. S.J. and Bielajew, A.F. (2000) DPM, a Fast, Accurate Monte Carlo Code Optimized for Photon and Electron Radiotherapy Treatment Planning Dose Calculations. Physics in Medicine &amp; Biology, 45, 2263-2291.  
&lt;br&gt;https://doi.org/10.1088/0031-9155/45/8/315</mixed-citation></ref><ref id="hanspub.27389-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Ren, S., He, K., Girshick, R., et al. (2015) Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 1137-1149.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2016.2577031</mixed-citation></ref><ref id="hanspub.27389-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J., Divvala, S., Girshick, R., et al. (2016) You Only Look Once: Unified, Real-Time Object Detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 779-788.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.91</mixed-citation></ref></ref-list></back></article>