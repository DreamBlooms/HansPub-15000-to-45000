<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114091</article-id><article-id pub-id-type="publisher-id">CSA-41705</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_43977121.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于迁移学习的端到端发音检错研究
  Research on End-to-End Pronunciation Error Detection Based on Transfer Learning
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>高</surname><given-names>文明</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>吴</surname><given-names>怡之</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>魏</surname><given-names>新享</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>东华大学，上海</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>885</fpage><lpage>891</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   自动发音检错是为了满足第二语言学习者发音练习的需求，而先进的自动发音检错系统通常取决于声学模型识别率。随着深度学习技术的发展，端到端声学模型算法已经逐渐成熟，为发音检错算法研究提供的新思路。本文首先构建了基于连接时序分类(Connectionist Temporal Classification, CTC)算法的端到端发音检错声学模型架构。其次，基于二语迁移现象，L2发音往往带有其母语的音素特征，本文利用迁移学习算法提高基于母语的声学模型性能，从而提高发音检错准确率。通过迁移中文母语音素特征的声学模型相比于只使用英文母语的声学模型在错误音素率上有所下降，并且训练时间减少了7.3%。在发音检错性能上检错正确率提升了2.06%。 Automatic pronunciation error detection is to meet the needs of second language learners’ pronunciation practice, and advanced automatic pronunciation error detection systems usually depend on the recognition rate of the acoustic model. With the development of deep learning technology, end-to-end acoustic model algorithms have gradually matured, providing new ideas for the research of pronunciation error detection algorithms. This paper first builds an end-to-end pronunciation error detection acoustic model architecture based on the Connectionist Temporal Classification (CTC) algorithm. Secondly, based on the phenomenon of second language transfer, L2 pronun-ciation often has the phoneme characteristics of its native language. This paper uses transfer learning algorithms to improve the performance of the acoustic model based on the native language, thereby improving the accuracy of pronunciation error detection. Compared with the acoustic model that only uses the native English language, the acoustic model that transfers the Chinese phoneme features has a lower error phoneme rate, and the training time is reduced by 7.3%. The correct rate of error detection in pronunciation error detection performance has increased by 2.06%. 
  
 
</p></abstract><kwd-group><kwd>音素，迁移学习，自动发音检错；CTC，长短记忆神经网络, Phoneme</kwd><kwd> Transfer Learning</kwd><kwd> Automatic Pronunciation Error Detection</kwd><kwd> CTC</kwd><kwd> Long and Short memory Neural Network</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>自动发音检错是为了满足第二语言学习者发音练习的需求，而先进的自动发音检错系统通常取决于声学模型识别率。随着深度学习技术的发展，端到端声学模型算法已经逐渐成熟，为发音检错算法研究提供的新思路。本文首先构建了基于连接时序分类(Connectionist Temporal Classification, CTC)算法的端到端发音检错声学模型架构。其次，基于二语迁移现象，L2发音往往带有其母语的音素特征，本文利用迁移学习算法提高基于母语的声学模型性能，从而提高发音检错准确率。通过迁移中文母语音素特征的声学模型相比于只使用英文母语的声学模型在错误音素率上有所下降，并且训练时间减少了7.3%。在发音检错性能上检错正确率提升了2.06%。</p></sec><sec id="s2"><title>关键词</title><p>音素，迁移学习，自动发音检错；CTC，长短记忆神经网络</p></sec><sec id="s3"><title>Research on End-to-End Pronunciation Error Detection Based on Transfer Learning</title><p>Wenming Gao, Yizhi Wu, Xinxiang Wei</p><p>Donghua University, Shanghai</p><p><img src="//html.hanspub.org/file/12-1542108x4_hanspub.png" /></p><p>Received: Mar. 15<sup>th</sup>, 2021; accepted: Apr. 10<sup>th</sup>, 2021; published: Apr. 21<sup>st</sup>, 2021</p><p><img src="//html.hanspub.org/file/12-1542108x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Automatic pronunciation error detection is to meet the needs of second language learners’ pronunciation practice, and advanced automatic pronunciation error detection systems usually depend on the recognition rate of the acoustic model. With the development of deep learning technology, end-to-end acoustic model algorithms have gradually matured, providing new ideas for the research of pronunciation error detection algorithms. This paper first builds an end-to-end pronunciation error detection acoustic model architecture based on the Connectionist Temporal Classification (CTC) algorithm. Secondly, based on the phenomenon of second language transfer, L2 pronunciation often has the phoneme characteristics of its native language. This paper uses transfer learning algorithms to improve the performance of the acoustic model based on the native language, thereby improving the accuracy of pronunciation error detection. Compared with the acoustic model that only uses the native English language, the acoustic model that transfers the Chinese phoneme features has a lower error phoneme rate, and the training time is reduced by 7.3%. The correct rate of error detection in pronunciation error detection performance has increased by 2.06%.</p><p>Keywords:Phoneme, Transfer Learning, Automatic Pronunciation Error Detection, CTC, Long and Short memory Neural Network</p><disp-formula id="hanspub.41705-formula6"><graphic xlink:href="//html.hanspub.org/file/12-1542108x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/12-1542108x7_hanspub.png" /> <img src="//html.hanspub.org/file/12-1542108x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 介绍</title><p>随着经济全球化的发展，越来越多的人渴望学习除母语外的第二语言。计算机辅助语言学习(Computer-Assisted Language Learning, CALL)技术 [<xref ref-type="bibr" rid="hanspub.41705-ref1">1</xref>] 可实现随时随地可访问性的语言自主学习 [<xref ref-type="bibr" rid="hanspub.41705-ref2">2</xref>]。如今，市场上的CALL软件如雨后春般出现，可满足第二语言教学老师的匮乏，也可满足日益增长的学习者个性化的需求 [<xref ref-type="bibr" rid="hanspub.41705-ref3">3</xref>]。专注于为学习者提供L2发音练习的CALL系统通常称为计算机辅助发音训练系统(Computer-Assisted Pronunciation Training, CAPT)，CAPT系统可以有效地处理和分析学习者说出的语音信号。然后对语音进行定量或定性评估。然后反馈给学习者其发音的评分。自动发音检错 [<xref ref-type="bibr" rid="hanspub.41705-ref4">4</xref>] (Automatic Pronunciation Error Detection, APED)模块是CAPT系统的核心。能够准确检测出学习者的错误发音。</p><p>最近，端到端声学模型在自动语言识别(Automatic Speech Recognition, ASR)上取得了优良成绩。端到端声学模型与传统基于隐马尔可夫模型(Hidden Markov Model, HMM) [<xref ref-type="bibr" rid="hanspub.41705-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.41705-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.41705-ref7">7</xref>] 不同，不需要在帧级别上对齐标签。另外，考虑到APED系统往往满足的是学习除母语外第二语言的人群，如欧洲人学习中文，日本人学习英文。基于二语迁移现象，L2发音往往带有其母语的音素特征。本文利用迁移学习算法提高母语的端到端声学模型性能，从而提高发音检错准确率。最终本文构建基于迁移学习的端到端APED系统。在本文中，我们的端到端APED系统在第2节中介绍。在第3节中，进行实验与分析。最后，结论是在第4节中描述。</p></sec><sec id="s6"><title>2. 端到端APED</title><p>本节主要对端到端APED系统进行介绍。端到端声学模型主要将音频信号识别为音素序列。选择性能优异的声学模型是提升APED系统性能关键任务。识别率高的声学模型识别的音素序列经过序列比对算法比对，能够在识别与纠正学习者口音的绝对发音错误工作中表现出色。即通过声学模型识别的句子音素序列直接判断发音中哪些音素出现漏读，插入，替换等错误。</p><sec id="s6_1"><title>2.1. 特征提取</title><p>特征提取的目的是将语音波形数据转换成适合声学模型训练的声学特征参数。特征参数应具有一定的区分能力，以便声学建模单元之间能够相互区分开，实现准确的识别。音素识别中常用的特征有Mel频率倒谱系数(MelFrequency Cepstrum Coefficients, MFCC)和感知线性预测系数(Perceptual Linear Predictive, PLP)。MFCC特征更符合人耳的听觉特性，在提取的过程中一般会把相位谱丢弃，使用“Mel滤波器组”进行滤波，再使用离散余弦变换去掉维与维之间的相关性 [<xref ref-type="bibr" rid="hanspub.41705-ref8">8</xref>]。PLP特征是一种基于听觉模型的参数，在噪声环境下性能会稍好一些。由于本文选取的语料库都是低噪的环境下录制完成的。所以选取的特征为Mel频率倒谱系数。MFCC 提取过程一般分为预加重，分帧，加窗，快速傅里叶变换，梅尔滤波器组，离散余弦变换。预加重，分帧，加窗是各种声学特征提取都会有的预处理过程。主要是将连续的音频模拟信号转换几十毫米一帧高频特性有所提升的数字信号。快速傅里叶变换将时域信号转成频域信号可以获得更多有效的发音信息。使用“Mel滤波器组”进行滤波，再使用离散余弦变换去掉维度之间的相关性。</p></sec><sec id="s6_2"><title>2.2. 基于CTC的端到端声学模型</title><p>当前，端到端自动识别模型有两种主要架构，一种是基于CTC算法模型，另一个是基于注意力的Seq2seq模型。前者更适合于时间序列建模。本文采用是基于连接时序分类(CTC)端到端模型算法。循环神经网络(Recurrent Neural Network, RNN)一般以序列数据为输入，通过网络内部的结构设计有效捕捉序列之间的关系特征，一般也是以序列形式进行输出。考虑到母语语音语料库和L2非母语语料库中音频数据是句子，选择一种特殊的卷积神经网络–长短记忆神经网络(Long And Short Memory Neural Network, LSTM)作为本模型神经网络 [<xref ref-type="bibr" rid="hanspub.41705-ref9">9</xref>]。相比与普通循环神经网络有效解决了长期依赖问题。LSTM通过加入3个门，即遗忘门、输入门及输出门，这种独特的结构使得误差在传播过程中无需逐层归因，部分误差可以直接传递给下一层网络。输入门通过Sigmoid函数决定要更新的值。</p><p>f t = σ ( W f ⋅ [ h t − 1 , x t ] + b f ) (1)</p><p>其中 W f 为输入门权重， b f 为输入门偏置。 f t 的值在0至1之间。</p><p>遗忘门维护着LSTM网络的“状态”值。</p><p>i t = σ ( W i ⋅ [ h t − 1 , x t ] + b i ) (2)</p><p>C t ˜ = tanh ( W c ⋅ [ h t − 1 , x t ] + b c ) (3)</p><p>C t = f t ∗ C t − 1 + i t ∗ C t ˜ (4)</p><p>其中 C t 为更新上一个神经元“状态”值 C t − 1 所得。其中 W i 、 W c 为遗忘门权重， b i 、 b c 为遗忘门偏置。</p><p>最终输出门基于神经元状态值 C t 要输出什么</p><p>o t = σ ( W o ⋅ [ h t − 1 , x t ] + b o ) (5)</p><p>h t = o t ∗ tanh ( C t ) (6)</p><p>其中 W o 为输出门权重， b o 为输出门偏置。“状态”值经过tanh函数输出与输出门输出 o t 相乘最终得到神经元输出 h t 。</p><p>CTC的损失函数最大似然为：</p><p>L ( S ) = ∑ ( x , z ) ∈ S L ( x , z ) (7)</p><p>L ( x , z ) = − ln ∑ u = 1 | z ′ | α ( t , u ) β ( t , u ) (8)</p><p>其中， | z ′ | 表示Z对应的标签长度， α ( t , u ) β ( t , u ) 表示t时刻经过节点u的所有路径的概率和。</p></sec><sec id="s6_3"><title>2.3. 基于迁移学习端到端APED</title><p>迁移学习通过模拟人使用类比进行学习的能力，将在源领域中学到的知识迁移应用到目标领域中，放宽了对训练数据的限制，可以很好地解决标注样本量少的问题。迁移学习尤其是深度迁移学习 [<xref ref-type="bibr" rid="hanspub.41705-ref10">10</xref>]，被广泛应用于图像和文本领域的小样本学习。APED系统往往是满足学习第二种语言的学习者需求。本文研究的是汉语母语的英语为第二语言(ChineseL2)的发音检测问题。利用迁移学习在深度神经网络上的应用，将声学模型在汉语母语，即，Chinese L1，语料库上训练后的参数迁移到训练英语母语，即，EnglishL1，语料库的过程中来。以这种方式去提升端到端APED性能。基于迁移学习的端到端APED如图1。</p><p>图1. 基于迁移学习端到端APED</p></sec></sec><sec id="s7"><title>3. 实验结果与分析</title><sec id="s7_1"><title>3.1. 语料库</title><p>本文选择Timit作为English L1母语语音语料库 [<xref ref-type="bibr" rid="hanspub.41705-ref11">11</xref>]。是由来自美国八个主要方言地区的630个人每人说出给定的10个句子，所有的句子都在音素级别上进行了手动分割，标记。ChineseL1母语语音语料库选取的是标贝科技有限公司录制标注的中文标准语音库，录音环境的信噪比不低于35 dB。音频格式为WAV，采样率为48 KHz [<xref ref-type="bibr" rid="hanspub.41705-ref12">12</xref>]。</p><p>另外，本文制作了小型的English L2语音语料库DHU10。收集10名东华大学同学(4名男性和6名女性)朗读的400句英文音频数据，并且有语言学老师对发音有错误的音素进行标注。</p></sec><sec id="s7_2"><title>3.2. 实验配置</title><p>本文实验参数设置：以帧长25 ms、帧移为10 ms对语言音频数据提取特征，Mel滤波器数目为39，最终得到39维MFCC特征。Batch size大小为64，数据输入LSTM网络前进行padding操作以保证输入数据的长度一致。每层LSTM均包含150个单元，实验对选择2至5层LSTM进行比对。Dense层输出维度和softmax层单元数为语料库的标签种类数目。声学模型的迭代次数统一为200次。</p></sec><sec id="s7_3"><title>3.3. 基于迁移学习的模型训练</title><p>端到端声学模型训练用时为12小时18分钟，基于迁移学习端到端声学模型用时为11小时24分钟。训练时间减少了7.3%。图二是声学模型的损失函数值随时间的下降趋势。如图2显示：迁移学习声学模型因为保留着中文音素的特征。所以训练前期损失函数值下降更加快。</p><p>图2. 损失函数随时间变化</p></sec><sec id="s7_4"><title>3.4. ASR性能与结果</title><p>单词错误率(Word Error Rate, WER)是评估ASR性能最重要指标。本文主要关注音素级别的识别和检测性能。因此，我们使用音素错误率(phone error rate, PER)作为自动识别系统性能指标。</p><p>PER = S + D + I N ∗ 100 % (9)</p><p>其中N是音素总数。S，D和I表示音素替换、插入错误、漏读个数，分别是通过Needlen man-Wunsch获得的算法比较模型识别后序列与原序列所得。</p><p>根据表1的结果得出。端到端声学模型在2~4层LSTM网络中，随着层数上升音素的错误率逐渐下降。在采用5层的LSTM网络时上升，是因为English L1 语料库TIMIT数据不够导致的欠拟合问题。迁移学习端到端声学模型采用5层的LSTM网络时音素错误率仍然下降，一定程度上解决了样本小的问题。且在2~5层上的LSTM网络音素错误率均小于迁移学习端到端声学模型的声学模型。说明L1、L2母语语料库之间进行迁移的声学模型比直接使用L2母语语料库的更适合APED。</p></sec><sec id="s7_5"><title>3.5. APED性能与结果</title><p>APED的性能指标相比与ASR的来说较为复杂。本文绘制了图3的参数变量类树图。更加方便理解各个参数变量的含义。对正确的音素发音识别正确为TA，反之为FR。对错误的音素发音识别为正确音素为FA，反之为TR。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Phoneme error rate of acoustic mode</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >网络层数 声学模型</th><th align="center" valign="middle"  colspan="4"  >phone error rate(%)</th></tr></thead><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >5</td></tr><tr><td align="center" valign="middle" >端到端声学模型</td><td align="center" valign="middle" >25.48</td><td align="center" valign="middle" >17.62</td><td align="center" valign="middle" >14.30</td><td align="center" valign="middle" >15.22</td></tr><tr><td align="center" valign="middle" >迁移学习端到端声学模型</td><td align="center" valign="middle" >22.31</td><td align="center" valign="middle" >14.31</td><td align="center" valign="middle" >13.64</td><td align="center" valign="middle" >13.23</td></tr></tbody></table></table-wrap><p>表1. 声学模型的音素错误率</p><p>图3. 参数变量类树图</p><p>APED的精度，召回率以及F-measure指标的定义分别为公式10、11、12：</p><p>Precision = T R T R + F R (10)</p><p>Recall = T R T R + F A (11)</p><p>F - m e a s u r e = 2 Precisio n ∗ Recall Precision + Recall (12)</p><p>系统的纠错正确率为如下公式：</p><p>DetectionAccuracy = T A + T R T A + F R + F A + T R (13)</p><p>采用English L2语音语料库DHU10验证APED系统的性能。验证的性能结果为下表2。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> APED system performance result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Precision</th><th align="center" valign="middle" >Recall</th><th align="center" valign="middle" >F-measure</th><th align="center" valign="middle" >Detection Accuracy</th></tr></thead><tr><td align="center" valign="middle" >端到端APED</td><td align="center" valign="middle" >55.31%</td><td align="center" valign="middle" >79.23%</td><td align="center" valign="middle" >60.08%</td><td align="center" valign="middle" >86.17%</td></tr><tr><td align="center" valign="middle" >基于迁移学习端到端APED</td><td align="center" valign="middle" >58.12%</td><td align="center" valign="middle" >81.05%</td><td align="center" valign="middle" >64.39%</td><td align="center" valign="middle" >88.23%</td></tr></tbody></table></table-wrap><p>表2. APED系统性能结果</p><p>实验数据显示。基于迁移学习端到端APED相比端到端APED检错正确率提高2.06%。经过L1母语语音语料迁移。APED系统学习了学习者的母语发音特征，有利于系统对学习者进行L2发音检错。</p></sec></sec><sec id="s8"><title>4. 总结</title><p>实验表明，基于迁移学习端到端声学模型相比端到端声学模型无论在ASR性能上和APED性能上都更优。APED系统主要满足学习者学习第二语言的需求，结合这一特点。本文提出的基于迁移学习APED有着实际应用意义。</p></sec><sec id="s9"><title>文章引用</title><p>高文明,吴怡之,魏新享. 基于迁移学习的端到端发音检错研究Research on End-to-End Pronunciation Error Detection Based on Transfer Learning[J]. 计算机科学与应用, 2021, 11(04): 885-891. https://doi.org/10.12677/CSA.2021.114091</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41705-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Akhtar, S., Hussain, F., Raja, F.R., et al. (2020) Improving Mispronunciation Detection of Arabic Words for Non-Native Learners Using Deep Convolutional Neural Network Features. Electronics, 9, 963.  
&lt;br&gt;https://doi.org/10.3390/electronics9060963</mixed-citation></ref><ref id="hanspub.41705-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Franco, H. Neumeyer, L. Ramos, M. and Bratt, H. (1999) Auto-matic Detection of Phone-Level Mispronunciation for Language Learning. Sixth European Conference on Speech Com-munication and Technology, Budapest, 5-9 September 1999, 851-854.</mixed-citation></ref><ref id="hanspub.41705-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">胡文凭. 基于深层神经网络的口语发音检测与错误分析[D]: [博士学位论文]. 中国科学技术大学, 2016.</mixed-citation></ref><ref id="hanspub.41705-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Majeed, M.N., Ghazanfar, M.A., et al. (2019) Mispronunciation Detection Using Deep Convolutional Neural Network Features and Transfer Learning Based Model for Arabic Phonemes. IEEE Access, 7, 52589-52608.  
&lt;br&gt;https://doi.org/10.1109/ACCESS.2019.2912648</mixed-citation></ref><ref id="hanspub.41705-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Lo, W.-K., Qian, X.-J., et al. (2009) Implementation of an Extended Recognition Network for Mispronunciation Detection and Diagnosis in Computer-Assisted Pronunciation Training. Speech and Language Technology in Education (SLaTE 2009), 1, 1-4.</mixed-citation></ref><ref id="hanspub.41705-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Huang, H., Xu, H., Wang, X., et al. (2015) Maximum F1-Score Discriminative Training Criterion for Automatic Mispronunciation Detection. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23, 787-797.  
&lt;br&gt;https://doi.org/10.1109/TASLP.2015.2409733</mixed-citation></ref><ref id="hanspub.41705-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Hinton, G., Deng, L., Yu, D., et al. (2012) Deep Neural Net-works for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. IEEE Signal Pro-cessing Magazine, 29, 82-97.  
&lt;br&gt;https://doi.org/10.1109/MSP.2012.2205597</mixed-citation></ref><ref id="hanspub.41705-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Davis, S. and Mermelstein, P. (1980) Comparison of Parametric Representations for Mono Syllabic Word Recognition in Continuously Spoken Sentences. IEEE Transactions on Acous-tics, Speech, and Signal Processing, 28, 357-366.  
&lt;br&gt;https://doi.org/10.1109/TASSP.1980.1163420</mixed-citation></ref><ref id="hanspub.41705-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Graves, A. and Schmidhuber, J. (2005) Frame Wise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures. Neural Networks, 18, 602-610. &lt;br&gt;https://doi.org/10.1016/j.neunet.2005.06.042</mixed-citation></ref><ref id="hanspub.41705-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Oquab, M., Bottou, L., Laptev, I., et al. (2014) Learning and Transferring Mid-Level Image Representations Using Convolutional Neural Networks. IEEE Conference on Computer Vision &amp; Pattern Recognition, Columbus, 23-28 June 2014, 1717-1724. &lt;br&gt;https://doi.org/10.1109/CVPR.2014.222</mixed-citation></ref><ref id="hanspub.41705-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Garofolo, J.S., Lamel, L.F., Fisher, W.M., et al. (1993) TIMIT Acoustic-Phonetic Continuous Speech Corpus. Philadelphia: Linguistic Data Consortium, LDC93S1.</mixed-citation></ref><ref id="hanspub.41705-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">标贝(北京)科技有限公司. 中文标准女声音库[EB/OL]. &lt;br&gt;https://www.data-baker.com, 2016.</mixed-citation></ref></ref-list></back></article>