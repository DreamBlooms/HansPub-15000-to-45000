<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">SA</journal-id><journal-title-group><journal-title>Statistical and Application</journal-title></journal-title-group><issn pub-type="epub">2325-2251</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/SA.2015.42007</article-id><article-id pub-id-type="publisher-id">SA-15509</article-id><article-categories><subj-group subj-group-type="heading"><subject>SA20150200000_64678309.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  线性回归模型中响应值的选取对二分类问题的影响
  The Effects of Different Response Values in Linear Regression Model on Binary Classification
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>小英</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>杨</surname><given-names>岩丽</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>常龙</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>华北电力大学数理学院，北京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>10</day><month>06</month><year>2015</year></pub-date><volume>04</volume><issue>02</issue><fpage>47</fpage><lpage>55</lpage><history><date date-type="received"><day>Sep.</day>	<month>12th,</month>	<year>2014</year></date><date date-type="rev-recd"><day>Oct.</day>	<month>11th,</month>	<year>2014</year>	</date><date date-type="accepted"><day>Oct.</day>	<month>20th,</month>	<year>2014</year></date></history><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   我们利用多元线性回归模型处理两个总体的分类问题，首先对响应变量按一定的规则赋值，并在最小二乘法的基础上构建判别函数及判别准则，进而论证了响应值的选取对平衡及不平衡数据二分类问题的影响。此外，我们将此判别方法与经典判别分析方法如：经典马氏距离判别法、Bayes判别法进行比较，并得到它们之间的内在联系及优缺点。&lt;br/&gt;We use the multiple linear regression model to deal with the classification problem of two popula-tions. Firstly, we assign the response variables and some corresponding values with certain rules, and then construct discriminant function and criterion via least square method. On this basis, we discuss the effects of different response values on classification for balanced and unbalanced data in linear model. In addition, we compare the mentioned discriminant method above with classic discriminant methods including the classical Mahalanobis distance discriminant and Bayes dis-criminant. At last, we find the inner relation between these methods as well as their advantages and disadvantages.
    
  
 
</p></abstract><kwd-group><kwd>二分类问题，响应值选取，判别分析，线性回归模型，最小二乘法, Binary Classification</kwd><kwd> Response Values</kwd><kwd> Discriminant Analysis</kwd><kwd> Linear Regression Model</kwd><kwd> Least Square</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>线性回归模型中响应值的选取 对二分类问题的影响<sup> </sup></title><p>王小英，杨岩丽，陈常龙</p><p>华北电力大学数理学院，北京</p><p>Email: yangyanlibang@163.com</p><p>收稿日期：2015年6月5日；录用日期：2015年6月20日；发布日期：2015年6月25日</p><disp-formula id="hanspub.15509-formula2477"><graphic xlink:href="http://html.hanspub.org/file/3-2580107x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>我们利用多元线性回归模型处理两个总体的分类问题，首先对响应变量按一定的规则赋值，并在最小二乘法的基础上构建判别函数及判别准则，进而论证了响应值的选取对平衡及不平衡数据二分类问题的影响。此外，我们将此判别方法与经典判别分析方法如：经典马氏距离判别法、Bayes判别法进行比较，并得到它们之间的内在联系及优缺点。</p><p>关键词 :二分类问题，响应值选取，判别分析，线性回归模型，最小二乘法</p><disp-formula id="hanspub.15509-formula2478"><graphic xlink:href="http://html.hanspub.org/file/3-2580107x6_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s3"><title>1. 引言</title><p>考虑二总体的分类问题，已知有两个总体<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x7_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x8_hanspub.png" xlink:type="simple"/></inline-formula>，且假定<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x9_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x10_hanspub.png" xlink:type="simple"/></inline-formula>。每个个体有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x11_hanspub.png" xlink:type="simple"/></inline-formula>种观测指标，如果进行了<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x12_hanspub.png" xlink:type="simple"/></inline-formula>次观测得到的观测矩阵<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x13_hanspub.png" xlink:type="simple"/></inline-formula>满足：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x14_hanspub.png" xlink:type="simple"/></inline-formula>，这里<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x15_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x16_hanspub.png" xlink:type="simple"/></inline-formula>。其中观测矩阵的前<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x17_hanspub.png" xlink:type="simple"/></inline-formula>行观测值来自第一个总体<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x18_hanspub.png" xlink:type="simple"/></inline-formula>，第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x19_hanspub.png" xlink:type="simple"/></inline-formula>行到第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x20_hanspub.png" xlink:type="simple"/></inline-formula>行观测值来自第二个总体<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x21_hanspub.png" xlink:type="simple"/></inline-formula>，且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x22_hanspub.png" xlink:type="simple"/></inline-formula>。如今给定一个新的样品，判别分析的目的是根据观测矩阵<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x23_hanspub.png" xlink:type="simple"/></inline-formula>判定此新样品属于两类中的哪一类。</p><p>对上述判别分类问题，已有了一些经典的方法，如：距离判别，Bayes判别等。经典马氏距离判别的思想是：新样品距离哪个总体近就判给哪个总体。而Bayes判别的原理是考虑错判损失，依据使总平均损失最小来进行分类判别。其判别准则如下：</p><p>当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x24_hanspub.png" xlink:type="simple"/></inline-formula>时，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x25_hanspub.png" xlink:type="simple"/></inline-formula>；否则，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x26_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x27_hanspub.png" xlink:type="simple"/></inline-formula>、<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x28_hanspub.png" xlink:type="simple"/></inline-formula>为总体<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x29_hanspub.png" xlink:type="simple"/></inline-formula>、<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x30_hanspub.png" xlink:type="simple"/></inline-formula>的先验概率；<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x31_hanspub.png" xlink:type="simple"/></inline-formula>即把本属于总体<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x32_hanspub.png" xlink:type="simple"/></inline-formula>的样品错判给<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x33_hanspub.png" xlink:type="simple"/></inline-formula>时造成的损失。然而通常情况下，总体参数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x34_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x35_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x36_hanspub.png" xlink:type="simple"/></inline-formula>未知，需要由样本数据来估计未知参数。这里我们记：</p><disp-formula id="hanspub.15509-formula2479"><graphic xlink:href="http://html.hanspub.org/file/3-2580107x37_hanspub.png"  xlink:type="simple"/></disp-formula><p>将求得的参数估计值代入判别函数中，可用相应的判别准则对新样品判别归类。当两正态总体协方差阵相等时，我们可根据距离判别和Bayes判别准则导出两个线性判别函数。由判别函数的线性特性及判别函数中指标的多元性，我们考虑多元线性回归模型：</p><disp-formula id="hanspub.15509-formula2480"><label>(1.1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-2580107x38_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x39_hanspub.png" xlink:type="simple"/></inline-formula>。这里我们按如下规则对响应变量y赋值：</p><disp-formula id="hanspub.15509-formula2481"><label>(1.2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-2580107x40_hanspub.png"  xlink:type="simple"/></disp-formula><p>我们不妨令：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x41_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x42_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x43_hanspub.png" xlink:type="simple"/></inline-formula>是一个<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x44_hanspub.png" xlink:type="simple"/></inline-formula>的列向量，其元素全为1。</p><p><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x45_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x46_hanspub.png" xlink:type="simple"/></inline-formula></p><p>张尧庭等[<xref ref-type="bibr" rid="hanspub.15509-ref1">1</xref>] 通过讨论回归分析与判别分析的关系指出：可以用回归分析方法来处理判别分析问题，并指出Fishe线性判别函数与线性回归方程(除常数项<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x47_hanspub.png" xlink:type="simple"/></inline-formula>以外)在形式上是一样的；Trevor等[<xref ref-type="bibr" rid="hanspub.15509-ref2">2</xref>] 提出了分类的线性方法，并认为最小二乘回归系数与LDA (linear discriminant analysis)中的判别系数成比例；Qing 和Hui [<xref ref-type="bibr" rid="hanspub.15509-ref3">3</xref>] 由LDA中的最小二乘公式推导并提出Lassoed判别分析。Jianqing Fan [<xref ref-type="bibr" rid="hanspub.15509-ref4">4</xref>] 在A ROAD to Classification in High Dimensional Space中提出了Regularized Optimal Affine Discriminant的判别方法。以上方法都是用线性回归的方法做判别，这涉及到对响应变量y的值的选取问题。张尧庭等[<xref ref-type="bibr" rid="hanspub.15509-ref1">1</xref>] 在多元统计分析引论中提出：为了使各类响应值的均值0，不妨令下(1.2)式中的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x48_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x49_hanspub.png" xlink:type="simple"/></inline-formula>。Trevor等[<xref ref-type="bibr" rid="hanspub.15509-ref2">2</xref>] 在文章中令<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x50_hanspub.png" xlink:type="simple"/></inline-formula>分别为−1，1；而Qing和Hui [<xref ref-type="bibr" rid="hanspub.15509-ref3">3</xref>] 则令<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x51_hanspub.png" xlink:type="simple"/></inline-formula>分别为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x52_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x53_hanspub.png" xlink:type="simple"/></inline-formula>。我们将在前人研究的基础上，进一步讨论回归分析中不同响应值的选取对判别结果的影响。</p></sec><sec id="s4"><title>2. 用线性回归方法做判别</title><p>这里我们将用回归分析的方法来处理判别问题。首先我们对观测数据做中心化处理，如邰淑彩等[<xref ref-type="bibr" rid="hanspub.15509-ref5">5</xref>] 中的：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x54_hanspub.png" xlink:type="simple"/></inline-formula>,<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x55_hanspub.png" xlink:type="simple"/></inline-formula>，得到(1.1)式的一个新的矩阵表达形式：</p><disp-formula id="hanspub.15509-formula2482"><label>(2.1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-2580107x56_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x57_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x58_hanspub.png" xlink:type="simple"/></inline-formula>。由多元线性回归模型中系数的最小二乘估计法知：</p><disp-formula id="hanspub.15509-formula2483"><label>(2.2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-2580107x59_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x60_hanspub.png" xlink:type="simple"/></inline-formula>分别是<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x61_hanspub.png" xlink:type="simple"/></inline-formula>的最小二乘估计。</p><p>Theorem 2.1：在多元线性回归方程(2.1)中，参数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x62_hanspub.png" xlink:type="simple"/></inline-formula>的最小二乘估计<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x63_hanspub.png" xlink:type="simple"/></inline-formula>满足式子：</p><disp-formula id="hanspub.15509-formula2484"><label>(2.3)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-2580107x64_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x65_hanspub.png" xlink:type="simple"/></inline-formula>，特别的，当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x66_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x67_hanspub.png" xlink:type="simple"/></inline-formula>时，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x68_hanspub.png" xlink:type="simple"/></inline-formula>满足如下式子：</p><disp-formula id="hanspub.15509-formula2485"><graphic xlink:href="http://html.hanspub.org/file/3-2580107x69_hanspub.png"  xlink:type="simple"/></disp-formula><p>由(2.2)式和(2.3)式，我们将系数的最小二乘估计代入判别函数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x70_hanspub.png" xlink:type="simple"/></inline-formula>，并设定判别准则如下(2.4)式，我们用此判别函数及准则对新样本判别归类时能得到下面的定理。</p><disp-formula id="hanspub.15509-formula2486"><label>(2.4)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-2580107x71_hanspub.png"  xlink:type="simple"/></disp-formula><p>Theorem 2.2：(1) 若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x72_hanspub.png" xlink:type="simple"/></inline-formula>正定，则判别结果只与<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x73_hanspub.png" xlink:type="simple"/></inline-formula>的符号有关，而与<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x74_hanspub.png" xlink:type="simple"/></inline-formula>的取值无关。即只要<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x75_hanspub.png" xlink:type="simple"/></inline-formula>的符号<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x76_hanspub.png" xlink:type="simple"/></inline-formula>相同，用该方法判别得到的结果就相同，且无论<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x77_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x78_hanspub.png" xlink:type="simple"/></inline-formula>相等与否，该结论都成立。</p><p>(2) 当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x79_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x80_hanspub.png" xlink:type="simple"/></inline-formula>满足<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x81_hanspub.png" xlink:type="simple"/></inline-formula>时，用该判别方法与用距离判别法对新样品分类时得到的判别函数及判别结果相同。</p></sec><sec id="s5"><title>3. 模拟</title><sec id="s5_1"><title>3.1. 平衡数据模拟</title><p>我们通过数据模拟来验证我们的结论。我们随机生成了两类数据<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x82_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x83_hanspub.png" xlink:type="simple"/></inline-formula>。它们均服从<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x84_hanspub.png" xlink:type="simple"/></inline-formula>元正态分布，其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x85_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x86_hanspub.png" xlink:type="simple"/></inline-formula>。这里我们取<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x87_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x88_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x89_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x90_hanspub.png" xlink:type="simple"/></inline-formula>满足：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x91_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x92_hanspub.png" xlink:type="simple"/></inline-formula></p><disp-formula id="hanspub.15509-formula2487"><label>(3.1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-2580107x93_hanspub.png"  xlink:type="simple"/></disp-formula><p>我们采用五折交叉验证的方法，此方法被Breiman等[<xref ref-type="bibr" rid="hanspub.15509-ref6">6</xref>] 提议并广泛应用于实际，取数据集的4/5作训练集，剩下的1/5作测试集。然后用训练集拟合判别函数，用测试集评估分类性能。我们分别在<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x94_hanspub.png" xlink:type="simple"/></inline-formula>的八种情况下，用距离判别、判别I-III这四种方法对测试集样本判别归类。其中，判别I-III即在Theorem 2.2提出的判别方法中分别取：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x95_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x96_hanspub.png" xlink:type="simple"/></inline-formula>；<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x97_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x98_hanspub.png" xlink:type="simple"/></inline-formula>；<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x99_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x100_hanspub.png" xlink:type="simple"/></inline-formula>得到的三种判别方法。我们重复模拟试验1000次，最终得到各自的平均错判率如表1。</p><p>显然，在判别I-III中：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x101_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x102_hanspub.png" xlink:type="simple"/></inline-formula>。由表1可知：用判别I-III三种方法判别时的模拟错判率相等，这与Theorem 2.2 (1)相符。此外，当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x103_hanspub.png" xlink:type="simple"/></inline-formula>时，用距离判别与用判别I-III这三种方法判别时的模拟错判率相等，满足Theorem 2.2 (2)。此外，我们还可从表格中看出：用以上四种方法对平衡数据<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x104_hanspub.png" xlink:type="simple"/></inline-formula>进行判别时，模拟得到的错判率随着维数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x105_hanspub.png" xlink:type="simple"/></inline-formula>的增加而增加，即判别效果随之降低。</p></sec><sec id="s5_2"><title>3.2. 不平衡数据模拟</title><p>基于上3.1中提到的两类数据，我们分别取p = 10,50,100，并分别在这三种情况下取<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x106_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x107_hanspub.png" xlink:type="simple"/></inline-formula>。之后分别用距离判别，Bayes判别，判别I-III这五种方法对前面9种情况做判别，我们重复模拟试验1000次并取平均值，模拟结果如表2。</p><p>这里我们将用一些特定的评价标准评估不同判别方法的分类性能。当数据不平衡<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x108_hanspub.png" xlink:type="simple"/></inline-formula>时，Weiss [<xref ref-type="bibr" rid="hanspub.15509-ref7">7</xref>] 指出：为提高分类准确率，分类器往往倾向于将新样品预测为多数类而导致少数类样本的识别率较低，所以错判率不能很好的反映判别方法对不平衡数据集的判别效果。因此我们采用不平衡数据集分类中常用的评价标准：F-value [<xref ref-type="bibr" rid="hanspub.15509-ref8">8</xref>] 和G-mean [<xref ref-type="bibr" rid="hanspub.15509-ref9">9</xref>] 来衡量不同方法判别效果的好坏。这里记：</p><disp-formula id="hanspub.15509-formula2488"><label>(3.2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-2580107x109_hanspub.png"  xlink:type="simple"/></disp-formula><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The misclassification rate of four discriminant method</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >p</th><th align="center" valign="middle" >距离判别错判率</th><th align="center" valign="middle" >判别I错判率</th><th align="center" valign="middle" >判别II错判率</th><th align="center" valign="middle" >判别III错判率</th></tr></thead><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >0.0925</td><td align="center" valign="middle" >0.0925</td><td align="center" valign="middle" >0.0925</td><td align="center" valign="middle" >0.0925</td></tr><tr><td align="center" valign="middle" >30</td><td align="center" valign="middle" >0.1153</td><td align="center" valign="middle" >0.1153</td><td align="center" valign="middle" >0.1153</td><td align="center" valign="middle" >0.1153</td></tr><tr><td align="center" valign="middle" >50</td><td align="center" valign="middle" >0.1430</td><td align="center" valign="middle" >0.1430</td><td align="center" valign="middle" >0.1430</td><td align="center" valign="middle" >0.1430</td></tr><tr><td align="center" valign="middle" >70</td><td align="center" valign="middle" >0.1741</td><td align="center" valign="middle" >0.1741</td><td align="center" valign="middle" >0.1741</td><td align="center" valign="middle" >0.1741</td></tr><tr><td align="center" valign="middle" >90</td><td align="center" valign="middle" >0.2095</td><td align="center" valign="middle" >0.2095</td><td align="center" valign="middle" >0.2095</td><td align="center" valign="middle" >0.2095</td></tr><tr><td align="center" valign="middle" >110</td><td align="center" valign="middle" >0.2542</td><td align="center" valign="middle" >0.2542</td><td align="center" valign="middle" >0.2542</td><td align="center" valign="middle" >0.2542</td></tr><tr><td align="center" valign="middle" >130</td><td align="center" valign="middle" >0.3089</td><td align="center" valign="middle" >0.3089</td><td align="center" valign="middle" >0.3089</td><td align="center" valign="middle" >0.3089</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >0.3910</td><td align="center" valign="middle" >0.3910</td><td align="center" valign="middle" >0.3910</td><td align="center" valign="middle" >0.3910</td></tr></tbody></table></table-wrap><p>表1. 四种判别方法的错判率比较</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> The discriminant outcome comparison of unbalanced dat</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x110_hanspub.png" xlink:type="simple"/></inline-formula></th><th align="center" valign="middle" >判别标准</th><th align="center" valign="middle" >距离判别</th><th align="center" valign="middle" >Bayes判别</th><th align="center" valign="middle" >判别I</th><th align="center" valign="middle" >判别II</th><th align="center" valign="middle" >判别III</th></tr></thead><tr><td align="center" valign="middle"  rowspan="3"  >(150, 10)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0897</td><td align="center" valign="middle" >0.0938</td><td align="center" valign="middle" >0.1062</td><td align="center" valign="middle" >0.1062</td><td align="center" valign="middle" >0.1062</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.8903</td><td align="center" valign="middle" >0.8738</td><td align="center" valign="middle" >0.8782</td><td align="center" valign="middle" >0.8782</td><td align="center" valign="middle" >0.8782</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.9090</td><td align="center" valign="middle" >0.8886</td><td align="center" valign="middle" >0.9004</td><td align="center" valign="middle" >0.9004</td><td align="center" valign="middle" >0.9004</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(300, 10)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0863</td><td align="center" valign="middle" >0.0942</td><td align="center" valign="middle" >0.1919</td><td align="center" valign="middle" >0.1919</td><td align="center" valign="middle" >0.1919</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.8417</td><td align="center" valign="middle" >0.7716</td><td align="center" valign="middle" >0.7212</td><td align="center" valign="middle" >0.7212</td><td align="center" valign="middle" >0.7212</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.9117</td><td align="center" valign="middle" >0.8019</td><td align="center" valign="middle" >0.8567</td><td align="center" valign="middle" >0.8567</td><td align="center" valign="middle" >0.8567</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(600, 10)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0840</td><td align="center" valign="middle" >0.0687</td><td align="center" valign="middle" >0.3002</td><td align="center" valign="middle" >0.3002</td><td align="center" valign="middle" >0.3002</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.7586</td><td align="center" valign="middle" >0.6874</td><td align="center" valign="middle" >0.4876</td><td align="center" valign="middle" >0.4876</td><td align="center" valign="middle" >0.4876</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.9137</td><td align="center" valign="middle" >0.7328</td><td align="center" valign="middle" >0.8027</td><td align="center" valign="middle" >0.8027</td><td align="center" valign="middle" >0.8027</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(150, 50)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.1279</td><td align="center" valign="middle" >0.1292</td><td align="center" valign="middle" >0.1439</td><td align="center" valign="middle" >0.1439</td><td align="center" valign="middle" >0.1439</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.8438</td><td align="center" valign="middle" >0.8259</td><td align="center" valign="middle" >0.8364</td><td align="center" valign="middle" >0.8364</td><td align="center" valign="middle" >0.8364</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.8693</td><td align="center" valign="middle" >0.8497</td><td align="center" valign="middle" >0.8627</td><td align="center" valign="middle" >0.8627</td><td align="center" valign="middle" >0.8627</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(300, 50)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.1058</td><td align="center" valign="middle" >0.1078</td><td align="center" valign="middle" >0.2124</td><td align="center" valign="middle" >0.2124</td><td align="center" valign="middle" >0.2124</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.8065</td><td align="center" valign="middle" >0.7363</td><td align="center" valign="middle" >0.6972</td><td align="center" valign="middle" >0.6972</td><td align="center" valign="middle" >0.6972</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.8871</td><td align="center" valign="middle" >0.7766</td><td align="center" valign="middle" >0.8377</td><td align="center" valign="middle" >0.8377</td><td align="center" valign="middle" >0.8377</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(600, 50)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0921</td><td align="center" valign="middle" >0.0743</td><td align="center" valign="middle" >0.3088</td><td align="center" valign="middle" >0.3088</td><td align="center" valign="middle" >0.3088</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.7346</td><td align="center" valign="middle" >0.6582</td><td align="center" valign="middle" >0.4792</td><td align="center" valign="middle" >0.4792</td><td align="center" valign="middle" >0.4792</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.8960</td><td align="center" valign="middle" >0.7118</td><td align="center" valign="middle" >0.7949</td><td align="center" valign="middle" >0.7949</td><td align="center" valign="middle" >0.7949</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(150, 100)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.1868</td><td align="center" valign="middle" >0.1833</td><td align="center" valign="middle" >0.2020</td><td align="center" valign="middle" >0.2020</td><td align="center" valign="middle" >0.2020</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.7735</td><td align="center" valign="middle" >0.7559</td><td align="center" valign="middle" >0.7727</td><td align="center" valign="middle" >0.7727</td><td align="center" valign="middle" >0.7727</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.8085</td><td align="center" valign="middle" >0.7932</td><td align="center" valign="middle" >0.8039</td><td align="center" valign="middle" >0.8039</td><td align="center" valign="middle" >0.8039</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(300, 100)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.1369</td><td align="center" valign="middle" >0.1264</td><td align="center" valign="middle" >0.2406</td><td align="center" valign="middle" >0.2406</td><td align="center" valign="middle" >0.2406</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.7514</td><td align="center" valign="middle" >0.6922</td><td align="center" valign="middle" >0.6642</td><td align="center" valign="middle" >0.6642</td><td align="center" valign="middle" >0.6642</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.8482</td><td align="center" valign="middle" >0.7480</td><td align="center" valign="middle" >0.8098</td><td align="center" valign="middle" >0.8098</td><td align="center" valign="middle" >0.8098</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(600, 100)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.1047</td><td align="center" valign="middle" >0.0833</td><td align="center" valign="middle" >0.3199</td><td align="center" valign="middle" >0.3199</td><td align="center" valign="middle" >0.3199</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.6985</td><td align="center" valign="middle" >0.6135</td><td align="center" valign="middle" >0.4675</td><td align="center" valign="middle" >0.4675</td><td align="center" valign="middle" >0.4675</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.8713</td><td align="center" valign="middle" >0.6815</td><td align="center" valign="middle" >0.7836</td><td align="center" valign="middle" >0.7836</td><td align="center" valign="middle" >0.7836</td></tr></tbody></table></table-wrap><p>表2. 不平衡数据的判别结果比较</p><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x111_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x112_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x113_hanspub.png" xlink:type="simple"/></inline-formula>。P：正类(少数类)，N：负类(多数</p><p>类)。TP与TN分别表示被正确分类的正类和负类样本的数目；FN表示真实类标是正类却被误分为负类的数目，FP表示真实类标是负类而被误分为正类的数目。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x114_hanspub.png" xlink:type="simple"/></inline-formula>：少数类的查全率，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x115_hanspub.png" xlink:type="simple"/></inline-formula>：多数类的查全率，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x116_hanspub.png" xlink:type="simple"/></inline-formula>：查准率。陶新民等 [<xref ref-type="bibr" rid="hanspub.15509-ref10">10</xref>] 指出：F-Value既考虑了查全率又考虑了查准率，只有在查全率和查准率的值都大时，F-value才会大。同样，只有少数类和多数类样本的查全率同时都大时，G-mean值才会大。因此，F-value和G-mean能综合考虑少数类和多数类两类样本的分类性能，是对不平衡数据分类性能的两个较好的评测指标。</p><p>由表2最后三列知：就错判率及不平衡数据的评价标准：F-Value，G-Mean而言，当数据不平衡<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x117_hanspub.png" xlink:type="simple"/></inline-formula>时，判别I-III的判别结果相同且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x118_hanspub.png" xlink:type="simple"/></inline-formula>，此时用距离判别与用判别I-III判别得到的结果不同。</p><p>此外，当数据的维数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x119_hanspub.png" xlink:type="simple"/></inline-formula>固定不变时，随着数据不平衡程度<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x120_hanspub.png" xlink:type="simple"/></inline-formula>的增加，距离判别及Bayes判别的错判率、F-value和G-mean值变化相对较小，即受不平衡程度的影响较小，判别效果较好；当数据不平衡程度<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x121_hanspub.png" xlink:type="simple"/></inline-formula>固定不变时，随着维数p的增加，距离判别及Bayes判别得的错判率较低且F-value，G-Mean值较高，判别效果较好；然而我们可以结合线性模型的变量选择方法如：Tibshirani [<xref ref-type="bibr" rid="hanspub.15509-ref11">11</xref>] 提出的LASSO (Least Absolute Shrinkage and Selection Operator)、Fan和Li [<xref ref-type="bibr" rid="hanspub.15509-ref12">12</xref>] 提出的SCAD (Smoothly Clipped Absolute Deviation)、AIC、BIC及Fan和Lv [<xref ref-type="bibr" rid="hanspub.15509-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.15509-ref14">14</xref>] 提出的SIS (Sure Independence Screening)等方法选择重要变量对数据降维，与此同时，用线性回归的方法对新样品判别归类。用这种方法，我们在降维的同时对数据做判别，可能会使得判别I-III的判别效果得到提高，然而这还有待我们以后做进一步研究。</p></sec><sec id="s5_3"><title>3.3. 实例分析</title><p>此外，我们对“Wisconsin Diagnostic Breast Cancer (WDBC)”中的真实数据进行了分析。本文采用的数据来源于http://www.datatang.com/data/515。</p><p>该数据集包含了关于人体细胞核的30个相关指标(如：细胞核面积、周长、平滑度等)的大量数据，我们从第一类(未患乳腺癌)和第二类(患乳腺癌)的观测样本中各取<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x122_hanspub.png" xlink:type="simple"/></inline-formula>、<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x123_hanspub.png" xlink:type="simple"/></inline-formula>个样品，用五折交叉验证的方法，并分别用距离判别、Bayes判别、判别I-III这五种方法对这<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x124_hanspub.png" xlink:type="simple"/></inline-formula>个样品“是否患有乳腺癌”进行分析判别，并得到各自的模拟结果如表3。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> WDBC discriminant result compariso</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x125_hanspub.png" xlink:type="simple"/></inline-formula></th><th align="center" valign="middle" >判别标准</th><th align="center" valign="middle" >距离判别</th><th align="center" valign="middle" >Bayes判别</th><th align="center" valign="middle" >判别I</th><th align="center" valign="middle" >判别II</th><th align="center" valign="middle" >判别III</th></tr></thead><tr><td align="center" valign="middle"  rowspan="3"  >(200, 200)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0375</td><td align="center" valign="middle" >0.0375</td><td align="center" valign="middle" >0.0375</td><td align="center" valign="middle" >0.0375</td><td align="center" valign="middle" >0.0375</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.9632</td><td align="center" valign="middle" >0.9632</td><td align="center" valign="middle" >0.9632</td><td align="center" valign="middle" >0.9632</td><td align="center" valign="middle" >0.9632</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.9623</td><td align="center" valign="middle" >0.9623</td><td align="center" valign="middle" >0.9623</td><td align="center" valign="middle" >0.9623</td><td align="center" valign="middle" >0.9623</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(200, 100)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0467</td><td align="center" valign="middle" >0.0867</td><td align="center" valign="middle" >0.0700</td><td align="center" valign="middle" >0.0700</td><td align="center" valign="middle" >0.0700</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.9651</td><td align="center" valign="middle" >0.9389</td><td align="center" valign="middle" >0.9445</td><td align="center" valign="middle" >0.9445</td><td align="center" valign="middle" >0.9445</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.9417</td><td align="center" valign="middle" >0.8630</td><td align="center" valign="middle" >0.9412</td><td align="center" valign="middle" >0.9412</td><td align="center" valign="middle" >0.9412</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(200, 50)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0320</td><td align="center" valign="middle" >0.0720</td><td align="center" valign="middle" >0.1120</td><td align="center" valign="middle" >0.1120</td><td align="center" valign="middle" >0.1120</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.9805</td><td align="center" valign="middle" >0.9577</td><td align="center" valign="middle" >0.9243</td><td align="center" valign="middle" >0.9243</td><td align="center" valign="middle" >0.9243</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.9216</td><td align="center" valign="middle" >0.7803</td><td align="center" valign="middle" >0.9198</td><td align="center" valign="middle" >0.9198</td><td align="center" valign="middle" >0.9198</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(200, 40)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0292</td><td align="center" valign="middle" >0.0583</td><td align="center" valign="middle" >0.1417</td><td align="center" valign="middle" >0.1417</td><td align="center" valign="middle" >0.1417</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.9831</td><td align="center" valign="middle" >0.9670</td><td align="center" valign="middle" >0.9069</td><td align="center" valign="middle" >0.9069</td><td align="center" valign="middle" >0.9069</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.9121</td><td align="center" valign="middle" >0.7734</td><td align="center" valign="middle" >0.9010</td><td align="center" valign="middle" >0.9010</td><td align="center" valign="middle" >0.9010</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(200, 20)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0318</td><td align="center" valign="middle" >0.0409</td><td align="center" valign="middle" >0.1818</td><td align="center" valign="middle" >0.1818</td><td align="center" valign="middle" >0.1818</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.9825</td><td align="center" valign="middle" >0.9782</td><td align="center" valign="middle" >0.8884</td><td align="center" valign="middle" >0.8884</td><td align="center" valign="middle" >0.8884</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.8817</td><td align="center" valign="middle" >0.6811</td><td align="center" valign="middle" >0.8719</td><td align="center" valign="middle" >0.8719</td><td align="center" valign="middle" >0.8719</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >(200, 10)</td><td align="center" valign="middle" >错判率</td><td align="center" valign="middle" >0.0143</td><td align="center" valign="middle" >0.0238</td><td align="center" valign="middle" >0.2524</td><td align="center" valign="middle" >0.2524</td><td align="center" valign="middle" >0.2524</td></tr><tr><td align="center" valign="middle" >F-value</td><td align="center" valign="middle" >0.9925</td><td align="center" valign="middle" >0.9877</td><td align="center" valign="middle" >0.8465</td><td align="center" valign="middle" >0.8465</td><td align="center" valign="middle" >0.8465</td></tr><tr><td align="center" valign="middle" >G-mean</td><td align="center" valign="middle" >0.8811</td><td align="center" valign="middle" >0.6811</td><td align="center" valign="middle" >0.8130</td><td align="center" valign="middle" >0.8130</td><td align="center" valign="middle" >0.8130</td></tr></tbody></table></table-wrap><p>表3. WDBC判别结果比较</p><p>由表3知：由于判别I-III中的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x126_hanspub.png" xlink:type="simple"/></inline-formula>相等，用判别I-III三种方法模拟的判别结果相等。当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x127_hanspub.png" xlink:type="simple"/></inline-formula>时，距离判别与判别I-III的判别结果相同；当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x128_hanspub.png" xlink:type="simple"/></inline-formula>时，它们的判别结果不同。而且，随着不平衡程度<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x129_hanspub.png" xlink:type="simple"/></inline-formula>的增加，距离判别及Bayes判别的错判率较低，F-value值较高，受不平衡程度的影响较小，判别效果较好。</p></sec></sec><sec id="s6"><title>4. 总结</title><p>本文主要研究了线性回归模型中响应值的选取对二分类问题的影响。首先，我们对响应变量按一定的规则赋值，然后用最小二乘法拟合，建立判别函数及判别准则，进而得到以下两个结论：1) 该判别方法下的判别结果只与<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x130_hanspub.png" xlink:type="simple"/></inline-formula>有关，即只要<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x131_hanspub.png" xlink:type="simple"/></inline-formula>相同，用此判别方法判别的结果就相同。2) 当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x132_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-2580107x133_hanspub.png" xlink:type="simple"/></inline-formula>时，用该判别方法得的判别结果与距离判别结果相同。此外，我们用r语言[<xref ref-type="bibr" rid="hanspub.15509-ref15">15</xref>] 分别对平衡数据、不平衡数据及真实数据WDBC进行了模拟，得到了与前面两个结论相符的模拟结果。</p></sec><sec id="s7"><title>基金项目</title><p>中央高校基本科研业务费专项资金；北京高等学校青年英才计划项目。</p></sec><sec id="s8"><title>文章引用</title><p>王小英,杨岩丽,陈常龙, (2015) 线性回归模型中响应值的选取对二分类问题的影响 The Effects of Different Response Values in Linear Regression Model on Binary Classification. 统计学与应用,02,47-55. doi: 10.12677/SA.2015.42007</p></sec><sec id="s9"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.15509-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">张尧庭, 方开泰 (1988) 多元统计分析引论. 科学出版社, 北京.</mixed-citation></ref><ref id="hanspub.15509-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Hastie, T., Tibshirani, R. and Friedman, J. (2009) Elements of statistical learning: data mining, inference and prediction. 2nd Edition, Springer, Berlin.</mixed-citation></ref><ref id="hanspub.15509-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Mai, Q. and Zou, H. (2012) A direct approach to sparse discriminant analysis in ultra-high dimensions. Biometrika, 99, 29-42.</mixed-citation></ref><ref id="hanspub.15509-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Fan, J.Q., Feng, Y. and Tong, X. (2012) A road to classification in high dimensional space. Journal of the Royal Statistical Society, Series B, Statistical Methodology, 74, 745-771.</mixed-citation></ref><ref id="hanspub.15509-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">邰淑彩, 孙韫玉, 何娟娟 (2005) 应用数理统计(第二版). 武汉大学出版社, 武汉.</mixed-citation></ref><ref id="hanspub.15509-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Breiman, L. and Spector, P. (1992) Submodel selection and evaluation in regression: the x-random case. International Statistical Review, 60, 291-319.</mixed-citation></ref><ref id="hanspub.15509-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Weiss, G.M. and Provost, F. (2003) Learning when training data are costly: The effect of class distribution on tree induction. Journal of Artificial Intelligence Research, 19, 315-354.</mixed-citation></ref><ref id="hanspub.15509-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Kubat, M., Holte, R. and Matwin, S. (1998) Machine learning for the detection of oil spills in satellite radar images. Machine Learning, 30, 195-215.</mixed-citation></ref><ref id="hanspub.15509-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Lewis, D. and Gale, W. (1994) Training text classifiers by uncertainty sampling. Proceedings of ACM-SIGIR Conference on Information Retrieval, New York, 73-79.</mixed-citation></ref><ref id="hanspub.15509-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">陶新民, 郝思媛, 张冬雪, 徐鹏 (2013) 不均衡数据分类算法的综述. 重庆邮电大学学报(自然科学版), 1, 106- 108.</mixed-citation></ref><ref id="hanspub.15509-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Tibshirani, R.J. (1996) Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society, Series B, 58, 267-288.</mixed-citation></ref><ref id="hanspub.15509-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Fan, J. and Li, R. (2001) Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96, 1348-1360.</mixed-citation></ref><ref id="hanspub.15509-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Fan, J. and Lv, J. (2008) Sure independence screening for ultrahigh dimensional feature space (with discussion). Journal of the Royal Statistical Society, Series B, 70, 849-911.</mixed-citation></ref><ref id="hanspub.15509-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Fan, J. and Lv, J. (2010) A selective overview of variable selection in high dimensional feature space. Statistica Sinica, 20, 101-148.</mixed-citation></ref><ref id="hanspub.15509-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">薛毅, 陈立萍 (2007) 统计建模与R软件. 清华大学出版社, 北京.</mixed-citation></ref></ref-list></back></article>