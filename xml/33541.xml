<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">SEA</journal-id><journal-title-group><journal-title>Software Engineering and Applications</journal-title></journal-title-group><issn pub-type="epub">2325-2286</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/SEA.2019.86044</article-id><article-id pub-id-type="publisher-id">SEA-33541</article-id><article-categories><subj-group subj-group-type="heading"><subject>SEA20190600000_91525667.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>工程技术</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于信息熵的离散化算法的研究与实现
  The Study and Implementation of Discretization Algorithm Based on Information Entropy
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>城霞</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>朱</surname><given-names>敏玲</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>北京信息科技大学计算机学院单位，北京</addr-line></aff><aff id="aff1"><addr-line>北京信息科技大学网络文化与数字传播北京市重点实验室，北京；北京信息科技大学计算机学院单位，北京</addr-line></aff><aff id="aff2"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>22</day><month>11</month><year>2019</year></pub-date><volume>08</volume><issue>06</issue><fpage>358</fpage><lpage>363</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  离散化算法将连续属性的取值范围划分为很多个小的区间,每个区间都对应着自己的离散化符号，合理的离散化能够更准确的表达信息。本课题研究并实现了一种基于信息熵的离散化算法，通过赋予断点信息熵来度量断点的重要性从而对集合S进行划分。首先计算连续的属性的候选断点属性集，其次从候选断点集合中选取一个使信息熵最小的断点加入到断点集合中，该断点把集合S分成了两个部分，之后对于每一个子集合确定断点直到对于集合S的划分足够表达不同信息，满足最小区分长度准则完成。本文最后用实验验证了此算法的正确性和有效性，并对多组数据进行了测试和比较。
   The values range of continuous attributes is divided by discretization algorithm into several parts, each of which corresponds to its own discrete symbol. The reasonable discretization determines the accuracy of information expressing. This article studies and implements a discretization algorithm based on information entropy. The set S is divided by measuring the importance of the breakpoint by giving the breakpoint information entropy. First, a set of candidate breakpoint attributes of continuous attribute are calculated. Secondly, a breakpoint from the set of candidate breakpoints is selected to add the breakpoint with the smallest value of information entropy to the set of breakpoints, which breaks up the set S into two parts. The third determines the breakpoint for each set of instances until the partitioning for set S satisfies the minimum discrimination length criterion. In the last part of the article, the correctness and validity of the algorithm are verified by experiments, and test as well as comparison of different groups of data is given.
 
</p></abstract><kwd-group><kwd>信息熵，离散化，断点, Information Entropy</kwd><kwd> Discretization</kwd><kwd> Breakpoint</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于信息熵的离散化算法的研究与实现<sup> </sup></title><p>刘城霞<sup>1,2</sup>，朱敏玲<sup>2</sup></p><p><sup>1</sup>北京信息科技大学网络文化与数字传播北京市重点实验室，北京</p><p><sup>2</sup>北京信息科技大学计算机学院单位，北京</p><p>收稿日期：2019年11月26日；录用日期：2019年12月12日；发布日期：2019年12月19日</p><disp-formula id="hanspub.33541-formula33"><graphic xlink:href="//html.hanspub.org/file/11-2690400x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>离散化算法将连续属性的取值范围划分为很多个小的区间,每个区间都对应着自己的离散化符号，合理的离散化能够更准确的表达信息。本课题研究并实现了一种基于信息熵的离散化算法，通过赋予断点信息熵来度量断点的重要性从而对集合S进行划分。首先计算连续的属性的候选断点属性集，其次从候选断点集合中选取一个使信息熵最小的断点加入到断点集合中，该断点把集合S分成了两个部分，之后对于每一个子集合确定断点直到对于集合S的划分足够表达不同信息，满足最小区分长度准则完成。本文最后用实验验证了此算法的正确性和有效性，并对多组数据进行了测试和比较。</p><p>关键词 :信息熵，离散化，断点</p><disp-formula id="hanspub.33541-formula34"><graphic xlink:href="//html.hanspub.org/file/11-2690400x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/11-2690400x7_hanspub.png" /> <img src="//html.hanspub.org/file/11-2690400x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>离散化是数据挖掘中非常重要的一步，它属于数据预处理阶段，是对连续属性数据进行的分析处理。具体离散化算法会把连续属性的数据取值范围划分为很多个小区间，划分的原则分为有监督和无监督，有监督的分为等宽度，等频率 [<xref ref-type="bibr" rid="hanspub.33541-ref1">1</xref>] 等，而无监督就是指没有指定要求的情况下的根据具体算法具体分析。而基于信息熵的离散化算法即是其中一种无监督的离散化方法，它通过判断断点信息熵的大小来划分实例集，达到离散化的目的。</p></sec><sec id="s4"><title>2. 信息熵基本概念</title><p>信息是个很抽象的概念，起初人们对信息的多少无法说清楚，直到香农在1948年信息论中提出的信息熵 [<xref ref-type="bibr" rid="hanspub.33541-ref2">2</xref>] 这一概念才使的量化度量信息的问题得以解决。在信息论中，信息熵被视为接收到的每条信息种包含的信息量的平均值，也被称为信源熵、平均自信息量。</p><p>用X表示随机变量，随机变量的取值为( x 1 , x 2 , ⋯ , x n )， p ( x i ) 表示事件x<sub>i</sub>发生的概率，且有 ∑ p ( x i ) = 1 。定义事件x<sub>i</sub>的信息量是它发生概率对数的负数，记为I(x<sub>i</sub>)，即： I ( x i ) = − log ( p ( x i ) ) 。</p><p>而H(X)为随机变量X的平均信息量，即X的信息熵。</p><p>H ( X ) = E [ − log P ( x i ) ] = − ∑ i = 1 N P ( x i ) log P (xi)</p><p>其中 P ( x i ) 表示事件x<sub>i</sub>发生的先验概率，且 ∑ P ( x i ) = 1 。</p><p>另外，公式中的log的底和信息熵的单位有关。被广泛应用的是以2为底，它的单位是bit。当使用以e为底，单位为Nat。</p></sec><sec id="s5"><title>3. 最小信息熵的离散化方法</title><p>在智能化信息处理中，经常遇到的技术难点主要是离散化问题和数据的不完整性问题。在应用粗糙集理论来处理信息决策表时，需要满足的是决策表中的值应该是离散的。不管是连续属性还是离散化的数据，都是需要进行离散化，连续属性在处理前必须进行离散化处理；对于离散化的数据，可以使用离散化方法将本来也就是离散的数据合并或者抽象得到更好的离散值。离散化算法应该满足：</p><p>1) 离散化后的信息系统的空间维数应该尽量少，即离散化后信息系统剩余的属性个数尽量少。</p><p>2) 离散化后的信息丢失尽量少。</p><p>1993年，Fayyad和Irani于提出最小信息熵离散化方法 [<xref ref-type="bibr" rid="hanspub.33541-ref3">3</xref>]。离散的门限边界值由候选区间的类信息熵来选择。子集S<sub>1</sub>和子集S<sub>2</sub>的信息熵为Ent(S<sub>1</sub>)和Ent(S<sub>2</sub>)，则由断点T关于属性A的类信息熵可以表示为</p><p>E ( A , T ; S ) = | S 1 | / | S | ⋅ E n t ( S 1 ) + | S 2 | / | S | ⋅ E n t (S2)</p><p>|S|表示集合中元素个数。</p><p>对于给定的属性A，将使得E(A;T;S)值最小的点作为离散化的划分点，并将其记为Tmin。划分点Tmin将区间划分为两个部分，样本集合也被分为两个集合S<sub>1</sub>和S<sub>2</sub>。在分别计算S<sub>1</sub>和S<sub>2</sub>的所有划分点的时候，假设T<sub>1</sub>和T<sub>2</sub>分别为S<sub>1</sub>和S<sub>2</sub>的最好的划分点，而它们所对应的类信息熵分别为E(A,T<sub>1</sub>,S<sub>1</sub>)和E(A,T<sub>2</sub>,S<sub>2</sub>)；如果E(A,T<sub>1</sub>,S<sub>1</sub>) &gt; E(A,T<sub>2</sub>,S<sub>2</sub>)，就继续对S<sub>1</sub>进行划分，反之则是S<sub>2</sub>。也就是说，哪个大就划分哪个。然后递归调用他们，直到满足递归停止条件。停止条件是利用最小区分长度准则作为判断递归离散化是否停止，就是说集合S中的递归的划分区间，当满足下列条件时停止，</p><p>G a i n ( A , T ; S ) &lt; log 2 ( N − 1 ) / N + Δ ( A , T ; S ) / N</p><p>其中：N是集合S中的样本数量， G a i n ( A , T ; S ) = E n t ( S ) − E n t ( A , T , S ) ，</p><p>Δ ( A , T ; S ) = log 2 ( 3 k − 2 ) − [ k ⋅ E n t ( S ) − k 1 ⋅ E n t ( S 2 ) − k 2 ⋅ E n t ( S 2 ) ]</p><p>其中k<sub>1</sub>和k<sub>2</sub>分别是集合S<sub>1</sub>和S<sub>2</sub>中的类别数量，即k = |S|，k<sub>1</sub> = |S<sub>1</sub>|，k<sub>2</sub> = |S<sub>2</sub>|。</p></sec><sec id="s6"><title>4. 基于最小信息熵的离散化算法的设计及实现</title><p>基于信息熵的离散化算法有很多，比如文献 [<xref ref-type="bibr" rid="hanspub.33541-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.33541-ref5">5</xref>] 等。本文采取的基于最小信息熵的离散化算法的主要步骤为：第一，计算一个连续属性的候选断点属性集。第二，从第一步得到的候选断点属性集中选取一个使信息熵的值最小的点，此断点把实例集合划分为两个部分，将此断点加入到断点集合中。第三，确定每个实例集合的断点，划分实例集合S直到它满足递归停止条件则退出程序，完成离散化工作。该算法是一个比较基础的算法，它每次仅从单个条件属性的候选断点中选择断点 [<xref ref-type="bibr" rid="hanspub.33541-ref6">6</xref>]，得到的最终结果断点数目较多，但时间复杂度较低。但本文中是以针对该算法采用多组数据进行比较，实验结果表明此算法是有效的。</p><p>具体算法描述表示如下：</p><p>步骤1：计算连续属性的候选断点属性集。</p><p>步骤2：从第一步得到的候选断点属性集中选取一个使信息熵的值最小的点，此断点把实例集合划分为两个部分，将此断点加入到断点集合中。信息熵计算方法如下：对于一个给定的实例的集合S，一个属性A，一个分类的边界T（把S分为S<sub>1</sub>和S<sub>2</sub>两个部分），那么由分类边界T确定的分类的信息熵为：</p><p>E ( A , T ; S ) = | S 1 | / | S | ⋅ E n t ( S 1 ) + | S 2 | / | S | ⋅ E n t (S2)</p><p>信息熵计算公式为 E n t ( S ) = − ∑ j = 1 k P j log (Pj)</p><p>步骤3：对每个实例集合都调用步骤2中的方法进行划分，确定断点。递归停止条件为：</p><p>G a i n ( A , T ; S ) &lt; log 2 ( N − 1 ) / N + Δ ( A , T ; S ) / N</p><p>具体算法伪代码如下：</p><disp-formula id="hanspub.33541-formula35"><graphic xlink:href="//html.hanspub.org/file/11-2690400x23_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s7"><title>5. 结果分析</title><sec id="s7_1"><title>5.1. 简单例子</title><p>测试一个较为简单的例子，条件属性：4个；决策属性：1个；记录条数：75条；属性名称：A，B，C，D，E；属性类型：float，float，float，float，integer。离散化前后结果如图1~2所示：</p><p>图1. 离散化前</p><p>图2. 离散化后</p><p>测试结果表明，对于float类型的属性进行离散化后得到了离散化后的属性符号，并且该例中本是离散属性的Ｅ不需要再离散化。由于数据量小，离散化时间几乎为0。</p></sec><sec id="s7_2"><title>5.2. 性能分析</title><p>本次测试主要测试了75~4000条数据，具体运行时间折线图如图3所示。</p><p>图3. 运行时间比较图</p><p>从图上可以看出，随着数据量的增大，离散化时间也在增大。但增长又不是线性的，这是因为离散化时间不但和记录的条数有关，还和断点的个数、需要离散化属性的个数以及属性的类型等有关。</p></sec></sec><sec id="s8"><title>6. 总结与展望</title><p>本文研究并实现了基于最小信息熵的离散化算法，它可以从这些含有连续属性的数据集中取得好的数据样本，得到简洁且有效的规则，如此可以方便数据在后续挖掘中的处理，并帮助企业及用户更有效的挖掘需要的数据。</p></sec><sec id="s9"><title>基金项目</title><p>本项目得到网络文化与数字传播北京市重点实验室开放课题资助；促进高校内涵发展–科研水平提高项目(5221823410)资助。</p></sec><sec id="s10"><title>文章引用</title><p>刘城霞,朱敏玲. 基于信息熵的离散化算法的研究与实现The Study and Implementation of Discretization Algorithm Based on Information Entropy[J]. 软件工程与应用, 2019, 08(06): 358-363. https://doi.org/10.12677/SEA.2019.86044</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.33541-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">侯利娟, 王国胤, 聂能, 等. 粗糙集理论中的离散化问题[J]. 计算机科学, 2000, 27(12): 89-94.</mixed-citation></ref><ref id="hanspub.33541-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Shannon, C.E. (1948) A mathematical theory of communication. The Bell System Technical Journal, 27, 379-423. &lt;br&gt;https://doi.org/10.1002/j.1538-7305.1948.tb00917.x</mixed-citation></ref><ref id="hanspub.33541-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Fayyad, U.M. and Irani, K.B. (1993) Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning. Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI-93), Chambèry, 28 August-3 September 1993, 1022-1027.</mixed-citation></ref><ref id="hanspub.33541-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">谢宏, 程浩忠, 牛东晓. 基于信息熵的粗糙集连续属性离散化算法[J]. 计算机学报, 2005, 28(9): 1570-1574.</mixed-citation></ref><ref id="hanspub.33541-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">高建国, 崔业勤. 基于信息熵理论的连续属性离散化方法[J]. 微电子学与计算机, 2011, 28(7): 187-189.</mixed-citation></ref><ref id="hanspub.33541-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">刘业政, 焦宁, 姜元春. 连续属性离散化算法比较研究[J]. 计算机应用研究, 2007 , 24(9): 28-30+33.</mixed-citation></ref></ref-list></back></article>