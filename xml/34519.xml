<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.103047</article-id><article-id pub-id-type="publisher-id">CSA-34519</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200300000_31253361.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于生成对抗网络的图标形状生成
  Icon Shape Generation Based on Generative Adversarial Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>赵</surname><given-names>璁</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>平华</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>28</day><month>02</month><year>2020</year></pub-date><volume>10</volume><issue>03</issue><fpage>456</fpage><lpage>463</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对实际中的图标设计流程过于漫长和复杂的问题，参考实际图标设计流程提出了基于生成对抗网络模型的图标生成模型。首先，将用户所给出的需求转化为条件特征与噪声数据一起传入生成器中生成图标，接着将生成的图标与真实的图标一起输入鉴别器中，鉴别器鉴别图标的真假以及图标是否满足需求特征，最后二者经过迭代训练达到平衡，能够生成以假乱真的且符合需求的图标。实验表明，这种模型能够快速地生成多样的图标，且生成的图标能够满足指定的需求。 Aiming at the problem that the icon design process is very long and complicated in the real world, with the reference of actual icon design process, an icon generation model based on the generative adversarial network model is proposed based on the actual icon design process. First, the requirements given by the user are converted into conditional features and passed into the generator to generate icons together with the noise data, and then the generated icons and the real icons are the input of the discriminator. The discriminator evaluates the icon is real or not and the icon meets the required features. Finally, the generator and discriminator come to a balanced state through the iterative training. The experiment shows that the model can quickly generate a variety of icons and the icons can meet the requirements given by user. 
  
 
</p></abstract><kwd-group><kwd>生成对抗网络，深度学习，图标，图像生成, Generative Adversarial Network</kwd><kwd> Deep Learning</kwd><kwd> Icon</kwd><kwd> Image Generation</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于生成对抗网络的图标形状生成<sup> </sup></title><p>赵璁，陈平华</p><p>广东工业大学，广东 广州</p><p>收稿日期：2020年2月20日；录用日期：2020年3月6日；发布日期：2020年3月13日</p><disp-formula id="hanspub.34519-formula47"><graphic xlink:href="//html.hanspub.org/file/8-1541699x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>针对实际中的图标设计流程过于漫长和复杂的问题，参考实际图标设计流程提出了基于生成对抗网络模型的图标生成模型。首先，将用户所给出的需求转化为条件特征与噪声数据一起传入生成器中生成图标，接着将生成的图标与真实的图标一起输入鉴别器中，鉴别器鉴别图标的真假以及图标是否满足需求特征，最后二者经过迭代训练达到平衡，能够生成以假乱真的且符合需求的图标。实验表明，这种模型能够快速地生成多样的图标，且生成的图标能够满足指定的需求。</p><p>关键词 :生成对抗网络，深度学习，图标，图像生成</p><disp-formula id="hanspub.34519-formula48"><graphic xlink:href="//html.hanspub.org/file/8-1541699x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/8-1541699x7_hanspub.png" /> <img src="//html.hanspub.org/file/8-1541699x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>设计一个图标是一个漫长而复杂的过程，需要用户和设计者的共同协作来完成。许多用户没有相关的专业背景，特别是对于设计内容没有特定想法的用户，一般通过自身的主观感受来评判设计的好坏，因此设计者需要对自己的设计进行不断地修改以满足用户的各种需求，在进行了多次循环之后，最终匹配了用户的需求。这样的工作流程消耗了双方大量的时间和精力，而且成本较高。因此本文的目标在于使用更加智能的方法帮助设计者和用户更好地来设计出更好的图标，加快图标设计的流程，提升工作效率。用户只需向预设的模型输入相关的特征参数，即可得到所需的图标原型，即使这种图标原型无法完美地满足用户需求，设计师可以通过这种图标原型了解客户的大致想法，在此基础上进行进一步的完善。</p><p>要想设计这样一个模型我们需要理解实际中设计图标的工作流程，一般来说，用户首先对设计者提出自己的需求，例如需要一个什么形状和结构的图标，设计者通过这种需求提炼出设计要点，根据设计要点创作出一些设计原型，反馈给用户进行评估，用户提出意见和建议，设计者再进行修改和完善，通过不断地改进最终满足用户的需求。因此，模型的输入是用户的设计需求特征，输出是满足用户需求特征的图标，并且模型需要能够循环地完善和改进以满足用户的需求。另外用户一般需要较多的图标来进行选择，因此模型所生成的图标不能单一化，需要各具特点，根据用户的特定需求而变化，给用户更多的选择空间。</p><p>目前，基于神经网络的生成模型正好契合这种图标生成的任务。当前的生成模型主流研究热点集中在变分自编码器(VAE) [<xref ref-type="bibr" rid="hanspub.34519-ref1">1</xref>] 和生成对抗网络(GAN) [<xref ref-type="bibr" rid="hanspub.34519-ref2">2</xref>] 两种神经网络模型上，这两个模型都可以生成出图像数据，但是变分自编码器对于图像的输出往往较为模糊，因此不适合本任务。而生成对抗网络是由生成器和鉴别器两个神经网络组成，分别对应设计者和用户两个角色，他们以相对的目标以对抗的方式同时进行训练，通过循环的训练最终达到一种平衡，因此这种模型十分契合图标设计的流程。</p><p>生成对抗网络是一种由Ian Goodfellow等人于2014年首次提出的神经网络模型，是一种十分流行的生成模型，它在多个领域都有相应的应用，从手写数字生成 [<xref ref-type="bibr" rid="hanspub.34519-ref1">1</xref>] 到漫画头像 [<xref ref-type="bibr" rid="hanspub.34519-ref3">3</xref>]，以及以假乱真的名人头像 [<xref ref-type="bibr" rid="hanspub.34519-ref4">4</xref>]。尽管生成对抗网络已经用于生成很多数据，但是对于图标的生成仍未得到深入的研究，而生成对抗网络的训练流程与实际的图标设计流程极为相似，因此不难得出对于图标的生成任务生成对抗网络也能发挥其作用。生成对抗网络分别由生成器和鉴别器两个神经网络模型合并组成，生成器负责通过神经网络模仿真实数据的分布来生成以假乱真的数据，这种假的数据和真实数据一起输入到鉴别器中，鉴别器通过其鉴别神经网络来分别输入数据的真假，两个模型通过轮流训练最终达到一种平衡 [<xref ref-type="bibr" rid="hanspub.34519-ref5">5</xref>]，生成器能够生成出无法分别真假的图片，鉴别器对于输入数据的真假的可能性相等。</p><p>原始的生成对抗网络在遇到较为复杂的数据时，其训练过程会变得很不稳定，因此文献 [<xref ref-type="bibr" rid="hanspub.34519-ref6">6</xref>] 在原始生成对抗网络的基础上提出了新的结构体系和训练方法，例如使用深层卷积层和修改其损失函数 [<xref ref-type="bibr" rid="hanspub.34519-ref7">7</xref>]。即使是在改进了结构之后，生成对抗网络仍然无法满足较为复杂的任务，对于简单的手写数字的生成或者较低分辨率的图像上，其效果较好，但是对于具有约束条件的情形，这样的模型就无法对应了，因此文献 [<xref ref-type="bibr" rid="hanspub.34519-ref8">8</xref>] 提出了条件生成对抗网络(CGAN, Conditional Generative Adversarial Nets)，这是一种带有条件约束的生成对抗网络，在模型中引入约束条件变量来控制数据的生成方向，是模型能够像用户所偏好的方向进行训练。文献 [<xref ref-type="bibr" rid="hanspub.34519-ref9">9</xref>] 提出的辅助条件生成对抗网络(ACGAN, Auxiliary Classifier Generative Adversarial Nets)在条件生成对抗网络的基础上更进一步，不仅在生成过程中加入约束条件变量，并且在鉴别网络中也加入了对约束条件变量的识别，提升了模型的定向生成与鉴别能力，使得生成出的图像更加有针对性。</p><p>对于图标生成的任务，ACGAN这种模型十分具有启发性，因此在ACGAN模型的基础上提出一种新的图标的生成方法，首先用户向模型输入需求信息，解析成模型所需的条件特征，将条件特征与噪声矢量输入到生成器中进行生成训练，其中噪声矢量的作用是保证生成出的图像的多样性，接着将生成出的图标与真实图标混合后输入到鉴别网络中训练，鉴别网络需要鉴别出图像的真假及其条件信息，将鉴别出的条件特征与输入的条件特征比对后得出条件损失，通过神经网络优化器迭代训练降低模型损失最后达到稳定。这种模型所生成的图标具有独特性，同一条件特征不同批次所生成的图标都会不同，其次所生成的图标更加满足用户的需求，模型的训练会不断迭代以满足用户给出的条件，因此这种模型可以更加贴合用户的需要，接着是这样图标生成方法无需像传统图标生成方法要保存庞大的素材库，只需保存生成器所产生的模型，最后本文是将基于形状特征的生成对抗网络应用于图标的生成上，提升了图标生成的效率和效果。</p></sec><sec id="s4"><title>2. 技术细节</title><sec id="s4_1"><title>2.1. 模型的选择与搭建</title><p>图标的生成模型需要根据实际的图标设计流程来选择和搭建，实际中的图标设计流程一般是从用户需求开始，抽象化用户所给出的需求，提炼出设计要点，根据要点结合设计师的创意设计得出图标的草图，用户对草图做出点评和意见，用户根据用户的意见做出修改和完善，经过循环的修改最后的出用户满意的图标，该流程可以用图1表示。</p><p>图1. 图标设计流程图</p><p>需要将这样的设计流程抽象化成为一个模型来进行设计，首先从用户需求入手，用户的需求一般是文字性的，将文字性需求的提炼出需求条件特征，将这种条件特征与噪声数据输入到生成器中，得出生成的图标，再将图标输入到鉴别器中，鉴别图标是否满足用户的需求，通过循环训练，最终得到可以生成出使用户满意的模型，该模型可以抽象为图2。</p><p>图2. 图标生成模型图</p><p>根据模型的概要设计，需要对已有的模型进行探索，生成对抗网络是一种十分契合的神经网络模型，生成对抗网络的原理是将一个服从概率分布(P<sub>z</sub>)的噪声矢量(z)输入到生成网络(G)中进行数据生成，再将假的数据(G(z))以及真实数据(x)输入到鉴别网络中进行鉴别，二者交替训练，最后到达平衡。生成对抗网络的目标函数定义为，需要使生成网络最小化，鉴别网络最大化，其公式和结构图3如下。</p><p>min G max D V ( D , G ) = E x ~ P d a t a ( x ) [ log D ( x ) ] + E z ~ P z ( z ) [ log ( 1 − D ( G ( z ) ) ) ] (1)</p><p>图3. 生成对抗网络模型图</p><p>这种原始的生成对抗网络对于具有生成条件限制的图标生成任务显然难以完成，因此条件生成对抗网络(CGAN)就解决了这一问题，在生成对抗网络的基础上加上的条件特征，使得生成方向得到了控制，其原理是在模型中加入了条件信息y，其公式和结构图4如下。</p><p>min G max D V ( D , G ) = E x ~ P d a t a ( x ) [ log D ( x | y ) ] + E z ~ P z ( z ) [ log ( 1 − D ( G ( z | y ) ) ) ] (2)</p><p>图4. 条件生成对抗网络模型图</p><p>辅助条件生成对抗网络(ACGAN)则在条件生成对抗网络的基础上使用了条件特征的信息，其中鉴别网络不仅需要鉴别图像的真假，还需要对图像进行分类，并鉴别图像的类别是否与给出的条件特征一致。在生成网络的输入端输入噪声矢量(z)以及需要服从概率分别的条件类别标签 c ~ P c ，鉴别器需要鉴别图像真假以及条件类别标签，因此辅助条件生成对抗网络的目标函数分为两个部分，首先是图像的真假 L Source ACGAN ，以及图像的标签 L Class ACGAN ，生成器需要最大化 L Class ACGAN − L Source ACGAN ，鉴别器需要最大化 L Source ACGAN + L Class ACGAN ，二者的公式和结构图5分别为：</p><p>L Source ACGAN = E [ log P ( S = r e a l | X r e a l ) ] + E [ log P ( S = f a k e | X f a k e ) ] (3)</p><p>L Class ACGAN = E [ log P ( C = c | X r e a l ) ] + E [ log P ( C = c | X f a k e ) ] (4)</p><p>图5. 辅助条件生成对抗网络模型图</p><p>图标生成模型的搭建基于实际的图标设计流程结合辅助条件生成对抗网络模型最终形成本文的图标生成模型，整个模型就由特征提取模块、图标生成模块、图标鉴别模块三个模块组成，如图6所示。</p><p>图6. 图标生成模型图</p><p>用户首先给出需求需要一个五角星，特征提取模型提取出图标条件特征c为五角星，与噪声矢量z一同输入到生成网络G中生成图标G(z)，G生成的图标与符合条件c的真实图标X一起输入到鉴别网络D中，鉴别网络鉴别出输入的图标是真实还是生成的，以及所输入的图标是否满足条件特征五角星。通过循环训练最后达到平衡，其中生成模型可以很好的生成满足条件的图标，鉴别网络可以鉴别图标的真假以及类别。</p></sec><sec id="s4_2"><title>2.2. 模型训练与评估</title><p>在选择和搭建了图标生成模型之后，需要确定模型的具体训练方法以及模型评估方法。根据原始的生成对抗网络模型主要有以下训练步骤：首先初始化创建生成网络模型(G)以及对抗网络模型(D)，然后将二者合并成联合训练模型，之后开始正式训练模型，先训练对抗模型然后在训练联合模型，然后对生成对抗网络模型模型进行评估，重复训练之后达到终止条件结束训练。</p><p>由于文献的WGAN-GP [<xref ref-type="bibr" rid="hanspub.34519-ref10">10</xref>] 模型提供了更加稳定的Wasserstein距离损失函数来评估模型的优劣，因此本文也采用WGAN-GP来进行模型的训练，同时也将原始的ACGAN模型的损失函数改为以下函数：</p><p>L D WGANGP = − E x ~ p d [ D ( x ) ] + E x ^ ~ p g [ D ( x ^ ) ] + λ E x ~ p g [ ( ‖ ∇ D ( α x + ( 1 − α x ^ ) ) ‖ 2 − 1 ) 2 ] (5)</p><disp-formula id="hanspub.34519-formula49"><label>(6)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/8-1541699x25_hanspub.png"  xlink:type="simple"/></disp-formula><p>L Q WGANGP = E x , y [ log Q ( y | x ) ] (7)</p><p>其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/8-1541699x27_hanspub.png" xlink:type="simple"/></inline-formula>表示鉴别器的损失函数， L G WGANGP 表示生成器的损失函数， L Q WGANGP 表示鉴别标签类别的损失函数。为了防止训练过程不稳定以及模型崩溃，参考文献 [<xref ref-type="bibr" rid="hanspub.34519-ref11">11</xref>]，使用Batch normalization正则化，以及输入的噪声变量是服从高斯分布。</p></sec></sec><sec id="s5"><title>3. 实验结果与分析</title><sec id="s5_1"><title>3.1. 实验细节</title><p>实验采用Ubuntu16.04系统，Tensorflow [<xref ref-type="bibr" rid="hanspub.34519-ref12">12</xref>] 机器学习框架，编程语言Python3.6，采取了来自互联网采集的10,000张50个形状类别的图标数据作为原始数据，图标的大小为128 * 128像素，进行图标生成实验，数据集被分为训练集和测试集两部分，训练集有7000张图片，测试集有3000张图片。</p><p>本文的实验使用了上文提到的图标生成网络模型，网络模型参数如表1所示，实验中为了获得更好更快的训练效果，训练过程的主要实验参数设置为：使用Adam [<xref ref-type="bibr" rid="hanspub.34519-ref13">13</xref>] 优化函数，其学习率为0.0002，Adam函数的β参数设为0.5，训练的批参数设置为100，噪声变量的长度设置为100，在鉴别器鉴别图像的真假使用binary cross entropy二分类损失函数，鉴别图像的类别时使用categorical cross entropy多类对数损失函数。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Icon generate network parameters tabl</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >层类型</th><th align="center" valign="middle" >卷积核</th><th align="center" valign="middle" >步长</th><th align="center" valign="middle" >深度</th><th align="center" valign="middle" >归一化</th><th align="center" valign="middle" >Dropout</th><th align="center" valign="middle" >激活函数</th></tr></thead><tr><td align="center" valign="middle" >生成器Input：150 * 1 * 1</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >Linear</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >768</td><td align="center" valign="middle" >否</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >ReLU</td></tr><tr><td align="center" valign="middle" >Transposed Convolution</td><td align="center" valign="middle" >5 &#215; 5</td><td align="center" valign="middle" >2 &#215; 2</td><td align="center" valign="middle" >384</td><td align="center" valign="middle" >是</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >ReLU</td></tr><tr><td align="center" valign="middle" >Transposed Convolution</td><td align="center" valign="middle" >5 &#215; 5</td><td align="center" valign="middle" >2 &#215; 2</td><td align="center" valign="middle" >256</td><td align="center" valign="middle" >是</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >ReLU</td></tr><tr><td align="center" valign="middle" >Transposed Convolution</td><td align="center" valign="middle" >5 &#215; 5</td><td align="center" valign="middle" >2 &#215; 2</td><td align="center" valign="middle" >192</td><td align="center" valign="middle" >是</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >ReLU</td></tr><tr><td align="center" valign="middle" >Transposed Convolution</td><td align="center" valign="middle" >5 &#215; 5</td><td align="center" valign="middle" >2 &#215; 2</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >否</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >Tanh</td></tr><tr><td align="center" valign="middle" >鉴别器Input：128 * 128 * 3</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >Convolution</td><td align="center" valign="middle" >3 &#215; 3</td><td align="center" valign="middle" >2 &#215; 2</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >否</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >Leaky ReLU</td></tr><tr><td align="center" valign="middle" >Convolution</td><td align="center" valign="middle" >3 &#215; 3</td><td align="center" valign="middle" >1 &#215; 1</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >是</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >Leaky ReLU</td></tr><tr><td align="center" valign="middle" >Convolution</td><td align="center" valign="middle" >3 &#215; 3</td><td align="center" valign="middle" >2 &#215; 2</td><td align="center" valign="middle" >64</td><td align="center" valign="middle" >是</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >Leaky ReLU</td></tr><tr><td align="center" valign="middle" >Convolution</td><td align="center" valign="middle" >3 &#215; 3</td><td align="center" valign="middle" >1 &#215; 1</td><td align="center" valign="middle" >128</td><td align="center" valign="middle" >是</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >Leaky ReLU</td></tr><tr><td align="center" valign="middle" >Convolution</td><td align="center" valign="middle" >3 &#215; 3</td><td align="center" valign="middle" >2 &#215; 2</td><td align="center" valign="middle" >256</td><td align="center" valign="middle" >是</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >Leaky ReLU</td></tr><tr><td align="center" valign="middle" >Convolution</td><td align="center" valign="middle" >3 &#215; 3</td><td align="center" valign="middle" >1 &#215; 1</td><td align="center" valign="middle" >512</td><td align="center" valign="middle" >是</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >Leaky ReLU</td></tr><tr><td align="center" valign="middle" >Linear</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >11</td><td align="center" valign="middle" >否</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >Soft Sigmoid</td></tr></tbody></table></table-wrap><p>表1. 图标生成网络参数表</p></sec><sec id="s5_2"><title>3.2. 实验结果</title><p>通过分析现有的生成对抗网络模型，结合实际的图标设计模式，如图7所示，经过100轮的循环训练结果可以看出，本文所提出的图标生成模型从第50轮已经可以分辨一些图标的形状了，到100轮可以清晰的体现各类图标的形状，与图8所示的真实图标十分相近，并且每一个图标的形状都各有特色，生成的多样性可以得到保证。而传统的模型由于其过于复杂，并且没有根据图标设计的实际来进行设计，想要一步到位的生成彩色的图标，其生成出的图形尽管能够一定程度根据颜色来进行生成，但是却无法分辨图标的形状，也就跟无从判别其图标的用途。</p><p>而本文的图标可以很清晰的表达出其功能和用途，并且可以使用训练好的模型生成大批量的图标来进行选择，使得设计者与用户能够有更多的选择和替代空间，并且可以随意的更改生成的类别，根据用户的要求来进行生成。</p><p>图7. 生成模型生成的图标</p><p>图8. 数据集中真实图标</p></sec></sec><sec id="s6"><title>4. 总结与展望</title><p>本文提出了一种新的基于条件生成对抗网络的图标生成模型，能够基于给出的形状类别标签生成出相应的图标。通过实验表明，模型相比于传统的生成对抗网络模型生成的图标更加清晰，图像也更具多样性。本模型未来的工作主要是将所生成的图标加上颜色等多种特征，进一步简化设计者的工作。</p></sec><sec id="s7"><title>基金项目</title><p>广东省科技计划项目2017B030307002、2019B101001021；广州市天河区科技计划项目201703YG029。</p></sec><sec id="s8"><title>文章引用</title><p>赵 璁,陈平华. 基于生成对抗网络的图标形状生成Icon Shape Generation Based on Generative Adversarial Network[J]. 计算机科学与应用, 2020, 10(03): 456-463. https://doi.org/10.12677/CSA.2020.103047</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.34519-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Doersch, C. (2016) Tutorial on Variational Autoencoders. arXiv preprint arXiv:1606.05908</mixed-citation></ref><ref id="hanspub.34519-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Goodfellow, I., Pouget-Abadie, J., Mirza, M., et al. (2014) Generative Adversarial Nets. Advances in Neural Information Processing Systems, 2014, 2672-2680.</mixed-citation></ref><ref id="hanspub.34519-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Jin, Y.H., et al. (2017) Towards the Automatic Anime Characters Creation with Generative Adversarial Networks. arXiv preprint arXiv:1708.05509</mixed-citation></ref><ref id="hanspub.34519-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">许哲豪, 陈玮. 基于生成对抗网络的图片风格迁移[J]. 软件导刊, 2018, 17(6): 207-209+212+228.</mixed-citation></ref><ref id="hanspub.34519-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Nash, M.J.F., et al. (1950) Equilibrium Points in n-Person Games. Proceedings of the National Academy of Sciences, 36, 48-49. &lt;br&gt;https://doi.org/10.1073/pnas.36.1.48</mixed-citation></ref><ref id="hanspub.34519-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Gulrajani, I., Ahmed, F., Arjovsky, M., et al. (2017) Improved Training of Wasserstein GANs.  .</mixed-citation></ref><ref id="hanspub.34519-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Radford, A., Metz, L. and Chintala, S. (2015) Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arXiv preprint arXiv:1511.06434</mixed-citation></ref><ref id="hanspub.34519-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Mirza, M. and Osindero, S. (2014) Conditional Generative Adversarial Nets. arXiv preprint arXiv:1411.1784</mixed-citation></ref><ref id="hanspub.34519-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Odena, A., Chris-topher, O. and Jonathon, S. (2016) Conditional Image Synthesis with Auxiliary Classifier Gans. arXiv preprint arXiv:1610.09585</mixed-citation></ref><ref id="hanspub.34519-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Wu, H., Zheng, S., Zhang, J., et al. (2017) GP-GAN: Towards Realistic High-Resolution Im-age Blending.  .</mixed-citation></ref><ref id="hanspub.34519-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Ioffe, S. and Szegedy, C. (2015) Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv preprint arXiv:1502.03167</mixed-citation></ref><ref id="hanspub.34519-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Abadi, M., Barham, P., Chen, J., et al. (2016) Tensorflow: A System for Large-Scale Machine Learning. 12th Symposium on Operating Systems Design and Imple-mentation, 2016, 265-283.</mixed-citation></ref><ref id="hanspub.34519-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Kingma, D.P. and Ba, J. (2014) Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980</mixed-citation></ref></ref-list></back></article>