<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CES</journal-id><journal-title-group><journal-title>Creative Education Studies</journal-title></journal-title-group><issn pub-type="epub">2331-799X</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CES.2018.64047</article-id><article-id pub-id-type="publisher-id">CES-26380</article-id><article-categories><subj-group subj-group-type="heading"><subject>CES20180400000_84232606.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>人文社科</subject></subj-group></article-categories><title-group><article-title>
 
 
  面向慕课的情绪识别系统
  A MOOC-Oriented Emotion Recognition System
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>徐</surname><given-names>姜琴</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>永锋</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>厦门大学外国语学院，福建 厦门</addr-line></aff><aff id="aff3"><addr-line>厦门亚伯锋天科技有限公司，福建 厦门</addr-line></aff><aff id="aff2"><addr-line>null</addr-line></aff><author-notes><corresp id="cor1">* E-mail:<email>xujiangqin@xmu.edu.cn(徐姜)</email>;</corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>08</month><year>2018</year></pub-date><volume>06</volume><issue>04</issue><fpage>299</fpage><lpage>305</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   准确而快速地发现学习者的情绪变化，对提高慕课的教学质量具有极为重要的价值。然而面向慕课的情绪识别工具必须解决鲁棒性和实时性这两个关键问题。在这项研究中，我们提出了使用Computer Unified Device Architecture (CUDA)技术来对深度时空推理网络进行加速，从而快速而准确的识别学习者的面部情绪状态。我们利用添加不同噪声的AR数据来测试了我们的方法，并将结果同其他深度学习方法进行了对比。实验结果证明了我们的方法的有效性。 Accurate and rapid detect changes of learner’s emotional state is of great importance to improve the teaching quality of Massive Open Online Courses (MOOC). However, the emotion recognition tools for MOOC must solve the two key issues: robustness and real-time. In this study, we proposed a deep learning approach which is based on the Computer Unified Device Architecture (CUDA) technology, called CUDA-DeSTIN, to quickly and accurately identify the learner’s facial emotional state. We tested our method using AR data with different noise, and compared the results with other deep learning methods. The experimental results prove the effectiveness of our method.  
  
 
</p></abstract><kwd-group><kwd>慕课，情绪识别，人工智能, MOOC</kwd><kwd> Emotion Recognition</kwd><kwd> Artificial Intelligence</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>面向慕课的情绪识别系统 <sup> </sup></title><p>徐姜琴<sup>1</sup>，张永锋<sup>2</sup></p><p><sup>1</sup>厦门大学外国语学院，福建 厦门</p><p><sup>2</sup>厦门亚伯锋天科技有限公司，福建 厦门</p><p><img src="//html.hanspub.org/file/9-3090459x1_hanspub.png" /></p><p>收稿日期：2018年7月24日；录用日期：2018年8月6日；发布日期：2018年8月13日</p><disp-formula id="hanspub.26380-formula65"><graphic xlink:href="//html.hanspub.org/file/9-3090459x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>准确而快速地发现学习者的情绪变化，对提高慕课的教学质量具有极为重要的价值。然而面向慕课的情绪识别工具必须解决鲁棒性和实时性这两个关键问题。在这项研究中，我们提出了使用Computer Unified Device Architecture (CUDA)技术来对深度时空推理网络进行加速，从而快速而准确的识别学习者的面部情绪状态。我们利用添加不同噪声的AR数据来测试了我们的方法，并将结果同其他深度学习方法进行了对比。实验结果证明了我们的方法的有效性。</p><p>关键词 :慕课，情绪识别，人工智能</p><disp-formula id="hanspub.26380-formula66"><graphic xlink:href="//html.hanspub.org/file/9-3090459x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2018 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/9-3090459x7_hanspub.png" /> <img src="//html.hanspub.org/file/9-3090459x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>近年来，随着大规模开放网络课程(Massive Open Online Courses, MOOC，以下简称慕课) [<xref ref-type="bibr" rid="hanspub.26380-ref1">1</xref>] 和其它类型的E-learning方法的兴起，越来越多的知识交互活动，借由这些新的形式开展。如何借助新的技术手段，来提高相应的教学活动质量，具有重要的研究价值，同时也极具挑战。较传统的教学手段而言 [<xref ref-type="bibr" rid="hanspub.26380-ref2">2</xref>] ，这类新兴的技术手段有很多优势，但与此同时也存在着一些新特性，需要加以认真考虑 [<xref ref-type="bibr" rid="hanspub.26380-ref3">3</xref>] 。例如，授课教师往往与学生不在同一地点，从而使得教授者难以及时发现而如何加强教学活动中的交互质量，是评价慕课是否真正成功的重要依据。</p><p>众多已有的研究已经表明，情绪对学习的效果 [<xref ref-type="bibr" rid="hanspub.26380-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.26380-ref5">5</xref>] 有着至关重要的影响。这也意味着，学习者的情绪会严重影响其知识掌握程度和总体学习目标。因此，对于以慕课为代表的E-Learning来说，有效教学活动的前提条件之一是教师能够快速响应学生的情绪变化。然而，如何在复杂的学习环境中准确、快速地理解学生的情感互动仍然是一项非常具有挑战性的任务。</p><p>利用人工智能技术来提高教学质量的需求吸引了相当多的研究者的兴趣 [<xref ref-type="bibr" rid="hanspub.26380-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.26380-ref7">7</xref>] 。在许多尝试中，使用新的模式识别技术来检测学习者情绪变化的方法被认为是一种有效的方法。例如，在 [<xref ref-type="bibr" rid="hanspub.26380-ref6">6</xref>] 中，我们提出了一种基于深度学习的方法来检测学习者面部表情，从而判断其情绪的变化。在 [<xref ref-type="bibr" rid="hanspub.26380-ref7">7</xref>] 中，作者提出了一种基于AdaBoost的人眼状态检测方法，并使用该方法来判断学习者的情绪状态。AutoTutor [<xref ref-type="bibr" rid="hanspub.26380-ref8">8</xref>] 研究项目希望检测和利用学习者的情绪，以加强学习和教学过程。在 [<xref ref-type="bibr" rid="hanspub.26380-ref9">9</xref>] 中，构造了一个神经网络体系结构，能够处理语音中的面部特征、韵律和词汇内容的融合。Yau等人在其论文 [<xref ref-type="bibr" rid="hanspub.26380-ref10">10</xref>] 中提出了一种鲁棒的人脸表情识别方法。</p><p>深度学习 [<xref ref-type="bibr" rid="hanspub.26380-ref11">11</xref>] 是近年来在人工智能领域受到极大关注的创新性技术手段，这类方法较其他模式识别方法而言，具有的最大优点是能够自动抽取出非常高质量的特征，而高质量的特征是一个成功的情感识别系统的重要组成部分。</p><p>依据我们已有的研究成果，EM-DeSTIN [<xref ref-type="bibr" rid="hanspub.26380-ref12">12</xref>] 方法在处理噪声图像时非常有效，而在E-Learning系统中获得的图像通常包含噪声，因此，使用EM-DeSTIN方法能够帮助我们准确地识别特定场景下参与者的情绪。但是，在交互学习的过程中，我们往往要求情绪识别系统具有较好的实时性，换句话说，现实世界中的E-Learning要求系统必须能够较快的识别出学习者的情绪变化。而现有的深度学习方法，尤其是深度时空推理网络，往往需要很长的训练时间。</p><p>因此，在本文中，我们利用了一种基于CUDA架构的深度时空推理网络 [<xref ref-type="bibr" rid="hanspub.26380-ref13">13</xref>] ，CUDA-DeSTIN，来构建面向E-Learning环境的情感识别系统。该系统的特点在于，它不仅能够在高噪声的环境中工作，同时其学习速度较现有的方法也有了较大的提升，更加适合慕课等新兴的教学方法。</p><p>本文的其余部分整理如下。在第二节中，我们将简要介绍深空推理网络(DeSTIN)和CUDA技术，第三节我们介绍基于CUDA架构的深度时空推理网络CUDA-DeSTIN。在第四节中，我们将介绍实验内容以及实验结果。最后，在第五节中，我们将给出了一个简短的结论和未来的研究方向。</p></sec><sec id="s4"><title>2. 背景知识</title><sec id="s4_1"><title>2.1. 深度时空推理网络</title><p>深度时空推理网络(Deep Spatio-Temporal Inference Network, DeSTIN) [<xref ref-type="bibr" rid="hanspub.26380-ref14">14</xref>] 是一种尝试模仿人类视觉和听觉皮层的概念结构和动力学模型。DeSTIN的特殊优势在于它具有较为出色的时间数据处理能力，这种模型能够从较少量的训练实例中学习时空特征，并与外部认知软件系统进行交互。而这种能力对于，面向慕课的系统而言，尤其重要。</p><p>如图1所示，在宏观层面上，DeSTIN是一种分层结构的模型。其每一层(Layer)都被分为若干个2 &#215; 2的区域。第N − 1层的某个2 &#215; 2的区域会作为第 N层的一个节点(Node)的输入，而第 N层的四个节点又会与第N + 1层的一个特定节点相连。在DeSTIN网络的最底层，原始的图像数据会直接输入。可以将 DeSTIN网络中的每一层都理解为原始输入数据在特定层次上的抽象。在中间层每一节点都包着数量聚类中心(Centrino)，而节点可以通过无监督学习产生与到本层的观察值特征。</p><p>在这样的结构中，每个节点输出在其所在层的信念状态，这些信念状态会包含输入数据中存在的空间和时间规律。该系统顶层的输出可以作为有监督学习算法(如神经网络，支持向量机(Support Vector Machine))的输入，从而进行有效的模式分类。</p><p>在图1所示的例子中，数字“9”被直接输入第四层(Layer 4)，第四层是该系统的底层，包含64 = 8 &#215; 8个节点，该层接受原始图像的像素作为输入，每个节点对应一个的4 &#215; 4像素区域，每个节点对应的</p><p>图1. DeSTIN结构示意图</p><p>像素区域不重叠，每个4 &#215; 4的像素区域按照行优先的方式存储成一个16维的列向量，这个16维的列向量会作为该层输入数据。第四层的上一层是第三层(Layer 3)，第三层包含16 = 4 &#215; 4个节点，每一个节点接受第四层相对应的4 = 2 &#215; 2个节点的输出作为输入，每个节点的对应的区域也是不重叠的。第三层的上一层是第二层(Layer 2)，第二层包含4 = 2 &#215; 2个节点，每一个节点接受第三层相对应的4 = 2 &#215; 2个节点的输出作为输入，同样每个节点的对应的区域也是不重叠的。第一层(Layer 1)是该系统的顶层，它包含1 = 1 &#215; 1个节点，该节点接受第二层4 = 2 &#215; 2个节点的输出作为输入。该节点输出也就是DESTIN的输出是一个10维的向量，该向量可以作为有监督学习算法(如神经网络，支持向量机)的输入，从而进行有效的模式分类。</p></sec><sec id="s4_2"><title>2.2. CUDA：可伸缩并行编程模型</title><p>CUDA (Computer Unified Device Architecture) [<xref ref-type="bibr" rid="hanspub.26380-ref15">15</xref>] 是英伟达(NVIDIA)公司开发的一种并行计算架构 [<xref ref-type="bibr" rid="hanspub.26380-ref15">15</xref>] 。这种架构的主要出发点在于，随着技术不断发展，GPU (Graphic Processing Unit)变的越来越强大，对于某些特定的计算任务，其能力已经远远超越了通用的CPU。在此情形下，CPU与GPU“协同处理”模式能够为我们带来更加高效的计算能力。</p><p>基于此，英伟达(NVIDIA)发明了CUDA并行计算架构。该架构通过利用GPU超强的处理能力，可以大幅度提升计算性能。CUDA采用C语言作为编程语言提供大量的高性能计算指令开发能力，使开发者能够在GPU的强大计算能力的基础上建立起一种效率更高的密集数据计算解决方案。</p><p>在CUDA的架构下，一个程序分为两个部份：宿主(host)端和设备(device)端 [<xref ref-type="bibr" rid="hanspub.26380-ref15">15</xref>] 。宿主(Host)端是指在CPU上执行的部份，而设备(device)端则是在显示芯片上执行的部份。设备(device)端的程序又称为“kernel (核心)”。通常宿主(host)端程序会将数据准备好后，复制到显卡的内存中，再由显示芯片执行设备(device)端程序，完成后再由宿主(host)端程序将结果从显卡的内存中取回。不同类型的代码由于其运行的物理位置不同，能够访问到的资源不同，因此对应的运行期组件也分为公共组件、宿主组件和设备组件三个部分。更多有关CUDA的细节，读者可以参阅 [<xref ref-type="bibr" rid="hanspub.26380-ref15">15</xref>] 。</p></sec></sec><sec id="s5"><title>3. CUDA-DESTIN</title><p>在这一节的前半部分，我们简要的介绍我们所提出的CUDA-DeSTIN [<xref ref-type="bibr" rid="hanspub.26380-ref13">13</xref>] 的技术细节。然后，我们讨论，如何利用这种方法来进行面部表情的情绪识别。</p><sec id="s5_1"><title>3.1. CUDA-DeSTIN</title><p>简要地将，CUDA-DeSTIN就是利用CUDA的架构下，重新实现了DeSTIN，从而使得训练速度得到了极大的提升。</p><p>有几点需要说明的是，在CUDA-DeSTIN中，聚类中心的数据存储到全局内存(global memory)中，每一个线程在计算相似度时会用到该数据。DeSTIN中的一个节点(node)对应一个区块(block)，每一个区块有一个共享(shared memory)，其大小为16 kilobytes。第四层(Layer 4)中的每个节点的输入大小为16 &#215; 4 = 64 bytes，第三层(Layer 3)中的每个节点的输入大小为100 &#215; 4 = 400 bytes，第二层(Layer 2)中的每个节点的输入大小为64 &#215; 4 = 256 bytes，第一层(Layer 1)中的每个node的输入大小为48 &#215; 4 = 192 bytes。</p><p>每个区块(block)中的每一线程(thread)等同于CUDA-DeSIN中的一个聚类中心(centroid)。线程的数据，也就是聚类中心的每一维数据存储在全局变量(global memory)中，输入数据存储在区块(block)的共享内存(shared memory)中。然后计算相似度，并将结果存储到共享内存(shared memory)中。所有的线程计算完成之后，进入排序阶段。</p><p>在CUDA-DeSTIN中，采用的是快速排序(Quicksort)，这种方法是对冒泡排序的一种改进。由C. A. R. Hoare在1962年提出。最后优胜的聚类中心(winning centroid)会存储在全局内存(global memory)中，在更新优胜的聚类中心(winning centroid)时，该数据会被再次使用。有关CUDA-DeSTIN的更多细节可以参考文献 [<xref ref-type="bibr" rid="hanspub.26380-ref13">13</xref>] 。</p></sec><sec id="s5_2"><title>3.2. CUDA-DeSTIN的情绪识别方法</title><p>利用CUDA-DeSTIN来进行基于面部图像的情绪识别，大体可以分为训练阶段和识别阶段。训练 CUDA-DeSTIN的过程可以分为如下步骤：</p><p>1) CUDA-DeSTIN 中的每一层(除最底层外)，从下一层获得其输出的belief作为观测值。</p><p>2) 通过聚类中心更新算法得到新的聚类中心，然后依据更新后的聚类中心，计算概率分布向量Belief，并将此概率分布向量作为输出，向上传输到父层作为父层的观察值。</p><p>3) 在进行在线类聚的同时，CUDA-DeSTIN也会将父层类聚的赢者信息反馈到本层，更新本层的PSSA表，从而起到父层对子层的指导作用。</p><p>4) CUDA-DeSTIN中的每一层都按照此流程进行，在金字塔顶端的belief就是输入图像的关键特征。</p><p>5) 利用最上层的 belief作为一种有标签的训练数据，训练支持向量机(Support Vector Machine, SVM)，从而实现对相应的分类器的训练过程。</p><p>当完成了训练过程之后，我们就得到了一个能够识别不同面部情绪的分类器(一个训练好的SVM和一个训练好的CUDA-DeSTIN模型)。可以利用以下的步骤来实现识别过程：</p><p>1) 将要识别的面部图片输入到CUDA-DeSTIN中，由此获得一组Belief作为观测值。</p><p>2) 根据训练得到的类聚中心与PSSA (不需要进行类聚中心的更新)，计算概率分布向量Belief，并将Belief传输到父层作为父层的观察值。</p><p>3) 按照上述的过程，在训练好的CUDA-DeSTIN的顶部，输出的Belief就是待识别图片的特征。</p><p>4) 将第三步得到的特征，送入已训练好的分类器SVM，实现情绪识别。</p></sec></sec><sec id="s6"><title>4. 实验</title><p>我们利用了AR人脸数据集<sup>1</sup>的一个子集来进行实验。该子集由100个人组成，每个人有26个灰度图像，分别是165 &#215; 120像素和24位深度。这些图像可根据拍摄日期分成两组。我们以一组作为训练样本，另一组作为测试组。数据集的样本显示在图2中。</p><p>图2(a)中的图AR样本没有特定的情绪倾向；图2(b)表示微笑；图2(c)是生气，而是愤怒。为了模拟在慕课环境中，视频图像质量往往不高的情况，高斯噪声和盐和椒盐噪声被添加到训练样本中。对于高斯噪声，均值为0，方差分别设为0.01，0.06和0.1。对于椒盐噪音，噪音密度分别设定为0.01，0.1和0.5。</p><p>在我们的实验中，我们将CUDA-DeSTIN和DeSTIN的顶层两层输出作为输入特征来训练SVM，并使用训练的SVM对相同的测试样本进行分类分类。为了测试其他深层网络，我们选择了三种不同的方法：深信念网(DBN)，卷积神经网络(CNN)和堆栈自动编码器(SAE)等于。这三种其他方法的源代码是从DeepLearnToolbox<sup>2</sup>获得的。</p><p>表1记录不同噪音环境下的实验结果。实验表明，该算法在处理低到中等噪声水平时具有较强的竞争力，即使在噪声较大的情况下，EM-DeSTIN也具有良好的性能。</p><p>图2. AR数据库中的样本</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Emotion Recognition of CUDA-DeSTIN, uniform DeSTIN, DBN, CNN and Autoencode</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  ></th><th align="center" valign="middle"  colspan="3"  >高斯噪声</th><th align="center" valign="middle"  colspan="3"  >椒盐噪声</th></tr></thead><tr><td align="center" valign="middle" >0.01</td><td align="center" valign="middle" >0.06</td><td align="center" valign="middle" >0.1</td><td align="center" valign="middle" >0.01</td><td align="center" valign="middle" >0.1</td><td align="center" valign="middle" >0.5</td></tr><tr><td align="center" valign="middle" >DBN</td><td align="center" valign="middle" >0.35</td><td align="center" valign="middle" >0.33</td><td align="center" valign="middle" >0.30</td><td align="center" valign="middle" >0.41</td><td align="center" valign="middle" >0.38</td><td align="center" valign="middle" >0.32</td></tr><tr><td align="center" valign="middle" >CNN</td><td align="center" valign="middle" >0.43</td><td align="center" valign="middle" >0.41</td><td align="center" valign="middle" >0.40</td><td align="center" valign="middle" >0.45</td><td align="center" valign="middle" >0.42</td><td align="center" valign="middle" >0.33</td></tr><tr><td align="center" valign="middle" >SAE</td><td align="center" valign="middle" >0.41</td><td align="center" valign="middle" >0.40</td><td align="center" valign="middle" >0.38</td><td align="center" valign="middle" >0.41</td><td align="center" valign="middle" >0.39</td><td align="center" valign="middle" >0.72</td></tr><tr><td align="center" valign="middle" >DeSTIN</td><td align="center" valign="middle" >0.40</td><td align="center" valign="middle" >0.38</td><td align="center" valign="middle" >0.32</td><td align="center" valign="middle" >0.40</td><td align="center" valign="middle" >0.40</td><td align="center" valign="middle" >0.27</td></tr><tr><td align="center" valign="middle" >CUDA-DeSTIN</td><td align="center" valign="middle" >0.41</td><td align="center" valign="middle" >0.42</td><td align="center" valign="middle" >0.35</td><td align="center" valign="middle" >0.40</td><td align="center" valign="middle" >0.43</td><td align="center" valign="middle" >0.27</td></tr></tbody></table></table-wrap><p>表1. 五种不同深度系统的识别正确率</p><p>另外尤其需要说明的是，在Tesla上实现了基于CUDA-DeSTIN的面部情绪识别，训练时间只有1200秒(约0.3小时)，而DeSTIN算法在2.4 GHz的CPU上训练需要50小时，速度提高了约166倍。在识别率在优的情况下，训练速度得到了极大的提升，这一特点，对于在线学习工具而言，具有至关重要的优势。</p></sec><sec id="s7"><title>5. 总结</title><p>在本文中，我们提出利用一种CUDA加速的深度学习方法，来实现慕课教学中的情感识别。通常情况下，面向慕课的教学工具，必须考虑两个问题。首先，慕课环境下的视频图像往往具有较多的噪声，图像质量不高。其次，情绪识别必须具有较快的响应速度，也就是实时性必须较好。我们的方法的优点是在处理嘈杂的面部表情图片时，它具有较高的识别率。同时，该方法较原有的深度时空推理网络而言，训练速度得到了极大的提高。在未来，我们将进一步研究如何利用其它先进的智能方法 [<xref ref-type="bibr" rid="hanspub.26380-ref16">16</xref>] [<xref ref-type="bibr" rid="hanspub.26380-ref17">17</xref>] [<xref ref-type="bibr" rid="hanspub.26380-ref18">18</xref>] [<xref ref-type="bibr" rid="hanspub.26380-ref19">19</xref>] [<xref ref-type="bibr" rid="hanspub.26380-ref20">20</xref>] 来为高质量的慕课教学服务。</p></sec><sec id="s8"><title>致谢</title><p>感谢厦门大学信息科学与技术学院仿脑智能实验室为本研究提供了相关的代码，并协助进行了实验。</p></sec><sec id="s9"><title>基金项目</title><p>此研究受到国家自然科学基金(No. 61673328)资助。</p></sec><sec id="s10"><title>文章引用</title><p>徐姜琴,张永锋. 面向慕课的情绪识别系统 A MOOC-Oriented Emotion Recognition System[J]. 创新教育研究, 2018, 06(04): 299-305. https://doi.org/10.12677/CES.2018.64047</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.26380-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Daradoumis, T., Bassi, R., Xhafa, F., et al. (2013) A Review on Massive E-Learning (MOOC) Design, Delivery and As-sessment. 8th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC), Compiegne, 28-30 October 2013, 208-213.</mixed-citation></ref><ref id="hanspub.26380-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Xu, J.Q., Liu, F. and Jiang, M. (2008) Task-Based Language Teaching: From the Practical Perspective. 2008 International Conference on Computer Science and Software Engineering, Hubei, 12-14 December 2008, 1054-1057.</mixed-citation></ref><ref id="hanspub.26380-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Binali, H.H., Wu, C. and Potdar, V. (2009) A New Significant Area: Emotion Detection in E-Learning Using Opinion Mining Techniques. 3rd IEEE International Conference on Digital Ecosystems and Technologies, Istanbul, 1-3 June 2009, 259-264.</mixed-citation></ref><ref id="hanspub.26380-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Sylwester, R. (1994) How Emotions Affect Learning. Educational Leadership, 52, 60-65.</mixed-citation></ref><ref id="hanspub.26380-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Bower, G.H. (1992) How Might Emotions Affect Learning? The Handbook of Emotion and Memory: Research and Theory, 3, 31.</mixed-citation></ref><ref id="hanspub.26380-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Xu, J., Huang, Z., Shi, M., et al. (2017) Emotion Detection in E-learning Using Expectation-Maximization Deep Spatial-Temporal Inference Network. In: UK Workshop on Computational Intelligence, Springer, 245-252.</mixed-citation></ref><ref id="hanspub.26380-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Shen, L., Wang, M. and Shen, R. (2009) Affective E-Learning: Using “Emotional” Data to Improve Learning in Pervasive Learning Environment. Journal of Educational Technology &amp; Society, 12, 176.</mixed-citation></ref><ref id="hanspub.26380-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Graesser, A.C., Wiemer-Hastings, K., Wiemer-Hastings, P., et al. (1999) AutoTutor: A Simulation of a Human Tutor. Cognitive Systems Research, 1, 35-51. &lt;br&gt;https://doi.org/10.1016/S1389-0417(99)00005-4</mixed-citation></ref><ref id="hanspub.26380-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Cowie, R., Douglas-Cowie, E., Tsapatsoulis, N., et al. (2001) Emotion Recognition in Human-Computer Interaction. IEEE Signal Processing Magazine, 18, 32-80. &lt;br&gt;https://doi.org/10.1109/79.911197</mixed-citation></ref><ref id="hanspub.26380-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Shojaeilangari, S., Yau, W.-Y., Nandakumar, K., et al. (2015) Robust Representation and Recognition of Facial Emotions using Extreme Sparse Learning. IEEE Transactions on Image Processing, 24, 2140-2152.  
&lt;br&gt;https://doi.org/10.1109/TIP.2015.2416634</mixed-citation></ref><ref id="hanspub.26380-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">LeCun, Y., Bengio, Y. and Hinton, G. (2015) Deep Learning. Nature, 521, 436-444.  
&lt;br&gt;https://doi.org/10.1038/nature14539</mixed-citation></ref><ref id="hanspub.26380-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Jiang, M., Ding, Y., Goertzel, B., et al. (2014) Improving Machine Vision via Incorporating Expectation-Maximization into Deep Spatio-Temporal Learning. International Joint Conference on Neural Networks, Beijing, 6-11 July 2014, 1804-1811.</mixed-citation></ref><ref id="hanspub.26380-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">张永锋. 基于CUDA架构的深度时空推理网络[D]: [硕士学位论文]. 厦门: 厦门大学, 2012.</mixed-citation></ref><ref id="hanspub.26380-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Arel, I., Rose, D.C. and Coop, R. (2009) DeSTIN: A Scalable Deep Learning Architecture with Application to High-Dimensional Robust Pattern Recognition. AAAI Fall Symposium: Biologically Inspired Cognitive Architectures, Arlington, 5-7 November 2009, 11-15.</mixed-citation></ref><ref id="hanspub.26380-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">NVIDIA (2011) NVIDIA CUDA C Programming Guide. Nvidia Corporation, Santa Clara, 120, 8.</mixed-citation></ref><ref id="hanspub.26380-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Jiang, M., Huang, Z., Qiu, L., et al. (2017) Transfer Learning Based Dynamic Multiobjective Optimization Algorithms. IEEE Transactions on Evolutionary Computation, 1. &lt;br&gt;https://doi.org/10.1109/TEVC.2017.2771451</mixed-citation></ref><ref id="hanspub.26380-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Goertzel, B., De Garis, H., Pennachin, C., et al. (2010) OpenCogBot: Achieving Generally Intelligent Virtual Agent Control and Humanoid Robotics via Cognitive Synergy. Proceedings of ICAI, 10, 1-12.</mixed-citation></ref><ref id="hanspub.26380-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Jiang, M., Huang, W., Huang, Z., et al. (2017) Integration of Global and Local Metrics for Domain Adaptation Learning via Dimensionality Reduction. IEEE Transactions on Cybernetics, 47, 38-51.  
&lt;br&gt;https://doi.org/10.1109/TCYB.2015.2502483</mixed-citation></ref><ref id="hanspub.26380-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, X., Jiang, M., Zhou, C., et al. (2012) Graded BDI Models for Agent Architectures Based on Lukasiewicz Logic and Propositional Dynamic Logic. In: International Conference on Web Information Systems and Mining, Springer, Berlin, 439-450.</mixed-citation></ref><ref id="hanspub.26380-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Jiang, M., Qiu, L., Huang, Z. and Yen, G.G. (2018) Dynamic Multi-Objective Estimation of Distribution Algorithm Based on Domain Adaptation and Nonparametric Estimation. Information Sciences, 435, 203-223.  
&lt;br&gt;https://doi.org/10.1016/j.ins.2017.12.058</mixed-citation></ref></ref-list></back></article>