<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.1012247</article-id><article-id pub-id-type="publisher-id">CSA-39404</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20201200000_76182804.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  一种基于深度学习的课堂学生学习状态研究
  A Study on the Learning State of Classroom Students Based on Deep Learning
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>秋会</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>梁</surname><given-names>明秀</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>林</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>贵州民族大学数据科学与信息工程学院，贵州 贵阳</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>12</month><year>2020</year></pub-date><volume>10</volume><issue>12</issue><fpage>2339</fpage><lpage>2345</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   为了对学生课堂学习情况及教师授课情况进行客观评价，需要掌握学生在课堂上的学习状态，随着计算机视觉技术的发展，对学生课堂学生状态的分析成为可能。本文采用深度学习网络yolov3与dropblock结合对教室监控视频进行分析，检测学生在老师上课时的听课状态，实现对学生在课堂上是否专心听讲的学习状态检测。实验结果表明，通过建议方法得到的学生学习状态与实际人工观察具有很好的吻合度。 In order to objectively evaluate students’ classroom learning and teachers’ teaching, it is necessary to master students’ learning status in class. With the development of computer vision technology, it is possible to analyze students’ classroom learning status. In this paper, the deep learning network yolov3 is combined with dropblock to analyze classroom surveillance video, detect the state of students listening to teachers in class, and realize the learning state detection of whether students are paying attention in class. The experimental results show that the students’ learning status obtained by the proposed method is in good agreement with the actual artificial observation. 
  
 
</p></abstract><kwd-group><kwd>深度学习，课堂学习状态检测，Yolov3，Dropblock, Deep Learning</kwd><kwd> Classroom Learning Status Detection</kwd><kwd> Yolov3</kwd><kwd> Dropblock</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>为了对学生课堂学习情况及教师授课情况进行客观评价，需要掌握学生在课堂上的学习状态，随着计算机视觉技术的发展，对学生课堂学生状态的分析成为可能。本文采用深度学习网络yolov3与dropblock结合对教室监控视频进行分析，检测学生在老师上课时的听课状态，实现对学生在课堂上是否专心听讲的学习状态检测。实验结果表明，通过建议方法得到的学生学习状态与实际人工观察具有很好的吻合度。</p></sec><sec id="s2"><title>关键词</title><p>深度学习，课堂学习状态检测，Yolov3，Dropblock</p></sec><sec id="s3"><title>A Study on the Learning State of Classroom Students Based on Deep Learning</title><p>Qiuhui Liu, Mingxiu Liang, Lin Wang</p><p>School of Data Science and Information Engineering, Guizhou Minzu University, Guiyang Guizhou</p><p><img src="//html.hanspub.org/file/22-1541962x4_hanspub.png" /></p><p>Received: Nov. 27<sup>th</sup>, 2020; accepted: Dec. 21<sup>st</sup>, 2020; published: Dec. 28<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/22-1541962x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In order to objectively evaluate students’ classroom learning and teachers’ teaching, it is necessary to master students’ learning status in class. With the development of computer vision technology, it is possible to analyze students’ classroom learning status. In this paper, the deep learning network yolov3 is combined with dropblock to analyze classroom surveillance video, detect the state of students listening to teachers in class, and realize the learning state detection of whether students are paying attention in class. The experimental results show that the students’ learning status obtained by the proposed method is in good agreement with the actual artificial observation.</p><p>Keywords:Deep Learning, Classroom Learning Status Detection, Yolov3, Dropblock</p><disp-formula id="hanspub.39404-formula27"><graphic xlink:href="//html.hanspub.org/file/22-1541962x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/22-1541962x7_hanspub.png" /> <img src="//html.hanspub.org/file/22-1541962x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>在教室课堂场景下的学生学习状态检测是指在课堂图片中检测出头部状态的过程，状态分为抬头与低头。学生作为受教育的主体，从学生课堂学习状态的研究出发，可作为学生课堂学习效率的评价指标之一。目前教师普遍通过课堂观察以及提问等方式来实时了解学生的课堂学习情况，这会造成课堂信息传递与反馈的滞后性和片面性 [<xref ref-type="bibr" rid="hanspub.39404-ref1">1</xref>]。并且，随着智能手机等电子设备的普及，目前的课堂教学过程中，出现了大批“低头族”。因此，通过统计“抬头率”可以在一定程度上判断学生的课堂专注度，从而有助于有效地提升课堂教学效率 [<xref ref-type="bibr" rid="hanspub.39404-ref2">2</xref>]。学生的课堂学习状态与课堂学习效率息息相关。目前课堂场景中对于学生课堂学习状态的研究主要从课堂学习行为 [<xref ref-type="bibr" rid="hanspub.39404-ref3">3</xref>]、课堂疲劳状态 [<xref ref-type="bibr" rid="hanspub.39404-ref4">4</xref>]、课堂人脸检测及关注度研究方面 [<xref ref-type="bibr" rid="hanspub.39404-ref1">1</xref>] 展开。</p><p>在计算机视觉和模式识别中，近几年深度学习网络得到了广泛的应用，比如，2012年，以AlexNet [<xref ref-type="bibr" rid="hanspub.39404-ref5">5</xref>] 为代表的卷积神经网络(CNN)方法被广泛应用在目标检测领域，精度取得了显著提升。2014年，GoogLeNet [<xref ref-type="bibr" rid="hanspub.39404-ref6">6</xref>] 的面世，在保持预算不变的情况下增加网络的深度与宽度，从而实现大规模图片的目标检测。2015年，ResNet [<xref ref-type="bibr" rid="hanspub.39404-ref7">7</xref>] 深度残差网络的提出，解决了深度网络过深而浪费现有资源的问题，Fast R-CNN [<xref ref-type="bibr" rid="hanspub.39404-ref8">8</xref>] 是一种快速基于区域的卷积网络方法，在提高训练和测试速度的同时，提高了检测精度。2016年，ResNeXt [<xref ref-type="bibr" rid="hanspub.39404-ref9">9</xref>] 通过重复一个构建块来构建，聚合了一组具有相同拓扑结构的转换，在保持复杂的限制条件下，增加基数也能很好地提高分类精度。而SSD [<xref ref-type="bibr" rid="hanspub.39404-ref10">10</xref>] 及YOLO [<xref ref-type="bibr" rid="hanspub.39404-ref11">11</xref>] 的提出，让目标检测算法从两个阶段向一个阶段(端到端)迈进。2017年，YOLOV2 [<xref ref-type="bibr" rid="hanspub.39404-ref12">12</xref>] 不仅提高了检测速度，而且检测类别高达9000种，在数据集PASCAL VOC和COCO上，是最先进的多尺度训练方法。2018年，YOLOV3 [<xref ref-type="bibr" rid="hanspub.39404-ref13">13</xref>] 的提出，不仅简化了网络模型，更是对小目标检测起到了领航的作用。</p><p>深度学习虽然应用广泛且效果较好，但是对于不同的研究对象有不同的研究方法，本文针对于教室课堂场景，研究学生学习状态，以头部为目标，把头部状态分为抬头和低头，我们借鉴了tinyyolov3与dropblock的结合 [<xref ref-type="bibr" rid="hanspub.39404-ref14">14</xref>]，把yolov3与dropblock相结合，实验证明它对于网络卷积层是特别有效的正则化方法。</p></sec><sec id="s6"><title>2. 基本原理</title><sec id="s6_1"><title>2.1. Yolov3</title><p>yolov3 [<xref ref-type="bibr" rid="hanspub.39404-ref13">13</xref>] 提出了一个阶段克服了两种操作缓慢的缺点阶段检测算法。它是一种卷积神经实现端到端的目标检测和识别。它只使用一个CNN网络直接预测不同目标的类别和位置节省了大量的时间来检测对象。在这项工作中，我们选择yolov3模型提取头部特性。yolo算法的基本思想是：首先通过特征提取网络对输入特征提取特征，得到特定大小的特征图输出。输入图像为460 &#215; 460，会分成13 &#215; 13、26 &#215; 26、52 &#215; 52的网格，接着如果真实框中某个物体的中心坐标落在某个网格中，那么就由该网格来预测该物体。每个物体有固定数量的边界框，Yolov3中有三个边界框，使用逻辑回归确定用来预测的回归框。图2是dropblock与yolov3的网络结构。</p></sec><sec id="s6_2"><title>2.2. Dropblock</title><p>过拟合在计算机视觉领域普遍存在，模型在已有的训练集上表现比较好，而在新的未知数据集上表现较差。对于这一现象，在深度神经网络中首次提出了dropout [<xref ref-type="bibr" rid="hanspub.39404-ref15">15</xref>] 算法。dropout一般放在全连接层后。在卷积层中添加dropout没有明显的效果。由于卷积层可以通过drop掉的神经元附近学习到相似的信息，因此为了在卷积层中防止过拟合现象，出现了dropblock [<xref ref-type="bibr" rid="hanspub.39404-ref16">16</xref>] 模块。dropblock [<xref ref-type="bibr" rid="hanspub.39404-ref16">16</xref>] 是一种用于卷积层的正则化方法。这两种算法的主要区别在于dropout随机灭活全连接层的神经元，而dropblock [<xref ref-type="bibr" rid="hanspub.39404-ref16">16</xref>] 随机灭活卷积层的单元。在实验中，我们在yolov3模型中加入了dropblock [<xref ref-type="bibr" rid="hanspub.39404-ref16">16</xref>] 模块，从而获得更好的模型。</p><p>dropblock层以块的形式丢弃特征单元，减少网络对某一特征的依赖。block_size和 γ 是dropblock的两个重要参数。block_size表示要丢弃的块的大小，而 γ 控制的是要删除活动单元格的数量。block_size的大小对于所有的特征图都是一样的，不管特征图的分辨率如何。实际上， γ 没有确定的值，但可以按如下方式进行计算</p><p>γ = 1 − kepp _ prob block _ size 2 feat _ size 2 ( feat _ size − block _ size + 1 ) 2 (1)</p><p>其中，kepp_prob可以理解为灭活中的单元格被保留的概率。有效种子区域的大小为 ( feat _ size − block _ size + 1 ) 2 ，feat_size是特征图的大小。</p></sec></sec><sec id="s7"><title>3. 方法</title>Yolov3与Dropblock结合<p>为了提高模型的泛化能力，我们在yolov3中加入了dropblock (下文简称db)层。在实践中，在yolov3中添加了8个dropblock层。在yolov3模型中，在第1个卷积层之后加入了第一个dropblock层。在dropblock层中，根据设定的参数，将这些被激活的神经元块随机灭活。将丢失一些活跃单位的特征图传递给下一层。在第1、2、3、4、5个resnet模块后添加了第2、3、4、5、6个dropblock层。在第1个上采样层的前面卷积层之间放入第7个dropblock，然后再放最后一层dropblock层在第2个上采样层的前面卷积层的中间(如图1所示)。db1-1代表的是第一个dropblock层，db1-2代表的是第二个dropblock层，依此类推，db1-8代表的是第八个dropblock层；图中Conv2D表示卷积层，BN表示批归一化处理，LeakyReLU表示激活函数，resunit表示一个残差单元，resunit*n表示n个残差单元，resn表示n个r残差模块，zero padding表示零填充层；DBL由卷积层、批归一化处理、激活函数组成，resunit由两个DBL层组成，resn由一个零填充层与一个DBL层和n个残差单元组成，block1-1表示第一个由1个DBL层和一个卷积层组成的模块，显然，图中有三个这样相同的模块。</p></sec><sec id="s8"><title>4. 实验结果</title><p>本实验在python3.6、框架tensorflow1.13.1及keras2.2.4环境下进行课堂环境学生抬头低头检测，整个训练过程的学习率及批量尺寸分别为0.01及4，并且迭代20次。整个实验在独立显卡AMD Radeon Pro WX3100并且有Intel(R)Core(TM)i7-9700 CPU和64GB储存的台式电脑上进行。</p><p>数据集ClassUD：此数据集是自己创建完成，摄像机的型号为SNOY HXR-MC2500，其中包含2820张教室上课时的学生图片作为训练及测试集，以及240张图片作为测试数据，这些测试数据皆是模仿监控视角的位置与高度拍摄所得数据，范围为一个教室的3~4排左右，且240张图片由40分钟视频以10秒一张图片的截取方式获得，实验数据分布如表1所示。</p><p>图1. Yolov3与db层结合的网络体系结构</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Experimental data distributio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >训练集</th><th align="center" valign="middle" >验证集</th><th align="center" valign="middle" >验证集</th></tr></thead><tr><td align="center" valign="middle" >ClassUD</td><td align="center" valign="middle" >2538</td><td align="center" valign="middle" >282</td><td align="center" valign="middle" >240</td></tr></tbody></table></table-wrap><p>表1. 实验数据分布</p><p>实验分为三个部分进行，第一个实验用的是yolov3模型在ClassUD数据集上进行训练及测试，第二个实验是在yolov3的基础模型上加了dropblock层(具体网络图如图2)，同时也是在ClassUD数据集上进行训练及测试。第三个实验用的是yolov3的精简版tinyyolov3在ClassUD数据集上进行训练及测试。</p><p>图2. Yolov3 P-R曲线图</p><sec id="s8_1"><title>4.1. 第二部分实验</title><p>在这个实验中，我们用yolov3加入dropblock在数据集ClassUD上进行测试，得到的实验结果如图3。其中up AP表示抬头平均精度，down AP表示低头平均精度；从图中我们可以看出抬头的平均精度为91.83%，低头的平均精度为87.01%。</p><p>图3. Yolov3 +db P-R曲线图</p></sec><sec id="s8_2"><title>4.2. 第三部分实验</title><p>在这个实验中，我们用模型tinyyolov3在数据集ClassUD上进行测试，得到的实验结果如图4。其中up AP表示抬头平均精度，down AP表示低头平均精度；从图中我们可以看出抬头的平均精度为78.06%，低头的平均精度为48.71%。</p><p>图4. Tinyyolov3 P-R曲线图</p></sec><sec id="s8_3"><title>4.3. 实验结果及分析</title><p>从上面的三个实验可以看出，在yolov3模型上的抬头检测精度为89.42%，低头检测精度为75.48%，经计算均值平均检测精度为82.45%；在yolov3+dropblock模型上的抬头检测精度为91.83%，低头的检测精度为87.01%，计算出均值平均检测精度为89.42%；在tinyyolov3模型上的抬头检测精度为78.06%，低头检测精度为48.71%，计算得到均值平均检测精度为63.39%。yolov3+dropblock模型相比yolov3模型在抬头的检测精度上提高了2.42%，低头的检测精度提高了11.53%，均值平均检测精度提高了6.97%；yolov3+dropblock模型相比tinyyolov3模型在抬头检测精度上提高了13.77%，低头的检测精度提高了38.3%，均值平均检测精度提高了26.03%；yolov3模型相比tinyyolov3模型在抬头的检测精度上提高了11.36%，低头的检测精度提高了26.77%，均值平均检测精度提高了19.06%。</p><p>实践中，我们在yolov3模型结构中加入了dropblock层，并且我们设置drop_size = 7和keep_prob = 0.9。yolov3+dropblock模型实验结果及yolov3 (tinyyolov3)模型实验结果如图5。图5(a)表示yolov3模型下的实验结果；图5(b)表示yolov3+db模型下drop_size = 7、keep_prob = 0.9的实验结果；图5(c)表示tinyyolov3模型下的实验结果，可以看出图5(b)检测的准确率比图5(a)、图5(c)情况都要好，这正是我们在yolov3中加入dropblock层且参数drop_size = 7、keep_prob = 0.9的实验结果。其中蓝色的框线表示状态的真实值，绿色的框线表示检测与真实值相符的结果，红色的框线表示错检的结果，粉红色的框线表示漏检的结果。</p><p>图5. 部分实验结果</p><p>在计算机视觉领域的目标检测中，使用深度学习来做目标检测的实验有很多，每一张图片都包含了不同的目标，我们仅对于我们的研究方向建立了数据集，数据图片中包含一种物体(人)，但我们把她们在课堂学习中的两种状态(抬头低头)看出是两种目标来进行检测，从而有了我们这篇论文的思想。在本论文中，我们通过均值平均精度(mAP)来评估实验模型，评估的结果如表2，所有的测试均在ClassUD数据集上进行。从表可以看出中，当drop_size = 7、keep_prob = 0.9时，精度比原有的模型提高了6.97%，可以看出，在yolov3+dropblock模型中，drop_size = 7、keep_prob = 0.9时好于yolov3的情况。而yolov3模型又好于tinyyolov3的情况。这证明了dropblock层对于yolov3是有效的，在相同的实验环境下，yolov3模型的训练时间为58.79 h，测试每帧图片的时间为0.57 s，而yolov3+db模型的训练时间为61.51 h，测试每帧图片的时间为0.59 s，tinyyolo3模型的训练时间为14.37 h，测试每帧图片的时间为0.12 s，虽然在训练过程中tinyyolov3的训练速度快于yolov3，但是它的精度却远远低于yolov3。在训练时间与测试时间相差不大的yolov3模型与yolov3+db模型下，显然yolov3+db模型对我们的检测任务效果更好。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Evaluation results of different model</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >均值平均精度</th><th align="center" valign="middle" >训练时间(/小时)</th><th align="center" valign="middle" >测试时间(帧/秒)</th></tr></thead><tr><td align="center" valign="middle" >yolov3</td><td align="center" valign="middle" >82.45%</td><td align="center" valign="middle" >58.79</td><td align="center" valign="middle" >0.57</td></tr><tr><td align="center" valign="middle" >yolov3 + db (7.0.9)</td><td align="center" valign="middle" >89.42%</td><td align="center" valign="middle" >61.51</td><td align="center" valign="middle" >0.59</td></tr><tr><td align="center" valign="middle" >tinyyolov3</td><td align="center" valign="middle" >63.38%</td><td align="center" valign="middle" >14.37</td><td align="center" valign="middle" >0.12</td></tr></tbody></table></table-wrap><p>表2. 不同模型的评估结果</p></sec></sec><sec id="s9"><title>5. 结论</title><p>本文建议的方法利用yolov3结合dropblock进行教室场景学生课堂抬头低头检测。当数据流入dropblock层时，语义信息区域被成块的丢弃，这使得网络不得不集中精力学习剩余语义信息区域中的特征。在ClassUD上的抬头低头检测结果证明我们提出的网络结合在性能上比原来的模型要好。该方法有效地提高模型的鲁棒性和泛化能力。但是教室场景的抬头低头检测仍然面临着一系列的问题，如光照、状态不明显(低头幅度较小，可能误检为抬头)、图像质量(摄像头清晰度较低会影响检测效果)和遮挡问题。未来的工作将集中在寻找一种更适合于教室场景抬头低头状态检测的算法，该算法可以实现更好的鲁棒性，提高检测的精度。</p></sec><sec id="s10"><title>文章引用</title><p>刘秋会,梁明秀,王 林. 一种基于深度学习的课堂学生学习状态研究A Study on the Learning State of Classroom Students Based on Deep Learning[J]. 计算机科学与应用, 2020, 10(12): 2339-2345. https://doi.org/10.12677/CSA.2020.1012247</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.39404-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">唐康, 先强, 李明勇. 基于人脸检测的大学课堂关注度研究[J]. 重庆师范大学学报(自然科学版), 2019, 36(5): 123.</mixed-citation></ref><ref id="hanspub.39404-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">郭秀兰, 赵佳敏. 本科课堂教学“出勤率、抬头率、满意率”的调查报告[J]. 改革与开放, 2016(19): 108-110.</mixed-citation></ref><ref id="hanspub.39404-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">左国才, 吴小平, 苏秀芝, 等. 基于CNN人脸识别模型的大学生课堂行为分析研究[J]. 智能计算机与应用, 2019, 9(6): 107-110.</mixed-citation></ref><ref id="hanspub.39404-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">屈梁浩. 基于深度学习的学生课堂疲劳状态的分析与研究[D]: [硕士学位论文]. 重庆: 重庆师范大学, 2019.</mixed-citation></ref><ref id="hanspub.39404-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky, A., Sutskever, I. and Hinton, G. (2012) ImageNet Classification with Deep Convolutional Neural Networks. 2012 NIPS, Lake Tahoe, NV, December 2012, 1097-1105.</mixed-citation></ref><ref id="hanspub.39404-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Liu, W., Jia, Y., et al. (2014) Going Deeper with Convolutions. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, 7-12 June 2015, 1-9. &lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298594</mixed-citation></ref><ref id="hanspub.39404-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S., et al. (2016) Deep Residual Learning for Image Recognition. IEEE Conference on Computer Vision &amp; Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.39404-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Girshick, R. (2015) Fast R-CNN. IEEE International Conference on Computer Vision (ICCV), Santiago, 7-13 December 2015, 1440-1448. &lt;br&gt;https://doi.org/10.1109/ICCV.2015.169</mixed-citation></ref><ref id="hanspub.39404-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Xie, S., Girshick, R., Dollár, P., et al. (2017) Aggregated Residual Transformations for Deep Neural Networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 5987-5995.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.634</mixed-citation></ref><ref id="hanspub.39404-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Liu, W., Anguelov, D., Erhan, D., et al. (2016) SSD: Single Shot MultiBox Detector. European Conference on Computer Vision, Amsterdam, 8-16 October 2016, 21-37. &lt;br&gt;https://doi.org/10.1007/978-3-319-46448-0_2</mixed-citation></ref><ref id="hanspub.39404-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J., Divvala, S., Girshick, R., et al. (2016) You Only Look Once: Unified, Real-Time Object Detection. IEEE Conference on Computer Vision &amp; Pattern Recognition, Las Vegas, 27-30 June 2016, 779-788.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.91</mixed-citation></ref><ref id="hanspub.39404-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J. and Farhadi, A. (2017) YOLO9000: Better, Faster, Stronger. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 6517-6525. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.690</mixed-citation></ref><ref id="hanspub.39404-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J. and Farhadi, A. (2018) YOLOv3: An In-cremental Improvement.</mixed-citation></ref><ref id="hanspub.39404-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Yang, Z., Xu, W., Wang, Z., et al. (2019) Combining Yolov3-Tiny Model with Dropblock for Tiny-Face Detection. 2019 IEEE 19th International Conference on Communication Technology (ICCT) IEEE, Xi’an, 16-19 October 2019, 1673-1677. &lt;br&gt;https://doi.org/10.1109/ICCT46805.2019.8947158</mixed-citation></ref><ref id="hanspub.39404-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R. (2014) Dropout: A Simple Way to Prevent Neural Networks from Overfitting. The Journal of Machine Learning Research, 15, 1929-1958.</mixed-citation></ref><ref id="hanspub.39404-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Ghiasi, G., Lin, T.-Y. and Le, Q.V. (2018) DropBlock: A Regularization Method for Convolutional Networks.</mixed-citation></ref></ref-list></back></article>