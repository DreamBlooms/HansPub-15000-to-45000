<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.107140</article-id><article-id pub-id-type="publisher-id">CSA-36691</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200700000_82134619.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于深度学习的古汉语命名实体识别研究
  Ancient Chinese Named Entity Recognition Based on Deeping Learning
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>卓</surname><given-names>玛措</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>桑杰</surname><given-names>端珠</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>才</surname><given-names>让加</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>青海师范大学藏文信息处理教育部重点实验室，青海 西宁；青海省藏文信息处理与机器翻译重点实验室，青海 西宁</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>15</day><month>07</month><year>2020</year></pub-date><volume>10</volume><issue>07</issue><fpage>1359</fpage><lpage>1366</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   命名实体识别是自然语言处理的基础任务之一。而目前中文命名实体识别研究大多是面向现代汉语的，针对古汉语的这方面研究工作涉及较少。因此，本文以《战国策》为例，根据古汉语独特的子语言特征，利用网格长短期记忆(Lattice LSTM)神经网络构建命名实体识别模型以解决古汉语中的信息提取问题。实验结果表明，Lattice LSTM能够学会从语境中自动找到所有与词典匹配的词汇，以取得较好的命名实体识别性能。实验结果中的F1值达到92.16%。 Named entity recognition is one of the basic tasks of natural language processing. At present, the research on Chinese named entities recognition is mostly for modern Chinese, and the research on it for ancient Chinese is less involved. So in this paper, taking the War State Policy as an example and according to the characteristics of ancient Chinese text, we use the Lattice Long and Short-Term memory (Lattice LSTM) neural network to construct a named entity recognition model to solve the problem of information extraction of ancient Chinese. Experiment result shows that Lattice LSTM can learn to automatically find all the dictionary-matched words from the context to achieve better named entity recognition performance. The F1 value reaches 92.16%. 
  
 
</p></abstract><kwd-group><kwd>神经网络模型，古汉语，命名实体识别，条件随机场, Neural Network Model</kwd><kwd> Ancient Chinese</kwd><kwd> Named Entity Recognition</kwd><kwd> Conditional Random Field</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于深度学习的古汉语命名实体识别研究</title><p>卓玛措<sup>1,2</sup>，桑杰端珠<sup>1,2</sup>，才让加<sup>1,2</sup></p><p><sup>1</sup>青海师范大学藏文信息处理教育部重点实验室，青海 西宁</p><p><sup>2</sup>青海省藏文信息处理与机器翻译重点实验室，青海 西宁</p><p>收稿日期：2020年7月2日；录用日期：2020年7月16日；发布日期：2020年7月24日</p><disp-formula id="hanspub.36691-formula150"><graphic xlink:href="//html.hanspub.org/file/7-1541830x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>命名实体识别是自然语言处理的基础任务之一。而目前中文命名实体识别研究大多是面向现代汉语的，针对古汉语的这方面研究工作涉及较少。因此，本文以《战国策》为例，根据古汉语独特的子语言特征，利用网格长短期记忆(Lattice LSTM)神经网络构建命名实体识别模型以解决古汉语中的信息提取问题。实验结果表明，Lattice LSTM能够学会从语境中自动找到所有与词典匹配的词汇，以取得较好的命名实体识别性能。实验结果中的F1值达到92.16%。</p><p>关键词 :神经网络模型，古汉语，命名实体识别，条件随机场</p><disp-formula id="hanspub.36691-formula151"><graphic xlink:href="//html.hanspub.org/file/7-1541830x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/7-1541830x7_hanspub.png" /> <img src="//html.hanspub.org/file/7-1541830x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>命名实体识别 [<xref ref-type="bibr" rid="hanspub.36691-ref1">1</xref>] (Named Entity Recognition, NER)是自然语言处理(Natural Language Processing, NLP)基础性工作之一，它可以准确地从文本中识别出人名、机构名、地名、时间等信息，为信息检索、机器翻译、舆情分析等下游自然语言处理任务提供重要的特征信息。过去，命名实体识别任务多采用基于规则的方法、基于统计的方法和基于规则和统计相结合的方法 [<xref ref-type="bibr" rid="hanspub.36691-ref2">2</xref>]。</p><p>近年来，深度神经网络在自然语言处理领域广泛地受到关注，与上述方法相比，基于深度神经网络的方法具有更强的泛化能力、对人工特征依赖较少的优点。因此，面向现代汉语和英语等大语种，研究者已提出了许多基于深度神经网络的命名实体识别模型 [<xref ref-type="bibr" rid="hanspub.36691-ref3">3</xref>] - [<xref ref-type="bibr" rid="hanspub.36691-ref18">18</xref>]，但针对古汉语在这方面的研究才刚刚起步。</p><p>鉴于此，本文以《战国策》为例，根据古汉语独特的子语言特征，利用网格长短期记忆(Lattice LSTM)神经网络构建命名实体识别模型以解决古汉语中的信息提取问题。该方法将传统的LSTM单元改进为网格LSTM，在字模型的基础之上显性利用词与词序信息，从而避免了分词错误传递的问题。实验结果表明，Lattice LSTM能够学会从语境中自动找到所有与词典匹配的词汇，以取得较好的命名实体识别性能。在本研究构建的数据集上F1值达到92.16%。</p></sec><sec id="s4"><title>2. 模型</title><p>在英文领域，第一个采用神经网络进行命名实体识别的是Hammerton等人，由于LSTM良好的序列建模能力，LSTM-CRF [<xref ref-type="bibr" rid="hanspub.36691-ref19">19</xref>] 模型成为命名实体识别的基础架构之一，很多方法都是以LSTM-CRF为主体框架，在此之上融入各种相关特征。本文将LSTM-CRF作为主要网络结构，并且在该模型对一系列输入字符进行编码的同时将所有与词典匹配的词汇网格结构融入模型中。</p><p>一般将输入序列表示为 s = c 1 , c 2 , ⋯ , c n 。其中， c j 代表第j个字符。本文中应用 t ( i , k ) 表示索引j，代表第i个词的第k个字符。比如“医扁鹊”，索引从1开始，那么 t ( 1 , 1 ) = 1 (医)， t ( 2 , 1 ) = 2 (扁)。本研究运用BIO标注策略进行字粒度和词粒度的命名实体识别标注，古汉语命名实体识别的字序列和标记序列举例说明如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Examples of character sequence and label sequence in ancient Chinese named entity recognitio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >字</th><th align="center" valign="middle" >舜</th><th align="center" valign="middle" >虽</th><th align="center" valign="middle" >贤</th><th align="center" valign="middle" >不</th><th align="center" valign="middle" >遇</th><th align="center" valign="middle" >尧</th><th align="center" valign="middle" >不</th><th align="center" valign="middle" >得</th><th align="center" valign="middle" >为</th><th align="center" valign="middle" >天</th><th align="center" valign="middle" >子</th></tr></thead><tr><td align="center" valign="middle" >标记</td><td align="center" valign="middle" >B-PER</td><td align="center" valign="middle" >O</td><td align="center" valign="middle" >O</td><td align="center" valign="middle" >O</td><td align="center" valign="middle" >O</td><td align="center" valign="middle" >B-PER</td><td align="center" valign="middle" >O</td><td align="center" valign="middle" >O</td><td align="center" valign="middle" >O</td><td align="center" valign="middle" >B-POS</td><td align="center" valign="middle" >I-POS</td></tr></tbody></table></table-wrap><p>表1. 古汉语命名实体识别的字序列和标记序列举例</p><p>注：“O”是实体外部标记，“B”是实体开始标记，“I”是实体内部标记，“PER”表示人名，“POS”表示官职。</p><sec id="s4_1"><title>2.1. 基于字的模型</title><p>基于字的命名实体识别(见图1)存在一种缺陷，即无法充分利用词的显性以及词序信息。</p><p>图1. 基于字符的模型</p><p>图2. 词–字符网格</p><p>图3. Lattice模型</p></sec><sec id="s4_2"><title>2.2. 基于词的模型</title><p>基于词的模型见图4。</p></sec><sec id="s4_3"><title>2.3. Lattice LSTM模型</title><p>本文利用Lattice LSTM [<xref ref-type="bibr" rid="hanspub.36691-ref20">20</xref>] 来处理句子中的词汇词(lexicon word)，从而将所有潜在词信息全部整合到基于字符的LSTM-CRF中，见图2。并使用一个自动获取的词典来匹配句子，进而构建基于词的Lattice，见图3。由于在网格中存在指数数量的单词到字符路径，因此使用Lattice LSTM结构来自动控制从句子的开头到结尾的信息流。门控单元用于将不同路径的信息动态的传输到每个字符。在训练数据集上训练后，Lattice LSTM能够学会从信息流中自动找到有用的词，从而提升命名实体识别性能，见图5。与基于字符和基于词的命名实体识别方法相比，本文采用的模型优势在于利用词汇的显性信息进行分词，而不是仅仅自动关注，从而减少分词误差。</p><p>图4. 基于词的模型</p><p>图5. Lattice LSTM模型</p></sec><sec id="s4_4"><title>2.4. LSTM层</title><p>RNN循环神经网络理论上可以处理任意长度的序列信息，但实际应用中，当序列过长时会出现梯度消失的问题，且很难学到长期依赖的特征。因此，Graves等人 [<xref ref-type="bibr" rid="hanspub.36691-ref21">21</xref>] 改进了循环神经网络，提出长短期记忆网络(Long Short-Term Memory) LSTM模型。LSTM单元通过输入门、遗忘门和输出门来控制信息传递。它是一种特殊的RNN，能够学习长期的规律，应用十分广泛。LSTM编码单元如图6所示。</p><p>具体计算过程如公式(1)~(6)所示：</p><p>i t = σ ( W i h t − 1 + U i + b i ) (1)</p><p>f t = σ ( W f h t − 1 + U f x t + b f ) (2)</p><p>c t ˜ = tanh ( W c h t − 1 + U c x t + b f ) (3)</p><p>图6. LSTM编码单元</p><p>c t = f t ⊙ c t − 1 + i t ⊙ c t ˜ (4)</p><p>o t = σ ( W o h t − 1 + U o x t + b o ) (5)</p><p>h t = o t ⊙ tanh ( c t ) (6)</p><p>其中， σ 是sigmoid函数， ⊙ 是点积。 x t 为时刻t的输入向量， h t 是隐藏状态，也是输出向量，包含前面t时刻所有有效信息。 c t 是一个更新门，控制信息流入下一个时刻； f t 是一个遗忘门，控制信息丢失；二者共同决定隐藏状态的输出。</p></sec></sec><sec id="s5"><title>3. 实验及结果分析</title><sec id="s5_1"><title>3.1. 实验数据</title><p>由于目前古汉语命名实体识别缺乏公开的标注数据集，因此本文人工构建了一个古汉语命名实体识别数据集。该数据集包括训练集、开发集、测试集，训练集共包含43.995 K个字，开发集包含5.843 K个字，测试集包含5.849 K个字。各类实体统计如表2所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Statistics of entity numbe</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >语料数量</th><th align="center" valign="middle" >人名数量</th><th align="center" valign="middle" >地名数量</th><th align="center" valign="middle" >职官数量</th></tr></thead><tr><td align="center" valign="middle" >训练集</td><td align="center" valign="middle" >43,995</td><td align="center" valign="middle" >1,255</td><td align="center" valign="middle" >2,671</td><td align="center" valign="middle" >101</td></tr><tr><td align="center" valign="middle" >开发集</td><td align="center" valign="middle" >5,843</td><td align="center" valign="middle" >125</td><td align="center" valign="middle" >285</td><td align="center" valign="middle" >37</td></tr><tr><td align="center" valign="middle" >测试集</td><td align="center" valign="middle" >5,849</td><td align="center" valign="middle" >144</td><td align="center" valign="middle" >315</td><td align="center" valign="middle" >26</td></tr></tbody></table></table-wrap><p>表2. 实体个数统计</p></sec><sec id="s5_2"><title>3.2. 标注策略与评价指标</title><p>命名实体识别的标注策略有BIO模式，BIOE模式，BIOES模式。本文采用的是BIO标注策略，其中B表示实体开始，I表示实体非开始部分。O表示不是实体的部分。在预测实体边界的时候需要同时预测实体类型，所以待预测的标签一共7种，分别是O，B-PER，I-PER，B-LOC，I-LOC，B-POS，I-POS。在测试过程中，只有当一个实体的边界和实体的类型完全正确时，才判断该实体预测正确。</p><p>命名实体识别的评价指标有精确率(P)、召回率(R)和F1值。具体定义如公式(7)：T<sub>p</sub>为模型识别正确的实体个数，F<sub>p</sub>为模型识别到的不相关实体个数，F<sub>n</sub>为相关实体但是模型没有检测到的个数。</p><p>P = T P T P + F P &#215; 100 % R = T P T P + F n &#215; 100 % F 1 = 2 &#215; P &#215; R P + R &#215; 100 % (7)</p></sec><sec id="s5_3"><title>3.3. 实验环境与超参设置</title><p>本研究中的实验环境为Python3.6，深度学习框架为Pytorch1.4.0神经网络超参的取值会影响神经网络的性能。本文的神经网络参数设定如表3所示。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Neural network hyperparameter value</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >参数</th><th align="center" valign="middle" >取值</th><th align="center" valign="middle" >参数</th><th align="center" valign="middle" >取值</th></tr></thead><tr><td align="center" valign="middle" >音节向量维度</td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >词向量维度</td><td align="center" valign="middle" >50</td></tr><tr><td align="center" valign="middle" >Lattice向量维度</td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >Lattice丢弃率</td><td align="center" valign="middle" >0.5</td></tr><tr><td align="center" valign="middle" >丢弃率(Dropout)</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >学习率(lr)</td><td align="center" valign="middle" >0.05</td></tr><tr><td align="center" valign="middle" >LSTM层</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >主LSTM隐藏层维度</td><td align="center" valign="middle" >200</td></tr></tbody></table></table-wrap><p>表3. 神经网络超参取值</p></sec><sec id="s5_4"><title>3.4. 实验设计与结果</title><p>为了验证本研究中所使用的模型对古汉语命名实体识别数据集中的人名、地名、官职三大类实体的识别性能，本文分别采用三种神经网络模型设计了三个实验。其中主要实验模型为Lattice LSTM，对比实验模型为BiLSTM-CRF和BiLSTM-CNN-CRF。实验的评价指标有准确率(P)、召回率(R)和综合指标F1值。各模型实验结果见表4。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Experiment results of each model (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >准确率</th><th align="center" valign="middle" >召回率</th><th align="center" valign="middle" >F1值</th></tr></thead><tr><td align="center" valign="middle" >BiLSTM-CRF</td><td align="center" valign="middle" >88.30</td><td align="center" valign="middle" >87.70</td><td align="center" valign="middle" >87.92</td></tr><tr><td align="center" valign="middle" >BiLSTM-CNN-CRF</td><td align="center" valign="middle" >89.50</td><td align="center" valign="middle" >89.10</td><td align="center" valign="middle" >89.25</td></tr><tr><td align="center" valign="middle" >Lattice LSTM</td><td align="center" valign="middle" >92.42</td><td align="center" valign="middle" >91.90</td><td align="center" valign="middle" >92.16</td></tr></tbody></table></table-wrap><p>表4. 各模型的实验结果(%)</p><p>实验结果表明Lattice LSTM模型能有效提升实体识别的性能。各模型随着训练轮数F1值变化如图7所示。</p></sec><sec id="s5_5"><title>3.5 实体识别实例</title><p>以本研究构建的数据集中的一个句子为例展示Lattice LSTM模型的实体识别效果。具体实例如表5所示。</p><p>图7. F1值变化图</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Examples of ancient Chinese entity recognitio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >句子</th><th align="center" valign="middle" >故以舜汤武不遭时不得帝王</th></tr></thead><tr><td align="center" valign="middle" >正确的分词</td><td align="center" valign="middle" >故以舜汤武不遭时不得帝王</td></tr><tr><td align="center" valign="middle" >自动分词</td><td align="center" valign="middle" >故以舜汤武不遭时不得帝王</td></tr><tr><td align="center" valign="middle" >Lattice分词</td><td align="center" valign="middle" >故以舜汤汤武舜汤武不遭遭时不遭时不得帝王</td></tr><tr><td align="center" valign="middle" >BiLSTM-CRF</td><td align="center" valign="middle" >故以舜汤武<sub>PER</sub>不遭时不得帝王<sub>POS</sub></td></tr><tr><td align="center" valign="middle" >BiLSTM-CNN-CRF</td><td align="center" valign="middle" >故以舜汤武<sub>PER</sub>不遭时不得帝王<sub>POS</sub></td></tr><tr><td align="center" valign="middle" >Lattice LSTM</td><td align="center" valign="middle" >故以舜<sub>PER</sub>汤<sub>PER</sub>武<sub>PER</sub>，不遭时不得帝王<sub>POS</sub></td></tr></tbody></table></table-wrap><p>表5. 古汉语实体识别实例</p><p>注：斜体表示识别不正确的实体，粗体表示识别正确的实体。</p></sec></sec><sec id="s6"><title>4. 结束语</title><p>针对古汉语命名实体识别所面临的问题，本文采用了一种同时关注字信息和词信息进行实体识别的深度学习模型。该模型将传统的LSTM单元改进为网格LSTM，在字符模型的基础之上显性利用词和词序信息，从而避免了分词错误传递的问题；利用具有长短期记忆功能的LSTM模型作为隐藏层，可以解决古汉语文本中部分实体结构较长的问题；最后使用CRF作为标签推理层以解决文本序列标签依赖问题。在已构建的古汉语命名实体识别数据集上进行实验，实验结果证明了Lattice LSTM模型的有效性。</p><p>今后，本文的研究工作应该在数据和词典规模方面加大力度，从而进一步提高模型的整体性能。另外，还应该针对古汉语文本进行广泛深入的语言信息处理方面的研究，以便获得更多有价值的知识。</p></sec><sec id="s7"><title>基金项目</title><p>国家自然科学基金(61662061, 61063033)、国家重点研发计划(2017YFB1402200)、青海省科技厅项目(2015-SF-520)。</p></sec><sec id="s8"><title>文章引用</title><p>卓玛措,桑杰端珠,才让加. 基于深度学习的古汉语命名实体识别研究Ancient Chinese Named Entity Recognition Based on Deeping Learning[J]. 计算机科学与应用, 2020, 10(07): 1359-1366. https://doi.org/10.12677/CSA.2020.107140</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.36691-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Hammerton, J. (2003) Named Entity Recognition with Long Short-Term Memory. In: Conference on Natural Language Learning, Association for Computational Linguistics, Stroudsburg, 172-175.  
&lt;br&gt;https://doi.org/10.3115/1119176.1119202</mixed-citation></ref><ref id="hanspub.36691-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">张海楠, 伍大勇, 刘悦, 等. 基于深度神经网络的中文命名实体识别[J]. 中文信息学报, 2017, 31(4): 28-35.</mixed-citation></ref><ref id="hanspub.36691-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Ma, X. and Hovy, E. (2016) End-to-End Sequence Labeling via Bi-Directional LSTM-CNNs-CRF. Proceedings of the 54th Annual Meeting of the Association for Computational Lin-guistics, Volume 1: Long Papers, Berlin, August 2016, 1064-1074. &lt;br&gt;https://doi.org/10.18653/v1/P16-1101</mixed-citation></ref><ref id="hanspub.36691-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Chiu, J.P.C. and Nichols, E. (2016) Named Entity Recognition with Bi-directional LSTM-CNNs. Transactions of the Association for Computational Linguistics, 4, 357-370. &lt;br&gt;https://doi.org/10.1162/tacl_a_00104</mixed-citation></ref><ref id="hanspub.36691-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Lample, G., Ballesteros, M., Subramanian, S., et al. (2016) Neural Ar-chitectures for Named Entity Recognition. Proceedings of NAACL-HLT 2016, San Diego, 12-17 June 2016, 260-270. &lt;br&gt;https://doi.org/10.18653/v1/N16-1030</mixed-citation></ref><ref id="hanspub.36691-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Nothman, J., Ringland, N., Radford, W., Murphy, T. and Curran, J.R. (2013) Learning Multilingual Named Entity Recognition from Wikipedia. Artificial Intelligence, 194, 151-175.</mixed-citation></ref><ref id="hanspub.36691-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Santos, C.N.D. and Guimarães, V. (2015) Boosting Named Entity Recognition with Neural Character Embeddings.</mixed-citation></ref><ref id="hanspub.36691-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">买买提阿依甫, 吾守尔•斯拉木, 帕丽旦•木合塔尔, 等. 基于BiLSTM-CNN-CRF模型的维吾尔文命名实体识别[J]. 计算机工程, 2018, 44(8): 230-236.</mixed-citation></ref><ref id="hanspub.36691-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">王洁, 张瑞东, 吴晨生. 基于GRU的命名实体识别方法[J]. 计算机系统应用, 2018, 27(9): 18-24.</mixed-citation></ref><ref id="hanspub.36691-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">李丽双, 郭元凯. 基于CNN-BLSTM-CRF模型的生物医学命名实体识别[J]. 中文信息学报, 2018, 32(1): 116-122.</mixed-citation></ref><ref id="hanspub.36691-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">周晓磊, 赵薛蛟, 刘堂亮, 宗子潇, 王其乐, 里剑桥. 基于SVM-BiLSTM-CRF模型的财产纠纷命名实体识别方法[J]. 计算机系统应用, 2019, 28(1): 245-250.</mixed-citation></ref><ref id="hanspub.36691-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">杨文明, 褚伟杰. 在线医疗问答文本的命名实体识别[J]. 计算机系统应用, 2019, 28(2): 8-14.</mixed-citation></ref><ref id="hanspub.36691-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">He, J. and Wang, H. (2008) Chinese Named Entity Recognition and Word Segmentation Based on Character. Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing, Hyderabad, 11-12 January 2008, 128-132.</mixed-citation></ref><ref id="hanspub.36691-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Liu, Z., Zhu, C. and Zhao, T. (2010) Chinese Named Entity Recognition with a Sequence Labeling Approach: Based on Characters, or Based on Words? In: Advanced Intelligent Computing Theories and Applications. With Aspects of Artificial Intelligence, Springer, Berlin Heidelberg, 634-640. &lt;br&gt;https://doi.org/10.1007/978-3-642-14932-0_78</mixed-citation></ref><ref id="hanspub.36691-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">杨培, 杨志豪, 罗凌, 林鸿飞, 王健. 基于注意机制的化学药物命名实体识别[J]. 计算机研究与发展, 2018, 55(7): 1548-1556.</mixed-citation></ref><ref id="hanspub.36691-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Li, H., Hagiwara, M., Li, Q., et al. (2014) Comparison of the Impact of Word Segmentation on Name Tagging for Chinese and Japanese. 2014 LREC, Reykjavik, 26-31 May 2014, 2532-2536.</mixed-citation></ref><ref id="hanspub.36691-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Chen, W., Zhang, Y. and Isahara, H. (2006) Chinese Named Entity Recognition with Conditional Random Fields. Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, Sydney, 22-23 July 2006, 118-121.</mixed-citation></ref><ref id="hanspub.36691-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Dong, C., Zhang, J., Zong, C., et al. (2016) Character-Based LSTM-CRF with Radical-Level Features for Chinese Named Entity Recognition. In: Natural Language Understanding and Intelligent Applications, Springer, Cham, 239-250. &lt;br&gt;https://doi.org/10.1007/978-3-319-50496-4_20</mixed-citation></ref><ref id="hanspub.36691-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Huang, Z., Xu, W. and Yu, K. (2015) Bidirectional LSTM-CRF Models for Sequence Tagging.</mixed-citation></ref><ref id="hanspub.36691-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Y. and Yang, J. (2018) Chinese NER Using Lattice LSTM. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Volume 1: Long Papers, Melbourne, July 2018, 1554-1564.  
&lt;br&gt;https://doi.org/10.18653/v1/P18-1144</mixed-citation></ref><ref id="hanspub.36691-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Graves, A. (2013) Generating Sequences with Recurrent Neural Net-works. Computer Science, 1-43.</mixed-citation></ref></ref-list></back></article>