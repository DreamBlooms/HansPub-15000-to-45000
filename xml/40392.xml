<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">JWRR</journal-id><journal-title-group><journal-title>Journal of Water Resources Research</journal-title></journal-title-group><issn pub-type="epub">2166-6024</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/JWRR.2021.101005</article-id><article-id pub-id-type="publisher-id">JWRR-40392</article-id><article-categories><subj-group subj-group-type="heading"><subject>JWRR20210100000_94189140.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>地球与环境</subject><subject> 合作期刊</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于机器集成学习的中长期径流预报研究
  Research on Medium and Long-Term Runoff Forecasting Based on Machine Integrated Learning
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>吕</surname><given-names>盼成</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>丽萍</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>源</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>华北电力大学，北京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>18</day><month>02</month><year>2021</year></pub-date><volume>10</volume><issue>01</issue><fpage>44</fpage><lpage>52</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   中长期径流预报对水库优化调度及水资源优化开发利用都有着重要的意义。首先采用基于Boosting算法的梯度提升回归树(Gradient Boosting Decision Tree, GBRT)和极端梯度提升树(Extreme Gradient Boosting, XGBoost)、基于Bagging算法的随机森林(Random Forest, RF)和极端随机树(Extreme Random Tree, ET)四种算法作为预报模型对锦屏一级水库月平均入库流量序列进行预报，并对预测结果进行对比分析。结果显示，RF预测效果最差，XGBoost预测效果最好。进一步选用其中预测效果较好的三个方法ET、XGBoost、GBRT作为初级学习器，以Logistic回归作为次学习器，进行Stacking集成学习预测。结果表明，Stacking集成学习的预测效果要优于单一模型中预测效果最好的XGBoost，其预测值的结果和实测值更为接近，为中长期径流预报提供了新思路。 Medium and long-term runoff forecast is of great significance to the optimal operation of reservoirs, development and utilization of water resources. Firstly, the gradient boosting decision tree (GBRT) and extreme gradient boosting (XGBoost) based on boosting algorithm are selected. There is also random forest (RF) and extreme random tree (ET) based on bagging algorithm. These four algorithms are used as forecasting models to forecast the average monthly inflow of the Jinping-I Reservoir, and then the prediction results are analyzed and compared. The results showed that the RF prediction was the worst, and XGBoost was the best. Then, the three methods with better prediction effect are ET, XGBoost and GBRT as primary learners, logistic regression as secondary learners, and stacking ensemble learning to predict. The first mock exam results show that the prediction result of Stacking ensemble learning is better than that of XGBoost with the best prediction result in a single model. The predicted value is closer to the measured value, which provides a new idea for medium and long-term runoff forecast. 
  
 
</p></abstract><kwd-group><kwd>径流预报，集成学习，机器学习，锦屏一级水库, Runoff Forecast</kwd><kwd> Ensemble Learning</kwd><kwd> Machine Learning</kwd><kwd> The Jinping-1 Reservoir</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>中长期径流预报对水库优化调度及水资源优化开发利用都有着重要的意义。首先采用基于Boosting算法的梯度提升回归树(Gradient Boosting Decision Tree, GBRT)和极端梯度提升树(Extreme Gradient Boosting, XGBoost)、基于Bagging算法的随机森林(Random Forest, RF)和极端随机树(Extreme Random Tree, ET)四种算法作为预报模型对锦屏一级水库月平均入库流量序列进行预报，并对预测结果进行对比分析。结果显示，RF预测效果最差，XGBoost预测效果最好。进一步选用其中预测效果较好的三个方法ET、XGBoost、GBRT作为初级学习器，以Logistic回归作为次学习器，进行Stacking集成学习预测。结果表明，Stacking集成学习的预测效果要优于单一模型中预测效果最好的XGBoost，其预测值的结果和实测值更为接近，为中长期径流预报提供了新思路。</p></sec><sec id="s2"><title>关键词</title><p>径流预报，集成学习，机器学习，锦屏一级水库</p></sec><sec id="s3"><title>Research on Medium and Long-Term Runoff Forecasting Based on Machine Integrated Learning<sup> </sup></title><p>Pancheng Lv, Liping Wang, Yuan Liu</p><p>North China Electric Power University, Beijing</p><p><img src="//html.hanspub.org/file/5-2410987x5_hanspub.png" /></p><p>Received: Dec. 18<sup>th</sup>, 2020; accepted: Jan. 29<sup>th</sup>, 2021; published: Feb. 18<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/5-2410987x6_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Medium and long-term runoff forecast is of great significance to the optimal operation of reservoirs, development and utilization of water resources. Firstly, the gradient boosting decision tree (GBRT) and extreme gradient boosting (XGBoost) based on boosting algorithm are selected. There is also random forest (RF) and extreme random tree (ET) based on bagging algorithm. These four algorithms are used as forecasting models to forecast the average monthly inflow of the Jinping-I Reservoir, and then the prediction results are analyzed and compared. The results showed that the RF prediction was the worst, and XGBoost was the best. Then, the three methods with better prediction effect are ET, XGBoost and GBRT as primary learners, logistic regression as secondary learners, and stacking ensemble learning to predict. The first mock exam results show that the prediction result of Stacking ensemble learning is better than that of XGBoost with the best prediction result in a single model. The predicted value is closer to the measured value, which provides a new idea for medium and long-term runoff forecast.</p><p>Keywords:Runoff Forecast, Ensemble Learning, Machine Learning, The Jinping-1 Reservoir</p><disp-formula id="hanspub.40392-formula21"><graphic xlink:href="//html.hanspub.org/file/5-2410987x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Wuhan University.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/5-2410987x8_hanspub.png" /> <img src="//html.hanspub.org/file/5-2410987x9_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>中长期径流预报是指根据前期的水文气象等要素，应用数理统计法或者物理成因法等对未来的一段时间(通常指3天以上1年以内)的径流进行预测 [<xref ref-type="bibr" rid="hanspub.40392-ref1">1</xref>]。由于径流过程是一个高度复杂的非线性过程，在人类活动和气候变换等因素的作用下，流域的径流形式发生了新的变化，导致传统中长期径流预报的精度较低。因此以机器学习为基础的数据挖掘技术在径流预报中逐渐受到了关注。</p><p>机器学习是一种人工智能，可以利用算法或者逻辑从数据中抽取模型，可以深度挖掘大数据的深度价值和内在联系 [<xref ref-type="bibr" rid="hanspub.40392-ref2">2</xref>]。将机器学习应用于水文领域，对提升径流预报的有效性有着重要作用。如：李伶杰等 [<xref ref-type="bibr" rid="hanspub.40392-ref3">3</xref>] 利用随机森林选取预报因子，并建立随机森林和支持向量机模型对龙江水库开展径流预报研究，总体精度较高，但是支持向量机泛化能力更强。左岗岗 [<xref ref-type="bibr" rid="hanspub.40392-ref4">4</xref>] 分别采用SVM、GBDT、DNN对渭河流域的月径流和年径流进行预测，在年径流预测中SVM表现最好，而在月径流上，GBDT综合表现水平最好。许斌等 [<xref ref-type="bibr" rid="hanspub.40392-ref5">5</xref>] 引入RF和GBDT两类机器学习算法，对丹江口水库未来一段时间的径流序列进行预报，得到两类模型精度相似，可用于丹江口中长期的径流预报。</p><p>然而传统机器学习有时候只能得到几个有偏好的模型，得到结果存在一定的误差，因此应用于实际情况往往不是很理想。Stacking集成学习则可以通过引入次学习器，提高单一学习算法的预测效果，使预测结果更接近实际结果。鉴于此，本文将Stacking集成思想引入到现有的基于机器学习的径流预报模型中，以雅砻江流域锦屏一级水库为研究对象，预测锦屏一级水库月平均入库流量。首先，采用GBRT、XGBoost、RF、ET进行预测，并对预测结果进行统计分析，评价各单一算法可靠性。在此基础上，结合Stacking集成学习理论，进一步提升预测效果，并对预测结果进行分析。</p></sec><sec id="s6"><title>2. 研究方法</title><sec id="s6_1"><title>2.1. 基于Boosting的单一算法</title><p>Boosting算法，是一种可以用来减小监督式学习中偏差的机器学习算法，其中各个预测函数必须按照顺序迭代生成。Boosting算法工作机制为：先从初始训练集训练出一个基学习器，再根据基学习器的表现对样本分布进行调整，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至整个集成结果达到退出条件，然后将这些学习器进行加权结合 [<xref ref-type="bibr" rid="hanspub.40392-ref6">6</xref>]。其具体流程描述如图1所示。</p><p>图1. Boosting算法描述示意图</p><p>GBRT是一种以回归树为基本分类器用Boosting策略训练出来的模型。其原理是由多棵决策树构成，输出为每棵决策树输出结果的累加，利用梯度提升和回归决策树的组合方式，每次建立新的决策树模型都是在之前模型损失函数的梯度下降方向，使得决策树模型能够不断的改进 [<xref ref-type="bibr" rid="hanspub.40392-ref7">7</xref>]。</p><p>XGBoost则是在梯度提升决策树的基础上进行了改进，其优势表现在数据处理效率高、效果好、泛化能力强。作为GBRT的高效实现，XGBoost主要从以下三个方面做了优化：</p><p>1) 算法本身优化：在损失函数上，加上了正则化部分，并对误差部分做了二阶泰勒展开，使得结果更加准确。</p><p>2) 运行效率优化：决策树的建立过程中采用了并行化的选择，提高了算法的运行速度。</p><p>3) 健壮性优化：算法加入了L1和L2正则化项，可以有效的防止过拟合，其泛化能力更强。</p></sec><sec id="s6_2"><title>2.2. 基于Bagging的单一算法</title><p>Bagging算法，又称装袋算法，是一种可降低方差的机器学习算法，与Boosting最主要区别是Bagging的各个预测函数是并行生成的，可提高运行效率，当与其他回归算法结合时，则可以提高准确率和稳定性。其原理是给定包含N个样本的数据集，先随机取出一个样本放入采样中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中 [<xref ref-type="bibr" rid="hanspub.40392-ref8">8</xref>]。其算法描述如图2所示。</p><p>RF作为一种监督式集成学习模型，采用Bagging思想利用多棵决策树对样本进行训练的预测的一种分类器。对于一个输入样本，m棵树会有m个分类结果，而RF集成了所有的分类投票结果，将投票最多的类别指定为最终的输出，处理回归问题时，以每棵决策树输出的均值为最终结果。ET与RF十分相似，都是有许多随机树构成，主要区别有两点：</p><p>1) 对于每个决策树的训练集，RF是对采样集进行随机采样，以其结果作为每个决策树的训练集。而ET使用的是所有样本，只有特征是随机选择的；</p><p>2) RF在特征点的划分上是和传统决策树一样，会基于信息增益、信息增益率、基尼系数、均方差等原则来选择最优特征值；而ET在划分决策树上是随机的选择一个特征值进行划分的。</p><p>图2. Bagging算法描述示意图</p></sec><sec id="s6_3"><title>2.3. 基于Stacking的集成算法</title><p>在Bagging和Boosting中，弱学习器一般为同一模型，如RF、ET、GBRT和XGBoost的弱学习器都是决策树。因此想要将不同的模型结合在一起，综合考虑不同模型的优势，则可以采用Stacking集成方法。Stacking集成的思路是先将原始数据的特征作为输入，选取一系列弱学习器作为初级学习器，初级学习器的输出作为次级学习器的输入，最后的得到的输出则为Stacking预测结果。其算法描述如图3所示。</p><p>图3. Stacking算法描述示意图</p><p>在初级学习器的选取上，学习器个数过多可能会导致过拟合，一般选用2~3个模型作为初级学习器，效果会最好。因此，本文在RF、ET、GBRT和XGBoost中选取结果较好的3个模型作为初级学习器，以第一层预测的结果作为预报特征，并采用Logistic回归作为次级学习器对最终的结果进行预测。</p></sec><sec id="s6_4"><title>2.4. 评价指标的选取</title><p>回归评价指标主要包括平均绝对误差、均方误差、均方根误差、平均绝对百分误差、拟合优度等 [<xref ref-type="bibr" rid="hanspub.40392-ref9">9</xref>]。本文选取均方误差(MSE)和拟合优度(R<sup>2</sup>)作为模型模拟精度的评价指标。MSE是各数据偏离真实值的距离平方和的平均数，即误差平方和的平均数，在相同预测长度中，其值越小说明预测结果越好；R<sup>2</sup>是回归平方和在总平方和中所占比率，其值越接近1，表明其拟合预测性能越好。计算公式分别如下所示：</p><p>M S E = 1 N ∑ i = 1 n ( y i − y i ^ ) 2 (1)</p><p>R 2 = 1 − ∑ i ( y i − y i ^ ) 2 ∑ i ( y i − y i &#175; ) 2 (2)</p><p>同时，根据《水文情报规范》中的中长期预报的规范要求，本文选取预测值和实测值之间的相对误差范围在20%以内的预测为合格预测，误差范围在10%以内的预测为优秀预测。</p></sec></sec><sec id="s7"><title>3. 实例分析</title><p>锦屏一级水电站水库位于四川省雅砻江流域，是雅砻江干流下游河段的龙头水库，库容约77.6 &#215; 10<sup>8</sup> m<sup>3</sup>，水库主要用于发电，兼蓄水、拦沙和防洪。同时，由于该水库是下游河段的控制性水库，对下游其他水库具有显著的补偿效益。因此，准确预报该水电站水库月平均径流量，有利于指导该地区水资源综合开发利用、科学管理和优化调度。</p><sec id="s7_1"><title>3.1. 单一算法结果对比分析</title><p>本文选取锦屏一级1990~2012年共计23年的月径流数据，选取的预报因子为降水量、月平均气温、月平均水汽压、月平均相对湿度、前一年月径流值以及多年平均月径流值，其中月尺度气象数据可从中国气象数据网站上直接获取。以1990年1月~2011年12月共264个月的资料作为训练集进行建模，并以2012年1月~2012年12月共计12个月的资料作为验证集进行验证。分别用RF、ET、GBRT、XGBoost四种算法作为回归模型进行预测，并通过网格搜索对各个模型进行调参得到最佳预测效果。训练集各单一模型的拟合值与实测值的对比结果如图4所示。</p><p>图4. 训练集RF、ET、GBRT、XGBoost拟合值与实测值对比结果图</p><p>训练集各模型的R<sup>2</sup>分别为：XGBoost (0.9874)、ET (0.9851)、GBRT (0.9812)、RF (0.9726)。同时，由上图可以看出在训练集中，各模型拟合结果与实测值总体比较接近，且各模型之间相差不大，说明在训练集中各个模型中都能很好的拟合出实际径流值。各个模型的预测结果和实际值的对比结果如表1所示。</p><p>由表1可得，合格次数从高到低的排序分别是XGBoost (9次)、GBRT (8次)、ET (8次)、RF (6次)；优秀次数从高到底的排序分别是XGBoost (3次)、GBRT (3次)、ET (2次)、RF (2次)。各单模型预测结果与实测值对比如图5所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison results of RF, ET, GBRT, XGBoost and measured value</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >月份</th><th align="center" valign="middle"  rowspan="2"  >实测值(m<sup>3</sup>/s)</th><th align="center" valign="middle"  colspan="2"  >RF月径流预测模型</th><th align="center" valign="middle"  colspan="2"  >ET月径流模型</th><th align="center" valign="middle"  colspan="2"  >GBRT月径流模型</th><th align="center" valign="middle"  colspan="2"  >XGBoost月径流模型</th></tr></thead><tr><td align="center" valign="middle" >预测值(m<sup>3</sup>/s)</td><td align="center" valign="middle" >相对误差</td><td align="center" valign="middle" >预测值(m<sup>3</sup>/s)</td><td align="center" valign="middle" >相对误差</td><td align="center" valign="middle" >预测值(m<sup>3</sup>/s)</td><td align="center" valign="middle" >相对误差</td><td align="center" valign="middle" >预测值(m<sup>3</sup>/s)</td><td align="center" valign="middle" >相对误差</td></tr><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >261</td><td align="center" valign="middle" >357</td><td align="center" valign="middle" >36.8</td><td align="center" valign="middle" >313</td><td align="center" valign="middle" >19.9</td><td align="center" valign="middle" >351</td><td align="center" valign="middle" >34.5</td><td align="center" valign="middle" >292</td><td align="center" valign="middle" >11.9</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >233</td><td align="center" valign="middle" >221</td><td align="center" valign="middle" >−5.2</td><td align="center" valign="middle" >273</td><td align="center" valign="middle" >17.2</td><td align="center" valign="middle" >269</td><td align="center" valign="middle" >15.5</td><td align="center" valign="middle" >258</td><td align="center" valign="middle" >10.7</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >250</td><td align="center" valign="middle" >334</td><td align="center" valign="middle" >33.6</td><td align="center" valign="middle" >397</td><td align="center" valign="middle" >58.8</td><td align="center" valign="middle" >297</td><td align="center" valign="middle" >18.8</td><td align="center" valign="middle" >287</td><td align="center" valign="middle" >14.8</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >401</td><td align="center" valign="middle" >460</td><td align="center" valign="middle" >14.7</td><td align="center" valign="middle" >479</td><td align="center" valign="middle" >19.5</td><td align="center" valign="middle" >457</td><td align="center" valign="middle" >14.0</td><td align="center" valign="middle" >447</td><td align="center" valign="middle" >11.5</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >664</td><td align="center" valign="middle" >654</td><td align="center" valign="middle" >−1.5</td><td align="center" valign="middle" >599</td><td align="center" valign="middle" >−9.8</td><td align="center" valign="middle" >626</td><td align="center" valign="middle" >−5.7</td><td align="center" valign="middle" >542</td><td align="center" valign="middle" >−18.4</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >1822</td><td align="center" valign="middle" >2296</td><td align="center" valign="middle" >26.0</td><td align="center" valign="middle" >2071</td><td align="center" valign="middle" >13.7</td><td align="center" valign="middle" >2085</td><td align="center" valign="middle" >14.4</td><td align="center" valign="middle" >1931</td><td align="center" valign="middle" >6.0</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >4558</td><td align="center" valign="middle" >3221</td><td align="center" valign="middle" >−29.3</td><td align="center" valign="middle" >3428</td><td align="center" valign="middle" >−24.8</td><td align="center" valign="middle" >3378</td><td align="center" valign="middle" >−25.9</td><td align="center" valign="middle" >3521</td><td align="center" valign="middle" >−22.8</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >2284</td><td align="center" valign="middle" >2708</td><td align="center" valign="middle" >18.6</td><td align="center" valign="middle" >1858</td><td align="center" valign="middle" >−18.7</td><td align="center" valign="middle" >1901</td><td align="center" valign="middle" >−16.8</td><td align="center" valign="middle" >1771</td><td align="center" valign="middle" >−22.5</td></tr><tr><td align="center" valign="middle" >9</td><td align="center" valign="middle" >2692</td><td align="center" valign="middle" >3080</td><td align="center" valign="middle" >14.4</td><td align="center" valign="middle" >3421</td><td align="center" valign="middle" >27.1</td><td align="center" valign="middle" >2763</td><td align="center" valign="middle" >2.6</td><td align="center" valign="middle" >2501</td><td align="center" valign="middle" >−7.1</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >1838</td><td align="center" valign="middle" >1130</td><td align="center" valign="middle" >−38.5</td><td align="center" valign="middle" >1492</td><td align="center" valign="middle" >−18.8</td><td align="center" valign="middle" >1267</td><td align="center" valign="middle" >−31.1</td><td align="center" valign="middle" >1357</td><td align="center" valign="middle" >−26.2</td></tr><tr><td align="center" valign="middle" >11</td><td align="center" valign="middle" >800</td><td align="center" valign="middle" >654</td><td align="center" valign="middle" >−18.3</td><td align="center" valign="middle" >728</td><td align="center" valign="middle" >−9.0</td><td align="center" valign="middle" >829</td><td align="center" valign="middle" >3.6</td><td align="center" valign="middle" >727</td><td align="center" valign="middle" >−9.1</td></tr><tr><td align="center" valign="middle" >12</td><td align="center" valign="middle" >390</td><td align="center" valign="middle" >531</td><td align="center" valign="middle" >36.2</td><td align="center" valign="middle" >523</td><td align="center" valign="middle" >34.1</td><td align="center" valign="middle" >490</td><td align="center" valign="middle" >25.6</td><td align="center" valign="middle" >458</td><td align="center" valign="middle" >17.4</td></tr></tbody></table></table-wrap><p>表1. RF、ET、GBRT、XGBoost与实测值对比结果</p><p>图5. RF、ET、GBRT、XGBoost预测值与实测值对比结果图</p><p>各个模型的预测结果回归指标、优秀率以及合格率如表2所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> RF, ET, GBRT, XGBoost evaluation index result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >RF</th><th align="center" valign="middle" >ET</th><th align="center" valign="middle" >GBRT</th><th align="center" valign="middle" >XGBoost</th></tr></thead><tr><td align="center" valign="middle" >MSE</td><td align="center" valign="middle" >242085</td><td align="center" valign="middle" >185886</td><td align="center" valign="middle" >163864</td><td align="center" valign="middle" >137347</td></tr><tr><td align="center" valign="middle" >R<sup>2</sup></td><td align="center" valign="middle" >0.8064</td><td align="center" valign="middle" >0.8507</td><td align="center" valign="middle" >0.8435</td><td align="center" valign="middle" >0.8696</td></tr><tr><td align="center" valign="middle" >合格率</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >0.67</td><td align="center" valign="middle" >0.67</td><td align="center" valign="middle" >0.75</td></tr><tr><td align="center" valign="middle" >优秀率</td><td align="center" valign="middle" >0.17</td><td align="center" valign="middle" >0.17</td><td align="center" valign="middle" >0.25</td><td align="center" valign="middle" >0.25</td></tr></tbody></table></table-wrap><p>表2. RF、ET、GBRT、XGBoost评价指标结果</p><p>从上图可以看出各个模型与实测结果的总体拟合结果都比较好，但是不同的模型结果还是有些差距的。在非汛期各个模型与的拟合结果与实测值都很接近，但是在汛期误差则相对比较大。通过上表可以得到RF的均方误差最大，并且R<sup>2</sup>、合格率以及优秀率都最小，综合四个模型，其效果最差。GBRT和ET总体比较接近，却各有优劣，GBRT的MSE和优秀率均优于ET，但是其R<sup>2</sup>差于ET。XGBoost各项指标在四个模型中均为最优，因此，在单一算法的结果中XGBoost的预测结果最好。</p></sec><sec id="s7_2"><title>3.2. 集成算法结果对比分析</title><p>由于单一模型的计算结果中，RF的计算结果相较于剩下三个模型来说，精度偏低，所以在Stacking结合策略集成学习模型中，选用GBRT、XGBoost和ET作为初级学习器，Logistic回归作为次学习器，采用5折交叉验证对初级学习器的结果进行Stacking集成。</p><p>以GBRT为例，首先把整个数据集分为训练集和测试集两部分，然后把训练集分成5份，先拿出其中4份作为训练集另外1份作为测试集，在第一次交叉验证后会得到一个关于这一份测试集的预测值TrainP1，然后对原来整个数据集的预测值TestP1。5折交叉验证，即将上述过程进行5次，得到针对Training Data数据预测5列数据TrainP1，TrainP2，TrainP3，TrainP4，TrainP5以及对Testing Data数据预测的5列数据TestP1，TestP2，TestP3，TestP4，TestP5。此时得到的新特征1就是由TrainP1，TrainP2，TrainP3，TrainP4，TrainP5拼凑而成，而新特征1对应的TestP即为estP1，TestP2，TestP3，TestP4，TestP5的平均值，这就是Stacking的一个完整的流程。接着再对ET和XGBoost这两个模型重复以上步骤即可得到新特征2和新特征3以及对应的TestP。最后将得到的新特征以及对应的预测值进入到下一层模型采用Logistic进行进一步训练得到最终的结果。其过程如图6所示。</p><p>图6. 5折交叉验证示意图</p><p>训练集中XGBoost和Stacking集成的拟合结果与实测值对比如图7所示。</p><p>图7. 训练集XGBoost、Stacking集成拟合值与实测值对比结果图</p><p>训练集Stacking集成的R<sup>2</sup>为0.9941，相较于XGBoost有所提升，说明Stacking集成在训练集中与实测值更为接近，其拟合效果最好。将Stacking集成算法和单一模型结果最好的XGBoost与实测值的拟合结果如图8所。</p><p>图8. XGBoost、Stacking集成预测值与实测值对比结果图</p><p>Stacking集成MSE、R<sup>2</sup>、合格率以及优秀率与XGBoost的对比结果如表3所示。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Stacking integration and XGBoost evaluation index result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >MSE</th><th align="center" valign="middle" >R<sup>2</sup></th><th align="center" valign="middle" >合格率</th><th align="center" valign="middle" >优秀率</th></tr></thead><tr><td align="center" valign="middle" >Stacking集成</td><td align="center" valign="middle" >129751</td><td align="center" valign="middle" >0.8934</td><td align="center" valign="middle" >0.83</td><td align="center" valign="middle" >0.25</td></tr><tr><td align="center" valign="middle" >XGBoost</td><td align="center" valign="middle" >137347</td><td align="center" valign="middle" >0.8696</td><td align="center" valign="middle" >0.75</td><td align="center" valign="middle" >0.25</td></tr></tbody></table></table-wrap><p>表3. Stacking集成与XGBoost评价指标结果</p><p>从上图可以看出，Stacking集成和XGBoost总体上与实测值更为接近。从上表的对比结果中可得Stacking集成除了优秀率效果没有发生变化外，其他三个指标都有正向提升，其中：MSE下降了5.854%、R<sup>2</sup>提升了2.737%、合格率提升了10.667%。从Stacking集成和XGBoost的对比结果中，进一步得到：Stacking集成相对于各单一机器学习模型而言，根据相关评价指标，可认为其预报效果最好。由此可见，相对于单一算法，Stacking集成对中长期径流预报结果更符合实际。</p></sec></sec><sec id="s8"><title>4. 总结</title><p>机器学习作为近几年兴起的一种热门算法，在各种竞赛中都取得了很好的成绩。本文选取机器学习算法中的集成算法对锦屏一级的月径流进行了模拟和预测。在单一算法中，除了RF月径流预测结果较差外，GBRT、XGBoost和ET模型结果表现都比较好，其中XGBoost月径流模型行综合效果最好，能够满足一般中长期径流预报精度要求。</p><p>在此基础上，择优选取其中三个模型作为初级学习器，以其结果作为下一层的输入，选取Logistic回归作为次学习器，采用5折交叉验证，构建基于Stacking集成策略的预测模型进行预测。其最终结果相对于单一模型的结果都有所提升，体现了Stacking集成算法在径流预报中的优势，为中长期径流预报提供了新的可行思路，为分析和研究中长期径流的变化规律提供了新的研究方法。</p></sec><sec id="s9"><title>基金项目</title><p>国家自然科学基金(51709105)；中央高校基本科研业务费专项资金资助(2020MS026; 2019MS031)。</p></sec><sec id="s10"><title>文章引用</title><p>吕盼成,王丽萍,刘 源. 基于机器集成学习的中长期径流预报研究Research on Medium and Long-Term Runoff Forecasting Based on Machine Integrated Learning[J]. 水资源研究, 2021, 10(01): 44-52. https://doi.org/10.12677/JWRR.2021.101005</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.40392-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">朱双. 流域中长期水文预报与水资源承载力评价方法研究[D]: [博士学位论文]. 武汉: 华中科技大学, 2017.  
ZHU Shuang. Studies on watershed long-term hydrological forecast and evaluation method of water resources carrying capacity. Ph.D. Thesis, Wuhan: Huazhong University of Science &amp; Technology. 2017. (in Chinese)</mixed-citation></ref><ref id="hanspub.40392-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">KIRK M. p ython机器学习实践: 测试驱动的开发方法[M]. 北京: 机械工业出版社, 2018.  
KIRK M. Thoughtful machine learning with Python: A test-driven approach. Beijing: China Machine Press, 2018. (in Chi-nese)</mixed-citation></ref><ref id="hanspub.40392-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">李伶杰, 王银堂, 胡庆芳, 刘定忠, 张安富, 巴亚荃. 基于随机森林与支持向量机的水库长期径流预报[J]. 水利水运工程学报, 2020(4): 33-40.  
LI Lingjie, WANG Yintang, HU Qingfang, LIU Dingzhong, ZHANG Anfu, and BAYAQUAN. Long-term reservoir runoff forecast based on random forest and support vector machine. Journal of Water Resources and Water Transport Engineering, 2020(4): 33-40. (in Chinese)</mixed-citation></ref><ref id="hanspub.40392-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">左岗岗. 基于机器学习的渭河流域径流预测系统研究[D]: [硕士学位论文]. 西安: 西安理工大学, 2017.  
ZUO Ganggang. The research of WEI River runoff prediction system based on machine learning. Master’s Thesis, Xi’an: Xi’an University of Technology, 2017. (in Chinese)</mixed-citation></ref><ref id="hanspub.40392-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">许斌, 杨凤根, 郦于杰. 两类集成学习算法在中长期径流预报中的应用[J]. 水力发电, 2020, 46(4): 21-24+34.  
XU Bin, YANG Fenggen, and LI Yujie. Application of two types of integrated learning algorithms in mid- and long-term runoff forecasting. Hydropower, 2020, 46(4): 21-24+34. (in Chinese)</mixed-citation></ref><ref id="hanspub.40392-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">胡丹. 面向视觉跟踪的深度学习模型设计与优化研究[D]: [博士学位论文]. 西安: 西北工业大学, 2017.  
HU Dan. Model design and optimization of deep learning for visual tracking. Ph.D. Thesis, Xi’an: Northwestern Polytechnical University, 2017. (in Chinese)</mixed-citation></ref><ref id="hanspub.40392-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">陈宏, 邓芳明, 吴翔, 付智辉. 基于梯度提升决策树的电力电子电路故障诊断[J]. 测控技术, 2017, 36(5): 9-12+20.  
CHEN Hong, DENG Fangming, WU Xiang, and FU Zhihui. Power electronic circuit fault diagnosis based on gradient boosting decision tree. Measurement and Control Technology, 2017, 36(5): 9-12+20. (in Chinese)</mixed-citation></ref><ref id="hanspub.40392-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">侯舒凯. 基于集成学习方法的MINIST手写数字识别[J]. 通讯世界, 2018(8): 236-237.  
HOU Shukai. MINIST handwritten digit recognition based on integrated learning method. Communication World, 2018(8): 236-237. (in Chinese)</mixed-citation></ref><ref id="hanspub.40392-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">宋俊杰. 三峡流域中长期径流预报模型精度评定综合分析及优化方法研究[D]: [硕士学位论文]. 武汉: 华中科技大学, 2013.  
SONG Junjie. Comprehensive analysis and optimization method of the accuracy assessment of the medium and long-term runoff forecast model in the Three Gorges Basin. Master’s Thesis, Wuhan: Huazhong University of Science and Technology, 2013. (in Chinese)</mixed-citation></ref></ref-list></back></article>