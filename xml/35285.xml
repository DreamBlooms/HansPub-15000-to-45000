<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2020.94069</article-id><article-id pub-id-type="publisher-id">AAM-35285</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20200400000_18753938.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于人工神经网络的WENO重构方法
  The WENO Reconstruction Based on the Artificial Neural Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>琪</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>温</surname><given-names>晓</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>中国海洋大学数学科学学院，山东 青岛</addr-line></aff><aff id="aff3"><addr-line>山东科技大学数学与系统科学学院，山东 青岛</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>04</month><year>2020</year></pub-date><volume>09</volume><issue>04</issue><fpage>574</fpage><lpage>583</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  本文提出了一种基于人工神经网络(ANN)的WENO重构方法，用ANN来近似WENO-JS中的非线性权重，其中ANN的输入为用于五阶WENO求解的模板中的五点函数值，输出为三个子模板的光滑性指标，光滑性指标可直接转化为对应的非线性权重，用于WENO重构。数值实验表明，与WENO-JS相比，WENO-ANN能保持精度，且数值色散耗散较低。
   A method for WENO reconstruction based on the artificial neural network (ANN) is proposed, which uses the ANN to approximate the non-linear weights in WENO-JS, where the input of the ANN is the value of the five-point function in the stencil for fifth-order WENO scheme, and the output is the smoothness indicators of the three sub-stencils. The smoothness indicators can be directly converted into corresponding non-linear weights for WENO reconstruction. Numerical experiments show that, compared with the WENO-JS scheme, the WENO-ANN can maintain accuracy, the numerical dispersion and dissipation are lower.
 
</p></abstract><kwd-group><kwd>人工神经网络，WENO格式，非线性权重, Artificial Neural Network</kwd><kwd> WENO Scheme</kwd><kwd> Non-Linear Weights</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于人工神经网络的WENO重构方法<sup> </sup></title><p>刘 琪<sup>1</sup>，温 晓<sup>2</sup></p><p><sup>1</sup>中国海洋大学数学科学学院，山东 青岛</p><p><sup>2</sup>山东科技大学数学与系统科学学院，山东 青岛</p><p>收稿日期：2020年4月5日；录用日期：2020年4月20日；发布日期：2020年4月27日</p><disp-formula id="hanspub.35285-formula43"><graphic xlink:href="//html.hanspub.org/file/13-2621184x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>本文提出了一种基于人工神经网络(ANN)的WENO重构方法，用ANN来近似WENO-JS中的非线性权重，其中ANN的输入为用于五阶WENO求解的模板中的五点函数值，输出为三个子模板的光滑性指标，光滑性指标可直接转化为对应的非线性权重，用于WENO重构。数值实验表明，与WENO-JS相比，WENO-ANN能保持精度，且数值色散耗散较低。</p><p>关键词 :人工神经网络，WENO格式，非线性权重</p><disp-formula id="hanspub.35285-formula44"><graphic xlink:href="//html.hanspub.org/file/13-2621184x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/13-2621184x7_hanspub.png" /> <img src="//html.hanspub.org/file/13-2621184x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>对于流体力学中的某些初边界值问题(IBVP)，在求解过程中，它们的数值解可能会在有限时间内形成不连续性，即冲击波。为了更好的处理这些不连续性，必须专门研究用于求解这类偏微分方程(PDE)的数值方法 [<xref ref-type="bibr" rid="hanspub.35285-ref1">1</xref>]。</p><p>1987年，Harten等人 [<xref ref-type="bibr" rid="hanspub.35285-ref2">2</xref>] 提出了一类称为本质无振荡(ENO)格式的高分辨率方法，该方法衡量了数值解在多个子模板上的光滑度，然后根据最光滑的子模板计算通量，以此来避免通过不连续性进行插值。1996年，Jiang和Shu等人 [<xref ref-type="bibr" rid="hanspub.35285-ref3">3</xref>] 对ENO进行了改进，提出了加权本质无振荡方法(WENO-JS)，该方法再次在多个子模板上计算光滑度。但是，WENO-JS不是仅采用最光滑的模板，而是采用每个模板上预测的通量的加权平均值来强调较平滑的模板。</p><p>之后的许多工作都是建立在原始的WENO-JS方法上，如修改光滑指示因子 [<xref ref-type="bibr" rid="hanspub.35285-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.35285-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.35285-ref6">6</xref>]，修改非线性权重 [<xref ref-type="bibr" rid="hanspub.35285-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.35285-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.35285-ref9">9</xref>] 等。但是通过这类WENO-JS方法得到的数值解都会依赖于光滑指示因子和灵敏度参数，尽管现在已经有比较精确的光滑指示因子计算方法，但并没有研究表明，它们就是最优的选择。并且根据传统的光滑指示因子 [<xref ref-type="bibr" rid="hanspub.35285-ref3">3</xref>] 得出的权重，在通量函数的一阶或高阶倒数消失的点上，无法恢复解的最高阶数 [<xref ref-type="bibr" rid="hanspub.35285-ref7">7</xref>]。</p><p>在过去的几十年中，机器学习已在数据分析中变得无处不在，并且在求解PDE的数值解方面也得到了很多的重视。Lagaris等人 [<xref ref-type="bibr" rid="hanspub.35285-ref10">10</xref>] 将微分方程解的形式参数化，用神经网络进行训练，并优化权重以减小解的残差。Yu等人 [<xref ref-type="bibr" rid="hanspub.35285-ref11">11</xref>] 训练了一个神经网络来对局部光滑度进行分类，并根据此分类计算了人工粘度。Bar-Sinai等人 [<xref ref-type="bibr" rid="hanspub.35285-ref12">12</xref>] 使用模拟数据将粗粒度模型嵌入到涉及神经网络的有限差分格式中，从而使它们可以在相对粗网格上实现低误差。Pfau等人 [<xref ref-type="bibr" rid="hanspub.35285-ref13">13</xref>] 将特征值问题的特征函数参数化为神经网络，并将训练转化为双级优化问题以减少偏差，从而显著降低了内存需求。Hsieh等人 [<xref ref-type="bibr" rid="hanspub.35285-ref14">14</xref>] 通过学习如何使用深度神经网络迭代地改进数值解，得到了特定领域的快速PDE求解器，与最先进的求解器 [<xref ref-type="bibr" rid="hanspub.35285-ref15">15</xref>] 相比，其速度提高了2~3倍 [<xref ref-type="bibr" rid="hanspub.35285-ref16">16</xref>]。</p><p>在本文中，我们构造了利用人工神经网络(ANN)求非线性权重的WENO方法(记为WENO-ANN)。其中ANN的输入为用于WENO求解的模板中的五点函数值，输出为三个子模板的光滑性指标, 光滑性指标可直接转化为对应的非线性权重，用于WENO重构。因此WENO-ANN在求解过程中不需要人工参数，提高了算法的稳定性。数值实验表明与WENO-JS相比，WENO-ANN能保持精度，且数值色散耗散较低。在高分辨率情况下，能有效提高计算效率。</p></sec><sec id="s4"><title>2. 加权本质无振荡WENO-JS格式</title><p>满足双曲守恒律的可压缩流数值模型为</p><p>∂ u ∂ t + ∇ ⋅ F ( u ) = 0 ， (1)</p><p>考虑均匀网格点 x i = i Δ x ,   i = 0 , ⋯ , N ，公式(1)的离散格式可表示为</p><p>d u i ( t ) d t = − ∂ f ∂ x | x = x i ,   i = 0 , ⋯ , N ， (2)</p><p>其中 u i ( t ) 是点值 u ( x i , t ) 的数值近似。用于双曲守恒律的守恒有限差分公式要求在单元边界 x i + 1 / 2 = x i + Δ x 2 处具有高阶一致的数值通量，以便在均匀分布的单元上形成通量差。可以通过隐式地定义</p><p>数值通量函数</p><p>f ( x ) = 1 Δ x ∫ x − Δ x 2 x + Δ x 2 h ( ξ ) d ξ ,</p><p>来达到空间离散的守恒性质，从而使得(2)式中的空间导数可以在单元边界处通过守恒有限差分公式得到精确近似，</p><p>d u i ( t ) d t = − 1 Δ x ( h i + 1 / 2 − h i − 1 / 2 ) ,</p><p>其中 h i &#177; 1 / 2 = h ( x i &#177; 1 / 2 ) 。</p><p>h i &#177; 1 / 2 处的高阶多项式插值可以通过已知网格点处的值 f i = f ( x i ) 来计算。经典的五阶WENO格式如图1所示，</p><p>图1. 五阶WENO示意图</p><p>它把包含五个点的大模板 S 5 分成了三个子模板 { S 0 , S 1 , S 2 } ，其中每个子模板包含三个点。通过插值点值 f ( k ) ( x i &#177; 1 / 2 ) 的凸组合建立五阶多项式逼近 f i &#177; 1 / 2 = h i &#177; 1 / 2 + O ( Δ x 5 ) ，这里 f ( k ) ( x ) 是定义在每个子模板 S k 上的三阶多项式，即单元边界 x i + 1 / 2 处的五阶WENO重构可表示为：</p><p>f i + 1 / 2 = ∑ k = 0 2 ω k f i + 1 / 2 ( k ) ,</p><p>其中 f i + 1 / 2 ( k ) ,   k = 0 , 1 , 2 为三个子模板上二次多项式 p k ( x ) 在 x i + 1 / 2 处的函数值：</p><p>{ f i + 1 / 2 ( 0 ) = 1 3 f i − 2 − 7 6 f i − 1 + 11 6 f i , f i + 1 / 2 ( 1 ) = − 1 6 f i − 1 + 5 6 f i + 1 3 f i + 1 , f i + 1 / 2 ( 2 ) = 1 3 f i + 5 6 f i + 1 − 1 6 f i + 2 .</p><p>五阶WENO-JS格式的非线性权重 ω k ,   k = 0 , 1 , 2 为</p><p>ω k = α k ∑ l = 0 2 α l ,     α k = c k ( ε + β k ) 2 ,</p><p>其中灵敏度参数为 ε = 10 − 12 ， c k 为理想权重且取值为 c 0 = 1 10 ,   c 1 = 3 5 ,   c 2 = 3 10 。 β k 为光滑指示因子，用来</p><p>衡量子模板上的光滑性，可显式地写为</p><p>{ β 0 = 13 12 ( f i − 2 − 2 f i − 1 + f i ) 2 + 1 4 ( f i − 2 − 4 f i − 1 + 3 f i ) 2 , β 1 = 13 12 ( f i − 1 − 2 f i + f i + 1 ) 2 + 1 4 ( f i − 1 − f i + 1 ) 2 , β 2 = 13 12 ( f i − 2 f i + 1 + f i + 2 ) 2 + 1 4 ( 3 f i − 4 f i + 1 + f i + 2 ) 2 .</p></sec><sec id="s5"><title>3. 基于BP神经网络的WENO权重算法</title><p>BP神经网络能学习和存储大量的输入–输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程，其网络模型拓扑结构包括输入层(input layer)、隐含层(hidden layer)和输出层(output layer)。BP神经网络的学习过程由前向计算过程和误差反向传播过程组成。在前向计算过程中，输入量从输入层经隐含层逐层计算，并传向输出层，每层神经元的状态只影响下一层神经元的状态。如果输出层不能得到期望的输出，则转入误差反向传播过程，误差信号沿原来的连接通路返回，逐次调整网络各层的权值和阈值，直至达到输入层，再重复向计算。这两个过程依次反复进行，不断调整各层的权重和阈值，使得网络误差最小或达到人们所期望的要求时，学习过程结束。</p><p>由公式(1)可知，计算每个单元边界 x i + 1 / 2 处的值时，都要重新计算非线性权重 ω k ( k = 0 , 1 , 2 ) ，并且在计算非线性权重时，会依赖于灵敏度参数和光滑指示因子的选择。因此我们考虑用BP神经网络来近似式(1)中的权重 ω k ( k = 0 , 1 , 2 ) ，从而避免人工参数的选择，输入 x i 及其附近的函数值 ( f i − 2 , f i − 1 , f i , f i + 1 , f i + 2 ) ∈ ℝ 5 ，可以快速的输出每个子模板上函数的光滑性，记为Index，然后将输出的光滑性指标Index转化为定每个子模板上的权重 ω k ( k = 0 , 1 , 2 ) ，两者的对应关系如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The corresponding relationship between the smoothness index and the weights of the sub-stencils ω k ( k = 0 , 1 , 2 </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Index</th><th align="center" valign="middle" >( 1 , 1 , 1 )</th><th align="center" valign="middle" >( 1 , 1 , 0 )</th><th align="center" valign="middle" >( 0 , 1 , 1 )</th><th align="center" valign="middle" >( 1 , 0 , 0 )</th></tr></thead><tr><td align="center" valign="middle" >ω</td><td align="center" valign="middle" >( 1 10 , 3 5 , 3 10 )</td><td align="center" valign="middle" >( 1 7 , 6 7 , 0 )</td><td align="center" valign="middle" >( 0 , 2 3 , 1 3 )</td><td align="center" valign="middle" >( 1 , 0 , 0 )</td></tr><tr><td align="center" valign="middle" >Index</td><td align="center" valign="middle" >( 0 , 0 , 1 )</td><td align="center" valign="middle" >( 0 , 0 , 0 )</td><td align="center" valign="middle" >( 1 , 0 , 1 )</td><td align="center" valign="middle" >( 0 , 1 , 0 )</td></tr><tr><td align="center" valign="middle" >ω</td><td align="center" valign="middle" >( 0 , 0 , 1 )</td><td align="center" valign="middle" >( 1 626 , 625 939 , 625 1878 )</td><td align="center" valign="middle" >( 169 730 , 27 365 , 507 730 )</td><td align="center" valign="middle" >( 625 4402 , 949 1114 , 27 4402 )</td></tr></tbody></table></table-wrap><p>表1. 光滑性指标Index与子模版上权重 ω k ( k = 0 , 1 , 2 ) 的对应关系</p><sec id="s6_0_1"><title>3.1. 网络的输入输出</title><p>网络的输入为 x i 及其附近的函数值 ( f i − 2 , f i − 1 , f i , f i + 1 , f i + 2 ) ∈ ℝ 5 ，因此输入层神经元个数 N I = 5 。网络输出层神经元个数 N O = 3 ，为子模版光滑性指标Index，其与三个子模板的光滑性有关，具体见表2：</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> The corresponding relationship between the smoothness index and the smoothness of the three sub-stencils S 0 , S 1 , S </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >光滑性</th><th align="center" valign="middle" >S 0 , S 1 , S 2 都光滑</th><th align="center" valign="middle" >S 0 , S 1 光滑， S 2 间断</th><th align="center" valign="middle" >S 0 间断， S 1 , S 2 光滑</th><th align="center" valign="middle" >S 0 光滑， S 1 , S 2 间断</th><th align="center" valign="middle" >S 0 , S 1 间断， 光滑</th><th align="center" valign="middle" >S 0 一阶导数不连续</th><th align="center" valign="middle" >S 1 一阶导数不连续</th><th align="center" valign="middle" >S 2 一阶导数不连续</th></tr></thead><tr><td align="center" valign="middle" >Index</td><td align="center" valign="middle" >( 1 , 1 , 1 )</td><td align="center" valign="middle" >( 1 , 1 , 0 )</td><td align="center" valign="middle" >( 0 , 1 , 1 )</td><td align="center" valign="middle" >( 1 , 0 , 0 )</td><td align="center" valign="middle" >( 0 , 0 , 1 )</td><td align="center" valign="middle" >( 0 , 0 , 0 )</td><td align="center" valign="middle" >( 1 , 0 , 1 )</td><td align="center" valign="middle" >( 0 , 1 , 0 )</td></tr></tbody></table></table-wrap><p>表2. 三个子模板 S 0 , S 1 , S 2 上光滑性与子模版光滑性指标Index的对应关系</p></sec><sec id="s6_1"><title>3.2. 数据集的设计</title><p>如3.1中所述，数据集的输入取函数 f ( x ) 定义域D内某一点 x i 及其左右两点处的函数值 ( f i − 2 , f i − 1 , f i , f i + 1 , f i + 2 ) ，函数 f ( x ) 、参数取值、定义域、间断标识及每类函数的选取数量如表3所示。在神经网络训练过程中，随机选取80%进行训练，剩余的20%用于网络测试。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> The function f ( x ) , the value of the parameters, the domain, the smoothness index and the number of the functions in the data bas</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >f ( x )</th><th align="center" valign="middle" >参数取值</th><th align="center" valign="middle" >定义域D</th><th align="center" valign="middle" >Index</th><th align="center" valign="middle" >数量</th></tr></thead><tr><td align="center" valign="middle" >sin ( k π x )</td><td align="center" valign="middle" >k = 2</td><td align="center" valign="middle" >( 0 , 1 )</td><td align="center" valign="middle" >( 1 , 1 , 1 )</td><td align="center" valign="middle" >18000</td></tr><tr><td align="center" valign="middle" >k x</td><td align="center" valign="middle" >k ∈ ( − 10 , 10 )</td><td align="center" valign="middle" ><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/13-2621184x105_hanspub.png" xlink:type="simple"/></inline-formula></td><td align="center" valign="middle" >( 1 , 1 , 1 )</td><td align="center" valign="middle" >40000</td></tr><tr><td align="center" valign="middle" >k x a</td><td align="center" valign="middle" >k ∈ ( − 10 , 10 ) , a ∈ ( 2 , 5 )</td><td align="center" valign="middle" >( − 1 k , 1 k + 4 d x )</td><td align="center" valign="middle" >( 1 , 1 , 1 )</td><td align="center" valign="middle" >40000</td></tr><tr><td align="center" valign="middle" >c</td><td align="center" valign="middle" >c ∈ ( − 1 , 1 )</td><td align="center" valign="middle" >( 0 , 1 )</td><td align="center" valign="middle" >( 1 , 1 , 1 )</td><td align="center" valign="middle" >20000</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >k | x |</td><td align="center" valign="middle"  rowspan="3"  >k ∈ ( − 10 , 10 )</td><td align="center" valign="middle"  rowspan="3"  ></td><td align="center" valign="middle" >( 0 , 0 , 0 )</td><td align="center" valign="middle" >10000</td></tr><tr><td align="center" valign="middle" >( 1 , 0 , 1 )</td><td align="center" valign="middle" >10000</td></tr><tr><td align="center" valign="middle" >( 0 , 1 , 0 )</td><td align="center" valign="middle" >10000</td></tr><tr><td align="center" valign="middle"  rowspan="4"  >l ( x &lt; x 0 ) + r ( x &gt; x 0 )</td><td align="center" valign="middle"  rowspan="4"  >l ∈ ( − 1 , 1 ) , r ∈ ( − 1 , 1 )</td><td align="center" valign="middle"  rowspan="4"  >( 0 , 1 )</td><td align="center" valign="middle" >( 1 , 1 , 0 )</td><td align="center" valign="middle" >20000</td></tr><tr><td align="center" valign="middle" >( 0 , 1 , 1 )</td><td align="center" valign="middle" >20000</td></tr><tr><td align="center" valign="middle" >( 1 , 0 , 0 )</td><td align="center" valign="middle" >20000</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >20000</td></tr></tbody></table></table-wrap><p>表3. 数据集中的函数 f ( x ) 、参数取值、定义域、光滑性指标及训练集中每类函数的数量</p></sec><sec id="s6_2"><title>3.3. 网络结构的确定</title><p>1) 网络的层数</p><p>1989年，Robert Hecht-Nielsen [<xref ref-type="bibr" rid="hanspub.35285-ref17">17</xref>] 证明了对于任何闭区间内的一个连续函数，都可以用一个隐含层的BP网络来逼近，当学习不连续函数时BP网络需要两个隐含层。因此，本文选择两个隐含层 H 1 , H 2 的BP网络。</p><p>2) 隐含层节点数</p><p>各隐藏层节点数的选取会影响神经网络的性能。若隐藏层中节点数太少，会出现网络识别未经训练样本能力差，容错性低等问题；若隐藏层节点数太多，则会导致网络学习时间大幅加长，样本中一些如干扰、噪声等非规律性内容存储进网络等问题。隐藏层的节点数往往与求解问题的要求，输入输出节点数有不可分割的联系。在本文中设置的隐含层节点数 { l 1 , l 2 } = { 20 , 15 } 。</p><p>3) 激活函数(activation function)</p><p>引入激活函数的目的是在模型中引入非线性。如果没有激活函数，那么无论神经网络有多少层，最终都是一个线性映射，单纯的线性映射无法解决线性不可分问题。</p><p>本文在隐含层 H 1 , H 2 中均选用Relu激活函数( f ( x ) = max ( 0 , x ) )，以增加特征效应；在输出层中选用</p><p>sigmoid激活函数( f ( x ) = 1 1 + e − x )，将输出数据映射到 ( 0 , 1 ) 的范围内，从而进行分类。</p><p>4) 数据的归一化处理。</p><p>对于大小不一致的输入函数值，本文中选取的S型激活函数(sigmoid)在输入数据过大时变化平缓，区分度太小。因此需要采取归一化算法对输入数据进行预处理，本文中采用线性归一化算法将数据归一化到 [ 0 , 1 ] 区间：</p><p>f ^ j = f j max k = − 2 , ⋯ , 2 ( | f i + k | , 1 ) , j = i − 2 , ⋯ , i + 2 ,</p><p>其中 f ^ 是 f 的归一化向量。</p><p>综上所述，本文设计采用的间断点检测的BP神经网络模型为(图2)：</p><p>图2. BP神经网络模型</p></sec></sec><sec id="s7"><title>4. WENO-ANN格式</title><p>应用第二节中构造的人工神经网络，通过五个点的大模板 S 5 上的函数值计算非线性权重 ω k ( k = 0 , 1 , 2 ) ，并应用于WENO重构。算法如下：</p><p>算法1 WENO-ANN方法</p><p>1) 构建并训练神经网络，得出由五点函数值 ( f i − 2 , f i − 1 , f i , f i + 1 , f i + 2 ) ∈ ℝ 5 计算三个子模板的光滑性指标Index的网络N。</p><p>2) 在计算过程中，对某一点 x i ，其左右两点处的函数值 ( f i − 2 , f i − 1 , f i , f i + 1 , f i + 2 ) 作为网络输入，计算得出三个子模板的光滑性指标Index。</p><p>3) 根据表1，将神经网络输出的光滑性指标Index转化为非线性权重 ω k ( k = 0 , 1 , 2 ) ；</p><p>4) WENO重构： f i + 1 / 2 = ∑ k = 0 2 ω k f i + 1 / 2 ( k ) 。</p></sec><sec id="s8"><title>5. 数值结果</title><p>在本节的数值实验中，PDEs离散后得到的关于时间的ODEs，使用了三阶TVD Runge-Kutta方法</p><p>Q ( 1 ) = Q n + Δ t L ( Q n ) , Q ( 2 ) = 3 4 Q n + 1 4 Q ( 1 ) + 1 4 Δ t L ( Q ( 1 ) ) , Q n + 1 = 1 3 Q n + 2 3 Q ( 2 ) + 2 3 Δ t L ( Q ( 2 ) ) ,</p><p>其中，L是空间离散算子，CFL条件数为0.45。</p><sec id="s8_1"><title>5.1. 标量方程</title><p>本节首先考虑具有周期边界条件的波动方程 Q t + Q x = 0 ，其中初始条件考虑了光滑和间断两种情况：光滑初始条件为 <inline-formula><inline-graphic xlink:href="//html.scirp.org/file/13-2621184x9_hanspub.png" xlink:type="simple"/></inline-formula> ，终止时间为 t = 0.32 ；间断初始条件为</p><p>Q ( x , 0 ) = { − sin ( π x ) − 1 2 x 3 , − 1 &lt; x &lt; 0 , − sin ( π x ) − 1 2 x 3 + 1 , 0 ≤ x ≤ 1 ,</p><p>终止时间 t = 8 。</p><p>表4给出了在光滑初始条件下，WENO-ANN格式的 L ∞ 误差和 L 2 误差，可以看出WENO-ANN格式能保持五阶精度。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> The precision test of the wave equation with smooth initial conditio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >N</th><th align="center" valign="middle" >L ∞ 误差</th><th align="center" valign="middle" >阶数</th><th align="center" valign="middle" >L 2 误差</th><th align="center" valign="middle" >阶数</th></tr></thead><tr><td align="center" valign="middle" >50</td><td align="center" valign="middle" >1.13E−4</td><td align="center" valign="middle" >−</td><td align="center" valign="middle" >2.05E−4</td><td align="center" valign="middle" >−</td></tr><tr><td align="center" valign="middle" >100</td><td align="center" valign="middle" >3.28E−8</td><td align="center" valign="middle" >11.75</td><td align="center" valign="middle" >2.32E−7</td><td align="center" valign="middle" >9.78</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >1.03E−9</td><td align="center" valign="middle" >5.00</td><td align="center" valign="middle" >1.03E−8</td><td align="center" valign="middle" >4.50</td></tr><tr><td align="center" valign="middle" >400</td><td align="center" valign="middle" >3.21E−11</td><td align="center" valign="middle" >5.00</td><td align="center" valign="middle" >4.54E−10</td><td align="center" valign="middle" >4.50</td></tr></tbody></table></table-wrap><p>表4. 具有光滑初始条件的波动方程的精度测试</p><p>图3(左)展示了在间断初始条件下，WENO-ANN和WENO-JS格式所求得的数值解的比较，可以看出WENO-ANN格式具有相对较高的分辨率，数值耗散较低。</p><p>其次考虑具有周期边界条件的Burgers方程</p><p>Q t + ( Q 2 2 ) x = 0 , ( x , t ) ∈ [ 0 , 1 ] &#215; ( 0 , 0.32 ] , Q ( x , 0 ) = sin ( 2 π x ) .</p><p>其数值解如图3(右)所示，与WENO-JS格式相比具有同等精度。此外，表5给出了Burgers方程在两种格式下的CPU时间对比，可以看出，WENO-ANN格式在高分辨率下，会明显地提高计算效率。</p><p>图3. (左) 具有间断初始条件的波动方程的数值解，(右) Burgers方程的数值解：网格节点数均为 N = 200</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> The comparison of the CPU time for Burgers equatio</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >N</th><th align="center" valign="middle"  colspan="2"  >CPU时间(s)</th></tr></thead><tr><td align="center" valign="middle" >WENO-ANN</td><td align="center" valign="middle" >WENO-JS</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >3.95</td><td align="center" valign="middle" >4.57</td></tr><tr><td align="center" valign="middle" >400</td><td align="center" valign="middle" >15.83</td><td align="center" valign="middle" >17.89</td></tr><tr><td align="center" valign="middle" >600</td><td align="center" valign="middle" >34.36</td><td align="center" valign="middle" >39.38</td></tr><tr><td align="center" valign="middle" >800</td><td align="center" valign="middle" >64.42</td><td align="center" valign="middle" >74.57</td></tr><tr><td align="center" valign="middle" >1000</td><td align="center" valign="middle" >94.00</td><td align="center" valign="middle" >110.01</td></tr></tbody></table></table-wrap><p>表5. Burgers方程的CPU时间对比</p></sec><sec id="s8_2"><title>5.2. 一维欧拉方程</title><p>在本节中，考虑强守恒形式的一维欧拉方程</p><p>Q t + F x = 0 ,</p><p>其中，</p><p>Q = ( ρ , ρ u , E ) T , F = ( ρ u , ρ u 2 + P , ( E + P ) u ) T ,</p><p>状态方程 P = ( γ − 1 ) ( E − 1 2 ρ u 2 ) , γ = 1.4 ， ρ , u , P , E 分别是密度，速度，压力和总能量。</p><p>下面将对一维欧拉方程中的典型问题Sod问题、Lax问题、Shock-Density问题以及Shock-Entropy问题进行数值实验，来验证所提方法的准确性和有效性。</p><p>Sod问题的初始条件为</p><p>( ρ , u , P ) = { ( 0.125 , 0 , 0.1 ) , − 5 ≤ x &lt; 0 , ( 1 , 0 , 1 ) , 0 ≤ x ≤ 5.</p><p>终止时间 t = 2 ，网格节点数 N = 200 。</p><p>Lax问题的初始条件为</p><p>( ρ , u , P ) = { ( 0.445 , 0.698 , 0.3528 ) , − 5 ≤ x &lt; 0 , ( 0.5 , 0 , 0.5710 ) , 0 ≤ x ≤ 5.</p><p>终止时间 t = 1.3 ，网格节点数 N = 200 。</p><p>Shock-Density问题的初始条件为</p><p>( ρ , u , P ) = { ( 27 7 , 4 35 9 , 31 3 ) , − 5 ≤ x &lt; − 4 , ( 1 +0 .2 sin ( k x ) , 0 , 1 ) , − 4 ≤ x ≤ 5.</p><p>其中， k = 5 ，终止时间 t = 1.8 ，网格节点数 N = 400 。</p><p>图4. Sod问题、Lax问题和Shock-Density问题的密度解</p><p>图中Sod问题、Lax问题和Shock-Density问题的参考解是五阶WENO-JS格式在网格节点数 N = 2400 下的数值解。如图4所示，WENO-ANN格式能很好的捕捉间断结构，且与WENO-JS格式相比，WENO-ANN格式的数值耗散更小，具有更高的精度。</p></sec></sec><sec id="s9"><title>6. 结论</title><p>本文在五阶WENO-JS的基础上，用人工神经网络(ANN)代替传统的非线性权重计算方法，将用于五阶WENO求解的模板中的五点函数值作为ANN的输入，三个子模板的光滑性指标作为输出，光滑性指标可直接转化为对应的非线性权重，用于WENO重构。数值实验表明，该网络能很好的捕捉间断，与WENO-JS相比，WENO-ANN能保持精度，且数值色散耗散较低，在高分辨率下，能有效提高计算效率。</p></sec><sec id="s10"><title>文章引用</title><p>刘 琪,温 晓. 基于人工神经网络的WENO重构方法The WENO Reconstruction Based on the Artificial Neural Network[J]. 应用数学进展, 2020, 09(04): 574-583. https://doi.org/10.12677/AAM.2020.94069</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.35285-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">LeVeque, R.J. (2002) Finite Volume Methods for Hyperbolic Problems. Cambridge University Press, Cambridge, 31.  
&lt;br&gt;https://doi.org/10.1017/CBO9780511791253</mixed-citation></ref><ref id="hanspub.35285-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Harten, A., Engquist, B., Osher, S. and Chakravarthy, S.R. (1987) Uniformly High Order Accurate Essentially Non-Oscillatory Schemes, III. In: Upwind and High-Resolution Schemes, Springer, Berlin, 231-303.  
&lt;br&gt;https://doi.org/10.1007/978-3-642-60543-7_12</mixed-citation></ref><ref id="hanspub.35285-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Jiang, G. and Shu, C.-W. (1996) Efficient Implementation of Weighted Eno Schemes. Journal of Computational Physics, 126, 202-228. &lt;br&gt;https://doi.org/10.1006/jcph.1996.0130</mixed-citation></ref><ref id="hanspub.35285-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Ha, Y., Kim, C., Lee, Y. and Yoon, J. (2013) An Improved Weighted Essentially Non-Oscillatory Scheme with a New Smoothness Indicator. Journal of Computational Physics, 232, 68-86. &lt;br&gt;https://doi.org/10.1016/j.jcp.2012.06.016</mixed-citation></ref><ref id="hanspub.35285-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Kim, C., Ha, Y. and Yoon, J. (2016) Modified Non-Linear Weights for Fifth-Order Weighted Essentially Non-Oscillatory Schemes. Journal of Computational Physics, 67, 299-323. &lt;br&gt;https://doi.org/10.1007/s10915-015-0079-3</mixed-citation></ref><ref id="hanspub.35285-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Rathan, S. and Raju, G. (2018b) A Modified Fifth-Order WENO Scheme for Hyperbolic Conservation Laws. Computers &amp; Mathematics with Applications, 75, 1531-1549. &lt;br&gt;https://doi.org/10.1016/j.camwa.2017.11.020</mixed-citation></ref><ref id="hanspub.35285-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Borges, R., Carmona, M., Costa, B. and Don, W.S. (2008) An Improved Weighted Essentially Non-Oscillatory Scheme for Hyperbolic Conservation Laws. Journal of Computational Physics, 227, 3191-3211.  
&lt;br&gt;https://doi.org/10.1016/j.jcp.2007.11.038</mixed-citation></ref><ref id="hanspub.35285-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Castro, M., Costa, B. and Don, W.S. (2011) High Order Weighted Essentially Non-Oscillatory Weno-Z Schemes for Hyperbolic Conservation Laws. Journal of Computational Physics, 230, 1766-1792.  
&lt;br&gt;https://doi.org/10.1016/j.jcp.2010.11.028</mixed-citation></ref><ref id="hanspub.35285-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Rathan, S. and Raju, G. (2018) Improved Weighted Eno Scheme Based on Parameters Involved in Nonlinear Weights. Applied Mathematics and Computation, 331, 120-129. &lt;br&gt;https://doi.org/10.1016/j.amc.2018.03.034</mixed-citation></ref><ref id="hanspub.35285-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Lagaris, I., Likas, A. and Fotiadis, D. (1998) Artificial Neural Networks for Solving Ordinary and Partial Differential Equations. IEEE Transactions on Neural Networks, 9, 987-1000. &lt;br&gt;https://doi.org/10.1109/72.712178</mixed-citation></ref><ref id="hanspub.35285-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Yu, J., Hesthaven, J.S. and Yan, C. (2018) A Data-Driven Shock Capturing Approach for Discontinuous Galerkin Methods. Technical Report.</mixed-citation></ref><ref id="hanspub.35285-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Bar-Sinai, Y., Hoyer, S., Hickey, J. and Brenner, M. (2019) Learning Data-Driven Discretizations for Partial Differential Equations. PNAS, 116, 15344-15349. &lt;br&gt;https://doi.org/10.1073/pnas.1814058116</mixed-citation></ref><ref id="hanspub.35285-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Pfau, D., Petersen, S., Agarwal, A., Barrett, D.G.T. and Stachenfeld, K.L. (2018) Spectral Inference Networks: Unifying Deep and Spectral Learning.</mixed-citation></ref><ref id="hanspub.35285-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Hsieh, J.T., Zhao, S.J., Eismann, S., Mirabella, L. and Ermon, S. (2019) Learning Neural PDE Solvers with Convergence Guaran-tees.</mixed-citation></ref><ref id="hanspub.35285-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Logg, A., Mardal, K.A. and Wells, G. (2012) Automated Solution of Differential Equations by the Finite Element Method: The FEniCS Book. Springer, Berlin. &lt;br&gt;https://doi.org/10.1007/978-3-642-23099-8</mixed-citation></ref><ref id="hanspub.35285-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Stevens, B. and Colonius, T. (2020) Enhancement of Shock-Capturing Methods via Machine Learning.</mixed-citation></ref><ref id="hanspub.35285-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Hecht-Nielsen, R. (1988) Theory of the Back-Propagation Neural Network. Neural Networks, 1, 445.  
&lt;br&gt;https://doi.org/10.1016/0893-6080(88)90469-8</mixed-citation></ref></ref-list></back></article>