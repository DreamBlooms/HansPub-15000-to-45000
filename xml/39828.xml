<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2021.101011</article-id><article-id pub-id-type="publisher-id">AAM-39828</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20210100000_50174812.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  一类改进广义岭估计的实例研究
  Improved Generalized Ridge Regression and Its Application
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>金灵</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>云南财经大学统计与数学学院，云南 昆明</addr-line></aff><pub-date pub-type="epub"><day>11</day><month>01</month><year>2021</year></pub-date><volume>10</volume><issue>01</issue><fpage>92</fpage><lpage>97</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    本文对广义岭回归方法进行了改进，并在真实数据中进行了实例验证。改进广义岭估计主要对广义岭估计在线性回归模型中当存在若干较大异常值影响模型精度的情况进行了修正，加入适当的修正参数，使模型达到对数据更精确的拟合以及预测作用，并对中俄贸易数据进行了实例验证。
    In this paper, the generalized ridge regression method is improved and verified by real data. The improved generalized ridge regression mainly corrects the generalized ridge estimation in the linear regression model when there are some large outliers that affect the accuracy of the model. Appropriate correction parameters are added to make the model achieve more accurate data fitting and prediction effects. Finally, we verified it in trade data. 
  
 
</p></abstract><kwd-group><kwd>多重共线性，改进广义岭估计，异常值处理，修正参数, Multicollinearity</kwd><kwd> Improved Generalized Ridge Regression</kwd><kwd> Outlier Handling</kwd><kwd> Correction Parameters</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>本文对广义岭回归方法进行了改进，并在真实数据中进行了实例验证。改进广义岭估计主要对广义岭估计在线性回归模型中当存在若干较大异常值影响模型精度的情况进行了修正，加入适当的修正参数，使模型达到对数据更精确的拟合以及预测作用，并对中俄贸易数据进行了实例验证。</p></sec><sec id="s2"><title>关键词</title><p>多重共线性，改进广义岭估计，异常值处理，修正参数</p></sec><sec id="s3"><title>Improved Generalized Ridge Regression and Its Application</title><p>Jinling Liu</p><p>School of Statistics and Mathematics, Yunnan University of Finance and Economics, Kunming Yunnan</p><p><img src="//html.hanspub.org/file/11-2621439x4_hanspub.png" /></p><p>Received: Dec. 14<sup>th</sup>, 2020; accepted: Jan. 3<sup>rd</sup>, 2021; published: Jan. 19<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/11-2621439x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In this paper, the generalized ridge regression method is improved and verified by real data. The improved generalized ridge regression mainly corrects the generalized ridge estimation in the linear regression model when there are some large outliers that affect the accuracy of the model. Appropriate correction parameters are added to make the model achieve more accurate data fitting and prediction effects. Finally, we verified it in trade data.</p><p>Keywords:Multicollinearity, Improved Generalized Ridge Regression, Outlier Handling, Correction Parameters</p><disp-formula id="hanspub.39828-formula6"><graphic xlink:href="//html.hanspub.org/file/11-2621439x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/11-2621439x7_hanspub.png" /> <img src="//html.hanspub.org/file/11-2621439x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>本文主要提出改进广义岭估计。广义岭估计是Horel和Kennard提出的对于一般岭估计岭参数设置改进的一种方法；Hawkins和Yin提出了针对实际情况中样本量n小于未知参数的p形成降秩矩阵的岭回归的快速迭代算法；由于实际研究中还没有一种能够确定岭参数的完美的方法，或多或少会造成估计误差不稳定，游、王和刘(2002) [<xref ref-type="bibr" rid="hanspub.39828-ref1">1</xref>] 提出了广义岭估计的直接解法，跳过岭参数的估计，直接求得具有最小均方误差的解；凌和叶对平衡损失下的双h岭估计的优良性进行了验证，给出了风险函数的表达式。改进岭估计在基于前人对该领域研究的基础上，适当改进，对于实际数据若干异常值较大影响回归结果的问题提出新的方法予以解决，将大大提高模型准确度。</p></sec><sec id="s6"><title>2. 模型建立及数据预处理</title><sec id="s6_1"><title>2.1. 模型建立</title><p>在将中蒙俄2008~2018的贸易数据整理，所有变量重新命名。定义 Y 1 和 Y 2 为因变量， X 1 、 X 2 、 X 3 、 X 4 、 X 5 、 X 7 、 X 8 、 X 10 、 X 12 和 X 14 是对应 Y 1 的自变量； X 1 、 X 2 、 X 3 、 X 4 、 X 6 、 X 7 、 X 9 、 X 11 、 X 13 和 X 15 是对应 Y 2 的自变量。所有数据均能在统计局官网获得，这里不予介绍。建立如下线性模型：</p><p>Y 1 = X ′ β 1 + ε ,     ε ~ N ( 0 , σ 2 I ) ,</p><p>Y 2 = X ″ β 2 + ε ,     ε ~ N ( 0 , σ 2 I ) ,</p><p>其中： X ′ = [ 1     X 1 ~ X 14 ] ， X ″ = [ 1     X 1 ~ X 15 ] ， β 1 , β 2 ∈ R 11 。</p></sec><sec id="s6_2"><title>2.2. 多重共线性诊断</title><p>如果数据设计阵存在多重共线性问题，就不能简单的用最小二乘估计对其进行参数估计，虽然看似是对数据进行了最小均方误差的拟合，但是有时可能导致拟合的结果完全和事实背离，造成数据分析无效等后果。为了使数据得到更好的处理，我们将引用岭估计、主成分回归和逐步回归等方法才能对数据进行更加合理的处理，所以进行多重共线性诊断是十分必要的，这关系到模型的合理性以及对数据的解释程度。</p>对Y<sub>1</sub>、Y<sub>2</sub>设计阵进行诊断<p>对矩阵 Z 1 和 Z 2 进行中心化和标准化：</p><p>Z 1 = [ ( X 1 i j − μ j σ j ) i j ] , Z 2 = [ ( X 2 i j − μ j σ j ) i j ] ,</p><p>其中 μ j 和 σ j 分别表示矩阵Z的第j列的均值和方差。计算 Z ′ 1 Z 1 和 Z ′ 2 Z 2 的特征值。</p><p>引入条件数公式 k = λ 1 / λ p − 1 ，k的值越大，说明存在的共线性越严重。</p><p>k 1 = λ 1 λ p − 1 = 1313.9 2.9 ≈ 453.81 &gt; 1000 ,</p><p>k 2 = λ 1 λ p − 1 = 1301.2 0.9 ≈ 1408.03 &gt; 1000 ,</p><p>从 k 1 和 k 2 的取值来看，不管是 Y 1 还是 Y 2 的设计阵，都存在略严重的多重共线性。此时如果要对 Y 1 和 Y 2 分别进行线性拟合，就不能再使用最小二乘估计了，处理此类问题常见的方法有常见的一般岭估计、逐步回归分析、主成分回归。</p></sec></sec><sec id="s7"><title>3. 理论推导</title><sec id="s7_1"><title>3.1. 改进广义岭估计理论推导</title><p>线性模型拟合中，对于参数的估计最常用的的是最小二乘估计，但是在遇到设计阵存在多重共线性的时候，往往最小二乘估计不是最好的，所以A.E.Hoerl提出了岭估计，引入岭参数 k I p &#215; p 来解决多重共线性问题，后来广义岭估计的提出又定义了岭参数 k 1 ~ k p 可以不相同来减小拟合结果的均方误差。叶、朱(1998) [<xref ref-type="bibr" rid="hanspub.39828-ref2">2</xref>] 提出了奇异值分解的解法。本节在前人建立的广义岭估计的基础上又提出对于若干异常值较大情况的一种广义岭估计的修正方法，改进广义岭估计。先对广义岭估计进行回顾，</p><p>岭估计参数 β ^ 的估计方法：</p><p>β ^ ( k ) = ( X ′ X + k I p &#215; p ) − 1 X ′ Y = ( X ′ X + k I p &#215; p ) − 1 ( X ′ X ) β ^ ,</p><p>建立典则方程：</p><p>Y = α 0 1 + Z α + ε ,</p><p>典则回归系数的岭估计：</p><p>α ^ ( k ) = ( Λ + k I p &#215; p ) − 1 Z ′ Y = ( Λ + k I ) − 1 Λ α ^ ,</p><p>其中 α ^ = ( Λ ) − 1 Z ′ Y ， Λ = ϕ ′ ( X ′ X ) ϕ = diag ( λ 1 , ⋯ , λ p ) ， ϕ = ( φ 1 , ⋯ , φ p ) 是 λ i 对应的特征根， Z = X ϕ ， α ^ = ϕ ′ β ^ 。定义 K = diag ( k 1 , ⋯ , k p ) ， k i 是不必全相等的岭参数，于是定义该模型广义岭估计的典则参数和平差参数估计如下：</p><p>α ^ ( K ) = ( Z ′ Z + K ) − 1 Z ′ Y = ( Λ + K ) − 1 Z ′ Y = ( Λ + K ) − 1 Λ α ^ = ( Λ + K ) − 1 C , β ^ ( K ) = ϕ α ^ ( K ) = ϕ ( Λ + K ) − 1 Λ ϕ ′ β ^ = ϕ ( Λ + K ) − 1 ϕ ′ X ′ Y = ( X ′ X + ϕ K ϕ ′ ) X ′ Y , (A)</p><p>当 K = k I p &#215; p 时，广义岭估计就简化成为了一般的岭估计。</p><p>由于实际数据或许存在若干较大异常值情况，故提出修正参数对广义岭估计进行改进：</p><p>γ i j = 1 − I { | x i j − x &#175; j | &gt; Q j a } ( x i j − x j &#175; ) ,</p><p>其中 I { x i j &gt; Q j a } 为示性函数， Q j a 为对应设计阵X的第j列进行中心化并求绝对值后的a%分位数， x j &#175; 为对应设计阵X的第j列的均值。提出修正参数 γ i j ，将消除由于自变量中存在若干较大异常值对模型拟合的影响。</p><p>相应的，加入修正参数后的典则方程和平差方程如下：</p><p>α ^ ( K ) = [ ( Z γ i j ) ′ ( Z γ i j ) + K ] − 1 ( Z γ i j ) ′ Y ,</p><p>β ^ ( K ) = [ ( X γ i j ) ′ ( X γ i j ) + ϕ K ϕ ′ ] − 1 ( X γ i j ) ′ Y ,</p><p>令 Z R = Z γ i j , X R = X γ i j 得：</p><p>α ^ ( K ) = [ Z R ′ Z R + K ] − 1 Z R ′ Y ,</p><p>β ^ ( K ) = [ X R ′ X R + ϕ K ϕ ′ ] − 1 X R ′ Y ,</p><p>易证 MSE [ α ^ ( K ) ] = MSE [ β ^ ( K ) ] ，这里略去证明过程，于是可以通过 α ^ ( K ) 来作为评价 β ^ ( K ) 估计值好坏的指标。</p><p>MSE [ α ^ ( K ) ] = MSE [ β ^ ( K ) ] = σ 2 ∑ i = 1 p − 1 λ i ( λ i + k i ) 2 + ∑ i = 1 p − 1 k i 2 α i 2 ( λ i + k i ) 2 .</p></sec><sec id="s7_2"><title>3.2. 改进广义岭估计解法介绍</title><p>对于岭参数的估计，将是建立合理的线性模型的至关重要的一点，虽然前人已经提出了各种确定岭参数的方法和准则，但是到目前为止，并没有一种公认的非常稳健的方法，所以本文将采用游、王和刘(2002) [<xref ref-type="bibr" rid="hanspub.39828-ref1">1</xref>] 提出的广义岭估计的直接解法DSGRE (derct solution to generalized ridge estimate)，其内容如下：</p><p>由于 λ i 和 α i 与 k i 没有关系且 α i 是非随机的，为了求得最小的 MSE [ α ^ ( K ) ] ，故对 MSE [ α ^ ( K ) ] 关于 k i 进行一阶求导：</p><p>f ′ ( k i ) = dMSE [ α ^ ( K ) ] d k i = − 2 σ 2 ∑ i = 1 p − 1 λ i ( λ i + k i ) 3 + 2 ∑ i = 1 p − 1 k i λ i α i 2 ( λ i + k i ) 3 ,</p><p>令 f ′ ( k i ) = 0 ,</p><p>解得：</p><p>k i = σ 2 α i 2 ,     i = 1 , 2 , ⋯ , p − 1 ,</p><p>由于真实的 σ 2 和 α i 2 是未知的，故用 σ 2 的估计值 σ 2 ^ 以及 α i 的迭代值 α i ^ ( j ) 来对 k i 进行迭代求解：</p><p>k i ^ ( j ) = σ 2 ^ α i ^ ( j ) α i ^ ( j ) , (B)</p><p>那么就可以用第j次 α i ^ ( j ) 的迭代值来估计与之相同次数的岭参数 k i ，由式(A)可知，第 j + 1 次估计为：</p><p>α i ^ ( j + 1 ) = c i λ i + k i ^ ( j ) ,</p><p>带入式(B)可知：</p><p>α i ^ ( j + 1 ) = c i λ i + σ 2 ^ α i ^ ( j ) α i ^ ( j ) = c i α i ^ ( j ) α i ^ ( j ) λ i α i ^ ( j ) α i ^ ( j ) + σ 2 . (C)</p></sec><sec id="s7_3"><title>3.3. 解的存在性证明</title><p>薛、梁(2002) [<xref ref-type="bibr" rid="hanspub.39828-ref3">3</xref>] 给出了参数的迭代算法。结合已有的证明过程，对改进广义岭估计的解的存在性予以证明。如果 α i ^ ( j + 1 ) 是一致收敛的，那么经过 j + 1 次迭代过程，必然能够求解出 α i ^ ( j + 1 ) 的数值解，故下证 α i ^ ( j + 1 ) 的一致收敛性：</p><p>证明：若 c i &gt; 0 ，根据 α i ^ ( j + 1 ) 的形式知： α i ^ ( 0 ) = c i λ i &gt; 0 ，由于 σ 2 &gt; 0 ⇒ α i ^ ( 0 ) &gt; α i ^ ( 1 ) &gt; ⋯ &gt; α i ^ ( ∞ ) &gt; 0 ，故</p><p>α i ^ ( j + 1 ) 在 j ∈ ( 0 , ∞ ) 上市有界并且单调的，有Dieichlet判别法可知， α i ^ ( j + 1 ) 在 j ∈ ( 0 , ∞ ) 上是一致收敛的。</p><p>若 c i &lt; 0 ，根据 α i ^ ( j + 1 ) 的形式知： α i ^ ( 0 ) = c i λ i &lt; 0 ，由于 σ 2 &gt; 0 ⇒ α i ^ ( 0 ) &lt; α i ^ ( 1 ) &lt; ⋯ &lt; α i ^ ( ∞ ) &lt; 0 ，故 α i ^ ( j + 1 ) 在</p><p>j ∈ ( 0 , ∞ ) 上是有界并且单调的，有Dieichlet判别法可知， α i ^ ( j + 1 ) 在 j ∈ ( 0 , ∞ ) 上是一致收敛的。</p><p>综上所知： α i ^ ( j + 1 ) 在 j ∈ ( 0 , ∞ ) 上是一致收敛的，迭代式 α i ^ ( j + 1 ) 必然有解。</p><p>于是，令 α i ^ ( ∞ ) = α i ^ ，则式(C)可以写成如下形式：</p><p>α i ^ = c i α i ^ 2 λ i α i ^ 2 + σ 2 , (D)</p><p>解式(D)得：</p><p>α i ^ = { 0 ,                                               c i = 0 或 c i 2 − 4 λ i σ 2 ≥ 0 , c i + c i 2 − 4 λ i σ 2 2 λ i ,     c i &gt; 0 , c i 2 − 4 λ i σ 2 ≥ 0 , c i + c i 2 − 4 λ i σ 2 2 λ i ,     c i &lt; 0 , c i 2 − 4 λ i σ 2 ≥ 0.</p><p>又因为 β ^ = ϕ α ^ ，故该方法直接给出了直接解改进广义岭估计参数的方式。本节将游、王和刘(2002) [<xref ref-type="bibr" rid="hanspub.39828-ref1">1</xref>] 提出的DSGRE在改进广义岭估计中应用，推导过程验证，也是适用的。</p></sec></sec><sec id="s8"><title>4. 实例探究</title><p>基于第3节中对于改进广义岭估计的研究，本节将采用该方法对于中蒙俄贸易数据进行回归分析，在广义岭估计的基础上引入修正参数形成改进广义岭估计，训练相应的模型对总体数据进行更加精确地拟合。</p><sec id="s8_1"><title>4.1. 算法</title><p>① 对X的每一列进行中心化后再求绝对值得到 A = ( | x i j − x j &#175; | ) i j ；</p><p>② 对 | x i j − x j &#175; | 按列进行排序，得到新的排序矩阵B；</p><p>③ 找到排序矩阵每一列的a%分位数 Q j a ，这里不妨取a = 95；</p><p>④ 通过判别函数将每一列小于对应这一列a%分位数 Q j a 的元素重置为0，大于 Q j a 的元素则保留得到矩阵E；</p><p>⑤ A中每个元素取绝对值得到矩阵C，用A点除C得到矩阵D；</p><p>⑥ 所以 Z R = Z − E   . * D ， X R = X − E   . * D ，其中的.*为MATLAB中的点乘语句；</p><p>⑦ 最后将ZR和XR进行标准中心化，任用ZR和XR表示。</p></sec><sec id="s8_2"><title>4.2. 改进广义岭估计参数 β ^</title><p>利用上述算法对相关参数直接求解：</p><p>c 1 i = ( 51.0     118.2     147.7     103.8     − 113.4     126.7     45.8     15.1     22.0     14.6 ) T ,</p><p>c 2 i = ( 59.21     36.1     142.3     125.9     − 48.8     125.9     37.4     13.2     − 28.5     35.9 ) T ,</p><p>λ 1 i = ( 5.0     44.7     80.6     101.2     196.0     270.6     309.3     330.7     623.8     1194.1 ) T ,</p><p>λ 2 i = ( 5.2     6.3     45.5     84.5     263.6     274.4     320.5     344.8     621.7     1193.2 ) T . .</p><p>最后被估的回归系数为：</p><p>β 1 ^ = ϕ α ^ = ( 0     − 0.17     − 0.05     − 0.08     0.021     − 0.25     0.22     0.05     0     0     0 ) T ,</p><p>β 2 ^ = ϕ α ^ = ( 0     − 0.04     0.17     − 0.08     0.021     0.24     0.12     − 0.16     0.1     0.04     0     0     0 ) T .</p><p>从回归系数来看，对于响应变量 Y 1 来说， X 1 、 X 10 、 X 12 、 X 14 的系数为0，与 Y 1 不存在线性关系，其余变量在线性回归中对 Y 1 起着决定性作用。对于响应变量 Y 2 来说 X 1 、 X 11 、 X 13 、 X 15 与 Y 2 几乎没有线性关系，其余变量决定 Y 2 的线性回归。</p></sec></sec><sec id="s9"><title>5. 结论</title><p>本文我们对广义岭估计进行了适当的改进，加入了修正参数，这使得岭估计在当数据存在部分异常点时，具有一定的稳健性。同时我们将中蒙俄的贸易数据引入到本文中，对改进广义岭估计进行了实例探究，从结果发现，改进广义岭估计能够估出线性回归系数。我们还将其结果与现有的岭估计、主成分回归、最小二乘法进行了比较，改进广义岭估计估计的结果具有最小的均方误差。岭估计、主成分回归、逐步回归是较为成熟的方法，其估计结果我们没有在文中给出。</p></sec><sec id="s10"><title>文章引用</title><p>刘金灵. 一类改进广义岭估计的实例研究Improved Generalized Ridge Regression and Its Application[J]. 应用数学进展, 2021, 10(01): 92-97. https://doi.org/10.12677/AAM.2021.101011</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.39828-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">游扬声, 王新洲, 刘星. 广义岭估计的直接解法[J]. 武汉大学学报(信息科学版), 2002, 27(2): 175-178.</mixed-citation></ref><ref id="hanspub.39828-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">叶松林, 朱建军. 矩阵奇异值分解与广义岭估计及其在测量中的应用[J]. 中国有色金属学报, 1998(1): 160-164.</mixed-citation></ref><ref id="hanspub.39828-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">薛美玉, 梁飞豹. 广义岭估计参数的迭代算法[J]. 福州大学学报(自然科学版), 2002, 30(2): 167-171.</mixed-citation></ref></ref-list></back></article>