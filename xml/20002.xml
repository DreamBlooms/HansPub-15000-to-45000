<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2017.73029</article-id><article-id pub-id-type="publisher-id">CSA-20002</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20170300000_63497293.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于链接的关系主题模型
  Relation Topic Model Based on Links
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>全民</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>孙</surname><given-names>艳峰</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>振国</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>谷</surname><given-names>实</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>开阳</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>北京工业大学信息学部计算机学院，北京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><author-notes><corresp id="cor1">* E-mail:<email>sunyanfeng0913@163.com(孙艳)</email>;</corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>03</month><year>2017</year></pub-date><volume>07</volume><issue>03</issue><fpage>232</fpage><lpage>239</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   LDA模型是一种完全的概率主题生成模型，可以利用有效的概率算法来训练和使用模型，但是LDA模型在训练过程中并没有考虑文档之间的链接对主题生成的影响，而在本文提出的RTM模型中，就加入对文档之间链接的计算，计算过程中使用EM算法来对潜在变量进行计算，因为无法准确计算，所以采用变分分布算法。最后我们对没有训练的数据进行预测，分别为根据文档预测链接和根据链接预测文档，在试验结果中可以看到在数据集中两个RTM模型的变型都表现良好。 LDA model is a complete model of probability topic generation, which can use effective probability algorithm to train and use model. However, LDA model does not consider the effect of link between documents on topic generation in training process. In the RTM model of this paper, the link between the documents is added to the calculation, and the calculation process uses the EM algorithm to calculate the potential variables. Because it cannot be accurately calculated, the variational distribution algorithm is used. Finally, we predict the data without training, according to the document prediction link and the link prediction document. We can see that the two RTM models in the dataset are all good.
    
  
 
</p></abstract><kwd-group><kwd>LDA模型，RTM模型，链接，变分分布, LDA Model</kwd><kwd> RTM Model</kwd><kwd> Links</kwd><kwd> Variational Distribution</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于链接的关系主题模型<sup> </sup></title><p>王全民，孙艳峰，李振国，谷实，王开阳</p><p>北京工业大学信息学部计算机学院，北京</p><p>收稿日期：2017年3月7日；录用日期：2017年3月25日；发布日期：2017年3月28日</p><disp-formula id="hanspub.20002-formula64"><graphic xlink:href="http://html.hanspub.org/file/7-1540690x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>LDA模型是一种完全的概率主题生成模型，可以利用有效的概率算法来训练和使用模型，但是LDA模型在训练过程中并没有考虑文档之间的链接对主题生成的影响，而在本文提出的RTM模型中，就加入对文档之间链接的计算，计算过程中使用EM算法来对潜在变量进行计算，因为无法准确计算，所以采用变分分布算法。最后我们对没有训练的数据进行预测，分别为根据文档预测链接和根据链接预测文档，在试验结果中可以看到在数据集中两个RTM模型的变型都表现良好。</p><p>关键词 :LDA模型，RTM模型，链接，变分分布</p><disp-formula id="hanspub.20002-formula65"><graphic xlink:href="http://html.hanspub.org/file/7-1540690x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2017 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="http://image.hanspub.org:8080\Html/htmlimages\1-2890033x\e70a10f1-7c93-45ea-9603-062237856e4b.png" /><img src="http://image.hanspub.org:8080\Html\htmlimages\1-2890033x\e898c85e-ffc4-45c9-b817-14224a4d6960.png" /></p></sec><sec id="s3"><title>1. 背景介绍</title><p>近年来，关于文本过滤的研究都集中在过滤模型方面，基本上是引入和改进机器学习领域的相关成果 [<xref ref-type="bibr" rid="hanspub.20002-ref1">1</xref>] 。隐含狄利克雷分布 [<xref ref-type="bibr" rid="hanspub.20002-ref2">2</xref>] 是近年来这方面发展起来的一种重要的离散数据集合的建模方法，首先，LDA模型是完全的概率生成模型 [<xref ref-type="bibr" rid="hanspub.20002-ref3">3</xref>] ，因此具有丰富的内在结构，并且可以利用成熟有效的概率算法来训练和使用模型，再者，LDA模型参数空间的规模是K*N (k是隐含主题的数量，N是此表中词的数量)，与文档的数量无关，使得LDA更适合在大规模语料库上构造文本表示模型。但是LDA模型只考虑文本本身的内容，而忽略了节点之间的联系。</p></sec><sec id="s4"><title>2. 相关研究</title><p>这个关系主体模型(RTM)是在统计和机器学习的研究基础上提出的，它是一个可以提供节点属性以及其网络结构的潜在空间模型，一些对网络结构的潜在空间模型已经被提出过 [<xref ref-type="bibr" rid="hanspub.20002-ref4">4</xref>] ，但是，这些模型只单单计算数据，并没有考虑节点属性；解释网络的链接结构或者是节点属性的模型的建立过程大都是通过降维 [<xref ref-type="bibr" rid="hanspub.20002-ref5">5</xref>] 和获取节点属性 [<xref ref-type="bibr" rid="hanspub.20002-ref6">6</xref>] 来建立，都趋向于研究它们中的一个或者它们之外的其他属性 [<xref ref-type="bibr" rid="hanspub.20002-ref7">7</xref>] ，包含一些主题模型 [<xref ref-type="bibr" rid="hanspub.20002-ref8">8</xref>] ，RTM模型综合考虑了节点属性和他们之间的链接结构，这样可以通过其中一个预测到另一个。</p><p>对一篇文档来说，每一个节点信息就是它包含的关键词，RTM模型研究的是文档关键词和文档之间的链接的关系。除了可以通过链接预测词语和词语预测链接，还可以对没有经过训练的文档数据进行预测。RTM模型是一种新的针对文档和文档之间链接的概率生成模型 [<xref ref-type="bibr" rid="hanspub.20002-ref9">9</xref>] ，关于生成模型 [<xref ref-type="bibr" rid="hanspub.20002-ref10">10</xref>] ，在之前研究中，通常把链接当作相互独立的单元，与本文提出的研究最相近的是Nallapati和Mei的研究 [<xref ref-type="bibr" rid="hanspub.20002-ref11">11</xref>] ，他们试图扩展混合成员模型 [<xref ref-type="bibr" rid="hanspub.20002-ref12">12</xref>] ，他们假设可交换，在模型中允许使用主题来解释链接并且使用其他词语来解释另外一些词语，但是这样影响了使用词语信息来预测链接信息，与此不同的是，在RTM模型中强制使用主题来解释全部词语和链接。RTM是一种新的概率生成模型，它可以被用来分析链接集，比如网页引用、网页链接、社交网络等，我们在实验中证实了它在分析这些数据的时候是适用的，与之前的模型相比在效果上有了明显的提升。</p></sec><sec id="s5"><title>3. RTM模型</title><sec id="s5_1"><title>3.1. RTM简介</title><p>RTM假设一组被观察到的文档<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x9_hanspub.png" xlink:type="simple"/></inline-formula>以及他们之间的双链接<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x10_hanspub.png" xlink:type="simple"/></inline-formula>是由以下方法产生的，见图1。</p><p>1.对于每一个文档d：</p><p>a.抽取主题比例<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x11_hanspub.png" xlink:type="simple"/></inline-formula></p><p>b.对于每一个词语<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x12_hanspub.png" xlink:type="simple"/></inline-formula></p><p>i.抽取主题分配<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x13_hanspub.png" xlink:type="simple"/></inline-formula></p><p>ii.抽取单词<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x14_hanspub.png" xlink:type="simple"/></inline-formula></p><p>图1. 大量文件的图形模型</p><p>2.对于每一对文档<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x16_hanspub.png" xlink:type="simple"/></inline-formula>：</p><p>a.抽取二元连接指示器<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x17_hanspub.png" xlink:type="simple"/></inline-formula></p><p>完整的模式是很难说明的。因为其中包含了所有从文档中观察的词语，以及他们之间的每个可能的链接变量。函数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x18_hanspub.png" xlink:type="simple"/></inline-formula>是两个文件之间的链接分布。这个函数依赖于生成他们的词语<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x19_hanspub.png" xlink:type="simple"/></inline-formula>，这里我们探讨两种可能性。</p><p>第一，我们考虑：</p><disp-formula id="hanspub.20002-formula66"><label>(1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/7-1540690x20_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x21_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x22_hanspub.png" xlink:type="simple"/></inline-formula>表示函数是S状的曲线。这个链接函数对每一对二元变量作隐藏协变量回归建模。它由<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x23_hanspub.png" xlink:type="simple"/></inline-formula>参数化、由<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x24_hanspub.png" xlink:type="simple"/></inline-formula>拦截。协变量是由<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x25_hanspub.png" xlink:type="simple"/></inline-formula>构造，捕捉两个文件隐含主题之间的相似性。</p><p>第二：我们考虑：</p><disp-formula id="hanspub.20002-formula67"><label>(2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/7-1540690x26_hanspub.png"  xlink:type="simple"/></disp-formula><p><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x27_hanspub.png" xlink:type="simple"/></inline-formula>使用和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x28_hanspub.png" xlink:type="simple"/></inline-formula>相同的协变量，但是由一个指数函数代替，这个函数返回的概率呈指数增长，可以被看作是Blei提出的建模方法 [<xref ref-type="bibr" rid="hanspub.20002-ref13">13</xref>] 的一个近似变体。</p><p>下面我们对这两种情况进行分析。</p><p>3.2计算过程</p><p>图2表示了文档间的关系。</p><p>变量y表示两个文件是否有联系。</p><p>通过上面的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x29_hanspub.png" xlink:type="simple"/></inline-formula>函数可以认为，潜在特性的期望函数是回应<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x30_hanspub.png" xlink:type="simple"/></inline-formula>这个公式，由监督LDA模型 [<xref ref-type="bibr" rid="hanspub.20002-ref14">14</xref>] 可确保用于生成文档内容的相同潜在主题分配，另外还负责生成他们的链接结构。</p><sec id="s5_1_1"><title>3.2.1. 算法建模</title><p>我们的计算过程参考变分 [<xref ref-type="bibr" rid="hanspub.20002-ref15">15</xref>] 推理过程，在变分推理中使用EM (期望最大化)算法来做参数估计，最大期望(EM)算法是在概率模型 [<xref ref-type="bibr" rid="hanspub.20002-ref16">16</xref>] 中寻找参数极大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测到的潜在变量，那么就是计算观测变量的潜在变量的后验分布，但是准确的后验分布是难</p><p>图2. 文档关系图</p><p>以计算的，所以只能使用变分分布。在变分分布中，首先假设存在一个由参数索引的潜在变量的分布函数，这些参数接近真实的后验，然后使用相对熵测量(见Jordan等，1999) [<xref ref-type="bibr" rid="hanspub.20002-ref17">17</xref>] 。经过分解后的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x32_hanspub.png" xlink:type="simple"/></inline-formula>是一组狄利克雷参数，每一个对应一个文档；而<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x33_hanspub.png" xlink:type="simple"/></inline-formula>是一组多项参数，对应每个文档中每个词语。</p><disp-formula id="hanspub.20002-formula68"><label>(3)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/7-1540690x34_hanspub.png"  xlink:type="simple"/></disp-formula><p>这个推理过程只对观测到的链接建模，即<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x35_hanspub.png" xlink:type="simple"/></inline-formula>，这样做的原因有两个：</p><p>首先，当文档d1和d2之间的链接被观测到就修改<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x36_hanspub.png" xlink:type="simple"/></inline-formula>，否则<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x37_hanspub.png" xlink:type="simple"/></inline-formula>，但是这种方法不能证明当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x38_hanspub.png" xlink:type="simple"/></inline-formula>时，d1和d2之间没有链接，此时，对待这些链接作为潜在变量是更符合真实的，例如，在大型的社交网络如Facebook中，两个人之间没有联系并不一定意味着他们不是朋友，他们可能是真正的朋友，只是谁都不知道在彼此的存在而以 [<xref ref-type="bibr" rid="hanspub.20002-ref18">18</xref>] 。</p><p>第二，隐藏未被观测到的链接可以降低计算成本，因为计算的复杂性与观察到的链接的数量成正比。</p><p>我们选择等式一时，由于等式难以计算，对它进行一次近似 [<xref ref-type="bibr" rid="hanspub.20002-ref19">19</xref>] ，得到等式4于是乎目标转化为计算等式4。</p><disp-formula id="hanspub.20002-formula69"><label>(4)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/7-1540690x39_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20002-formula70"><label>(5)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/7-1540690x40_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x41_hanspub.png" xlink:type="simple"/></inline-formula>以及<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x42_hanspub.png" xlink:type="simple"/></inline-formula>，当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x43_hanspub.png" xlink:type="simple"/></inline-formula>是目标函数时，这个式子可以被准确的计算为：</p><disp-formula id="hanspub.20002-formula71"><label>(6)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/7-1540690x44_hanspub.png"  xlink:type="simple"/></disp-formula><p>使用坐标上升方法来优化变分参数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x45_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x46_hanspub.png" xlink:type="simple"/></inline-formula>可得到：</p><disp-formula id="hanspub.20002-formula72"><graphic xlink:href="http://html.hanspub.org/file/7-1540690x47_hanspub.png"  xlink:type="simple"/></disp-formula><p><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x48_hanspub.png" xlink:type="simple"/></inline-formula>的计算取决于方程式5或者6中对于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x49_hanspub.png" xlink:type="simple"/></inline-formula>的选择。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x50_hanspub.png" xlink:type="simple"/></inline-formula>可以通过对<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x51_hanspub.png" xlink:type="simple"/></inline-formula>元素取对数被计算。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x52_hanspub.png" xlink:type="simple"/></inline-formula>是<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x53_hanspub.png" xlink:type="simple"/></inline-formula>，而<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x54_hanspub.png" xlink:type="simple"/></inline-formula>是双伽马函数。</p><p><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x55_hanspub.png" xlink:type="simple"/></inline-formula>的更新和LDA模型中变分参数一样。即<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x56_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec><sec id="s5_1_2"><title>3.2.2. 估算参数</title><p>我们通过对每个参数计算其极大似然估计的方法来对模型进行调整，主要是多项主题向量<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x57_hanspub.png" xlink:type="simple"/></inline-formula>和链接函数参数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x58_hanspub.png" xlink:type="simple"/></inline-formula>、<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x59_hanspub.png" xlink:type="simple"/></inline-formula>，但是我们发现直接计算是比较困难的，所以我们转向求近似值，采用变分EM，在优化式子4的变分分布和模型参数之间迭代。</p><p>因为包含<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x60_hanspub.png" xlink:type="simple"/></inline-formula>的式子4中术语和传统LDA中的一样，所以估算主题向量可以通过相同的方法，即对称的狄利克雷来估算<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x61_hanspub.png" xlink:type="simple"/></inline-formula>。</p><disp-formula id="hanspub.20002-formula73"><graphic xlink:href="http://html.hanspub.org/file/7-1540690x62_hanspub.png"  xlink:type="simple"/></disp-formula><p>但是不能在没有经过观察的情况下直接优化链路概率函数的参数，而是通过使用分等级的正则化任意参数化，这种正则化是先假设网络中存在一些潜在的负面意见，然后将其纳入参数估计，负观测的频率由<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x63_hanspub.png" xlink:type="simple"/></inline-formula>控制。当使用公式1的逻辑，我们使用基于梯度的优化 [<xref ref-type="bibr" rid="hanspub.20002-ref20">20</xref>] 来估计参数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x64_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x65_hanspub.png" xlink:type="simple"/></inline-formula>。使用公式5中使用的近似，ELBO的相关梯度是</p><p><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x66_hanspub.png" xlink:type="simple"/></inline-formula>，</p><p><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x67_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>当使用等式2的指数函数时，可以分析出参数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x68_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x69_hanspub.png" xlink:type="simple"/></inline-formula>。</p><disp-formula id="hanspub.20002-formula74"><graphic xlink:href="http://html.hanspub.org/file/7-1540690x70_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x71_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>我们的最终目标是要对新的数据进行预测，分为两个类别的预测：从文字预测链接的和从链接预测文字。</p><p>在链接预测中，给定一个新的文档及其中的词语。我们需要从这个文档到其他文件的链接。这需要计算一个关于后验的期望，这个后验我们是无法计算的。</p><disp-formula id="hanspub.20002-formula75"><graphic xlink:href="http://html.hanspub.org/file/7-1540690x72_hanspub.png"  xlink:type="simple"/></disp-formula><p>通过之前介绍的推理算法，根据变分分布的优化方法，我们得到了使用训练文本集 [<xref ref-type="bibr" rid="hanspub.20002-ref21">21</xref>] 中的词语和链接以及测试文件中的词语来进行计算的方法，使用近似值<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x73_hanspub.png" xlink:type="simple"/></inline-formula>来替换上面提到的后验，那么预测是值就约等于：</p><disp-formula id="hanspub.20002-formula76"><label>(7)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/7-1540690x74_hanspub.png"  xlink:type="simple"/></disp-formula><p>在词语预测中，我们仅仅基于链接预测一篇未知文档中的词语，与链路预测一样，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x75_hanspub.png" xlink:type="simple"/></inline-formula>无法计算，使用和上面相同的技术，使用变分分布来近似这个后验，就产生了预测等式：</p><disp-formula id="hanspub.20002-formula77"><graphic xlink:href="http://html.hanspub.org/file/7-1540690x76_hanspub.png"  xlink:type="simple"/></disp-formula><p>通过文件和链接，我们的模型能够根据词语预测链接，以及根据链接预测词语，或者两者混合。</p></sec></sec></sec><sec id="s6"><title>4. 实验结果</title><p>我们使用一组数据集来做实验，完成分词处理的词语(删除停顿词和常用词)，由定向链接转化为不定向链接，删除没有直接链接的文件 [<xref ref-type="bibr" rid="hanspub.20002-ref22">22</xref>] ，Cora数据 [<xref ref-type="bibr" rid="hanspub.20002-ref23">23</xref>] 包含使用论文搜素引擎搜索出来的摘要，以及文件间相互引用的链接 [<xref ref-type="bibr" rid="hanspub.20002-ref2">2</xref>] 。</p>评估<p>RTM模型对未经训练的数据定义了概率分布，根据第三部分所描述的，从数据中推断潜在变量，我们需要知道的是在预测未经训练的数据时这个模型的效果有多好，我们研究上面所说的RTM的两种变体：</p><p>图3. 链接预测结果对比图</p><p>图4. 词语预测结果对比图</p><p>利用等式1使用逻辑链接的逻辑RTM01和利用等式二使用指数链接的指数RTM02，通过两种备选方案来比较这两种模型，首先是基准模型，在这个模型中词语和链接是相互独立的，使用多项分布对词语建模，使用伯努利对链接进行建模；第二种是回归LDA，首次拟合LDA模型对文件处理，然后对观测到的链接进行逻辑回归，并输入每对文件的潜在链接，而不是进行降维和回归，该方法首先进行无监督降维，然后回归潜在空间和潜在的连结结构之间的关系。所有的模型都进行了训练，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/7-1540690x79_hanspub.png" xlink:type="simple"/></inline-formula>的值取5.0。</p><p>通过对上面所说模型关于词语预测和链接预测的计算，我们将数据集分成四份，对于每一份数据对应一种模型，我们提出两个预测查询：给定一个未知文档中的词语，他们之间的链接是怎样的；给定文档中的链接，它包含的词语有哪些。其次，预测查询是针对未经过观测训练的文件，在训练测试文档时，它们之间的链接，在图3中显示了链接预测的结果，图4显示的是词语预测的结果。</p><p>在预测链接上，RTM的两个变体比另外两个模型在所有数据集上表现都要好，Cora数据集是例证，指数RTM比基准模型提高了6%，比回归LDA提高了5%；逻辑RTM比基准模型提高了将近5%，比回归LDA提高了4%。在词语预测上，RTM的两个变体又一次的比其他模型都要好，这是因为RTM模型使用链接信息去影响词语的预测分布，LDA回归模型和预测和基准模型相似。</p></sec><sec id="s7"><title>文章引用</title><p>王全民,孙艳峰,李振国,谷 实,王开阳. 基于链接的关系主题模型Relation Topic Model Based on Links[J]. 计算机科学与应用, 2017, 07(03): 232-239. http://dx.doi.org/10.12677/CSA.2017.73029</p></sec><sec id="s8"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.20002-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">李文波, 孙乐, 黄瑞红. 基于Labeled-LDA模型的文本分类新算法[J]. 计算机学报, 2008, 31(4): 620-627.</mixed-citation></ref><ref id="hanspub.20002-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">张启蕊, 张凌, 董守斌, 谭景华. 训练集类别分布对文本分类的影响[J]. 清华大学学报(自然科学版), 2005, 45(S1): 76-79.</mixed-citation></ref><ref id="hanspub.20002-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Blei, D., Ng, A. and Jordan, M. (2003) Latent Dirichlet Allocation. Journal of Machine Learning Research, 3, 993- 1022.</mixed-citation></ref><ref id="hanspub.20002-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Cohn, D. and Hofmann, T. (2011) The Missing Link—A Probabilistic Model of Document Content and Hypertext Connectivity. Neural Information Processing Systems.</mixed-citation></ref><ref id="hanspub.20002-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Newman, M. (2012) The Structure and Function of Networks. Computer Physics Communications, 147, 40-45.</mixed-citation></ref><ref id="hanspub.20002-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Taskar, B., Wong, M., Abbeel, P. and Koller, D. (2014) Link Prediction in Relational Data. Neural Information Processing Systems.</mixed-citation></ref><ref id="hanspub.20002-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">McCallum, A., Nigam, K., Rennie, J. and Seymore, K. (2009) Automating the Construction of Internet Portals with Machine Learning. Information Retrieval, 3, 127-163. &lt;br&gt;https://doi.org/10.1023/A:1009953814988</mixed-citation></ref><ref id="hanspub.20002-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">McCallum, A., Corrada-Emmanuel, A. and Wang, X. (2010) Topic and Role Discovery in Social Networks. IJCAI.</mixed-citation></ref><ref id="hanspub.20002-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Nallapati, R., Ahmed, A., Xing, E.P. and Cohen, W.W. (2008) Joint Latent Topic Models for Text and Citations. Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, 24-27 August 2008, 542-550. &lt;br&gt;https://doi.org/10.1145/1401890.1401957</mixed-citation></ref><ref id="hanspub.20002-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Nallapati, R. and Cohen, W. (2013) Link-pLSA-LDA: A New Unsupervised Model for Topics and Influence of Blogs. 2nd International Conference on Weblogs and Social Media (ICWSM), Seattle, 2008.</mixed-citation></ref><ref id="hanspub.20002-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Mei, Q., Cai, D., Zhang, D. and Zhai, C. (2008) Topic Modeling with Network Regularization. Proceedings of the 17th international conference on World Wide Web, Beijing, 21-25 April 2008, 101-110.  
&lt;br&gt;https://doi.org/10.1145/1367497.1367512</mixed-citation></ref><ref id="hanspub.20002-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Airoldi, E., Blei, D., Fienberg, S. and Xing, E. (2008) Mixed Membership Stochastic Blockmodels. Journal of Machine Learning Research, 9, 1981-2014.</mixed-citation></ref><ref id="hanspub.20002-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Blei, D. and Jordan, M. (2013) Modeling Annotated Data. Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Toronto, 28 July-1 August 2003, 127-134.</mixed-citation></ref><ref id="hanspub.20002-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Blei, D.M. and McAuliffe, J.D. (2007) Supervised Topic Models. Neural Information Processing Systems.</mixed-citation></ref><ref id="hanspub.20002-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Braun, M. and McAuliffe, J. (2010) Variational Inference for Large-Scale Models of Discrete Choice. Journal of the American Statistical Association, 105, 324-334. arXiv:0712.2526. &lt;br&gt;https://doi.org/10.1198/jasa.2009.tm08030</mixed-citation></ref><ref id="hanspub.20002-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Sinkkonen, J., Aukia, J. and Kaski, S. (2008) Component Models for Large Networks. arXiv:0803.1628</mixed-citation></ref><ref id="hanspub.20002-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Gruber, A., Rosen-Zvi, M. and Weiss, Y. (2008) Latent Topic Models for Hypertext. Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence.</mixed-citation></ref><ref id="hanspub.20002-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Hoff, P., Raftery, A. and Handcock, M. (2012) Latent Space Approaches to Social Network Analysis. Journal of the American Statistical Association, 97, 1090-1098. &lt;br&gt;https://doi.org/10.1198/016214502388618906</mixed-citation></ref><ref id="hanspub.20002-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Getoor, L., Friedman, N., Koller, D. and Taskar, B. (2011) Learning Probabilistic Models of Relational Structure. Proceedings of the 18th International Conference on Machine Learning, 28 June-1 July 2001, 170-177.</mixed-citation></ref><ref id="hanspub.20002-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Wang, X., Mohanty, N. and McCallum, A. (2005) Group and Topic Discovery from Relations and Text. Proceedings of the 3rd International Workshop on Link Discovery, Chicago, 21-25 August 2005, 28-35.  
&lt;br&gt;https://doi.org/10.1145/1134271.1134276</mixed-citation></ref><ref id="hanspub.20002-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Hofman, J. and Wiggins, C. (2008) Bayesian Approach to Network Modularity. Physical Review Letters, 100, Article ID: 258701. arXiv:0709.3512. &lt;br&gt;https://doi.org/10.1103/physrevlett.100.258701</mixed-citation></ref><ref id="hanspub.20002-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Dietz, L., Bickel, S. and Scheffer, T. (2007) Unsupervised Prediction of Citation Influences. Proceedings of the 24th International Conference on Machine Learning, Corvalis, Oregon, 20-24 June 2007, 233-240.  
&lt;br&gt;https://doi.org/10.1145/1273496.1273526</mixed-citation></ref><ref id="hanspub.20002-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Kemp, C., Griffiths, T. and Tenenbaum, J. (2014) Discovering Latent Classes in Relational Data. MIT AI Memo 2004- 019.</mixed-citation></ref></ref-list></back></article>