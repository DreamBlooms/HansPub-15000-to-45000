<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">HJDM</journal-id><journal-title-group><journal-title>Hans Journal of Data Mining</journal-title></journal-title-group><issn pub-type="epub">2163-145X</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/HJDM.2017.72004</article-id><article-id pub-id-type="publisher-id">HJDM-20396</article-id><article-categories><subj-group subj-group-type="heading"><subject>HJDM20170200000_61628239.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于随机森林方法的北京市二手房价格研究
  Analysis of Beijing Second-Hand House Price Based on Random Forest
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>晓童</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郭</surname><given-names>萱</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>成杰</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff2"><addr-line>中国石油大学(北京)理学院，北京</addr-line></aff><pub-date pub-type="epub"><day>04</day><month>05</month><year>2017</year></pub-date><volume>07</volume><issue>02</issue><fpage>37</fpage><lpage>45</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    随着经济的发展和可供开发土地的减少，二手房价一路飙高。截止到2016年5月底，北京城内六区二手房均价已超6万。对二手房价格进行评估预测将对居民生活产生重要影响，也可以给政府宏观调控提供一定参考。目前关于房价的数学模型多使用线性回归模型，神经网络模型和支持向量机模型。线性回归模型中对房价与预测变量线性关系的设定易造成较大误差，神经网络与支持向量机解释性较差。本文针对北京市16,795套在售二手房，对多类别变量建立随机森林模型，进行房价影响因素研究以及房价预测，通过方差解释性变化得到lat (小区所处纬度)，long (小区所处经度)和cate (小区所处区域)三个预测变量对房价的影响最为显著，通过随机森林变量重要性输出得到cate，lat和long对房价的影响最大。然后通过00B (out-of bag)样本得到随机森林二手房价格预测精度为0.69。最后将房价数据输入神经网络模型与支持向量机模型，得到房价预测精度分别为：5.15、1.10。结果表明，随机森林预测效果最佳；支持向量机模型次之，预测结果不够稳定；而神经网络预测误差较大，不适用于本文二手房价格预测。
    With the development of economy and reducing of available land, the price of second-hand house is rising continuously. By the end of May 2016, average price of second-hand house in Beijing has been more than &#165;60,000/m2. Evaluating the price of second-hand house will not only produce important influence on residents’ life, but also bring effective reference on the government’s macroeconomic regulation and control. Current mathematical model about housing price includes linear regression model, neural network model (NN) and support vector machine model (SVM). In linear regression model, the suppose of linear relationship may cause more error. NN and SVM are proved to have poor explanatory. Based on the price of 16,795 second-hand houses in Beijing, the random forest model was established to study the influence factors of house price and the forecast of house price. Method of variance explanatory changes shows lat (Residential latitude), long (Residential longitude) and cate (Residential area) are the three main significant prediction variables on housing price, while random forest model picks up cate, lat and long to be the most important. Through analysis of OOB (out-of bag) samples, random forest gets a precision of 0.69 in second-hand housing forecast. Finally, put price data into NN and SVM model and forecast, precision 5.15 and 1.10 were got respectively. The result shows that random forest forecast is the best, followed by SVM. NN prediction does not apply to the second-hand house data in this paper. 
  
 
</p></abstract><kwd-group><kwd>二手房，房价预测，Boostrap抽样，决策树，随机森林, Second-Hand House</kwd><kwd> Housing Forecast</kwd><kwd> Bootstrap Sampling</kwd><kwd> Decision Trees</kwd><kwd> Random Forest Model</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于随机森林方法的北京市二手房价格研究<sup> </sup></title><p>李晓童，郭萱，王成杰</p><p>中国石油大学(北京)理学院，北京</p><p>收稿日期：2017年4月5日；录用日期：2017年4月27日；发布日期：2017年4月30日</p><disp-formula id="hanspub.20396-formula5"><graphic xlink:href="http://html.hanspub.org/file/1-1760115x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>随着经济的发展和可供开发土地的减少，二手房价一路飙高。截止到2016年5月底，北京城内六区二手房均价已超6万。对二手房价格进行评估预测将对居民生活产生重要影响，也可以给政府宏观调控提供一定参考。目前关于房价的数学模型多使用线性回归模型，神经网络模型和支持向量机模型。线性回归模型中对房价与预测变量线性关系的设定易造成较大误差，神经网络与支持向量机解释性较差。本文针对北京市16,795套在售二手房，对多类别变量建立随机森林模型，进行房价影响因素研究以及房价预测，通过方差解释性变化得到lat (小区所处纬度)，long (小区所处经度)和cate (小区所处区域)三个预测变量对房价的影响最为显著，通过随机森林变量重要性输出得到cate，lat和long对房价的影响最大。然后通过00B (out-of bag)样本得到随机森林二手房价格预测精度为0.69。最后将房价数据输入神经网络模型与支持向量机模型，得到房价预测精度分别为：5.15、1.10。结果表明，随机森林预测效果最佳；支持向量机模型次之，预测结果不够稳定；而神经网络预测误差较大，不适用于本文二手房价格预测。</p><p>关键词 :二手房，房价预测，Boostrap抽样，决策树，随机森林</p><disp-formula id="hanspub.20396-formula6"><graphic xlink:href="http://html.hanspub.org/file/1-1760115x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2017 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="http://image.hanspub.org:8080\Html/htmlimages\1-2890033x\e70a10f1-7c93-45ea-9603-062237856e4b.png" /><img src="http://image.hanspub.org:8080\Html\htmlimages\1-2890033x\e898c85e-ffc4-45c9-b817-14224a4d6960.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>经济的快速发展使得城市可供利用开发的土地越来越少，房地产市场越来越成熟使得二手房市场交易日益活跃。今年三月北京市西城区文昌胡同的一处学区房卖出46万一平。有些房子是小朋友上好学校的通行证，可能是房子很贵的原因。同样房子周围有没有地铁也是影响房价的因素。通过对北京市二手房价格影响因素进行分析并对房价做出预测，为二手房价格评估提供理论依据和实践指导具有重要意义。对二手房的已有研究成果较多。仲小瑾 [<xref ref-type="bibr" rid="hanspub.20396-ref1">1</xref>] (2008)将多元线性回归模型应用于房价影响因素研究预测中。李菲等 [<xref ref-type="bibr" rid="hanspub.20396-ref2">2</xref>] (2004)使用灰色系统理论对线性回归模型进行完善和改进，建立房价预测模型。张辉 [<xref ref-type="bibr" rid="hanspub.20396-ref3">3</xref>] (2013)将非线性模型神经网络用于房价评估领域。陈静 [<xref ref-type="bibr" rid="hanspub.20396-ref4">4</xref>] (2008)将支持向量机模型用于西安市房价评估中。郭志强 [<xref ref-type="bibr" rid="hanspub.20396-ref5">5</xref>] (2013)将支持向量机用于房价预测中，发现预测结果明显优于岭回归与神经网络模型。这些研究成果中，线性回归模型遇到非线性问题会产生较大误差，神经网络稳定性较差以及对结果解释性较差，支持向量机解释性差。</p><p>随机森林由Brieman 2001年提出至今，已经被广泛应用于生态学、经济管理、生物医学、信用评价等领域。其不易出现过度拟合、很好的处理类别变量、解释性好、对噪声数据的容忍性、精度高等优点使其成为一种广泛使用的回归分类算法。本文用随机森林方法对北京市二手房价格影响因素进行分析并对房价做出预测，首先根据问题背景以及获取数据的局限性初步给出九个影响二手房价格因素，分别为：subway、school、long、lat、cate、bedrooms、halls、area、floor。基于16,795套在售二手房数据建立随机森林模型，由随机森林变量重要性输出以及逐步删除变量得到解释性变化值，从而得到影响北京市二手房价格的主要因素并进行分析，最后利用随机森林OOB样本数据对房价进行预测。同时本文还将神经网络模型、支持向量机模型作为对比模型，进行房价预测。发现在预测方面，随机森林有着更加精确的预测效果。</p></sec><sec id="s4"><title>2. 随机森林模型介绍</title><p>近年来，作为机器学习方法之一的随机森林受到越来越广泛的关注。随机森林 [<xref ref-type="bibr" rid="hanspub.20396-ref6">6</xref>] 是一种统计学习理论，利用bootstrap抽样的方式从原始数据集中抽取多个样本，对每个bootstrap样本进行决策树建模，组合多个决策树投票得到最终预测结果。大量理论实践研究都表明随机森林具有很高的预测准确率，对异常值和噪声具有很好的容忍度，且不易出现过度拟合。随机森林作为一种非线性的建模工具，是目前数据挖掘、生物信息学最热门最前言的研究领域之一。</p><p>决策树是构成随机森林的基本单位，一个简单的决策树模型如图1，其是一个树状结构，由根节点、中间节点以及叶结点组成。每一步根据变量的分类效果选择合适的划分，最终做出分类和预测</p><p>随机森林(图2)是由决策树组成，通过组合多个决策树分类器进行分类和预测。其工作机制大致为：首先通过bootstrap抽样选择一系列训练集，在每个训练集上对特征进行随机的选取并通过基尼指标等其他指标对数据集进行合适的划分，生成一系列不剪枝的决策树。最后投票决定最优分类做出预测。</p><p>图1. 决策树模型</p><p>图2. 随机森林模型</p><p>随机森林具体算法如下：</p><p>输入：1.训练集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x11_hanspub.png" xlink:type="simple"/></inline-formula></p><p>2.待测样本<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x12_hanspub.png" xlink:type="simple"/></inline-formula></p><p>For: <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x13_hanspub.png" xlink:type="simple"/></inline-formula></p><p>1) 对原式训练集S Bootstrap抽样，生成训练集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x14_hanspub.png" xlink:type="simple"/></inline-formula></p><p>2) 使用<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x15_hanspub.png" xlink:type="simple"/></inline-formula>生成一棵不剪枝的树<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x16_hanspub.png" xlink:type="simple"/></inline-formula></p><p>a.从d个特征中随机选取<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x17_hanspub.png" xlink:type="simple"/></inline-formula>个特征</p><p>b.在每个节点上从<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x18_hanspub.png" xlink:type="simple"/></inline-formula>个特征依据Gini指标选取最优特征</p><p>c.分裂直到树生长到最大</p><p>End</p><p>输出：1.树的集合<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x19_hanspub.png" xlink:type="simple"/></inline-formula></p><p>2.对待测样本<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x20_hanspub.png" xlink:type="simple"/></inline-formula>，决策树<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x21_hanspub.png" xlink:type="simple"/></inline-formula>输出<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x22_hanspub.png" xlink:type="simple"/></inline-formula></p><p>回归：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x23_hanspub.png" xlink:type="simple"/></inline-formula></p><p>分类：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x24_hanspub.png" xlink:type="simple"/></inline-formula></p><p>由上述随机森林算法可得，随机森林的随机性主要体现在如下两方面：1) bootstrap抽样产生的样本随机性。本文关于北京市16,795套二手房数据，通过bootstrap抽样，假设我们得到500个训练集，每个训练集中将近37%的数据不会出现，训练集之间两两差异很大，由此对数据进行了充分利用；2) 在每个训练集上选择特征的随机性。在每个训练集上每一步进行特征选择时，不同于bagging的方法，随机森林会根据变量的个数确定选择几个特征。本文关于二手房数据的9个变量中，每一棵树生成时每一步划分我们选择3个变量，从3个变量中根据Gini指标确定最优的划分变量，生成不剪枝的决策树，依次生成一系列不剪枝决策树，相对于bagging方法，通过这样的特征选取进一步提高了数据的利用率，从而提高了预测精度。由这两点的随机性决定着随机森林的分类预测效果。</p></sec><sec id="s5"><title>3. 二手房的随机森林模型</title><sec id="s5_1"><title>3.1. 变量的选取</title><p>本文主要对北京市二手房价格影响因素进行分析研究并对房价进行预测。收集某二手房中介网站2016年5月底北京城内六区(东城、西城、朝阳、海淀、丰台、石景山) 16,795套在售二手房相关数据。</p><p>下面介绍本文各变量的选取：</p><p>1) 响应变量</p><p>本文选取在售二手房Price (每平米的均价)作为响应变量。</p><p>2) 预测变量</p><p>美国学者Butler提出了影响房地产价格的三大特征变量 [<xref ref-type="bibr" rid="hanspub.20396-ref7">7</xref>] ：区位特征，建筑特征以及邻里环境。区位特征指的是住宅小区位于城市哪个区域，包含与固定区位属性相关的一些特征。一般选取到城镇中心区的距离量化该特征。建筑特征简而言之即为住宅本身的客观状况，包括：户型、面积、建筑年龄、建筑结构、装修、车库等。邻里环境具体指住宅小区的人文环境、自然环境以及治安管理等。这三种特征变量包含着属性的隐含价格，因为消费者对于属性的支付意愿是从住宅价格间接得到的。</p><p>据此我们根据三大特征变量选择出九个变量作为本文的预测变量。区位特征选择：subway (是否地铁沿线)、school (是否为学区房)、long (所在小区所处的经度)、lat (所在小区所处的纬度)、cate (东城、西城、海淀、朝阳、丰台石、景山)。建筑特征选择：bedrooms (卧室数)、halls (厅总数)、area (房屋总面积)、floor (basement, low, middle, high)。由于住宅小区及治安管理缺乏统计标准以及环境变量的数据获取途径有限。邻里环境变量没有出现在本文中。</p></sec><sec id="s5_2"><title>3.2. 随机森林模型的建立</title><p>随机森林模型是Breiman (2001)首次提出，通过建立一系列的决策树组成随机森林模型，最终投票做出最后的预测。该算法具有需要调整的参数较少、不必担心过度拟合、分类速度快、能高效处理大样本数据、能估计特征因素的重要性、很好的处理类别变量、有较强的抗噪声能力等优点。与线性回归相比，避免了线性回归事先假定的线性关系不符合实际造成较大误差的情况。且随机森林不用对函数形式事先进行假设，避免了假设误差。</p><p>运用随机森林方法进行二手房价格评估，随机森林可以处理分类和回归问题。本文对于二手房的研究属于回归预测问题。随机森林回归的基本思想是：首先利用自助抽样法，从原始数据集中抽取B个样本，且每个样本容量都与原始数据集相同；然后对B个样本分别建立B棵树，得到B个结果；最后，对这B个结果取平均值得到最终的预测结果。基于随机森林的二手房价格评估模型计算如下：</p><p>二手房的随机森林模型由B棵树组成，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x25_hanspub.png" xlink:type="simple"/></inline-formula>，其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x26_hanspub.png" xlink:type="simple"/></inline-formula>是二手房的P维特征向量。结果会产生B个结果<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x27_hanspub.png" xlink:type="simple"/></inline-formula>。其中，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x28_hanspub.png" xlink:type="simple"/></inline-formula>是第b棵树的预测结果。对于回归问题预测值为所有树预测结果的平均。算法流程如下：</p><p>1) 原始数据含样本量为16,795，应用bootstrap方式抽样选择500个样本集，构建500棵决策树。每次抽样未被抽到的样本构成OOB样本作为随机森林的验证样本。</p><p>2) 样本中变量个数为9，每一棵决策树每一个节点随机选择<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x29_hanspub.png" xlink:type="simple"/></inline-formula>个变量进行基尼指标计算，确定合适的变量得到合适的划分。使用随机森林做回归时，通常选取<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x30_hanspub.png" xlink:type="simple"/></inline-formula>。本文每一次划分选择3个变量。</p><p>3) 每一棵决策树生长到最大，无需进行剪枝，重复上述步骤直到生成500棵决策树。</p><p>通过如上步骤，建立得到二手房的随机森林价格评估模型，将OOB样本输入随机森林模型得到房价预测精度。</p></sec><sec id="s5_3"><title>3.3. 特征变量重要性评价</title><p>随机森林可以给出变量重要性排序，本文据此得出影响二手房价格的重要预测变量。其次，本文通过依此删除预测变量的方式计算方差解释性差值，得到变量重要性排序。删除某个变量后解释性差值变化越大，证明这个变量越重要；解释性差值变化越小，证明这个变量越不重要。</p><p>记删除变量后方差的解释性为：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x31_hanspub.png" xlink:type="simple"/></inline-formula></p><p>方差解释性变化为：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x32_hanspub.png" xlink:type="simple"/></inline-formula></p><p>变量分别如表1。</p><p>为了提高计算准确性，随机森林运行十次得到方差解释性如表2。</p><p>方差平均解释性为：85.12%。</p><p>逐个删除自变量，输入随机森林模型，方差解释性如表3。</p><p>方差的解释性变化如表4。</p><p>由此可得，可以按照重要性将变量分为三个层次：第一层次包括lat、long、cate三个方差的解释性差值最大的变量，这表明大多数人选择二手房时首先考虑房子所在的地理位置(纬度、经度和区域)。选择了房子的地理位置后，第二层次变量包括房子的总面积(area)以及卧室的数目(bedrooms)。第三层次的学区房和是否临近地铁这两个变量方差解释性差值较小，为重要性相对较弱的变量，分析其原因，是否临</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Explaining variabl</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >x1</th><th align="center" valign="middle" >Subway</th><th align="center" valign="middle" >x2</th><th align="center" valign="middle" >school</th></tr></thead><tr><td align="center" valign="middle" >x3</td><td align="center" valign="middle" >Long</td><td align="center" valign="middle" >x4</td><td align="center" valign="middle" >lat</td></tr><tr><td align="center" valign="middle" >x5</td><td align="center" valign="middle" >Cate</td><td align="center" valign="middle" >x6</td><td align="center" valign="middle" >bedrooms</td></tr><tr><td align="center" valign="middle" >x7</td><td align="center" valign="middle" >Halls</td><td align="center" valign="middle" >x8</td><td align="center" valign="middle" >area</td></tr><tr><td align="center" valign="middle" >x9</td><td align="center" valign="middle" >Floor</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >　</td></tr></tbody></table></table-wrap><p>表1. 预测变量</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Variance of explanator</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >次数</th><th align="center" valign="middle" >解释性</th><th align="center" valign="middle" >次数</th><th align="center" valign="middle" >解释性</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >85.10%</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >84.90%</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >85.22%</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >84.85%</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >85.30%</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >85.26%</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >84.73%</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >85.65%</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >85.37%</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >84.80%</td></tr></tbody></table></table-wrap><p>表2. 方差解释性</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Erase variables of variance explanator</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >w1</th><th align="center" valign="middle" >82.95%</th><th align="center" valign="middle" >w2</th><th align="center" valign="middle" >81.92%</th></tr></thead><tr><td align="center" valign="middle" >w3</td><td align="center" valign="middle" >74.91%</td><td align="center" valign="middle" >w4</td><td align="center" valign="middle" >71.95%</td></tr><tr><td align="center" valign="middle" >w5</td><td align="center" valign="middle" >76.16%</td><td align="center" valign="middle" >w6</td><td align="center" valign="middle" >80.92%</td></tr><tr><td align="center" valign="middle" >w7</td><td align="center" valign="middle" >81.72%</td><td align="center" valign="middle" >w8</td><td align="center" valign="middle" >79.97%</td></tr><tr><td align="center" valign="middle" >w9</td><td align="center" valign="middle" >81.19%</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >　</td></tr></tbody></table></table-wrap><p>表3. 逐个删除变量方差解释性</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Changes of variance explanator</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >t1</th><th align="center" valign="middle" >2.17%</th><th align="center" valign="middle" >t2</th><th align="center" valign="middle" >3.20%</th></tr></thead><tr><td align="center" valign="middle" >t3</td><td align="center" valign="middle" >10.21%</td><td align="center" valign="middle" >t4</td><td align="center" valign="middle" >13.17%</td></tr><tr><td align="center" valign="middle" >t5</td><td align="center" valign="middle" >8.96%</td><td align="center" valign="middle" >t6</td><td align="center" valign="middle" >4.20%</td></tr><tr><td align="center" valign="middle" >t7</td><td align="center" valign="middle" >3.40%</td><td align="center" valign="middle" >t8</td><td align="center" valign="middle" >5.15%</td></tr><tr><td align="center" valign="middle" >t9</td><td align="center" valign="middle" >3.93%</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >　</td></tr></tbody></table></table-wrap><p>表4. 方差解释性变化</p><p>近地铁是房子所处地理位置的一部分因素，大多数情况下可以由房子的地理位置确定，因此作为单独变量影响较小；而学区房受众群体比较单一，其重要性只针对有孩子需要上学的家庭，样本较大时这种重要性会被减弱。</p><p>随机森林输出的变量重要性如表5。</p><p>由表5可得，cate，lat和long同上述方差解释性差值一样，为最重要的三个变量，表明大多数人选择二手房时首先考虑房子所在的地理位置。school，subway和area为接下来重要的变量。这个结果与上述方差解释性得到的结果具有大致相同的趋势。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Output of RF variable importanc</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >变量</th><th align="center" valign="middle" >节点纯度</th></tr></thead><tr><td align="center" valign="middle" >CATE</td><td align="center" valign="middle" >1.77E+12</td></tr><tr><td align="center" valign="middle" >Bedrooms</td><td align="center" valign="middle" >9.65E+10</td></tr><tr><td align="center" valign="middle" >Halls</td><td align="center" valign="middle" >8.75E+10</td></tr><tr><td align="center" valign="middle" >AREA</td><td align="center" valign="middle" >4.30E+11</td></tr><tr><td align="center" valign="middle" >floor</td><td align="center" valign="middle" >1.09E+11</td></tr><tr><td align="center" valign="middle" >Subway</td><td align="center" valign="middle" >1.09E+11</td></tr><tr><td align="center" valign="middle" >School</td><td align="center" valign="middle" >6.59E+11</td></tr><tr><td align="center" valign="middle" >LONG</td><td align="center" valign="middle" >8.73E+11</td></tr><tr><td align="center" valign="middle" >LAT</td><td align="center" valign="middle" >1.06E+12</td></tr></tbody></table></table-wrap><p>表5. RF变量重要性输出</p></sec><sec id="s5_4"><title>3.4. 二手房房价预测</title><p>通过bootstrap抽样，未被抽到的样本组成了B个袋外数据(out-of-bag, OOB)，构成OOB样本。每次bootstrap抽样，将近37%的样本不会被抽中。本文将入袋样本作为测试集，将袋外样本作为验证集。采用下述的方式衡量房价的预测精度：</p><disp-formula id="hanspub.20396-formula7"><label>(1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/1-1760115x33_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20396-formula8"><label>(2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/1-1760115x34_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中n为16,795套二手房数据的袋外数据量。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x35_hanspub.png" xlink:type="simple"/></inline-formula>为袋外数据的预测价格，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x36_hanspub.png" xlink:type="simple"/></inline-formula>为袋外数据的实际价格。ESS为残差平方和。J为残差平方和取平方根。</p><p>随机森林每一次bootstrap抽样，会产生不同的OOB样本，不同的OOB样本计算ESS会得到不同的预测精度，为了保证预测准确性，对十次bootstrap得到的袋外数据计算预测误差并取平均，为了方便与下文其他模型对比，我们取预测误差平均的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/1-1760115x37_hanspub.png" xlink:type="simple"/></inline-formula>，计算结果如表6。</p><p>为了更加直观的看到随机森林的预测效果，我们使用R软件在16795个数据集的OOB样本中随机抽取15个样本，得到其预测价格与实际价格并计算预测误差如表7。我们看到预测误差基本可以控制在10%左右，说明随机森林预测效果良好。</p></sec></sec><sec id="s6"><title>4. 模型对比</title><p>分类和回归模型使用较好且常用的有神经网络模型与支持向量机模型。本文将数据输入这两个对比模型得到预测误差如下：</p><p>计算得到支持向量机十次的预测误差并取平均如表8。</p><p>计算得到神经网络十次的预测误差并取平均如表9。</p><p>本文关于北京市二手房数据我们得到方差的解释性达到85.12%，表明所得数据里包含着大量可提取的有效信息，进一步变量重要性的输出对预测精度高做出合理的解释。将北京市二手房的9个预测变量分为三个层次，第一层次包括lat、long和cate，三个表明房屋地理位置的变量；第二层次包括area和bedrooms，两个表明房屋建筑特征的变量；第三层次包含subway、school等重要性相对较弱的变量。相对于神经网络模型和支持向量机模型直接给出预测精度，随机森林变量重要性的输出对房价进行了合理</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Prediction accuracy of RF model OOB sampl</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >次数</th><th align="center" valign="middle" >J</th><th align="center" valign="middle" >次数</th><th align="center" valign="middle" >J</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >697303</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >646405.7</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >700391.7</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >665065</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >711579.9</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >707365.6</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >681810</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >680221.9</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >698288</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >690601.6</td></tr><tr><td align="center" valign="middle" >平均</td><td align="center" valign="middle" >687903.2</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >0.687903</td></tr></tbody></table></table-wrap><p>表6. RF模型OOB样本预测精度</p><table-wrap id="table7" ><label><xref ref-type="table" rid="table7">Table 7</xref></label><caption><title> Some predictions of house price</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >序号</th><th align="center" valign="middle" >实际价格</th><th align="center" valign="middle" >预测价格</th><th align="center" valign="middle" >预测误差</th><th align="center" valign="middle" >序号</th><th align="center" valign="middle" >实际价格</th><th align="center" valign="middle" >预测价格</th><th align="center" valign="middle" >预测误差</th></tr></thead><tr><td align="center" valign="middle" >10736</td><td align="center" valign="middle" >56389</td><td align="center" valign="middle" >63590.84</td><td align="center" valign="middle" >12.77%</td><td align="center" valign="middle" >5735</td><td align="center" valign="middle" >42639</td><td align="center" valign="middle" >39447.53</td><td align="center" valign="middle" >7.48%</td></tr><tr><td align="center" valign="middle" >6690</td><td align="center" valign="middle" >45376</td><td align="center" valign="middle" >39248.68</td><td align="center" valign="middle" >13.50%</td><td align="center" valign="middle" >13378</td><td align="center" valign="middle" >50477</td><td align="center" valign="middle" >49622.64</td><td align="center" valign="middle" >1.69%</td></tr><tr><td align="center" valign="middle" >365</td><td align="center" valign="middle" >66679</td><td align="center" valign="middle" >62198.62</td><td align="center" valign="middle" >6.72%</td><td align="center" valign="middle" >7204</td><td align="center" valign="middle" >38637</td><td align="center" valign="middle" >38118.09</td><td align="center" valign="middle" >1.34%</td></tr><tr><td align="center" valign="middle" >1876</td><td align="center" valign="middle" >41243</td><td align="center" valign="middle" >43597.08</td><td align="center" valign="middle" >5.71%</td><td align="center" valign="middle" >9673</td><td align="center" valign="middle" >58352</td><td align="center" valign="middle" >59597.04</td><td align="center" valign="middle" >2.13%</td></tr><tr><td align="center" valign="middle" >9823</td><td align="center" valign="middle" >92527</td><td align="center" valign="middle" >99416.12</td><td align="center" valign="middle" >7.45%</td><td align="center" valign="middle" >11472</td><td align="center" valign="middle" >86402</td><td align="center" valign="middle" >78716.34</td><td align="center" valign="middle" >8.90%</td></tr><tr><td align="center" valign="middle" >8017</td><td align="center" valign="middle" >75377</td><td align="center" valign="middle" >74278.09</td><td align="center" valign="middle" >1.46%</td><td align="center" valign="middle" >11522</td><td align="center" valign="middle" >70198</td><td align="center" valign="middle" >56844.28</td><td align="center" valign="middle" >19.02%</td></tr><tr><td align="center" valign="middle" >4030</td><td align="center" valign="middle" >36539</td><td align="center" valign="middle" >34855.84</td><td align="center" valign="middle" >4.61%</td><td align="center" valign="middle" >14333</td><td align="center" valign="middle" >114183</td><td align="center" valign="middle" >88392.41</td><td align="center" valign="middle" >22.59%</td></tr><tr><td align="center" valign="middle" >826</td><td align="center" valign="middle" >39797</td><td align="center" valign="middle" >38470.58</td><td align="center" valign="middle" >3.33%</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >　</td></tr></tbody></table></table-wrap><p>表7. 房价部分预测情况</p><table-wrap id="table8" ><label><xref ref-type="table" rid="table8">Table 8</xref></label><caption><title> Prediction accuracy of SVM mode</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >次数</th><th align="center" valign="middle" >J</th><th align="center" valign="middle" >次数</th><th align="center" valign="middle" >J</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1101259</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >1113980</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1093232</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >1089112</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1111184</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >1119870</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >1120403</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >1104404</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >1089643</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >1096588</td></tr><tr><td align="center" valign="middle" >平均</td><td align="center" valign="middle" >1103968</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >1.103968</td></tr></tbody></table></table-wrap><p>表8. SVM模型预测精度</p><table-wrap id="table9" ><label><xref ref-type="table" rid="table9">Table 9</xref></label><caption><title> Prediction accuracy of NN mode</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >次数</th><th align="center" valign="middle" >J</th><th align="center" valign="middle" >次数</th><th align="center" valign="middle" >J</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >5166610</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >5149572</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >5160513</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >5130260</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >5182932</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >5196509</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >5165548</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >5125228</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >5113012</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >5137804</td></tr><tr><td align="center" valign="middle" >平均</td><td align="center" valign="middle" >5152799</td><td align="center" valign="middle" >　</td><td align="center" valign="middle" >5.152799</td></tr></tbody></table></table-wrap><p>表9. NN模型预测精度</p><p>的解释。由上神经网络和支持向量机对比模型可得，支持向量机模型预测的误差仅次于随机森林模型预测误差，但误差较大，约为随机森林误差的一倍。神经网络的误差较大，不适合于本文二手房房价评估模型。</p></sec><sec id="s7"><title>5. 总结</title><p>本文构建了二手房价格评估的随机森林模型。在三大特征变量中选择了9个预测变量，对北京市城内六区16,795套在售的二手房数据进行了房价影响因素以及房价预测研究。研究表明，cate，lat和long为影响房价的最重要变量。进一步本文利用OOB样本实现了对随机森林模型预测精度的外推，得到了随机森林有着较好的预测精度。最后本文引入对比模型神经网络模型以及支持向量机模型对房价进行预测，得到支持向量机模型的预测效果仅次于随机森林模型的预测效果，而神经网络模型预测误差较大，不适用于本文的房价预测。</p></sec><sec id="s8"><title>基金项目</title><p>中国石油大学(北京)本科教育教学改革项目，项目编号21G16091。</p></sec><sec id="s9"><title>文章引用</title><p>李晓童,郭 萱,王成杰. 基于随机森林方法的北京市二手房价格研究 Analysis of Beijing Second-Hand House Price Based on Random Forest[J]. 数据挖掘, 2017, 07(02): 37-45. http://dx.doi.org/10.12677/HJDM.2017.72004</p></sec><sec id="s10"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.20396-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">仲小瑾. 基于多元线性回归分析法的房地产价格评估[J]. 商业时代, 2014: 133-134.</mixed-citation></ref><ref id="hanspub.20396-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">李菲, 孙文彬. 灰色理论在商品住宅价格预测中的应用[J]. 辽宁工程大学学报, 2004, 6(3): 271-273.</mixed-citation></ref><ref id="hanspub.20396-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">张辉. 关于多当今社会BP神经网络的房地产价格评估与研究方向[J]. 房地产导刊, 2013.</mixed-citation></ref><ref id="hanspub.20396-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">陈静. 基于支持向量机的房地产估价方法研究[D]: [硕士学位论文]. 西安: 长安大学, 2008.</mixed-citation></ref><ref id="hanspub.20396-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">郭志强. 基于支持向量机回归的房地产批量估价[D]: [硕士学位论文]. 广州: 暨南大学, 2013.</mixed-citation></ref><ref id="hanspub.20396-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">James, G. (2014) An Introduction to Statistical Learning with Applications in R. University of Southern California, 303-324.</mixed-citation></ref><ref id="hanspub.20396-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">杨沐晞. 基于随机森林模型的二手房价格评估研究[D]: [硕士学位论文]. 长沙: 中南大学, 2012.</mixed-citation></ref></ref-list></back></article>