<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114098</article-id><article-id pub-id-type="publisher-id">CSA-41746</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_71983298.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  融合知识图谱和用户行为信息的个性化推荐算法研究
  Research on Personalized Recommendation Algorithm Based on Knowledge Graph and User Behavior Information
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>程</surname><given-names>静文</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>全民</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>北京工业大学信息学部，北京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>948</fpage><lpage>961</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对传统协同过滤存在的稀疏性和冷启动问题，通常使用深度神经网络(DNN)构建融合知识图谱和推荐系统的推荐任务。但目前的方法未曾考虑特征间的低阶线性关系，虽可加入因子分解机(FM)，但不同的特征对模型的贡献不同，FM可能会因所有特征交互设置相同的权重而受到阻碍；DNN解决知识图谱这种具有不规则、可扩展、多重结构特性的数据结构不具普适性。针对以上问题，提出MKAFG模型，推荐部分加入具有注意力机制的FM，通过注意力网络区分不同特征交互的重要性，使FM提取到对目标预测起到重要作用的一阶、二阶线性交互特征。知识嵌入部分使用图卷积神经网络(GCN)，提高推荐系统推荐效果。实验结果表明，MKAFG在MovieLens-1M数据集上的推荐效果优于主流推荐模型。 To solve the sparsity and cold start problems of traditional collaborative filtering, deep neural net-work (DNN) is usually used to construct the recommendation task of fusion knowledge map and recommendation system. However, the current method does not consider the low-order linear rela-tionship between features. Although factor decomposition machine (FM) can be added, different features have different contributions to the model, and FM may be hindered because all features set the same weight interactively. DNN solves the problem that knowledge map, a data structure with irregular, extensible and multiple structural characteristics, is not universal. To solve the above problems, the MKAFG model is proposed, and FM with attention mechanism is recommended. The importance of interaction between different features is distinguished by attention network, so that FM can extract first-order and second-order linear interactive features which play an important role in target prediction. In the part of knowledge embedding, graph convolution neural network (GCN) is used to improve the recommendation effect of recommendation system. Experimental results show that MKAFG's recommendation effect on MovieLens-1M dataset is better than that of main-stream recommendation models. 
  
 
</p></abstract><kwd-group><kwd>推荐系统，知识图谱，深度神经网络，注意力因子分解机，图卷积神经网络, Recommendation System</kwd><kwd> Knowledge Graph</kwd><kwd> Deep Neural Network</kwd><kwd> Attentional Factorization Machines</kwd><kwd> Graph Convolutional Networks</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>针对传统协同过滤存在的稀疏性和冷启动问题，通常使用深度神经网络(DNN)构建融合知识图谱和推荐系统的推荐任务。但目前的方法未曾考虑特征间的低阶线性关系，虽可加入因子分解机(FM)，但不同的特征对模型的贡献不同，FM可能会因所有特征交互设置相同的权重而受到阻碍；DNN解决知识图谱这种具有不规则、可扩展、多重结构特性的数据结构不具普适性。针对以上问题，提出MKAFG模型，推荐部分加入具有注意力机制的FM，通过注意力网络区分不同特征交互的重要性，使FM提取到对目标预测起到重要作用的一阶、二阶线性交互特征。知识嵌入部分使用图卷积神经网络(GCN)，提高推荐系统推荐效果。实验结果表明，MKAFG在MovieLens-1M数据集上的推荐效果优于主流推荐模型。</p></sec><sec id="s2"><title>关键词</title><p>推荐系统，知识图谱，深度神经网络，注意力因子分解机，图卷积神经网络</p></sec><sec id="s3"><title>Research on Personalized Recommendation Algorithm Based on Knowledge Graph and User Behavior Information</title><p>Jingwen Cheng, Quanmin Wang</p><p>Information Department, Beijing University of Technology, Beijing</p><p><img src="//html.hanspub.org/file/19-1542084x4_hanspub.png" /></p><p>Received: Mar. 21<sup>st</sup>, 2021; accepted: Apr. 15<sup>th</sup>, 2021; published: Apr. 22<sup>nd</sup>, 2021</p><p><img src="//html.hanspub.org/file/19-1542084x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>To solve the sparsity and cold start problems of traditional collaborative filtering, deep neural network (DNN) is usually used to construct the recommendation task of fusion knowledge map and recommendation system. However, the current method does not consider the low-order linear relationship between features. Although factor decomposition machine (FM) can be added, different features have different contributions to the model, and FM may be hindered because all features set the same weight interactively. DNN solves the problem that knowledge map, a data structure with irregular, extensible and multiple structural characteristics, is not universal. To solve the above problems, the MKAFG model is proposed, and FM with attention mechanism is recommended. The importance of interaction between different features is distinguished by attention network, so that FM can extract first-order and second-order linear interactive features which play an important role in target prediction. In the part of knowledge embedding, graph convolution neural network (GCN) is used to improve the recommendation effect of recommendation system. Experimental results show that MKAFG's recommendation effect on MovieLens-1M dataset is better than that of mainstream recommendation models.</p><p>Keywords:Recommendation System, Knowledge Graph, Deep Neural Network, Attentional Factorization Machines, Graph Convolutional Networks</p><disp-formula id="hanspub.41746-formula15"><graphic xlink:href="//html.hanspub.org/file/19-1542084x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/19-1542084x7_hanspub.png" /> <img src="//html.hanspub.org/file/19-1542084x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>推荐系统旨在解决信息爆炸问题并解决用户的个性化需求。目前使用的协同过滤推荐算法使用用户–物品交互矩阵存在稀疏性和冷启动等问题。因此，通常在推荐系统中加入辅助信息来提高推荐性能。本文将知识图谱作为辅助信息，因其包含丰富的实体和关系。目前经常使用的知识图谱包括：NELL1、DBpedia2、Google Knowledge Graph3和Microsoft Satori4。由于知识图谱高维性和异构性，使用知识图谱嵌入(Knowledge Graph Embedding, KGE)将知识图谱中的实体和关系映射到低维向量中，并保持其原有的语义和结构 [<xref ref-type="bibr" rid="hanspub.41746-ref1">1</xref>]。</p><p>文献 [<xref ref-type="bibr" rid="hanspub.41746-ref2">2</xref>] 提出了CKE模型，将结构化、文本、视觉等知识输入到贝叶斯框架中，但推荐部分和知识嵌入部分在贝叶斯框架下是松散耦合的 [<xref ref-type="bibr" rid="hanspub.41746-ref3">3</xref>]，因此知识图谱对提高推荐系统的推荐效果起到的辅助性不太明显。文献 [<xref ref-type="bibr" rid="hanspub.41746-ref4">4</xref>] 设计了基于内容的深度知识感知网络DKN，其结合了实体嵌入和文字嵌入进行新闻推荐。但其只针对文本数据，使用场景受限，也无法做到端到端的训练。文献 [<xref ref-type="bibr" rid="hanspub.41746-ref5">5</xref>] 设计了RippleNet记忆网络的类似模型，在知识图谱中传播用户的潜在偏好，探索分级兴趣，但关系的嵌入矩阵很难捕捉关系的重要性。文献 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>] 设计了一个通用的端到端的深度推荐框架MKR，通过交叉压缩感知单元让推荐模块和知识嵌入模块进行交替学习，通过知识图谱提高推荐系统的性能。但在推荐过程中只考虑到了特征间的高阶非线性关系；在知识嵌入过程中，深度学习虽能够学习到更高效的特征与模式，但处理图数据结构不具普适性，使用深度神经网络存在一定的限制。</p><p>基于上述背景，本文的主要贡献如下：</p><p>a) 基于知识图谱的思想提出了基于注意力分解机和图卷积神经网络的知识图谱的多任务推荐模型(Multi-Task Recommendation of Knowledge Graph Based on Attention Factorization Machine and Graph Convolution Neural Network, MKAFG)；</p><p>b) 本文将加入Attention [<xref ref-type="bibr" rid="hanspub.41746-ref7">7</xref>] 思想的因子分解机(Factorization Machines, FM)引入到深度推荐模型中，Attention机制对目标有用的特征赋予更高的权重，提取对目标预测起到重要作用的低阶线性特征交互。</p><p>c) 使用图卷积神经网络(Graph Convolutional Networks, GCN) [<xref ref-type="bibr" rid="hanspub.41746-ref8">8</xref>] 对知识图谱进行的知识嵌入，解决深度学习模型处理图数据结构不具有普适性的问题。</p></sec><sec id="s6"><title>2. 相关理论</title><sec id="s6_1"><title>2.1. 基于因子分解机的深度神经网络推荐模型</title><p>在推荐领域中，对辅助信息进行One-hot编码时，会带来数据稀疏问题，而因子分解机(FM) [<xref ref-type="bibr" rid="hanspub.41746-ref9">9</xref>] 是一种较好的特征组合的方法来构建新特征 [<xref ref-type="bibr" rid="hanspub.41746-ref10">10</xref>]。FM只能构建特征之间的低阶线性关系。深度神经网络具有学习复杂特征的交互潜力，使用多层感知机(MLP) [<xref ref-type="bibr" rid="hanspub.41746-ref11">11</xref>] 实现高阶非线性的特征组合，但无法实现低阶线性特征组合。针对以上Cheng等人提出了Wide &amp; Deep [<xref ref-type="bibr" rid="hanspub.41746-ref12">12</xref>]，在FM的基础上引入了深度神经网络(Deep Neural Network, DNN)，加强模型的非线性能力，但Wide &amp; Deep没有将二阶交叉特征的信息完全表征出来，造成DNN部分学习更高阶交叉信息效率低下。因此，Xiang等人提出NFM [<xref ref-type="bibr" rid="hanspub.41746-ref13">13</xref>]，使用双向交互池结构对二阶交叉信息进行处理，使高阶交叉特征的信息能更好的被DNN学习。Guo等人提出了DeepFM [<xref ref-type="bibr" rid="hanspub.41746-ref14">14</xref>]，FM和Deep分别进行特征间低阶线性和高阶非线性组合，提高训练效果。为了提高预测性，阿里提出了融合Attention机制的Det Interest Network，在模型嵌入层和拼接层加入Attention单元，根据特征的贡献程度调整不同的特征权重，提高预测性 [<xref ref-type="bibr" rid="hanspub.41746-ref15">15</xref>]。</p><p>本文提出将Attention机制的FM融入到基于知识图谱的推荐算法中，将具有Attention机制的FM与DNN相结合运用到推荐模块进行推荐。</p></sec><sec id="s6_2"><title>2.2. 基于知识图谱的特征表示学习</title><p>知识图谱表示学习 [<xref ref-type="bibr" rid="hanspub.41746-ref16">16</xref>]，是知识图谱研究的重点。Mikolov等人提出了word2vec词表示学习模型，并发现词向量空间存在平移不变现象 [<xref ref-type="bibr" rid="hanspub.41746-ref17">17</xref>]。针对上述的启发，Bordes等人提出了TransE [<xref ref-type="bibr" rid="hanspub.41746-ref18">18</xref>] 模型，其便于计算、参数少，但TransE无法处理知识图谱复杂的实体和关系的建模。后人依次提出TransH [<xref ref-type="bibr" rid="hanspub.41746-ref19">19</xref>]、TransR [<xref ref-type="bibr" rid="hanspub.41746-ref20">20</xref>]。但这些模型虽然在低维空间中可以重构出原有的网络结构，但学习到的表征无法进一步学习。网络表示学习，代表有深度神经网络、卷积神经网络等。深度学习虽能够学习到更高效的特征与模式，但图数据结构，具有不规则、多重结构、可扩展性等特点，因此知识表示过程中深度神经网络存在一定的限制。</p><p>本文使用图卷积神经网络(GCN)对知识图谱进行嵌入。通过给边和节点赋予特征，不仅可以学习到知识图谱自身的关系和特征，在卷积层传播过程中，节点会接受邻居节点的信息不断更新自身的节点。</p></sec></sec><sec id="s7"><title>3. 方法的提出</title><p>本文提出了基于注意力分解机和图卷积神经网络的知识图谱的多任务推荐模型(Multi-Task Recommendation of Knowledge Graph Based on Attention Factorization Machine and Graph Convolution Neural Network, MKAFG)。由推荐部分、知识嵌入部分、交叉压缩感知部分组成。其中推荐部分由具有注意力机制的因子分解机(AFM)和深度神经网络(DNN)两块组成。</p><sec id="s7_1"><title>3.1. 问题的定义</title><p>在推荐场景中，假设有m位用户 U = { u 1 , u 2 , ⋯ , u m } ，n个物品 V = { v 1 , v 2 , ⋯ , v n } 。组成用户–物品交互矩阵 Y m &#215; n ，并用隐式反馈。 y u v = 1 表示用户u对物品v有过历史行为，否则 y u v = 0 。同时加入知识图谱G来提高推荐效果。G由头实体–关系–尾实体 ( h , r , t ) 三元组组成。在很多推荐场景中， v ∈ V 与知识图谱中一个或者多个实体有关联。</p><p>本文中，给定用户-物品交互矩阵 Y m &#215; n 和知识图谱G，去预测用户u是否会对之前没有过交互的物品v产生兴趣。定义了预测函数 y ^ u v = F ( u , v | Θ , Y , G ) ， y ^ u v 表示用户u对物品v点击的概率， Θ 为函数 F 的参数。</p></sec><sec id="s7_2"><title>3.2. MKAFG算法框架</title><p>MKAFG算法框架如图1所示。由推荐部分、知识嵌入部分、交叉压缩感知部分三个模块组成。左边的推荐模块以用户和物品作为输入，右边的知识嵌入模块以实体和关系作为输入。左右两部分使用One-hot编码，对输入的数据采用稀疏表示，通过嵌入层将每个非零特征嵌入到一个稠密向量中。</p><p>图1. MKAFG算法推荐框架</p><p>左边的推荐部分由具有注意力机制的因子分解机(Attentional Factorization Machines, AFM)与深度神经网络(DNN)组成。AFM部分如图2，通过Attention机制对目标有用的特征赋予更高的权重，从而使FM提取对目标预测起到重要作用的一阶和二阶线性交互特征；DNN部分，融合用户和物品特征向量之间高阶非线性关系。两部分学习到的特征输入到预测函数中，得到预测概率；右边的知识嵌入模块通过图卷积神经网络(GCN)从三元组的头实体和关系实体中提取特征，并在函数f和真实的尾实体t的监督下输出预测的尾部实体 t ^ ；推荐模块和知识嵌入模块通过交叉压缩单元连接，可以自动学习推荐系统中物品和知识图谱中实体的高阶特征交互。</p><p>图2. 基于注意力机制的FM</p></sec><sec id="s7_3"><title>3.3. 推荐部分</title><p>本文将注意力机制(Attention)与FM加入到文献 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>] 提出的MKR左边的推荐部分中，由具有注意力机制的因子分解机(AFM)与深度神经网络(DNN)两部分共享相同的输入和嵌入层的输出，模型输出见公式(1)，用来预测用户u对物品v的点击概率。其中 y AFM 是AFM部分的输出， y DNN 为DNN部分的输出。</p><p>y ^ u v = s i g m o i d ( y AFM + y DNN ) (1)</p><sec id="s7_3_1"><title>3.3.1. AFM模块</title><p>AFM是用来学习加权组合特征的，是具有注意力机制的FM [<xref ref-type="bibr" rid="hanspub.41746-ref7">7</xref>]。AFM输入层将输入的物品和用户数据进行One-hot编码后，通过嵌入层，将每个向量投入到一个密集的向量中，得到如下公式：</p><p>y FM ( x ) = w 0 + ∑ i = 1 n w i x i + ∑ i = 1 n ∑ j = i + 1 n w i j x i x j (2)</p><p>其中 w 0 代表全局偏差，n是经过One-hot编码后特征空间的维度， x i 代表第i个特征。我们将上述 结果通过文献提出的成对交互层 [<xref ref-type="bibr" rid="hanspub.41746-ref7">7</xref>]，如图2，其是将m个向量扩展为 m ( m − 1 ) / 2 个相互作用的向量，其中每个相互作用的向量是两个不同向量元素的点积，来编码它们的相互作用。模型表达式为：</p><p>y FM ( x ) = w 0 + ∑ i = 1 n w i x i + p T ∑ ( i , j ) ∈ R x ( v i ⊙ v j ) x i x j (3)</p><p>p ∈ R k 表示预测层的偏差， ⊙ 为两个元素的点积， v i 用于表达特征之间的相互作用。经过成对交互层，AFM学到了特征向量之间低阶线性组合特征。</p><p>并非所有的组合特征会对目标预测产生相同的作用，因此本文采用文献 [<xref ref-type="bibr" rid="hanspub.41746-ref7">7</xref>] 提出的Attention机制，对越重要的特征赋予越高的权重。使用多层感知机参数化注意力得分。如图2，其由两个特征的交互向量 ( v i ⊙ v i ) x i x j 作为输入，输入向量的加权算数平均值为输出。得到的权重代表特征各个组合特征的重要程度，权重函数为：</p><p>a ′ i j = W ′ σ ( W ( v i ⊙ v j ) x i x j + b ) (4)</p><p>a i j = exp ( a ′ i j ) ∑ ( i , j ) ∈ R x exp ( a ′ i j ) (5)</p><p>σ 是激活函数， W ′ ，W，b为模型学习的参数。对于输入的组合特征，其返回的是非标准化的权重分值 a ′ i j 。当得到所有组合特征的分值后，带入到公式(5)就可以得到标准化的权重分值 a i j 。将 a i j 带入到公式(3)。AFM提取到对目标预测起到重要作用的一阶和二阶线性交互特征。AFM模型公式如下：</p><p>y AFM ( x ) = w 0 + ∑ i = 1 n w i x i + p T ∑ i = 1 n ∑ j = i + 1 n a i j ( v i ⊙ v j ) x i x j (6)</p></sec><sec id="s7_3_2"><title>3.3.2. 深度神经网络(DNN)模块</title><p>给定用户u的原始特征向量u，经过特征嵌入后，使用一个L层MLP提取其隐性压缩特征，见公式(7)。其中 M ( x ) = σ ( w x + b ) 为全连接的神经网络，w为权重(使用高斯分布初始化权重)，b为偏差， σ 为非线性激活函数。</p><p>u L = M ( M ( ⋯ M ( u e m ) ) ) = M L ( u e m ) (7)</p><p>对于物品v，通过特征嵌入层，使用交叉压缩单元共享知识图谱中对应实体的信息之后，获取深层次的特征 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>]，见公式(8)。其中 S ( v ) 是物品v相关联的实体h集合， [ v ] 是区分物品向量的标志 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>]。</p><p>v L = E h ~ S ( v ) [ A L ( v e m , h ) [ v ] ] (8)</p><p>有了用户和物品的潜在特征 u L 和 v L 后，投入到DNN中，得到用户和物品的高阶组合特征。输出为：</p><p>y DNN = σ ( F ( u L , v L ) ) (9)</p></sec></sec><sec id="s7_4"><title>3.4. 知识嵌入部分</title><p>知识图谱的嵌入，是将实体和关系嵌入到连续的向量空间中，同时保持它原有的信息和结构。本文使用GCN提取知识图谱的空间特征，利用边的信息对节点信息进行聚合从而生成新的节点表示，信息可沿着边直接从一个节点传递到另一个节点 [<xref ref-type="bibr" rid="hanspub.41746-ref8">8</xref>]。</p><p>对于给定的知识三元组 ( h , r , t ) ，首先对One-hot编码的特征进行嵌入操作。让头部实体h经过多个交叉压缩单元来学习深层次的特征，对于关系实体r使用L层的MLP提取隐性压缩特征。然后将 h L 和 r L 一起投入到图卷积神经网络中，来预测尾实体 t ^ 。见如下公式：</p><p>h L = E v ~ S ( h ) [ A L ( v e m , h ) [ h ] ] (10)</p><p>r L = M ( M ( ⋯ M ( r e m ) ) ) = M L ( r e m ) (11)</p><p>t ^ = σ ( F g c n k ( h L , r L ) ) = σ ( C * σ ( C * H ( l ) W ( l ) ) W ( l + 1 ) ) (12)</p><p>S ( h ) 是与头实体h相关联的物品集合， [ h ] 区分头实体向量的标志 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>]。 t ^ 是尾实体的预测向量。 F g c n k 是k层的图卷积神经网络，本文设置k = 2，叠加两层GCN，每个节点可以把2-hops邻居的特征加以聚合，得到自身特征。 C ˜ = C + I N ，表示通过添加自环，把自身特征和邻居特征结合起来，更新自身节点。A为描述节点之间关系的邻接矩阵。 C * = D ˜ − 1 / 2 C ˜ D ˜ − 1 / 2 对自身节点和邻居节点所传播的信息进行归一化。 W ( l ) 为特定层的可训练权重矩阵。 σ 为非线性激活函数。使用分数函数 f K G 来计算三元组分数：</p><p>s c o r e ( h , r , t ) = f KG ( t , t ⌢ ) (13)</p></sec><sec id="s7_5"><title>3.5. 交叉压缩部分</title><p>本文使用交叉压缩单元与MKR [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>] 一致。物品v向量和实体h向量通过交叉特征共享单元进行信息交互，弥补自身信息稀疏性的不足 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>]，如图3。</p><p>对于物品v与其相关联的实体h，其L层的隐特征分别为 v L ∈ R d 和 h L ∈ R d 。通过计算 v L 与 h L 的外积，构建了一个 d &#215; d 维的隐特征成对相互作用的交叉特征矩阵。如下：</p><p>A L = v L h L T = [ v L ( 1 ) h L ( 1 ) ⋯ v L ( 1 ) h L ( d ) ⋮ ⋱ ⋮ v L ( d ) h L ( 1 ) ⋯ v L ( d ) h L ( d ) ] (14)</p><p>其中 C L ∈ R d &#215; d 是L层的交叉特征矩阵，d是嵌入向量的维度。由公式(14)可知，交叉特征矩阵 A L 显示建模了物品v和其相关联的实体h的每个特征间的交互 v L ( i ) h L ( j ) , ∀ ( i , j ) ∈ { 1 , ⋯ , d } 2 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>]。然后，将 A L 投影到隐性表示空间，输出下一层的项目和实体的特征向量，此过程为压缩操作，见如下公式：</p><p>v L + 1 = A L w L v v + A L T w L h v + b L v = v L h L T w L v v + h L v L T w L h v + b (15)</p><p>h L + 1 = A L w L v h + A L T w L h h + b L h = v L h L T w L v h + h L v L T w L h h + b L h (16)</p><p>其 w L .. ∈ R d 和 b L .. ∈ R d 是可训练的权重和偏差向量。权重向量将 A L 从 R d &#215; d 维压缩到 R d 维。 A L 沿着，水平和垂直两个方向进行对称压缩。交叉压缩单元可简单表示为：</p><p>[ v L + 1 , h L + 1 ] = A ( v L , h L ) (17)</p><p>[v]和[h]分别表示物品向量和实体向量的输出，通过交叉压缩感知单元，MKAFG可以自适应地调整知识转移的权重，并学习两个任务之间的相关性 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>]。</p><p>图3. 交叉压缩感知单元</p></sec><sec id="s7_6"><title>3.6. 算法设计</title><p>L = L R S + L K G + L θ = Σ u ∈ U , v ∈ V H ( y ⌢ u v , y u v ) − λ 1 ( ∑ ( h , r , t ) ∈ G s c o r e ( h , r , t ) − ∑ ( h , r , t ) ∉ G s c o r e ( h ′ , r ′ , t ′ ) ) + λ 2 ‖ W ‖ 2 2 (18)</p><p>MKAFG的损失函数如公式(18)。 L R S 为推荐部分的损失函数， H 为交叉熵函数； L K G 为知识嵌入部分的计算损失，旨在提高正确三元组得分，降低错误三元组得分； L θ 为正则化项，用来防止过拟合，其中 λ 1 和 λ 2 表示平衡系数， λ 1 可视为两项任务的两个学习率之比，本文设置 λ 1 = 0.5 ， λ 2 = 10 − 6 。上述公式遍历了所有可能的用户–交互矩阵和知识三元组。本文在训练过程中使用负采样策略 [<xref ref-type="bibr" rid="hanspub.41746-ref21">21</xref>] 来提高计算的有效性。见“算法1”(表1)，由推荐部分与知识嵌入部分联合训练。每次迭代过程中，更加倾向于提高推荐性能，所以RS训练t次(t是超参，t &gt; 1)后，KGE训练1次。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Model Training Structure about MKAF</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法1：MKAFG算法流程</th></tr></thead><tr><td align="center" valign="middle" >输入：用户--物品交互矩阵 Y m &#215; n ，知识图谱G 输出：预测函数： y ^ u v = F ( u , v | Θ , Y , G )</td></tr><tr><td align="center" valign="middle" >1. 初始化参数 2. for k do //推荐部分(AFM模块+DNN模块) 3. fortstepsdo 4. 从Y中采集正负相互作用的小批量样本，得到物品v相关联的实体 h ∼ S ( v ) ； 5. 对输入的物品、用户数据进行One-hot编码； 6. One-hot编码后的用户和物品特征向量进行嵌入操作； //AFM模块 7. 使用神经注意网络，公式(4)-(5)，以不同的权重 a i j 对所有的特征建模后，再通过公式(6) 获取特征间的低阶线性交互关系。 //DNN模块 8. 获取用户和物品高阶组合特征，见公式(7)-(9)； 9. 通过公式(1)预测用户u对物品v的点击概率； 10. 通过公式(18)更新推荐模块的所有参数； 11. endfor //GCN知识嵌入部分 12. 从知识图谱G中采集正负相互作用的小批量样本，得到头实体h相关联的物品 v ∼ S ( h ) ； 13. 对输入的头实体、关系实体数据进行One-hot编码； 14. 头实体、关系实体特征向量进行嵌入操作； 15. 头实体h和关系实体r构建邻接矩阵，预测尾实体 t ^ ； 16. 通过等式上的梯度下降更新 F 参数，公式(13)，(18)； 17. endfor</td></tr></tbody></table></table-wrap><p>表1. MKAFG模型训练结构</p></sec></sec><sec id="s8"><title>4. 实验</title><sec id="s8_1"><title>4.1. 实验环境</title><p>本实验的运行环境为：Python3.6、TensorFlow2.1、32G内存、Intel Core i5CPU、GeForce GTX 1050 2G显卡。</p></sec><sec id="s8_2"><title>4.2. 数据集及预处理</title><sec id="s8_2_1"><title>4.2.1. 数据集</title><p>推荐模块数据集：选用公开的MovieLens-1M数据集。其中评分数据集含有来自6036名用户对3952部电影约100万条显示评分数据(评分值从1~5，值越高表示用户对该电影越喜欢)；用户数据集包含了用户属性信息(年龄、性别等)；电影属性数据集包含了电影属性(类别、年代等)。</p><p>知识图谱数据集：使用Microsoft Satori来构建电影知识图谱。Microsoft Satori存储实体(真实世界中的人物、地点、事件等)的信息和关系，索引量并在不断增长。</p></sec><sec id="s8_2_2"><title>4.2.2. 数据预处理</title><p>推荐模块数据预处理：在MovieLens-1M中需要将显示反馈数据转换为隐式反馈数据。用户对电影的评分为最高为5，最低为1。本文根据电影评分的经验，设阈值为4，当用户对某一部电影的评分&gt;=4时，表示用户对该部电影很喜欢，是正反馈，用1表示；反之是负反馈，用0表示。0不只意味着用户不喜欢这个电影，它是用户不喜欢此电影和暂无交互(用户还未注意到此电影)的混合。</p><p>知识图谱数据预处理：电影知识图谱的构建与MKR [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>] 中保持一致。先从整个Microsoft Satori中选出置信度大于0.9的三元组子集，进一步选择出关系名称为“电影”的三元组集合，来减少知识图谱的大小。在得到的子集中，通过将所有有效电影与三元组的尾部(head, film.film.name, tail)进行匹配来收集所有有效电影的ID标识，将没有匹配或多个匹配的实体ID删除，然后将剩下的ID集合与KG子集中的三元组头尾部进行匹配，从KG中选择能够匹配上的三元组 [<xref ref-type="bibr" rid="hanspub.41746-ref6">6</xref>]。但用户、项目及交互的数量比原始数据集少，因为过滤掉了KG中没有对应到的实体的项目。</p></sec></sec><sec id="s8_3"><title>4.3. 衡量指标及实验结果</title><p>本文将MovieLens-1M数据集，按照6:2:2的比例划分为训练集、测试集、验证集。实验重复3次，计算3次平均的结果。</p><sec id="s8_3_1"><title>4.3.1. 衡量指标</title><p>使用ACC和AUC来评估点击率预估(CTR)预测MKAFG推荐框架的性能。其中ACC可以从混淆矩阵中导出，混淆矩阵见表2。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Confusion Matri</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >推荐框架</th><th align="center" valign="middle" >用户喜欢</th><th align="center" valign="middle" >用户不喜欢</th></tr></thead><tr><td align="center" valign="middle" >推荐</td><td align="center" valign="middle" >TP</td><td align="center" valign="middle" >FP</td></tr><tr><td align="center" valign="middle" >未推荐</td><td align="center" valign="middle" >FN</td><td align="center" valign="middle" >TN</td></tr></tbody></table></table-wrap><p>表2. 混淆矩阵</p><p>Accuracy = TP + TN TP + FP + TN + FN (19)</p><p>AUC被定义为ROC曲线下的面积。ROC曲线的横坐标是伪阳性率(FPR)，纵坐标是真阳性率(TPR)。AUC越大模型的推荐效果更好。</p><p>FPR = FP FP + TN (20)</p><p>TPR = TP TP + TN (21)</p></sec><sec id="s8_3_2"><title>4.3.2. 实验结果</title><p>本实验使用Movielens-1M电影数据集进行了点击率预估任务。第一部分将MKAFG与主流推荐模型进行比较；第二部分是自身实验的比较，见表2。结果最好已经加粗表示。</p><p>PER在电影数据集上表现不佳，因为PER需要手动进行元路径的设计，手动设计很难使结果达到最佳。DKN在所有推荐模型中表现最差，可能是DKN主要应用在新闻领域的推荐，新闻文本的特点是语言高度浓缩，而电影名称较短无法提供有用的信息。RippleNet在所有模型中表现较好，说明通过兴趣点传播可以更好的捕获用户的兴趣偏好。MKR在所有模型中表现较佳，说明知识图谱作为辅助信息可以辅助推荐任务进行学习，其物品、实体通过交叉压缩单元共享信息表现出多任务学习的有效性。</p><p>自身实验的比较。MKFM表示在推荐模块加入FM，学习用户、物品的低阶交互特征。由表3可知，MKFM与基础模型相比要好，说明在推荐模块加入FM的有效性，但推荐效果不如 MKAFM，其是加入了具有Attention的FM，能更好的能获取到，起到重要作用的一阶二阶交互特征。推荐效果最好的是本文提出的MKAFG模型，AUC分别比MKR、MKFM、MKAFM提高了2.8%、1.1%、0.6%。说明加入的图卷积神经网络能更好的学习知识图谱的特征。因此，在电影推荐领域，本文提出的MKAFM效果最佳。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Click rate estimation results of the mode</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >Approachs</th><th align="center" valign="middle"  colspan="2"  >Movielens-1M</th></tr></thead><tr><td align="center" valign="middle" >AUC</td><td align="center" valign="middle" >ACC</td></tr><tr><td align="center" valign="middle" >PER</td><td align="center" valign="middle" >0.710</td><td align="center" valign="middle" >0.664</td></tr><tr><td align="center" valign="middle" >CKE</td><td align="center" valign="middle" >0.801</td><td align="center" valign="middle" >0.742</td></tr><tr><td align="center" valign="middle" >DKN</td><td align="center" valign="middle" >0.655</td><td align="center" valign="middle" >0.589</td></tr><tr><td align="center" valign="middle" >RippleNet</td><td align="center" valign="middle" >0.920</td><td align="center" valign="middle" >0.842</td></tr><tr><td align="center" valign="middle" >MKR</td><td align="center" valign="middle" >0.917</td><td align="center" valign="middle" >0.843</td></tr><tr><td align="center" valign="middle" >MKFM</td><td align="center" valign="middle" >0.934</td><td align="center" valign="middle" >0.858</td></tr><tr><td align="center" valign="middle" >MKAFM</td><td align="center" valign="middle" >0.939</td><td align="center" valign="middle" >0.866</td></tr><tr><td align="center" valign="middle" >MKAFG</td><td align="center" valign="middle" >0.945</td><td align="center" valign="middle" >0.876</td></tr></tbody></table></table-wrap><p>表3. 模型的点击率预估结果</p></sec></sec><sec id="s8_4"><title>4.4. 推荐模块超参数分析</title><p>本部分实验研究在推荐模块引入AFM机制后，推荐模块的DNN部分的激活函数、神经元数量、隐藏层数量使MKAFG达到推荐效果最佳。</p><sec id="s8_4_1"><title>4.4.1. 激活函数</title><p>此部分研究了ReLU和Tanh作为左侧推荐模块的激活函数时，对总体框架结果的影响。由图4可得ReLU比Tanh性能更好。</p><p>图4. 激活函数对结果的影响</p></sec><sec id="s8_4_2"><title>4.4.2. 隐藏层数量</title><p>一般认为，隐藏层的个数越多，模型的学习能力越强。但随着隐藏层数量的不断增加，更容易产生过拟合，同时训练难度也不断加大。如图5，当神经网络隐藏层数为2时，模型的推荐效果最佳。</p><p>图5. 隐藏层的数量对结果的影响</p></sec><sec id="s8_4_3"><title>4.4.3. 隐因子维度分析</title><p>如图6，研究了隐因子的个数对MKAFG模型的影响。此处隐藏层数设置为2。随着隐因子维度的增加，并不能总给推荐效果带来提升。当隐因子维度为128时，推荐效果最佳。</p><p>图6. 隐因子个数对实验结果的影响</p></sec></sec><sec id="s8_5"><title>4.5. MKAFG模型参数分析</title><sec id="s8_5_1"><title>4.5.1. 训练集稀疏程度分析</title><p>实验将MovieLens-1M的训练集大小设置为10%到100%。测试集和验证集不变。所有模型的AUC全部升高；同时r = 100%与r = 10%时，各个模型AUC差分别为：PER (11.2%)、CKE (12.7%)、DKN (7.6%)、RippleNet (7.7%)、MKR (5.5%)、MKAFG (5.1%)。如图7，MKAFG面对数据稀疏性问题时，依然保持良好的性能。</p><p>图7. 训练集稀疏程度分析</p></sec><sec id="s8_5_2"><title>4.5.2. 知识图谱KG大小分析</title><p>本部分研究辅助信息数据集的大小对推荐效果的影响。将知识图谱的大小设置为原来的0.1倍到1倍。由图8，随着知识图谱数据集增大，推荐效果不断上升。且AUC和ACC分别增加了14.1%、12.8%。说明引入辅助信息可以提高推荐效果，同时辅助信息数据集越大，推荐效果会越好。</p><p>图8. 知识图谱大小分析</p></sec><sec id="s8_5_3"><title>4.5.3. 推荐模块训练频次分析</title><p>每个epoch，推荐部分训练t次，知识嵌入部分训练1次。本文设计t (1 - 10)，同时保持其他参数不变。如图9，当t = 4时，MKAFG性能最佳，当t &gt; 4时，随着t的增大时，MKAFG性能降低。因为如果知识图谱嵌入模块训练频次太少，不能充分利用知识图谱的知识；训练频次过高，则会影响MKAFG的目标函数。</p></sec><sec id="s8_5_4"><title>4.5.4. 嵌入维度分析</title><p>在嵌入层，用户、物品、实体、关系的嵌入维度会影响MKAFG的表现。当嵌入维度增加时，会带来更多的编码信息。如图10，当dim = 8时，MKAFG的推荐结果达到最优。原因是，随着嵌入维度的增加，会引入噪声，这会误导预测函数的预测。</p><p>图9. 推荐模块训练频次分析</p><p>图10. 嵌入维度分析</p></sec></sec></sec><sec id="s9"><title>5. 结束语</title><p>本文提出了融合知识图谱和用户行为信息的深度算法推荐模型。在推荐模块加入具有注意力机制的FM，通过注意力网络区分不同特征交互的重要性，使FM提取到对目标预测起到重要作用的一阶、二阶线性交互特征。在知识嵌入模块通过GCN学习知识图谱自身的关系和特征，还可以通过不断学习让每个节点接受邻居节点的信息从而更新自身的节点表示。通过实验表明，该模型的性能相比目前主流推荐模型有所提升。由于本文数据只在Movielens-1M电影数据集上进行研究，未来将应用到图书、新闻、音乐等领域进行推荐研究，以能在所有数据集上获取较好的效果。</p></sec><sec id="s10"><title>文章引用</title><p>程静文,王全民. 融合知识图谱和用户行为信息的个性化推荐算法研究Research on Personalized Recommendation Algorithm Based on Knowledge Graph and User Behavior Information[J]. 计算机科学与应用, 2021, 11(04): 948-961. https://doi.org/10.12677/CSA.2021.114098</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41746-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Wang, H., Wang, J., Wang, J., et al. (2017) GraphGAN: Graph Representation Learning with Generative Adversarial Nets. IEEE Transactions on Knowledge and Data Engineering.</mixed-citation></ref><ref id="hanspub.41746-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, F.Z., Yuan, N.J., Lian, D.F., et al. (2016) Collaborative Knowledge Base Embedding for Recommender Systems. In: Proceedings of the 22nd ACM SIGKDD In-ternational Conference on Knowledge Discovery and Data Mining, ACM, New York, 353-362. &lt;br&gt;https://doi.org/10.1145/2939672.2939673</mixed-citation></ref><ref id="hanspub.41746-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Bahdanau, D., Cho, K. and Bengio, Y. (2016) Neural Machine Translation by Jointly Learning to Align and Translate. Proceedings of the 3rd International Conference on Learning Representations, Banff, September 2014, 353-362.</mixed-citation></ref><ref id="hanspub.41746-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Wang, H.W., Zhang, F.Z., Xie, X., et al. (2018) DKN: Deep Knowledge-Aware Network for News Recommendation. Proceedings of the 2018 World Wide Web Conference on World Wide Web, Lyon, 23-27 April 2018, 1835-1844.  
&lt;br&gt;https://doi.org/10.1145/3178876.3186175</mixed-citation></ref><ref id="hanspub.41746-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Wang, H.W., Zhang, F.Z., Wang, J.L., et al. (2018) RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. In: Proceedings of the 27th ACM International Conference on Information and Knowledge Management, ACM Press, Torino, 417-426. &lt;br&gt;https://doi.org/10.1145/3269206.3271739</mixed-citation></ref><ref id="hanspub.41746-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Wang, H.W., Zhang, F.Z., Zhao, M., et al. (2019) Multi-Task Fea-ture Learning for Knowledge Graph Enhanced Recommendation. Proceedings of the 2019 World Wide Web Conference on World Wide Web (WWW 2019), New York, May 2019, 2000-2010. &lt;br&gt;https://doi.org/10.1145/3308558.3313411</mixed-citation></ref><ref id="hanspub.41746-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Xiao, J., Ye, H. and He, X.N. (2017) Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks. &lt;br&gt;https://doi.org/10.24963/ijcai.2017/435</mixed-citation></ref><ref id="hanspub.41746-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Kipf, T.N. and Welling, M. (2017) Semi-Supervised Classification with Graph Convolutional Networks. International Conference on Learning Representations, Toulon, 24-26 April 2017, 14.  
&lt;br&gt;https://arxiv.org/abs/1609.02907</mixed-citation></ref><ref id="hanspub.41746-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Rendle, S. (2010) Factorization Machines. Proceedings of the 10th IEEE In-ternational Conference on Data Mining (ICDM), Sydney, 13-17 December 2010, 995-1000. &lt;br&gt;https://doi.org/10.1109/ICDM.2010.127</mixed-citation></ref><ref id="hanspub.41746-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Rendle, S. (2012) Factorization Machines with LibFM. ACM Trans-actions on Intelligent Systems and Technology, 3, 57:1-57:22. &lt;br&gt;https://doi.org/10.1145/2168752.2168771</mixed-citation></ref><ref id="hanspub.41746-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Pal, S.K. and Mitra, S. (1992) Multilayer Perceptron, Fuzzy Sets, and Classification. IEEE Transactions on Neural Networks, 3, 683-697. &lt;br&gt;https://doi.org/10.1109/72.159058</mixed-citation></ref><ref id="hanspub.41746-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Cheng, H.T., Koc, L., Harmsen, J., et al. (2016) Wide &amp; Deep Learning for Recommender Systems. Workshop on Deep Learning for Recommender Systems, Boston, 15 September 2016, 7-10.  
&lt;br&gt;https://doi.org/10.1145/2988450.2988454</mixed-citation></ref><ref id="hanspub.41746-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">He, X.N. and Chua, T.S. (2017) Neural Factorization Machines for Sparse Predictive Analytics. Proceedings of the 40th International ACM SIGIR Conference on Research and Develop-ment in Information Retrieval (SIGIR), Tokyo, 7-11 August 2017, 355-364.</mixed-citation></ref><ref id="hanspub.41746-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Guo, H.F., Tang, R.M., Ye, Y.M., et al. (2017) DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. Proceedings of the 26th In-ternational Joint Conference on Artificial Intelligence (IJCAI), Melbourne, 19-25 August 2017, 1725-1731. &lt;br&gt;https://doi.org/10.24963/ijcai.2017/239</mixed-citation></ref><ref id="hanspub.41746-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Zhou, G., Song, C., Zhu, X., et al. (2017) Deep Interest Network for Click-Through Rate Prediction. Proceedings of the 40th ACM SIGKDD International Conference on Knowledge Dis-covery &amp; Data Mining, June 2017, 1058-1068.</mixed-citation></ref><ref id="hanspub.41746-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">刘知远, 孙茂松, 林衍凯, 等. 知识表示学习研究进展[J]. 计算机研究与发展, 2016, 53(2): 247-261.</mixed-citation></ref><ref id="hanspub.41746-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Bordes, A., Weston, J., Collobert, R., et al. (2011) Learning Structured Embeddings of Knowledge Bases. National Conference on Artificial Intelligence, Barcelona, 16-22 July 2011, 301-306.</mixed-citation></ref><ref id="hanspub.41746-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Bordes, A., Usunier, N., Garciaduran, A., et al. (2013) Translating Embeddings for Modeling Mul-ti-Relational Data. Proceedings of the 26th International Conference on Neural Information Processing Systems, Volume 2, 2787-2795.</mixed-citation></ref><ref id="hanspub.41746-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Wang, Z., Zhang, J., Feng, J., et al. (2014) Knowledge Graph Embedding by Translating on Hyper-planes. National Conference on Artificial Intelligence, Québec, 27-31 July 2014, 1112-1119.</mixed-citation></ref><ref id="hanspub.41746-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Y., Liu, Z., Sun, M., et al. (2015) Learning Entity and Relation Embedding for Knowledge Graph Completion. In: Proceedings of AAAI, AAAI, Menlo Park, 2181-2187.</mixed-citation></ref><ref id="hanspub.41746-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Yang, S., Korayem, M., Aljadda, K., et al. (2017) Combining Content-Based and Collaborative Filtering for Job Recommendation System: A Cost-Sensitive Statistical Relational Learning Approach. Knowledge-Based Systems, 136, 37-45.  
&lt;br&gt;https://doi.org/10.1016/j.knosys.2017.08.017</mixed-citation></ref></ref-list></back></article>