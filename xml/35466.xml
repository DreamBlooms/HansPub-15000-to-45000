<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.105087</article-id><article-id pub-id-type="publisher-id">CSA-35466</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200500000_60321628.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于Grad-CAM与B-CNN的细粒度图像分类方法研究
  Fine-Grained Image Classification Algorithm Based on Grad-CAM and B-CNN
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>邓</surname><given-names>绍伟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>伯泉</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学计算机学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>29</day><month>04</month><year>2020</year></pub-date><volume>10</volume><issue>05</issue><fpage>841</fpage><lpage>850</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   细粒度图像具有类间差异小，类内差异大的特点。图像之间的差异主要存在于细微的局部区域，局部区域定位及其代表性特征提取成为细粒度图像分类的主要研究问题之一。本文基于Grad-CAM和双线性卷积神经网络B-CNN模型对细粒度图像分类方法进行研究，它利用Grad-CAM模型定位原图像中的显著区域，并裁剪出显著性区域图像作为双线性CNN的输入，融合全局和局部的特征，从而完成分类。在CUB-200-2011、Stanford Dogs和Stanford Cars三个数据集上的实验表明，相较于传统模型，该方法能够更加准确定位图像特征显著区域，具有更好的分类效果。 Fine-grained images are characterized by small differences between classes and large differences within classes. The differences between images mainly exist in subtle local areas, and local area localization and its representative feature extraction have become one of the main research issues in fine-grained image classification. In this paper, the fine-grained categorization method is studied based on the Grad-CAM and the Bilinear Convolution Neural Networks B-CNN. It uses the Grad-CAM model to locate the salient region in the original image, and crops the salient region image as the input of the bilinear CNN, fusing the global and local features to complete the classification. Experiments on the three datasets of CUB-200-2011, Stanford Dogs and Stanford Cars show that compared with the traditional model, this method can more accurately locate areas with significant image features and have better classification effects. 
  
 
</p></abstract><kwd-group><kwd>细粒度图像分类，双线性卷积神经网络，Grad-CAM，显著性区域, Fine-Grained Categorization</kwd><kwd> Bilinear Convolution Neural Networks</kwd><kwd> Grad-CAM</kwd><kwd> Salient Regions</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于Grad-CAM与B-CNN的细粒度图像分类方法研究<sup> </sup></title><p>邓绍伟，张伯泉</p><p>广东工业大学计算机学院，广东 广州</p><p>收稿日期：2020年4月16日；录用日期：2020年5月1日；发布日期：2020年5月8日</p><disp-formula id="hanspub.35466-formula27"><graphic xlink:href="//html.hanspub.org/file/4-1541759x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>细粒度图像具有类间差异小，类内差异大的特点。图像之间的差异主要存在于细微的局部区域，局部区域定位及其代表性特征提取成为细粒度图像分类的主要研究问题之一。本文基于Grad-CAM和双线性卷积神经网络B-CNN模型对细粒度图像分类方法进行研究，它利用Grad-CAM模型定位原图像中的显著区域，并裁剪出显著性区域图像作为双线性CNN的输入，融合全局和局部的特征，从而完成分类。在CUB-200-2011、Stanford Dogs和Stanford Cars三个数据集上的实验表明，相较于传统模型，该方法能够更加准确定位图像特征显著区域，具有更好的分类效果。</p><p>关键词 :细粒度图像分类，双线性卷积神经网络，Grad-CAM，显著性区域</p><disp-formula id="hanspub.35466-formula28"><graphic xlink:href="//html.hanspub.org/file/4-1541759x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/4-1541759x7_hanspub.png" /> <img src="//html.hanspub.org/file/4-1541759x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>近年来，随着图像数据的大规模增长，人们对图像分类提出了迫切需求，图像分类成为了热门研究领域 [<xref ref-type="bibr" rid="hanspub.35466-ref1">1</xref>]。图像分类一般分为对象级分类和细粒度图像分类(Fine-grained image categorization) [<xref ref-type="bibr" rid="hanspub.35466-ref2">2</xref>]，细粒度图像分类又被称为子类别图像分类(Sub-category recognition) [<xref ref-type="bibr" rid="hanspub.35466-ref3">3</xref>]，细粒度图像分类是对粗粒度图像中的更小的子类别进行分类，如飞机型号、鸟类品种、服装款式与菜肴样式等。由于同一个粗粒度图像下的各子类图像在几何结构上十分相似，造成细粒度图像类间差异小；而同一个细粒度类别下的图像，从物体的形状、姿态、背景等角度来看都有可能产生极大的差异，致使子类内图像区分难度大。细粒度图像在工业界与学术界都应用广泛，比如在道路交通管理上，可以识别不同车型的数量，计算实时的交通状况；在生物学领域，可以帮助研究人员快速识别不同种类的物种，而不用受太大专业知识的限制。因此细粒度图像分类成为研究热点和难点 [<xref ref-type="bibr" rid="hanspub.35466-ref4">4</xref>]。</p><p>细粒度图像分类主要分为强监督的分类方法和弱监督的分类方法。强监督细粒度分类方法除了需要图像的类别标签之外，还需要标注框、局部位置等额外的人工标注信息，而这些人工的标注信息往往是需要丰富的专业知识才能够获得，所以这一类方法的代价较高。文献 [<xref ref-type="bibr" rid="hanspub.35466-ref5">5</xref>] 提出的Part-based R-CNN，该算法采用R-CNN对图像生成大量的候选区域，然后再对这些候选区域检测，给出每一个局部区域的评分值，根据评分值确定最后的定位检测结果。结合全局特征(物体级别特征)和判别行更强的局部特征进行分类取得了不错的效果，但是需要额外的人工标注开销，并且由于R-CNN算法产生大量的候选区域会大大增加计算复杂度，造成检测速度较慢。文献 [<xref ref-type="bibr" rid="hanspub.35466-ref6">6</xref>] 提出的Part-Stacked CNN，它与文献 [<xref ref-type="bibr" rid="hanspub.35466-ref5">5</xref>] 类似，也是分为两个步骤，由定位网络与分类网络两部分组成，在定位网络中用到了FCN (Fully Convolutional Network)提高了分类准确率，并且加快了算法的效率，但同样是需要对象与部位级的标签。文献 [<xref ref-type="bibr" rid="hanspub.35466-ref7">7</xref>] 提出了一种新颖的局部区域检测模型，在细粒度图像分类中引入了协同分割，提出了一种无需借助局部区域标注信息，只需要标注框就可以完成分割与对齐操作，分类准确度能够达到82%。</p><p>由于获取人工标注信息代价大，弱监督图像分类方法越来越受到重视。文献 [<xref ref-type="bibr" rid="hanspub.35466-ref8">8</xref>] 提出两级注意力(Two level attention)算法，它关注对象级和局部级两个不同层次的特征。但是利用聚类算法得到的局部区域并不十分准确，所以分类准确率有限。文献 [<xref ref-type="bibr" rid="hanspub.35466-ref9">9</xref>] 提出基于双线性卷积神经网络(Bilinear Convolution Neural Networks, B-CNN)的弱监督分类模型，它由两路VGGNet构成。该模型将两个网络提取的卷积特征进行双线性操作，以提高图像特征表达能力，实现了一个端到端训练的弱监督分类网络。但是该模型利用VGGNet作为特征提取网络，没有能充分关注物体判别性区域对分类准确率的影响。</p><p>综合以上，影响细粒度图像分类准确率的两个主要因素，一是对图像局部显著性区域的关注，二是对局部区域特征的提取和表达。本文在B-CNN的基础上，提出一种基于Grad-CAM [<xref ref-type="bibr" rid="hanspub.35466-ref10">10</xref>] 与B-CNN的细粒度图像分类方法，该方法首先利用Grad-CAM模型提高对象级显著性区域检测结果，聚焦判别性区域，去除无关区域对分类结果的影响，并且改用更加有效的特征提取函数，采用B-CNN模型对判别性区域进行特征提取与分类，从而提高细粒度图像的分类准确率。这种方法不需要标注框和局部位置等人工标记信息，也能够减少背景区域的干扰，在理论上将讲这种方法是一种有效的细粒度图像分类方法。</p></sec><sec id="s4"><title>2. 相关理论</title><sec id="s4_1"><title>2.1. 类别激活映射</title><sec id="s4_1_1"><title>2.1.1. CAM</title><p>具有区别性的显著性区域特征是分类的关键。类别激活映射(Class Activation Mapping, CAM) [<xref ref-type="bibr" rid="hanspub.35466-ref11">11</xref>] 提供了一种解释图像分类结果的方法，它采用全局池化层(global average pooling, GAP)，解决全连接层参数过多、网络不易训练和容易过拟合等问题。该方法以HeatMap来映射图像中与该类别的最相近的即显著性区域，使得模型有更强的解释性。是一种寻找图像中显著性区域的更好的方法。CAM模型如图1所示。</p><p>图1. CAM模型</p><p>CAM方法用GAP层替换全连接层来接收最后一层卷积的结果，输出每一个特征图在每一个通道的平均值，接着是全连接层接收这些平均值生成最后的预测值。选择最终全连接层节点的权重作为最后一层卷积层的特征图的权值，并对特征图像按通道加权形成最后的类别激活映射图。</p></sec><sec id="s4_1_2"><title>2.1.2. Grad-CAM</title><p>CAM能够反映特定类别的显著性区域，实现分类解释。但是CAM需要改变原模型结构，并且重新训练，这大大限制了CAM的应用场景。Grad-CAM与CAM的基本思路是一样，都是通过得到每一个通道特征图的权重，最后加权求和。但是，与CAM不同的是Grad-CAM不需要改变原模型结构，只需要通过梯度的全局平均求取通道映射为类别的权重,这样可以保留卷积之后的全连接层，并且经过数学推导证明Grad-CAM与CAM得到的通道特征图的权重是一致的。Grad-CAM模型如图2所示。</p><p>图2. Grad-CAM模型</p><p>参考图1、图2模型，设 f k ( x , y ) 为最后一个卷积层输出的特征图中的第k个通道的 ( x , y ) 位置上的激活值， F k 为通道k的特征图，则：</p><p>F k = ∑ x , y f k ( x , y ) (1)</p><p>对于某一个特定类别标签c， S c 表示的是softmax层的输入，则：</p><p>S c = ∑ k w k c F k (2)</p><p>其中 w k c 是通道k映射为类别c的权重。所以根据式(1)、式(2)：则：</p><p>S c = ∑ k w k c ∑ x , y f k ( x , y ) = ∑ x , y ∑ k w k c f k ( x , y ) (3)</p><p>用 M c 表示类别c对应的激活映射， M c ( x , y ) 表示激活映射图 ( x , y ) 位置上的激活值，则：</p><p>M c ( x , y ) = ∑ k w k c f k ( x , y ) (4)</p><p>第k个通道对应的类别c的权重值 w k c 为：</p><p>w k c = 1 Z ∑ i ∑ j ∂ y c ∂ A i , j k (5)</p><p>其中Z为特征图中像素的个数， y c 是对应类别c的分数(在代码中一般用logits表示，是输入softmax层之前的值)， A i , j k 表示第k个特征图中， ( x , y ) 位置处的像素值。</p><p>类别映射图反映了原图中各个显著性区域与特定分类分别之间的相关性。因此可以利用Grad-CAM来进行位置定位，用于检测相应物体在原图中的区域，并获取HeatMap中的最大联通区域的边界框，将边界框作为定位框。</p></sec></sec><sec id="s4_2"><title>2.2. B-CNN</title><p>双线性卷积神经网络(B-CNN)是一个典型的弱监督的细粒度图像分类算法,不需要任何的人工标记就已经在鸟、飞机、狗等数据集上达到了较好的准确率，其模型如图3所示：</p><p>图3. B-CNN模型结构</p><p>一个双线性卷积神经网络模型可由一个四元组 β 表示， β = ( f A , f B , P , C ) 。其中 f A 、 f B 分别表示两个特征提取函数，P是池化函数，C是分类函数。在每一个位置对两个网络提取到的特征做外积，组合成双线性特征(bilinear feature)，如式(6)所示：</p><p>b i l i n e a r ( l , I , f A , f B ) = f A ( l , I ) T f B ( l , I ) (6)</p><p>其中，l表示每一个局部位置，I表示原图。</p><p>对所有位置得到的双线性特征进行求和池化，作为原图像的特征：</p><p>ϕ ( I ) = ∑ l ∈ L b i l i n e a r ( l , I , f A , f B ) (7)</p></sec><sec id="s4_3"><title>2.3. 残差模型</title><p>经典的双线性卷积神经网络是由两个VGG分支网络组成的，Stream A进行图像中的目标定位，检测局部区域；Stream B进行定位后区域的特征提取。两个网络相互协调工作，最终完成对图像的分类。通常情况下，深度卷积神经网络的层数较少时，可以增加深度来获得更好的特征提取效果；一旦网络层数过高，会使得网络产生大量的参数，也难以使得网络收敛。VGG对于细粒度图像分类的有一定的局限性，对特征的提取和表达不能更加精确。文献 [<xref ref-type="bibr" rid="hanspub.35466-ref12">12</xref>] 表明，随着网络层数的增加，网络发生了退化(degradation)的现象。因此，在文献 [<xref ref-type="bibr" rid="hanspub.35466-ref12">12</xref>] 中何凯明等人提出了深度残差网络(ResNet)。其残差模块结构如图4所示。</p><p>其中，X是第一层残差模块的输入， F ( X ) 是经过第一层线性变化并激活后的输出。在第二层线性变化之后激活之前， F ( X ) 加入了第一层输入值X，然后激活输出。残差网络的提出解决了深层网络梯度消失的问题，提升了网络的分类准确度，相比于双线性卷积网络所使用的VGG网络，ResNet有更深的网络结构，也能更加准确识别图片中的细节特征，从而实现精细化识别。</p></sec></sec><sec id="s5"><title>3. 本文模型结构及算法流程</title><p>结合Grad-CAM与B-CNN模型，本文采用的细粒度图像分类模型如图5所示。</p><p>本模型分为两个模块，原图经过Grad-CAM检测之后，原图中更精细区域被定位，在Grad-CAM中依旧采用VGG网络来实现检测；显著性区域检测出来为提取特征做准备，B-CNN双线性模型中的VGG网络利用ResNet50代替，更加高层的双线性特征有利于最后的分类任务。</p><p>图4. 残差模块结构</p><p>图5. 基于Grad-CAM和B-CNN的细粒度图像分类模型</p><p>算法描述如下：</p><p>1) 输入原图，对原图进行统一尺度缩放为 H &#215; W 的图像I；</p><p>2) 将图像I进行卷积，并计算最后一层特征图 A ∈ R w &#215; h &#215; d ， w &#215; h 表示A的空间维度，d表示通道数量。A的第k个通道对应的c类别的权重值为 α k c ，分别表示每一个通道的重要程度，则：</p><p>α k c = 1 Z ∑ i ∑ j   ︷ globalaveragepooling ∂ y c ∂ A i , j k ︸ gradientsviabackprop (8)</p><p>A i , j k 表示通道k中位置 ( i , j ) 的像素值，用梯度 ∂ y c ∂ A k 的全局平均来计算每一个通道的权重值 α k c 。</p><p>3) 求得所有特征图的权重之后对其加权求和并ReLU激活；</p><p>L G r a d − C A M c = ReLU ( ∑ k α k c A k ) ︸ linearcombination (9)</p><p>A k 表示通道k的卷积特征图， L G r a d − C A M c 表示类别c判别定位图，用ReLU线性组合所有的加权特征图，输出突出目标类别的热力图；</p><p>4) 得到图像I的热力图 L c G r a d − C A M ，设定一个像素阈值 β ，如果热力图中 L i , j c ≥ β ，则令 L i , j c = 255 ；如果 L i , j c ≤ β ，则令 L i , j c = 0 ，通过热力图得到掩码图M；</p><p>5) 显著图 = L G r a d − C A M c ⋅ M ，通过点乘去掉不显著区域，得到显著图；</p><p>6)根据显著图中像素值为255(即白色部分)区域，用矩形框框出显著图区域并从原图中裁剪出显著图部分的图像 I 2 ，并对 I 2 的尺寸归一化；</p><p>7) 双路残差网络把 I 2 映射成同一维度的特征，两个特征通过一个双线性池化操作P汇聚，得到一维的双线性特征B；</p><p>8) 对所有位置的双线性组合特征B求和；</p><p>9) 训练网络，完成分类。</p><p>在算法开始前，将原图缩放到统一的尺寸。然后利用Grad-CAM对图像的显著性区域进行定位，生成显著性区域的热图。根据热图得到掩码图，并计算掩码图中的最大联通区域，该连通区域就是目标物体所在的位置。从原图中相应位置裁剪出显著性区域，并缩放图像尺寸，并输入双线性残差卷积神经网络以完成分类</p></sec><sec id="s6"><title>4. 实验与分析</title><p>在算法开始前，将原图缩放到统一的尺寸。然后利用Grad-CAM对图像的显著性区域进行定位，生成显著性区域的热图。根据热图得到掩码图，并计算掩码图中的最大联通区域，该连通区域就是目标物体所在的位置。从原图中相应位置裁剪出显著性区域，并缩放图像尺寸，并输入双线性残差卷积神经网络以完成分类。</p><sec id="s6_1"><title>4.1. 实验设计</title><p>论文使用开源深度学习框架Keras作为实验平台，基于一台英伟达GTX1070显卡和16G DDR4内存的ubuntu16.04计算机系统上采用Python编程实现。</p><p>实验采用加州理工大学鸟类数据集CUB-200-2011 [<xref ref-type="bibr" rid="hanspub.35466-ref13">13</xref>]、斯坦福大学狗类数据集Stanford Dogs [<xref ref-type="bibr" rid="hanspub.35466-ref14">14</xref>] 和斯坦福大学汽车数据集Stanford Cars [<xref ref-type="bibr" rid="hanspub.35466-ref15">15</xref>] 等三个经典的细粒度图像分类数据集。</p><p>CUB-200-2011鸟类数据集是在细粒度图像分类领域使用最为广泛的一个数据集。该数据集包括11,788张鸟类图片，一共分为200个类别。其中5994张图片用于训练模型，5794张图片用于测试模型。每一张图片都有详细的人工标注标签，物体标注框和局部位置标注点。Stanford Dogs狗类数据集总共包括20,580张图片，其中12,000张图片用于训练模型，8058张图片用于测试模型，一共分为120个类别。Stanford Cars汽车数据集总共包括16185张图片，其中8144张图片用于训练模型，8041张图片用于测试模型，一共分为196个类别。</p><p>因为细粒度图像分类的3个数据集都比较小，用于训练和测试的样本数较少，如果直接在这3个数据集上训练可能会导致网络无法收敛，因此，利用在ImageNet数据集上预训练好的参数对网络进行初始化，然后在三个细粒度图像数据集上对模型进行微调(fine-tuning)，这样会有更好的训练效果。</p><p>输入图像统一缩放成448 * 448的三通道的彩色图像，裁剪出来的显著性区域统一缩放为224 * 224 * 3，学习率设置为0.005，训练批次为32，迭代次数为100,000次，使用随机梯度下降优化器来训练和优化模型。</p></sec><sec id="s6_2"><title>4.2. 性能指标</title><p>对于以上所述的三个经典数据集，为了直观体现算法的性能与效果，使用分类准确度Accuracy作为评价指标。</p><p>Accuracy = N t e s t N (10)</p><p>其中，N表示总共用于测试样本的数量 N t e s t ，表示测试样本中正确预测的数量。用准确度Accuracy可以直观的反应算法的分类性能。</p></sec><sec id="s6_3"><title>4.3. 实验结果</title><p>通过实验，计算出原图的掩码图与显著性图如图6所示。</p><p>图6. 细粒度图像显著性区域检测。(a) 原图；(b) 热图；(c) 显著性区域</p><p>将本文提出的基于Grad-CAM与B-CNN的细粒度分类方法与PDFR [<xref ref-type="bibr" rid="hanspub.35466-ref16">16</xref>]、Two-level [<xref ref-type="bibr" rid="hanspub.35466-ref17">17</xref>]、Low-rank [<xref ref-type="bibr" rid="hanspub.35466-ref18">18</xref>]、Constellations [<xref ref-type="bibr" rid="hanspub.35466-ref19">19</xref>]、DVAN [<xref ref-type="bibr" rid="hanspub.35466-ref20">20</xref>]、B-CNN等主流细粒度分类算法对比，在三个数据集上的实验结果如表1所示。</p><p>实验结果表明本文方法在CUB-200-2011数据集上的分类效果比其他的方法略有提高，比B-CNN模型提高了0.6%；在Stanford Dogs数据集上，本文算法优于B-CNN、Low-Rank、Two-Level等弱监督分类模型87.1%；与原模型B-CNN相比较，在3个数据集上的分类效果都有所提高。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison of experimental resul</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle"  colspan="3"  >分类准确度</th></tr></thead><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >CUB-200-2011</td><td align="center" valign="middle" >Stanford Dogs</td><td align="center" valign="middle" >Stanford Cars</td></tr><tr><td align="center" valign="middle" >PDFR [<xref ref-type="bibr" rid="hanspub.35466-ref16">16</xref>]</td><td align="center" valign="middle" >80.3</td><td align="center" valign="middle" >79.3</td><td align="center" valign="middle" >82.3</td></tr><tr><td align="center" valign="middle" >Two-level [<xref ref-type="bibr" rid="hanspub.35466-ref17">17</xref>]</td><td align="center" valign="middle" >82.8</td><td align="center" valign="middle" >83.4</td><td align="center" valign="middle" >85.1</td></tr><tr><td align="center" valign="middle" >Low-rank [<xref ref-type="bibr" rid="hanspub.35466-ref18">18</xref>]</td><td align="center" valign="middle" >81.7</td><td align="center" valign="middle" >82.8</td><td align="center" valign="middle" >86.3</td></tr><tr><td align="center" valign="middle" >Constellations [<xref ref-type="bibr" rid="hanspub.35466-ref19">19</xref>]</td><td align="center" valign="middle" >81.0</td><td align="center" valign="middle" >68.61</td><td align="center" valign="middle" >\</td></tr><tr><td align="center" valign="middle" >DVAN [<xref ref-type="bibr" rid="hanspub.35466-ref20">20</xref>]</td><td align="center" valign="middle" >79.0</td><td align="center" valign="middle" >81.5</td><td align="center" valign="middle" >87.1</td></tr><tr><td align="center" valign="middle" >B-CNN [<xref ref-type="bibr" rid="hanspub.35466-ref7">7</xref>]</td><td align="center" valign="middle" >84.1</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >91.3</td></tr><tr><td align="center" valign="middle" >Grad-CAM + B-CNN</td><td align="center" valign="middle" >84.7</td><td align="center" valign="middle" >87.1</td><td align="center" valign="middle" >91.8</td></tr></tbody></table></table-wrap><p>表1. 实验结果对比</p></sec></sec><sec id="s7"><title>5. 结束语</title><p>本文在双线性卷积神经网络的基础上提出改进的基于梯度类别激活映射与双线性残差网络的细粒度分类方法。首先基于Grad-CAM提取图像中的显著性区域，将显著性区域从原图中裁剪出来并预处理，用两个ResNet50网络作为特征提取函数，提取显著性区域更加细致的特征，然后通过结合全局与局部的特征信息进行分类。在3个经典数据集上实验结果表明，在不使用物体包围框以及局部位置标注点的情况下，本文方法可以提升双线性卷积神经网络的分类性能，能够优于其它分类方法对细粒度图像进行有效分类。</p></sec><sec id="s8"><title>基金项目</title><p>本文得到广东省自然科学基金项目(No.2019A1515011056，2018A030313868)的资助。</p></sec><sec id="s9"><title>文章引用</title><p>邓绍伟,张伯泉. 基于Grad-CAM与B-CNN的细粒度图像分类方法研究Fine-Grained Image Classification Algorithm Based on Grad-CAM and B-CNN[J]. 计算机科学与应用, 2020, 10(05): 841-850. https://doi.org/10.12677/CSA.2020.105087</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.35466-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">杨兴. 基于B-CNN模型的细粒度分类算法研究[D]: [硕士学位论文]. 北京: 中国地质大学, 2017.</mixed-citation></ref><ref id="hanspub.35466-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Fu, J., Zheng, H. and Mei, T. (2017) Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fi-ne-Grained Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Hon-olulu, 21-26 July 2017, 4438-4446. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.476</mixed-citation></ref><ref id="hanspub.35466-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">罗建豪, 吴建鑫. 基于深度卷积特征的细粒度图像分类研究综述[J]. 自动化学报, 2017, 43(8): 1306-1318.</mixed-citation></ref><ref id="hanspub.35466-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">盛纾纬. 基于弱监督学习的细粒度图像识别技术研究[D]: [硕士学位论文]. 成都: 电子科技大学, 2019.</mixed-citation></ref><ref id="hanspub.35466-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, N., Donahue, J., Girshick, R., et al. (2014) Part-Based RCNNs for Fine-Grained Category Detection. In: Proceedings of the 13th European Conference on Computer Vision, Springer, Zurich, 834-849.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-10590-1_54</mixed-citation></ref><ref id="hanspub.35466-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Huang, S., Xu, Z., Tao, D., et al. (2016) Part-Stacked CNN for Fine-Grained Visual Categorization. Computer Vision and Pattern Recognition IEEE, Las Vegas, 27-30 June 2016, 1173-1182. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.132</mixed-citation></ref><ref id="hanspub.35466-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Krause, J., Jin, H.L., Yang, J.C., et al. (2015) Fi-ne-Grained Recognition without Part Annotations. Proceedings of the 15th IEEE International Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 5546-5555.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2015.7299194</mixed-citation></ref><ref id="hanspub.35466-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Xiao, T.J., Xu, Y.C., Yang, K.Y., et al. (2015) The Application of Two-Level Attention Models in Deep Convolutional Neural Network for Fine-Grained Image Classification. Pro-ceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 842-850. &lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298685</mixed-citation></ref><ref id="hanspub.35466-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Lin, T.Y., Aruni, R., Maji, S., et al. (2015) Bilinear CNN Mod-els for Fine-Grained Visual Recognition. Proceedings of the 15th IEEE International Conference on Computer Vision, Santiago, 7-13 December 2015, 1449-1457.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2015.170</mixed-citation></ref><ref id="hanspub.35466-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Selvaraju, R., Cogswell, M., Das, A., et al. (2017) Grad-Cam: Visual Explanations from Deep Networks via Gradient-Based Localization. Proceedings of the IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 618-626. &lt;br&gt;https://doi.org/10.1109/ICCV.2017.74</mixed-citation></ref><ref id="hanspub.35466-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Zhou, B., Khosla, A., Lapedriza, A., et al. (2016) Learning Deep Features for Discriminative Localization. Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 2921-2929.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.319</mixed-citation></ref><ref id="hanspub.35466-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">He, K.M., Zhang, X.Y., Ren, S.Q., et al. (2016) Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.35466-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Wah, C., Branson, S., Welinder, P., et al. (2011) The Caltech-UCSD Birds-200-2011 Dataset. Computation &amp; Neural Systems Technical Report, CNS-TR, California Institute of Technology, Pasadena.</mixed-citation></ref><ref id="hanspub.35466-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Khosla, A., Jayadevaprakkash, N., Yao, B.P., et al. (2011) Novel Dataset for Fine-Grained Image Cate-gorization: Stanford Dogs. Proceedings of the 1st Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition, 1-2.</mixed-citation></ref><ref id="hanspub.35466-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Krause, J., Stark, M., Deng, J., et al. (2013) 3d Object Repre-sentations for Fine-Grained Categorization. Proceedings of the 4th IEEE Workshop on 3D Representation, IEEE Interna-tional Conference on Computer Vision, Sydney, 2-8 December 2013, 554-561. &lt;br&gt;https://doi.org/10.1109/ICCVW.2013.77</mixed-citation></ref><ref id="hanspub.35466-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, X.P., Xiong, H.K., Zhou, W.G., et al. (2016) Picking Deep Filter Responses for Fine-Grained Image Recognition. IEEE Conference on Computer Visio and Pattern Recognition, Las Vegas, 27-30 June 2016, 1134-1142.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.128</mixed-citation></ref><ref id="hanspub.35466-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Xiao, T., Xu, Y., Yang, K., et al. (2015) The Application of Two-Level Attention Models in Deep Convolutional Neural Network for Fine-Grained Image Classification. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 842-850.</mixed-citation></ref><ref id="hanspub.35466-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Kong, S. and Fowlkes, C. (2017) Low-Rank Bilinear Pooling for Fine-Grained Classification. 2017 IEEE Conference on Com-puter Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 7025-7034.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.743</mixed-citation></ref><ref id="hanspub.35466-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Simon, M. and Rodner, E. (2015) Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks. Proceedings of the 15th IEEE International Confer-ence on Computer Vision, Santiago, 7-13 December 2015, 1143-1151. &lt;br&gt;https://doi.org/10.1109/ICCV.2015.136</mixed-citation></ref><ref id="hanspub.35466-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Zhao, B., Wu, X., Feng, J., et al. (2017) Diversified Visual Attention Networks for Fine-Grainde Object Classification. IEEE Transactions on Multimedia, 19, 1245-1256. &lt;br&gt;https://doi.org/10.1109/TMM.2017.2648498</mixed-citation></ref></ref-list></back></article>