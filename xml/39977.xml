<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.111010</article-id><article-id pub-id-type="publisher-id">CSA-39977</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210100000_21664619.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  OpenMPI环境下的VASP软件的并行与进程
  The Parallel and Progress of VASP in OpenMPI Environment
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>潘</surname><given-names>勇</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>史</surname><given-names>海洪</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>赵</surname><given-names>琉涛</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>彤</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>北京北科融智云计算科技有限公司，北京</addr-line></aff><aff id="aff2"><addr-line>北京市计算中心，北京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>08</day><month>01</month><year>2021</year></pub-date><volume>11</volume><issue>01</issue><fpage>84</fpage><lpage>94</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   VASP是材料微观物性模拟的重要软件，并行性能优良，其中矩阵对角化在其运算过程中占据大量时间。在HPC作业调度下，VASP并行的进程分配表现单节点和跨节点的差异。通过对MPI并行的节点内和节点间进程、EDDIAG和RMM-DIIS运行时间分析，明确了VASP程序并行中，矩阵对角化的重要性和OpenMPI对矩阵对角化并行的作用。并编写了一个通讯优化的矩阵对角化程序，通过测试并行程序托比串行的马斯算法表现出强大的可扩展性，基于OpenMPI的并行程序能够极大提高对角化的并行效率，但其并行度、通讯代价与计算规模之间的关系需要进一步研究。 VASP is an important software for material microscopic physical properties simulation, with excellent parallel performance, among which matrix diagonalization takes up a lot of time in its calculation process. Under HPC job scheduling, the parallel process allocation of VASP shows the difference between the single node and across multiple nodes. Through the analysis of the intra-node and internode processes of MPI parallel, the running time of EDDIAG and RMM-DIIS, we clarified the im-portance of matrix diagonalization in VASP program parallelism and the role of OpenMPI on matrix diagonalization parallelism. Furthermore, we write a communication-optimized matrix diagonalization program. By test, the parallel program using the Thomas algorithm shows strong scalability than the serial algorithm. The OpenMPI-based parallel program can significantly improve diago-nalization’s parallel efficiency, but the relationship between its parallelism, communication cost, and computing scale needs further study. 
  
 
</p></abstract><kwd-group><kwd>VASP，矩阵对角化，MPI，跨节点并行, VASP</kwd><kwd> Matrix Diagonalization</kwd><kwd> MPI</kwd><kwd> Inter-Node Parallelism</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>VASP是材料微观物性模拟的重要软件，并行性能优良，其中矩阵对角化在其运算过程中占据大量时间。在HPC作业调度下，VASP并行的进程分配表现单节点和跨节点的差异。通过对MPI并行的节点内和节点间进程、EDDIAG和RMM-DIIS运行时间分析，明确了VASP程序并行中，矩阵对角化的重要性和OpenMPI对矩阵对角化并行的作用。并编写了一个通讯优化的矩阵对角化程序，通过测试并行程序托比串行的马斯算法表现出强大的可扩展性，基于OpenMPI的并行程序能够极大提高对角化的并行效率，但其并行度、通讯代价与计算规模之间的关系需要进一步研究。</p></sec><sec id="s2"><title>关键词</title><p>VASP，矩阵对角化，MPI，跨节点并行</p></sec><sec id="s3"><title>The Parallel and Progress of VASP in OpenMPI Environment<sup> </sup></title><p>Yong Pan<sup>1</sup>, Haihong Shi<sup>2</sup>, Liutao Zhao<sup>1</sup>, Tong Liu<sup>1</sup></p><p><sup>1</sup>Beijing Computing Center, Beijing</p><p><sup>2</sup>Beijing Beike Rongzhi Cloud Computing Science and Technology Ltd., Beijing</p><p><img src="//html.hanspub.org/file/10-1541993x4_hanspub.png" /></p><p>Received: Dec. 24<sup>th</sup>, 2020; accepted: Jan. 18<sup>th</sup>, 2021; published: Jan. 26<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/10-1541993x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>VASP is an important software for material microscopic physical properties simulation, with excellent parallel performance, among which matrix diagonalization takes up a lot of time in its calculation process. Under HPC job scheduling, the parallel process allocation of VASP shows the difference between the single node and across multiple nodes. Through the analysis of the intra-node and inter-node processes of MPI parallel, the running time of EDDIAG and RMM-DIIS, we clarified the importance of matrix diagonalization in VASP program parallelism and the role of OpenMPI on matrix diagonalization parallelism. Furthermore, we write a communication-optimized matrix diagonalization program. By test, the parallel program using the Thomas algorithm shows strong scalability than the serial algorithm. The OpenMPI-based parallel program can significantly improve diagonalization’s parallel efficiency, but the relationship between its parallelism, communication cost, and computing scale needs further study.</p><p>Keywords:VASP, Matrix Diagonalization, MPI, Inter-Node Parallelism</p><disp-formula id="hanspub.39977-formula2"><graphic xlink:href="//html.hanspub.org/file/10-1541993x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/10-1541993x7_hanspub.png" /> <img src="//html.hanspub.org/file/10-1541993x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 简介</title><p>近年来，材料计算与模拟在新材料研发和材料物性分析中发挥越来越重要的作用，特别是基于量子力学的第一原理(ab initio)计算，构成材料计算与模拟的微观基础 [<xref ref-type="bibr" rid="hanspub.39977-ref1">1</xref>] - [<xref ref-type="bibr" rid="hanspub.39977-ref8">8</xref>]，由于实验技术和条件的限制，也是对计算需求特别大的阶段。描述微观粒子运动需要求解偏微分方程，对计算能力提出了巨大的挑战。随着高性能计算的能力提升，材料计算软件的并行性能也不断提高，出现了越来越多的适应高性能计算的第一原理并行计算软件，如VASP (Vienna Ab-initio Simulation Package) [<xref ref-type="bibr" rid="hanspub.39977-ref9">9</xref>]、QE (Quantum Espresso) [<xref ref-type="bibr" rid="hanspub.39977-ref10">10</xref>] 和PWmat [<xref ref-type="bibr" rid="hanspub.39977-ref11">11</xref>] 等。考虑到构成宏观物质的微观粒子的数量高达1023量级，现有的高性能计算能力面对材料设计的计算需求，还有不小的距离。</p><p>第一原理材料计算软件是基于密度泛函理论(Density Functional Theory) [<xref ref-type="bibr" rid="hanspub.39977-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.39977-ref13">13</xref>] 和周期势条件下电子运动的偏微分方程求解器。计算过程中主要涉及的高性能计算包括：1) 周期势条件下的平面波基组和FFT变换所需的网格分布；2) 矩阵对角化求解DFT基态方程本征值的迭代优化算法。以VASP为例，在网格划分方面，采取了“双网格技术(dual grid)” [<xref ref-type="bibr" rid="hanspub.39977-ref14">14</xref>]，即在倒空间用粗网格计算基础上，将网格加密，然后实施FFT到正空间(密网格)，计算有关物理量后，变换回到倒空间(密网格)，最后得到粗网格上的物理量，确保计算精度。而矩阵迭代优化算法则采用了Davison方法、共轭梯度(CG)和RMM-DIIS [<xref ref-type="bibr" rid="hanspub.39977-ref15">15</xref>] [<xref ref-type="bibr" rid="hanspub.39977-ref16">16</xref>] 等多种优化算法实现。</p><p>对称特征值问题作为数值计算的重要问题，存在于很多学科领域中，尤其是材料计算第一性原理，该问题的求解中，矩阵三对角化和三对角矩阵特征值的求解是两个重要的步骤，鉴于问题规模问题，本文中使用分治策略和OpenMPI进行矩阵并行求解，并测试了在分布式模式下的并行求解效率。</p><p>本文按以下顺序组织：第二部分介绍HPC并行环境，OpenMPI和LSF集群管理软件，第三部分介绍VASP在HPC作业下的调度、跨节点并行测试和结果进行分析，分别从VASP并行、MPI的体系结构和作业管理系统的资源分配讨论；第四部分为矩阵对角化程序并行算法及测试效果；最后为本文结论。</p></sec><sec id="s6"><title>2. 使用须知</title><p>作为以理论计算为基础的计算材料科学来说，计算研究是第一要务。通过模拟计算，可以在不实际制备的前提下深入理解材料的微观结构性质，从而节约研发成本；同时还可以模拟特殊/极限环境下的实验结果，比如对人体健康有害，或者处在压、超低温、强磁场等某些极端条件下，实验测量很难实现或者耗费巨大，可以使用模拟计算来代替或指导实验。</p><p>随着HPC能力飞速提高，材料科学应用领域大幅度扩展，更大尺度的体系、更长时间的动力学演化、更精确的理论计算描述成为可能。高性能计算在材料科学中的应用也更加广泛。高性能计算给材料科学研究带来极大便利的同时，材料科学的发展也推动了高性能计算的进步。</p><sec id="s6_1"><title>2.1. OpenMPI的体系结构</title><p>适用于机群、MPP等分布式内存结构的并行编程环境，通常由“并行虚拟机”(Parallel Virtual Machine, PVM)或“消息传递接口”(Message Passing Interface, MPI)来实现。利用PVM工具，可以把互连的各种计算机虚拟为一台并行机，从而为编程人员提供了一个便于管理和使用的编程环境，而由PVM的编译库对程序进行转换，将程序的计算任务分解为若干子任务后合理分配到各个节点机进行并行处理。MPI是一种基于消息传递的并行计算规范，消息(Message)一般包括数据、指令或其它各种控制信号等，MPI提供了一套消息传递库，基于消息传递的并行编程实际上就是通过调用MPI的消息传递库函数实现节点机之间的数据交换，并提供并行处理任务之间的同步等。目前，基于PVM和MPI并行编程环境，都可以支持C、C++和FORTRAN等的并行编程。</p><p>Open MPI的基于构件的体系结构不仅为第三方研究提供了稳定的平台，也使得独立软件附件能够在运行时组合。Open MPI的运行时环境提供了启动和管理并行应用的基本服务。Open MPI的设计以MPI构件架构(MPI Component Architecture, MCA)为中心，为所有其它层次提供管理服务的基础组件结构。对Open MPI的架构分析表明，OpenMPI并不会开启额外进程，因此跨节点时的新增进程的唯一来源是作业管理系统对计算资源的分配与监控过程。</p></sec><sec id="s6_2"><title>2.2. 作业管理系统的资源分配与监控</title><p>高性能计算是以大型集群和计算队列为硬件基础。作业管理系统包括集群管理和作业管理两个方面，集群管理系统将一组独立的计算机组合成资源池，共同协作以完成任务。集群广泛应用于高性能计算领域，提供低成本，可扩展及高性能的计算能力，作业管理系统是构成集群的重要软件系统，它的主要任务是对集群的资源进行集中的监控和管理，为用户提交的任务分配可用的计算资源，并监控和管理作业的执行及结果的返回；同时，还提供系统容错和错误恢复能力，可以在异常或故障发生时最大程度的减少任务中断的代价。</p><p>随着数据中心在规模和复杂性上的快速增加，使得对集群工作负载和应用的管理更加困难，同时也难以确保计算硬件和软件等资源的正常使用。用户希望在任何地方都能灵活使用应用程序，并自动操控数据流，管理者希望能够监督集群的资源和负载，管理软件使用权，确定瓶颈问题，监督面向用户的服务协议是否满足，并计划系统的扩容数量。</p><p>北京市计算中心采用IBM Spectrum LSF (Load Sharing Facility)平台系列软件管理集群和作业。对于要求高的分布式任务型高性能计算环境，该软件能够通过综合的基于智能的，策略驱动的调度策略高效地管理负载，方便客户使用所有计算基础设施资源，保障最佳的应用性能。软件将集群内服务器分为两类，分别是管理节点和计算节点。管理节点负责监控集群中所有节点状态，并分配任务到计算节点，计算节点负责运行用户提交的任务，图1是LSF平台在集群中的系统环境下的任务提交执行过程。</p><p>作业管理系统展示了任务提交的完整过程：</p><p>1) 作业提交</p><p>用户通过LSF客户端，或者可以执行bsub命令的服务器上提交一个作业，当提交这份作业时，如果不指定哪个队列，这份作业就会被提交到系统默认的队列中，作业在队列中等待安排，这些作业处于等待状态。</p><p>图1. LSF平台在集群中的系统环境下的任务提交执行过程</p><p>2) 作业调度</p><p>后台的主进程mbatchd将处理队列中的作业，在一个预定的时间间隔里将这些作业按设定的计划，传递给主调度进程mbschd。</p><p>主调度进程mbschd评估这份工作时，根据作业的优先权制定调度决策、调度机制和可利用资源。主调度进程选择可以运行作业的最佳计算节点，并将它的决策返回给后台主进程mbatchd。主负载信息管理进程(lim)收集资源信息，主lim与mbatchd主进程交流这些信息，反过来mbatchd主进程使用之前交流信息支持调度决定。</p><p>3) 作业分配</p><p>mbatchd主进程收到mbschd发过来的决定后，立即分配作业到计算节点。</p><p>4) 作业运行</p><p>从批处理进程(sbatchd)，从mbatchd主进程接到要求，为作业创建子sbatchd和一个执行环境，通过使用一个远程执行服务器开始这个作业。对于使用MPI执行的作业，LSF会在MPI和应用中间创建一个TaskStarter进程，用于管理和监控每个作业的执行。</p><p>5) 返回输出</p><p>当一个作业完成时，如果这个作业没有出现任何问题，它处于一个完成状态。如果有错误作业无法完成，这份作业处于退出状态。sbatchd传达作业信息，包括错误提示和给mbatchd的输出信息。</p><p>6) 信息反馈</p><p>mbatchd通过预定方式给提交者反馈作业输出信息、作业错误、提示信息、作业信息。</p><p>通过上述作业提交和管理、监控过程，不难看出，在跨节点计算任务中，作业管理系统将在MPI通信启动时，在计算节点上开启两类进程，一类用于监督作业运行状况(节点数−1)*CPU数，另一类是管理进程，即检查跨节点计算资源的运行和稳定状况。因此是(节点数−1)。如果计算中的资源不跨节点，作业管理系统启动MPI的进程同时充当监督和守护进程，因此作业管理系统没有启动额外的进程。</p></sec></sec><sec id="s7"><title>3. VASP并行效果测试</title><p>VASP是维也纳大学开发PAW方法 [<xref ref-type="bibr" rid="hanspub.39977-ref17">17</xref>] [<xref ref-type="bibr" rid="hanspub.39977-ref18">18</xref>] 的第一原理材料物性模拟的主要软件，VASP的MPI并行效率较高。在北京市计算中心的服务器的计算环境为Intel&#174; Xeon (R) CPU E5-2680 V3 2.50 GHz，内存DDR4 64 G，2133 MHz，数据网络连接为Infiniband 56 Gb/s。每个节点有24核，每核4线程，通过作业管理系统，分别分配的计算资源包括单节点8核、16核、24核和跨节点24核(N = 2)、48核(N = 3)、72核(N = 4)等几种情况，VASP正常结束后，作业管理系统将统计并输出的最大进程数和线程数。由此确定VASP并行过程中节点、进程等资源的使用情况，为进一步优化计算资源作准备。</p><sec id="s7_1"><title>3.1. VASP的并行实现</title><p>本文实验的运行环境中，VASP的并行计算，是由Open MPI支持的。Open MPI是在LAM/MPI，LA-MPI和FT-MPI的基础上的一种全新的基于构件概念的MPI实现，VASP的MPI实现只关注高性能计算的部分方面，即倒空间布点的分配和能带并行快速求解的问题。Open MPI并不是LAM/MPI，LA-MPI和FT-MPI的简单组合，而是一种全新的MPI实现，完全实现了MPI-1.2和MPI-2规约，并且完全支持并发和多线程应用(也就是MPI_THREAD_MULTIPLE)。</p><p>VASP运行过程中，并行计算部分主要集中在主要倒空间网格点的并行分配、k点能带本征值求解(矩阵对角化)和DFT总能量计算这几部分。VASP的主程序将计算资源(分配到的CPU)按控制参数NPAR和NCORE分解(要求满足NPAR*NCORE = NCPU)，在实际并行计算过程中，各部分对计算资源的利用方式不尽相同：</p><p>1) 倒空间网格点并行分配和FFT变换，在该阶段，NCPU个计算资源是作为整理对待的，即空间划分的网格被均分到NCPU个进程上，形成网格点和NCPU个进程的拓扑映射，在此基础上，根据NCPU个进程的网络关系完成MPI支持的FFT变换，实现最高效的并发式FFT变换。</p><p>2) k点能带本征值求解时，全部CPU分解为NPAR个组，每组NCORE个CPU上实现MPI通信和数据共享(通过共享内存)来求解一个能带的本征值，每次可同时求解NPAR个能带本征值。</p><p>3) DFT总能量计算，与k点能带本征值求解时相似，将NPAR组计算的结果汇总，得到体系的总能量。只是简单的MPI进程。</p></sec><sec id="s7_2"><title>3.2. VASP并行测试</title><p>VASP并行计算的节点、进程统计结果列于表1，计算结果表明，单节点(N = 1)时，进程数即CPU数目，当出现跨节点(N &gt; 1)计算时，进程数显著增加，并且进程数随着每个节点上的CPU数目增加呈现出有规律的变化，线程数则始终保持CPU数目的4倍递增。为讨论跨节点计算额外新增进程的可能起源。我们从VASP软件的并行实现、MPI的体系结构和作业管理系统的资源分配和监控三个方面讨论计算过程中启动的进程数目。</p><p>从图2中可以看出总的计算时间(紫色线，单位小时)随着并行核数的增加和而降低。当全部核被占据时，并行效率明显降低(计算时间增加)。VASP程序的并行执行主要是自洽迭代(SCF)中矩阵对角化，可以看出，每次自洽迭代的对角化过程RM-DIIS的耗时(蓝色线，单位秒)也是类似规律。在单机最大节点24核的情况下，并行核数达到20核时，RM-DIIS达到并行峰值。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The number of threads when using different number of nodes and CPU</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >N<sub>node</sub></th><th align="center" valign="middle" >N<sub>CPU</sub></th><th align="center" valign="middle" >N<sub>Process</sub></th><th align="center" valign="middle" >N<sub>Thread</sub></th><th align="center" valign="middle" >N<sub>node</sub></th></tr></thead><tr><td align="center" valign="middle" >N = 1</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >N = 1</td></tr><tr><td align="center" valign="middle" >N = 1</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >64</td><td align="center" valign="middle" >N = 1</td></tr><tr><td align="center" valign="middle" >N = 1</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >96</td><td align="center" valign="middle" >N = 1</td></tr><tr><td align="center" valign="middle" >N = 2</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >25</td><td align="center" valign="middle" >96</td><td align="center" valign="middle" >N = 2</td></tr><tr><td align="center" valign="middle" >N = 2</td><td align="center" valign="middle" >48</td><td align="center" valign="middle" >49</td><td align="center" valign="middle" >192</td><td align="center" valign="middle" >N = 2</td></tr><tr><td align="center" valign="middle" >N = 3</td><td align="center" valign="middle" >48</td><td align="center" valign="middle" >66</td><td align="center" valign="middle" >192</td><td align="center" valign="middle" >N = 3</td></tr><tr><td align="center" valign="middle" >N = 4</td><td align="center" valign="middle" >72</td><td align="center" valign="middle" >111</td><td align="center" valign="middle" >288</td><td align="center" valign="middle" >N = 4</td></tr><tr><td align="center" valign="middle" >N = 4</td><td align="center" valign="middle" >96</td><td align="center" valign="middle" >147</td><td align="center" valign="middle" >384</td><td align="center" valign="middle" >N = 4</td></tr></tbody></table></table-wrap><p>表1. VASP并行计算时不同节点产生进程和线程</p><p>跨节点并行时(图3)：满负荷(24核)并行时，随着并行节点数的增加，单次自洽迭代的矩阵对角化时间逐渐降低，但降低的梯度逐渐变小(计算效率有所下降)。半负荷(12核)并行时，计算效率比满负荷并行效率显著提高，由此可以推断，LSF作业调度系统下，每个阶段满负荷运行计算效率不高，考虑到单节点并行的情况，建议20核作为跨节点并行的推荐值。</p><p>综上所述，VASP计算过程中，无论实际计算中是否需要跨节点，VASP将NCPU个计算资源作为总体对待，虽然有一定的控制参数将计算资源进行划分，但是VASP运行中并未开启额外的进程，总进程保持为NCPU，因此额外的进程可能由MPI跨节点并行和作业管理系统对作业和计算资源监控产生的。</p><p>图2. 单节点VASP并行测试</p><p>图3. 跨节点的VASP并行测试</p><p>根据上述讨论，我们可以得到以下结论：在高性能集群中的VASP计算，从作业管理的角度考虑，当指定的计算资源是跨节点计算时，除了VASP作业自身需开启的流程，作业管理系统会开启一部分进程，实现对作业和计算资源的管理、监控和维护。这些额外的进程通过主进程mbatchd监控。一般情况下，这些进程占用的系统资源不大。因此，对于VASP这样的计算作业，由于作业管理产生的进程，对计算资源和计算效率的影响可以忽略不计。但是，由于这些进程伴随作业任务的全过程，因此Wall-time的影响，则必须考虑。</p></sec></sec><sec id="s8"><title>4. 矩阵并行对角化及实验效果</title><p>VASP软件中的矩阵对角化计算采用两个阶段，分别采用EDDIAG和RMM-DIIS算法，为了降低问题复杂度并体现MPI并行对矩阵对角化的效率提升，我们采用分治法对矩阵进行分块，每个块独立求解后再用Thomas算法进行归约计算。</p><sec id="s8_1"><title>4.1. 算法描述</title><p>对角矩阵是指只有主对角线上含有非零元素的矩阵，即，已知一个n &#215; n矩阵M，如果对于i ≠ j，M<sub>ij</sub> = 0，则该矩阵为对角矩阵。如果存在一个矩阵A，使A<sup>−1</sup>MA的结果为对角矩阵，则称矩阵A将矩阵M对角化。对于一个矩阵来说，不一定存在将其对角化的矩阵，但是任意一个n &#215; n矩阵如果存在n个线性不相关的特征向量，则该矩阵可被对角化。</p><p>图4. 矩阵并行对角化程序伪代码</p><p>算法描速：</p><p>1) 矩阵分块，假设矩阵规模为n，并行数量为p，令k = n/p，则矩阵划分为k*k的矩阵块；</p><p>2) 在每个进程中对矩阵进行独立求解；</p><p>3) 用Thomas算法对2)中的结果进行归约。</p><p>求解算法的伪代码见(图4矩阵并行对角化程序伪代码)</p></sec><sec id="s8_2"><title>4.2. 实验结果</title><p>实验以串行的Thomas算法作为参考，求解规模分为1,000,000，5,000,000，10,000,000，50,000,000四种，分别在2、4、10、16、20、32核的并行度下完成计算。计算时间、加速比和并行效率如表2所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Parameters of matrix diagonalizatio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >求解规模</th><th align="center" valign="middle" >进程数</th><th align="center" valign="middle" >运行时间</th><th align="center" valign="middle" >加速比</th><th align="center" valign="middle" >效率</th></tr></thead><tr><td align="center" valign="middle"  rowspan="7"  >1,000,000</td><td align="center" valign="middle"  colspan="4"  >串行：0.057035</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0.110394</td><td align="center" valign="middle" >0.88302806</td><td align="center" valign="middle" >0.44151403</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >0.045438</td><td align="center" valign="middle" >1.25522414</td><td align="center" valign="middle" >0.31380604</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >0.018939</td><td align="center" valign="middle" >3.00200644</td><td align="center" valign="middle" >0.30020064</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >0.012024</td><td align="center" valign="middle" >4.85033974</td><td align="center" valign="middle" >0.30314623</td></tr><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >0.009271</td><td align="center" valign="middle" >6.10399584</td><td align="center" valign="middle" >0.30519979</td></tr><tr><td align="center" valign="middle" >32</td><td align="center" valign="middle" >0.005903</td><td align="center" valign="middle" >9.60506589</td><td align="center" valign="middle" >0.30015831</td></tr><tr><td align="center" valign="middle"  rowspan="7"  >5,000,000</td><td align="center" valign="middle"  colspan="4"  >串行：0.28042</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0.327978</td><td align="center" valign="middle" >0.82309179</td><td align="center" valign="middle" >0.4115459</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >0.219792</td><td align="center" valign="middle" >1.27037836</td><td align="center" valign="middle" >0.31759459</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >0.095686</td><td align="center" valign="middle" >2.92946722</td><td align="center" valign="middle" >0.29294672</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >0.06291</td><td align="center" valign="middle" >4.88983486</td><td align="center" valign="middle" >0.30561468</td></tr><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >0.035386</td><td align="center" valign="middle" >7.87165017</td><td align="center" valign="middle" >0.39358251</td></tr><tr><td align="center" valign="middle" >32</td><td align="center" valign="middle" >0.021872</td><td align="center" valign="middle" >12.8209583</td><td align="center" valign="middle" >0.40065495</td></tr><tr><td align="center" valign="middle"  rowspan="7"  >10,000,000</td><td align="center" valign="middle"  colspan="4"  >串行：0.561426</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0.644677</td><td align="center" valign="middle" >0.876874776</td><td align="center" valign="middle" >0.438437388</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >0.309206</td><td align="center" valign="middle" >1.729714171</td><td align="center" valign="middle" >0.432428543</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >0.183201</td><td align="center" valign="middle" >3.048149301</td><td align="center" valign="middle" >0.30481493</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >0.121082</td><td align="center" valign="middle" >4.61117259</td><td align="center" valign="middle" >0.28819828</td></tr><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >0.068256</td><td align="center" valign="middle" >8.225310926</td><td align="center" valign="middle" >0.411265546</td></tr><tr><td align="center" valign="middle" >32</td><td align="center" valign="middle" >0.042963</td><td align="center" valign="middle" >12.96180434</td><td align="center" valign="middle" >0.405056386</td></tr><tr><td align="center" valign="middle"  rowspan="7"  >50,000,000</td><td align="center" valign="middle"  colspan="4"  >串行：2.79557</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >3.07504</td><td align="center" valign="middle" >0.898105391</td><td align="center" valign="middle" >0.449052695</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >2.12244</td><td align="center" valign="middle" >1.254122614</td><td align="center" valign="middle" >0.313530653</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >0.895866</td><td align="center" valign="middle" >3.09856608</td><td align="center" valign="middle" >0.309856608</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >0.569611</td><td align="center" valign="middle" >4.899396255</td><td align="center" valign="middle" >0.306212266</td></tr><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >0.321817</td><td align="center" valign="middle" >8.706314458</td><td align="center" valign="middle" >0.435315723</td></tr><tr><td align="center" valign="middle" >32</td><td align="center" valign="middle" >0.202909</td><td align="center" valign="middle" >13.77745689</td><td align="center" valign="middle" >0.430545528</td></tr></tbody></table></table-wrap><p>表2. 矩阵对角化参数</p><p>实验结果表明利用分治算法，在OpenMPI和分布式并行环境下，矩阵对角化计算具有较强的扩展能力，在文中的规模上并行效率基本稳定，运行时间随着核数增加线性减少，直观效果见图5~8。</p><p>图5. 1 M，5 M，10 M矩阵规模下执行时间</p><p>图6. 50 M矩阵规模下的执行时间</p><p>图7. 不同核数的并行加速比</p><p>图8. 不同矩阵规模和并行核数下系统的使用效率</p></sec></sec><sec id="s9"><title>5. 结论</title><p>在HPC环境中的材料计算的核心问题是计算分解、通讯和归约。应用在实际运行中需要兼顾计算资源、通讯效率等多方面问题。在指定的计算资源是跨节点计算时，除了应用软件自身需的资源，操作系统、存储系统和管理软件也会占用一定资源。虽然，这些程序占用的系统资源不大，但对于VASP之类的CPU密集型应用，由于辅助程序产生的进程，对作业任务的全过程均会产生影响，在系统满负荷运行时，会导致计算任务的不均衡问题，不如空置少量核作为预留资源。</p><p>矩阵对角化在科学计算领域占有重要地位，如何更加合理地解构问题，处理分块求解通讯和同步问题，保证加速比随着并行度提升线性增加是矩阵优化的重要研究方向。</p></sec><sec id="s10"><title>致谢</title><p>本项目由国家重点研发计划项目“产学研用协同的高通量材料计算融合服务平台”(2018YFB0703900)，课题3高通量材料计算的工作流设计与交互图形化(2018YFB0703903)支持。</p></sec><sec id="s11"><title>文章引用</title><p>潘 勇,史海洪,赵琉涛,刘 彤. OpenMPI环境下的VASP软件的并行与进程The Parallel and Progress of VASP in OpenMPI Environment[J]. 计算机科学与应用, 2021, 11(01): 84-94. https://doi.org/10.12677/CSA.2021.111010</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.39977-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Thijssen, J.M. (2007) Computational Physics. 2nd Edition, Cambridge University Press, Cambridge, England.</mixed-citation></ref><ref id="hanspub.39977-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Ong, S.P., Chlia, S., Jain, A., Brafman, M., Gunter, D., Ceder, G. and Persson, K.A. (2015) The Materials Application Programming In-terface (API): A Simple, Flexible and Efficient API for Materials Data Based on REpresentational State Transfer (REST) Princi-ples. Computational Materials Science, 97, 209R.  
&lt;br&gt;https://doi.org/10.1016/j.commatsci.2014.10.037</mixed-citation></ref><ref id="hanspub.39977-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Potyrailo, A. Chisholm, B.J., Morris, W.G., Cawse, J.N., Flanagan, W.P., Hassib, L., Molaison, C.A., Ezbiansky, K., Medford, G. and Reitz, H. (2003) Development of Combinatorial Chemistry Methods for Coatings:  High-Throughput Adhesion Evaluation and Scale-Up of Combinatorial Leads. Journal of Computational Chemistry, 5, 472.  
&lt;br&gt;https://doi.org/10.1021/cc030022s</mixed-citation></ref><ref id="hanspub.39977-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Potyrailo, R.A. and Takeuchi, I. (2005) Role of High-Throughput Characteriza-tion Tools in Combinatorial Materials Science. Materials Science and Technology, 16, 1. &lt;br&gt;https://doi.org/10.1088/0957-0233/16/1/E01</mixed-citation></ref><ref id="hanspub.39977-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Persson, K. (2020) Materials Project. http://www.materialsgenome.org</mixed-citation></ref><ref id="hanspub.39977-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Setyawan, W. and Curtarolo, S. (2010) High-Throughput Electronic Band Structure Calculations: Challenges and Tools. Computational Materials Science, 49, 299. &lt;br&gt;https://doi.org/10.1016/j.commatsci.2010.05.010</mixed-citation></ref><ref id="hanspub.39977-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Jain, A. and Hautier, G., Moore, C.J., Ong, S.P., Fischer, C.C., Mueller, T., Persson, K.A. and Ceder, G. (2011) A High-Throughput Infrastructure for Density Functional Theory Calculations. Computational Materials Science, 50, 2295. &lt;br&gt;https://doi.org/10.1016/j.commatsci.2011.02.023</mixed-citation></ref><ref id="hanspub.39977-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Yang, X., Wang, Z., Zhao, X., Song, J., Zhang, M. and Liu, H. (2018) MatCloud: A High-Throughput Computational Infrastructure for Integrated Management of Materials Simulation, Data and Resources. Computational Materials Science, 146, 319. &lt;br&gt;https://doi.org/10.1016/j.commatsci.2018.01.039</mixed-citation></ref><ref id="hanspub.39977-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Kresse, G., Marsman, M. and Furthmüller, J. (2016) VASP the Guide. http://cms.mpi.univie.ac.at/VASP/</mixed-citation></ref><ref id="hanspub.39977-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Giannozzi, P., Baroni, S., Bonini, N., Calandra, M., Car, R., Cavazzoni, C., Ceresoli, D., Chiarotti, G.L., Cococcioni, M., Dabo, I., Dal Corso, A., Fabris, S., Fratesi, G., de Gironcoli, S., Gebauer, R., Gerstmann, U., Gougoussis, C., Kokalj, A., Lazzeri, M., Martin-Samos, L., Marzari, N., Mauri, F., Mazzarello, R., Paolini, S., Pasquarello, A., Paulatto, L., Sbraccia, C., Scandolo, S., Sclauzero, G., Seitsonen, A.P., Smogunov, A., Umari P. and Wentzcovitch, R.M. (2009) QUANTUM ESPRESSO: A Modular and Open-Source Software Project for Quantum Simulations of Materials. Journal of Physics: Condensed Matter,21, Article ID: 395502. &lt;br&gt;https://doi.org/10.1088/0953-8984/21/39/395502</mixed-citation></ref><ref id="hanspub.39977-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Jia, W., Fu, J., Cao, Z., Wang, L., Chi, X., Gao, W. and Wang, L.-W. (2013) Fast Plane Wave Density Functional Theory Molecular Dynamics Calculations on Multi-GPU Machines. Journal of Computational Physics, 251, 102.  
&lt;br&gt;https://doi.org/10.1016/j.jcp.2013.05.005</mixed-citation></ref><ref id="hanspub.39977-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Hohenberg, P. and Kohn, W. (1964) Inhomogeneous Electron Gas. Physi-cal Review, 136, B864.  
&lt;br&gt;https://doi.org/10.1103/PhysRev.136.B864</mixed-citation></ref><ref id="hanspub.39977-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Kohn, W. and Sham, L.J. (1965) Self-Consistent Equations Including Exchange and Correlation Effect. Physical Review, 140, A1133. &lt;br&gt;https://doi.org/10.1103/PhysRev.140.A1133</mixed-citation></ref><ref id="hanspub.39977-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Kresse, G. and Hafner, J. (1994) Norm-Conserving and Ultrasoft Pseudopotentials for First-Row and Transition Elements. Journal of Physics: Condensed Matter, 6, 8245. &lt;br&gt;https://doi.org/10.1088/0953-8984/6/40/015</mixed-citation></ref><ref id="hanspub.39977-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Kresse, G. and Furthmüller, J. (1996) Efficiency of Ab-Initio Tottal Energy Calculations for Metals and Semiconductors Using a Plane-Wave Basis Set. Com-putational Materials Science, 6, 15.  
&lt;br&gt;https://doi.org/10.1016/0927-0256(96)00008-0</mixed-citation></ref><ref id="hanspub.39977-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Kresse, G. and Furthmüller, J. (1996) Efficient Iterative Schemes for Ab Initio Total-Energy Calculations Using a Plane-Wave Basis Set. Physical Review, 54, Article ID: 11169. &lt;br&gt;https://doi.org/10.1103/PhysRevB.54.11169</mixed-citation></ref><ref id="hanspub.39977-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Blöchl, P.E. (1994) Projector Augmented-Wave Method. Physical Re-view B, 50, Article ID: 17953.  
&lt;br&gt;https://doi.org/10.1103/PhysRevB.50.17953</mixed-citation></ref><ref id="hanspub.39977-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Kresse, G. and Joubert, D. (1999) From Ultrasoft Pseudopotentials to the Projector Augmented-Wave Method. Physical Review B, 59, 1758. &lt;br&gt;https://doi.org/10.1103/PhysRevB.59.1758</mixed-citation></ref></ref-list></back></article>