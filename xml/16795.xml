<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">JISP</journal-id><journal-title-group><journal-title>Journal of Image and Signal Processing</journal-title></journal-title-group><issn pub-type="epub">2325-6753</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/JISP.2016.51006</article-id><article-id pub-id-type="publisher-id">JISP-16795</article-id><article-categories><subj-group subj-group-type="heading"><subject>JISP20160100000_47678318.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于角点特征的遥感图像快速配准
  Fast Registration of Remote Sensing Image Based on Corner Feature
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>钱</surname><given-names>社军</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>正勇</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>何</surname><given-names>小海</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>四川大学电子信息学院，四川 成都</addr-line></aff><aff id="aff2"><addr-line>null</addr-line></aff><author-notes><corresp id="cor1">* E-mail:<email>380574425@qq.com(钱社)</email>;</corresp></author-notes><pub-date pub-type="epub"><day>01</day><month>12</month><year>2015</year></pub-date><volume>05</volume><issue>01</issue><fpage>43</fpage><lpage>51</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对遥感图像传统配准算法匹配速度较慢、不满足实时性要求等问题，本文提出了一种结合改进AGAST与FREAK算法的遥感图像快速配准方法。首先，利用改进AGAST检测算法分别快速检测参考图像和待配准图像中的特征点；然后用FREAK算法获取二进制描述符串，利用级联匹配计算特征向量之间的汉明距离，获得特征点匹配对；最后利用改进的相似三角形剔除方法去掉错误的匹配对，并结合最小二乘法，估算出空间几何变换参数，实现两幅图像的配准。实验结果表明，本文方法在保证遥感图像配准精度的同时，配准速度相比于传统配准方法得到较大提升。 In this paper, because the traditional registration algorithms for remote sensing image are slower and don’t meet the requirements of real-time problem, a new method is proposed based on the combination of improved AGAST and FREAK for fast remote sensing images registration. Firstly, the improved AGAST is used to detect the feature points between reference image and image that is to be registered; Secondly, FREAK algorithm is used to obtain a binary string descriptor, and hamming distance between features vector is computed by using a cascade match to get matching feature points; Finally, wrong match pairs are eliminated by using the improved similar triangle method, and the optimal spatial geometric transform parameters are estimated using the least square method to accomplish the two images registration. Experimental results show that the pro- posed method improves the registration rate compared to the traditional registration methods, and ensures accuracy at the same time.
    
  
 
</p></abstract><kwd-group><kwd>遥感图像，快速配准，AGAST，FREAK，相似三角形，最小二乘法, Remote Sensing Image</kwd><kwd> Fast Registration</kwd><kwd> AGAST</kwd><kwd> FREAK</kwd><kwd> Similar Triangles</kwd><kwd> 
The Least Square Method</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于角点特征的遥感图像快速配准<sup> </sup></title><p>钱社军，王正勇<sup>*</sup>，何小海</p><p>四川大学电子信息学院，四川 成都</p><disp-formula id="hanspub.16795-formula114"><graphic xlink:href="http://html.hanspub.org/file/6-2670068x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>收稿日期：2016年1月4日；录用日期：2016年1月19日；发布日期：2016年1月22日</p><disp-formula id="hanspub.16795-formula115"><graphic xlink:href="http://html.hanspub.org/file/6-2670068x8_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>针对遥感图像传统配准算法匹配速度较慢、不满足实时性要求等问题，本文提出了一种结合改进AGAST与FREAK算法的遥感图像快速配准方法。首先，利用改进AGAST检测算法分别快速检测参考图像和待配准图像中的特征点；然后用FREAK算法获取二进制描述符串，利用级联匹配计算特征向量之间的汉明距离，获得特征点匹配对；最后利用改进的相似三角形剔除方法去掉错误的匹配对，并结合最小二乘法，估算出空间几何变换参数，实现两幅图像的配准。实验结果表明，本文方法在保证遥感图像配准精度的同时，配准速度相比于传统配准方法得到较大提升。</p><p>关键词 :遥感图像，快速配准，AGAST，FREAK，相似三角形，最小二乘法</p><disp-formula id="hanspub.16795-formula116"><graphic xlink:href="http://html.hanspub.org/file/6-2670068x9_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s3"><title>1. 引言</title><p>基于局部不变特征是目前非常重要的图像配准手段，能够对旋转、尺度缩放、仿射变换、视角变化、光照变化等图像变化因素保持一定的不变性，同时对物体运动、遮挡等因素也能够保持较好的可匹配性。2004年，Lowe [<xref ref-type="bibr" rid="hanspub.16795-ref1">1</xref>] 提出的SIFT (ScaleInvariant Feature Transform) [<xref ref-type="bibr" rid="hanspub.16795-ref2">2</xref>] 是一种提取局部特征的算法，除了具有对旋转、尺度缩放和亮度变化保持不变性的特性外，且对噪声、仿射变换及视角变化具有很好的稳定性；2006年，Bay等人 [<xref ref-type="bibr" rid="hanspub.16795-ref3">3</xref>] 提出的SURF特征检测算子与SIFT一样计算量较大，特征向量维数过高导致匹配的时间较长；Rosten等人 [<xref ref-type="bibr" rid="hanspub.16795-ref4">4</xref>] 提出FAST角点检测方法特征点提取速度特别快，但FAST算法对于相机旋转或任何新的环境，需要从新的训练图提取角点并重新构建决策树，并且FAST使用机器学习ID3贪心算法构建决策树，其结果有很大可能仅为局部最优解。</p><p>特征点描述是指将检测到的特征点通过一系列运算，生成一个用于特征匹配的高维向量的过程。Low提出的SIFT是目前最著名的特征描述符之一。该描述符通过计算特征点附近局部方向直方图来生成128维浮点型向量，特征匹配计算量大；Bay提出的SURF特征描述算法结合积分直方图对SIFT描述子进行降维，在保证不降低描述符性能的基础上，提高了描述符生成速度。不过，上述这些方法在实时图像拼接、图像检索及图像融合等实时性要求比较高的应用中，仍然因为高维浮点型描述符的生成导致匹配较慢而无法满足实时性要求 [<xref ref-type="bibr" rid="hanspub.16795-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.16795-ref6">6</xref>] 。Calonder等人 [<xref ref-type="bibr" rid="hanspub.16795-ref7">7</xref>] 提出的BRIEF，Leutenegger等人 [<xref ref-type="bibr" rid="hanspub.16795-ref8">8</xref>] 提出的BRISK和Rublee等人 [<xref ref-type="bibr" rid="hanspub.16795-ref9">9</xref>] 提出的ORB算法均生成二进制链码描述符。但BRIEF描述符通过随机抽样模式选取像素点对，不具备旋转不变性、尺度不变性，而且对噪声敏感；BRISK描述符算法采用固定抽样模式来生成二进制向量，解决了BRIEF不具备旋转不变性、尺度不变性和噪声鲁棒性的问题，但对信息具有冗余性且描述向量的长度过大；ORB描述符也是对BRIEF描述符的改进，具有旋转不变性和噪声鲁棒性，但不具备尺度不变性。</p><p>针对以上算法的不足，本文提出了一种结合AGAST [<xref ref-type="bibr" rid="hanspub.16795-ref10">10</xref>] 与FREAk局部特征二进制描述符的遥感图像配准方法。FREAK描述符是Alahi等人 [<xref ref-type="bibr" rid="hanspub.16795-ref11">11</xref>] 提出的一种局部特征检测算法，该算法是基于人眼视网膜细胞的分布，中间密集，四周稀疏，从而在图像中构建很多的区域，采用越靠近特征点中心的区域采样点越密集，四周区域采集稀疏的抽样方式，利用扫视搜索进行特征向量的匹配，具有旋转不变性和噪声鲁棒性。同时，本文使用的AGAST检测算法主要针对FAST的不足，利用AGAST算法对FAST底层的AST (the accelerated segment test)进行改进，通过在扩展配置空间寻找最优决策树，使用特定树的组合获得自适应。</p></sec><sec id="s4"><title>2. 改进的AGAST特征点检测</title><p>AGAST算法主要是是对FAST算法的改进，通过将FAST算法中的ID3决策树改造成二叉树，并且能够根据当前处理的图像信息动态且高效的分配决策树，从而提高算法检测速度。</p><p>AGAST寻求使算法不依赖在特定场景下的训练，从而更具泛用性，具有如同FAST算法一样的可重用性。此决策树对在AST mask下相似的像素来说有一定的几率是最佳的。通过合并俩个决策树，角检测算法自动适应环境并且为图像区域提供最高效的决策树(最多只有一个像素延迟)，也就是说，当在其中一棵树上到达决策路径的末端，即已经满足角点判据或不可能满足角点判据，则将路径切换到另外一棵树。图1所示为AGAST算法决策树，其中树叶亮度越高则表示对应集合中角点与非角点的数量越接近相等。左侧决策树专用于单一环境,右侧决策树专用于复杂环境。</p><p>AGAST算法未提供关键点方向指定,且均在单一尺度的图像上进行检测，本文使用文献 [<xref ref-type="bibr" rid="hanspub.16795-ref8">8</xref>] 提出的尺度空间构造方法进行AGAST尺度空间构建来解决此问题。另外，AGAST角点检测方法沿用了FAST角点判断依据，如图2所示。记任意像素点p的灰度为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x10_hanspub.png" xlink:type="simple"/></inline-formula>，若16个圆周点中至少有n个连续点灰度均大于或等于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x11_hanspub.png" xlink:type="simple"/></inline-formula>、或均小于或等于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x12_hanspub.png" xlink:type="simple"/></inline-formula>，则判定p为角点，其中t为设定阈值。传统应用FAST算法时，对于图像中的每一个点，都要去遍历其邻域圆上的16个点的像素，效率较低。为了快速地排除图像中的非角点，本文采取一种高效的方法来快速排除一大部分非角点的像素，该方法仅仅检查在位置1，5，9和13四个位置的像素。首先检测位置1和位置9，如果它们都比阈值暗或比阈值亮，再检测位置5和位置13。如果P是一个角点，那么上述四个像素点中至少有3个应该都大于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x13_hanspub.png" xlink:type="simple"/></inline-formula>或者小于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x14_hanspub.png" xlink:type="simple"/></inline-formula>，因为若是一个角点，超过四分之三圆的部分应该满足判断条件。如果不满足，那么p不可能是一个角点。在通过对4个圆周点的考查后，若像素p仍然可能为角点，则需要按照判据考查所有邻域圆周点，判断p是否为关键点。</p><p>阈值t表示所能检测角点的最小对比度，也是能忽略的噪声的最大容限。阈值t的选取尤为重要，它主要决定了能够提取的特征点数量，t越小，可从对比度越低的图像中提取越多的特征。因此，对于不同对比度情况的图像，选取的阈值t应相应的不同。原始的FAST角点检测算法中阈值t的选择依赖于人为的干涉，本文对其进行改进，根据不同对比度图像自动给出最优值。通过对图像灰度值和对比度进行分析，提出不同对比度下t的自适应取值方法，即：</p><disp-formula id="hanspub.16795-formula117"><label>(1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-2670068x15_hanspub.png"  xlink:type="simple"/></disp-formula><p>式中：n为对图像像素值进行降序排序后，所取得的最大或最小灰度值的数量；a表示比例系数，一般取0.10~0.25；<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x16_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x17_hanspub.png" xlink:type="simple"/></inline-formula>(<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x18_hanspub.png" xlink:type="simple"/></inline-formula>)分别代表最大的前n个值中排在位置j处的像素值和最小的后n个值中排在位置j处的像素值。利用公式(1)获得特征点检测所需的阈值t，然后用该阈值进行角点检测，最终获得生成特征描述符所需的的特征点，本文比例系数a取值0.15，n取值100。</p></sec><sec id="s5"><title>3. FREAK特征描述符</title><p>人类的视网膜有某区域可被划分为中央凹、旁中央凹、原中央凹和周边区四个区域，其具有感知光线的功能，该区域被称为感受域，如图3所示。神经节细胞的密度与感受域中枝状结构随着与中央凹距离的增大而变得越来越小。在目前的目标检测和识别等应用中，感受域的不同部分完成的功能也不尽相</p><p>图1. AGAST决策树</p><p>图2. AGAST检测模板</p><p>图3. 人类视网膜的结构示意图</p><p>同：周边区获得低分辨图像，用于识别目标的轮廓信息，中央凹区能够感知获得高分辨率图像，用于识别目标物的细节等信息。FREAK描述符就是通过模仿人类视觉的感受域区域来获取信息的结构而来的。</p><p>FREAK描述符的采样点分布类似于视网膜感受域随着与中央凹随着神经节细胞的分布结构，采取了更加接近人眼视网膜接收图像信息的采样模型。如图4所示展示了人眼视网膜拓扑结构：采样点均匀分布在以特征点为中心的同心圆上。离中心特征点越近，采样点越密集，该圆内的高斯函数半径越小；离中心特征点越远，采样点越稀疏，该圆内的高斯函数半径越大。Fovea区域主要是对高精度的图像信息进</p><p>图4. FREAK算子的采样点结构</p><p>行处理，Para主要是对低精度的图像信息进行处理。从图中可知，该结构有很多大小不同并有重叠的圆构成，从外到内采样点分布模式为：6、6、6、6、6、6、6、1，其中的1表示最中心的点，即特征点，其它的圆心是采样点。对于每个采样点，首先要进行高斯平滑，高斯核的大小与当前采样点的同心圆半径成正比。相邻采样点的感受域(即以采样点为中心，高斯核为半径的圆)设计成重叠的结构，以获取更充分的信息。</p><p>最终生成的FREAK描述子是二进制比特串，由感受域对的差值强度阈值比较结果级联而成。假设F为某个特征点的FREAK二进制描述子，则：</p><disp-formula id="hanspub.16795-formula118"><label>(2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-2670068x23_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.16795-formula119"><label>(3)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-2670068x24_hanspub.png"  xlink:type="simple"/></disp-formula><p>式中：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x25_hanspub.png" xlink:type="simple"/></inline-formula>是感受域对，N表示期望的二进制特征点描述子长度，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x26_hanspub.png" xlink:type="simple"/></inline-formula>、<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x27_hanspub.png" xlink:type="simple"/></inline-formula>分别表示采样点经过高斯平滑后的感受域对中前后感受域的强度值，r1、r2分别代表感受域对前后感受域的半径。</p><p>FREAK描述符采样的位置和半径随着尺度的变化使其具有尺度不变性，而强度对比生成二进制描述子使其对于光照也具有不变性，对每个采样点进行高斯模糊在一定程度上能削弱噪声的影响。所以理论上FREAK特征是对于各种变换和噪声均具有鲁棒性的局部不变特征。</p></sec><sec id="s6"><title>4. FLANN匹配搜索</title><p>FREAK算子生成的二进制描述符是由0和1组成，使用汉明距离表示参考图像与待配准图像描述符之间的关系。当两个二进制描述符之间的汉明距离小于阈值T<sub>d</sub>时，认为其是匹配的一对特征点，否则，认为其不是匹配的特征点。当然，选择的阈值T<sub>d</sub>要适中，过大容易导致误匹配点的增多，过小小则会导致正确匹配点数量的下降，严重的话甚至无法找到匹配关系。针对FREAK特征向量维数比较高，参考文献 [<xref ref-type="bibr" rid="hanspub.16795-ref12">12</xref>] 提出了基于FLANN的高维二进制特征最近邻搜索算法，显著提高搜索效率。本文首先寻找最近邻与次近邻特征点。通过FLANN搜索算法去查找每个特征点的最近邻与次近邻特征点。假设参考遥感图像中特征点K的最近邻和次近邻分别为待配准遥感图像中的两个特征点p与q，则设定它们与特征点k的汉明距离分别为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x28_hanspub.png" xlink:type="simple"/></inline-formula>与<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x29_hanspub.png" xlink:type="simple"/></inline-formula>；其次，通过计算匹配比率<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x30_hanspub.png" xlink:type="simple"/></inline-formula>=<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x31_hanspub.png" xlink:type="simple"/></inline-formula>，如果<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x32_hanspub.png" xlink:type="simple"/></inline-formula>小于之前规定的阈值<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x33_hanspub.png" xlink:type="simple"/></inline-formula>(本文取值0.5)，则可以认为点k与点i是匹配的特征点对，否则匹配失败，利用该方法获得粗略的匹配特征点对集；然后利用相似三角形方法对之前获取的特征点集剔除无匹配点，获得最佳匹配特征点对集；最后采用最小二乘法计算图像配准变换参数。</p></sec><sec id="s7"><title>5. 基于相似三角形的误匹配点去除</title><p>上文介绍了基于FLANN的高维二进制特征最近邻搜索算法，本文利用该方法获得粗略的匹配特征点对集，并且运用基于相似三角形的错误匹配点剔除算法来剔除错误匹配点。文献 [<xref ref-type="bibr" rid="hanspub.16795-ref13">13</xref>] 提出了采用相似三角形去除SIFT错误匹配点的方法，该方法在剔除错误匹配点的同时能够较好的保留正确的匹配对，并且速度比RANSANC方法有大幅度提高。但该方法只是从满足相似三角形的任意三对匹配点中选取的基准点，其并不具有代表性，依然会存在错误匹配对的问题，本文对该方法选取基准点步骤进行了改进，方法核心步骤如下：</p><p>1) 通过最近邻搜索算法FLANN获取粗略的图像匹配对后，根据欧式距离最小原则从最匹配的点开始，对其中任意相邻的三对匹配对<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x34_hanspub.png" xlink:type="simple"/></inline-formula>中对应的三点组成两个三角形，其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x35_hanspub.png" xlink:type="simple"/></inline-formula>是待配准图中的点，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-2670068x36_hanspub.png" xlink:type="simple"/></inline-formula>是参考图中的点；验证其是否满足相似三角形条件(允许有微小的误差)，找到两组满足相似三角条件的匹配点对，然后再对该两组六对匹配点对中做任意三三组合，判断其所有可能组合中是否有60%以上的组合满足相似三角形条件，如果达到60%以上，则选取其中任意的两对匹配点对作为基准点对，若不满足则重复上述操作；</p><p>2) 除了上述所获取的作为基准点对的匹配点对外，遍历所获取的粗略图像匹配对，从中逐一选取任一自认为是正确的匹配点对，判断与基准点对一起构成的两组三角形是否相似，若相似，则可以认为该匹配点对是真正匹配的，否则忽略该匹配点对。利用相似三角形方法获得最佳匹配特征点对集后；最后采用最小二乘法计算图像配准变换参数。本文针对遥感图像的配准整个过程如图5所示。</p></sec><sec id="s8"><title>6. 实验结果及分析</title><sec id="s8_1"><title>6.1. 实验测试</title><p>本文选取两组遥感图像作为实验用图。第一组实验图是通过Google Earth针对同一乡村区域拍摄的两幅遥感图像，如图6所示，第二组是针对某城镇采集的两幅遥感图像，如图7所示，图(a)作为参考图像，图(b)作为待配准图像，且两组序列图大小均为512 &#215; 512，试验环境：操作系统Windows7，CPU的主频为2.80 GHz，VS2010+OpenCV2.4.2。图8中(a)、(b)分别为图6和图7配准图像序列经过本文方法错误匹配剔除后得到的匹配结果。</p></sec><sec id="s8_2"><title>6.2. 与各配准算法性能比较</title><p>为了验证本算法性能优势，本文与其它5种配准算法在匹配率与配准速率两方面进行了比较。其中SIFT和SURF利用Low [<xref ref-type="bibr" rid="hanspub.16795-ref11">11</xref>] 的比值方法进行控制点匹配，比值阈值设置为0.8，ORB为Ethan Rublee所提方法，其它两种分别为FAST检测 + FREAK描述 + Hamming匹配、SURF检测 + FREAK描述 + Hamming匹配，本文方法通过公式(1)求得两组遥感图像阈值t分别为25和20。此五种算法均采用RANSANC随机一致性进行错误匹配剔除，各算法的性能比较如表1~4所示。</p><p>由表1与表3可知，本文提出的改进配准算法在提取的总匹配点数不如SIFT、SURF方法，但均高于其它三种算法；匹配正确率上接近SIFT算法，均高于其它四种配准算法。另外，由表2与表4可知本文方法相比于其它几种配准算法，在运行时间行上有了大幅度的提高。由此可知，本文所提遥感图像匹配方法不仅提取的匹配点数相对较多且匹配正确率高，还具有较好的实时性。</p><p>图5. 遥感图像配准方法流程图</p><p>图6. 第一组遥感图像。(a) 参考图像；(b) 待配准图像</p><p>图7. 第二组遥感图像。(a) 参考图像图；(b) 待配准图像</p><p>图8. 图像序列匹配结果图。(a) 图6图像序列匹配结果图；(b) 图7图像序列匹配结果图</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The matching performance comparison about image sequence for some algorithms in <xref ref-type="fig" rid="fig">Figure </xref></title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >总匹配点数</th><th align="center" valign="middle" >正确匹配点数</th><th align="center" valign="middle" >正确率(%)</th></tr></thead><tr><td align="center" valign="middle" >SIFT</td><td align="center" valign="middle" >957</td><td align="center" valign="middle" >729</td><td align="center" valign="middle" >76.2</td></tr><tr><td align="center" valign="middle" >SURF</td><td align="center" valign="middle" >643</td><td align="center" valign="middle" >417</td><td align="center" valign="middle" >64.9</td></tr><tr><td align="center" valign="middle" >ORB</td><td align="center" valign="middle" >296</td><td align="center" valign="middle" >135</td><td align="center" valign="middle" >45.7</td></tr><tr><td align="center" valign="middle" >FAST + FREAK</td><td align="center" valign="middle" >346</td><td align="center" valign="middle" >195</td><td align="center" valign="middle" >56.4</td></tr><tr><td align="center" valign="middle" >SURF + FREAK</td><td align="center" valign="middle" >584</td><td align="center" valign="middle" >326</td><td align="center" valign="middle" >55.8</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >395</td><td align="center" valign="middle" >265</td><td align="center" valign="middle" >67.1</td></tr></tbody></table></table-wrap><p>表1. 图6图像序列各算法配准匹配率性能比较</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> The speed performance comparison about image sequence for some algorithms in <xref ref-type="fig" rid="fig">Figure </xref></title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >检测(秒)</th><th align="center" valign="middle" >提取(秒)</th><th align="center" valign="middle" >匹配(秒)</th><th align="center" valign="middle" >总用时(秒)</th></tr></thead><tr><td align="center" valign="middle" >SIFT</td><td align="center" valign="middle" >0.565</td><td align="center" valign="middle" >0.637</td><td align="center" valign="middle" >1.378</td><td align="center" valign="middle" >2.580</td></tr><tr><td align="center" valign="middle" >SURF</td><td align="center" valign="middle" >0.314</td><td align="center" valign="middle" >0.285</td><td align="center" valign="middle" >0.392</td><td align="center" valign="middle" >0.991</td></tr><tr><td align="center" valign="middle" >ORB</td><td align="center" valign="middle" >0.046</td><td align="center" valign="middle" >0.035</td><td align="center" valign="middle" >0.197</td><td align="center" valign="middle" >0.278</td></tr><tr><td align="center" valign="middle" >FAST + FREAK</td><td align="center" valign="middle" >0.007</td><td align="center" valign="middle" >0.275</td><td align="center" valign="middle" >0.237</td><td align="center" valign="middle" >0.519</td></tr><tr><td align="center" valign="middle" >SURF + FREAK</td><td align="center" valign="middle" >0.411</td><td align="center" valign="middle" >0.060</td><td align="center" valign="middle" >0.412</td><td align="center" valign="middle" >0.883</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >0.005</td><td align="center" valign="middle" >0.065</td><td align="center" valign="middle" >0.156</td><td align="center" valign="middle" >0.226</td></tr></tbody></table></table-wrap><p>表2. 图6图像序列各算法配准速率性能比较</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> The matching performance comparison about image sequence for some algorithms in <xref ref-type="fig" rid="fig">Figure </xref></title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >总匹配点数</th><th align="center" valign="middle" >正确匹配点数</th><th align="center" valign="middle" >正确率(%)</th></tr></thead><tr><td align="center" valign="middle" >SIFT</td><td align="center" valign="middle" >828</td><td align="center" valign="middle" >615</td><td align="center" valign="middle" >74.3</td></tr><tr><td align="center" valign="middle" >SURF</td><td align="center" valign="middle" >543</td><td align="center" valign="middle" >330</td><td align="center" valign="middle" >60.8</td></tr><tr><td align="center" valign="middle" >ORB</td><td align="center" valign="middle" >264</td><td align="center" valign="middle" >107</td><td align="center" valign="middle" >40.4</td></tr><tr><td align="center" valign="middle" >FAST + FREAK</td><td align="center" valign="middle" >306</td><td align="center" valign="middle" >164</td><td align="center" valign="middle" >53.5</td></tr><tr><td align="center" valign="middle" >SURF + FREAK</td><td align="center" valign="middle" >513</td><td align="center" valign="middle" >265</td><td align="center" valign="middle" >51.7</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >256</td><td align="center" valign="middle" >171</td><td align="center" valign="middle" >66.7</td></tr></tbody></table></table-wrap><p>表3. 图7图像序列各算法配准性能比较</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> The speed performance comparison about image sequence for some algorithms in <xref ref-type="fig" rid="fig">Figure </xref></title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >检测(秒)</th><th align="center" valign="middle" >提取(用时)</th><th align="center" valign="middle" >匹配(秒)</th><th align="center" valign="middle" >总用时(秒)</th></tr></thead><tr><td align="center" valign="middle" >SIFT</td><td align="center" valign="middle" >0.478</td><td align="center" valign="middle" >0.526</td><td align="center" valign="middle" >1.127</td><td align="center" valign="middle" >2.131</td></tr><tr><td align="center" valign="middle" >SURF</td><td align="center" valign="middle" >0.225</td><td align="center" valign="middle" >0.273</td><td align="center" valign="middle" >0.286</td><td align="center" valign="middle" >0.784</td></tr><tr><td align="center" valign="middle" >ORB</td><td align="center" valign="middle" >0.061</td><td align="center" valign="middle" >0.044</td><td align="center" valign="middle" >0.154</td><td align="center" valign="middle" >0.413</td></tr><tr><td align="center" valign="middle" >FAST + FREAK</td><td align="center" valign="middle" >0.006</td><td align="center" valign="middle" >0.136</td><td align="center" valign="middle" >0.112</td><td align="center" valign="middle" >0.254</td></tr><tr><td align="center" valign="middle" >SURF + FREAK</td><td align="center" valign="middle" >0.294</td><td align="center" valign="middle" >0.055</td><td align="center" valign="middle" >0.299</td><td align="center" valign="middle" >0.648</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >0.004</td><td align="center" valign="middle" >0.056</td><td align="center" valign="middle" >0.124</td><td align="center" valign="middle" >0.184</td></tr></tbody></table></table-wrap><p>表4. 图7图像序列各算法配准性能比较</p></sec></sec><sec id="s9"><title>7. 结束语</title><p>本文针对传统方法速度较慢，主要做了两点改进：第一点，将自适应通用加速分割角点检测算法AGAST与FREAK算法相结合；第二点，基于FLANN快速匹配搜索结合相似三角形的错误匹配对快速剔除。针对FAST算法的不足，本文使用AGAST算法进行角点快速检测，且对AGAST角点检测的阈值获取进行了改进，并结合二进制描述子FREAK算法，实现遥感图像的配准。在实现遥感图像配准的过程中，针对误匹配问题，改进了相似三角形方法剔除误匹配点，最后利用最小二乘拟合方法估算几何空间变换参数，最终实现遥感图像的配准。本文选取两组遥感图像进行了测试，并与传统的算法进行了比较。根据上述试验结果可以得出本文方法不仅保证遥感图像配准精度，而且大幅度提高遥感图像的配准速度，基本满足对遥感图像配准的实时性要求。本文方法针对遥感图像的配准精度虽然有所提高，但相对来说配准精度依然不足，后续工作可以针对遥感图像配准做深入研究，提高图像的配准精度。</p></sec><sec id="s10"><title>文章引用</title><p>钱社军,王正勇,何小海. 基于角点特征的遥感图像快速配准 Fast Registration of Remote Sensing Image Based on Corner Feature[J]. 图像与信号处理, 2016, 05(01): 43-51. http://dx.doi.org/10.12677/JISP.2016.51006</p></sec><sec id="s11"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.16795-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Lowe, D.G. (2004) Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision, 60, 91-110. &lt;br&gt;http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94</mixed-citation></ref><ref id="hanspub.16795-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">刘志文, 刘定生, 刘鹏. 应用尺度不变特征变换的多源遥感影像特征点匹配[J]. 光学精密工程, 2013, 21(8): 2146-2153.</mixed-citation></ref><ref id="hanspub.16795-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Bay, H., Tuytelaars, T. and Van Gool, L. (2006) SURF: Speeded up Robust Features. Proceedings of the European Conference on Computer Vision, 404-417.</mixed-citation></ref><ref id="hanspub.16795-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Rosten, E. and Drummond, T. (2010) Faster and Better: A Machine Learning Approach to Comer Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32, 105-119. &lt;br&gt;http://dx.doi.org/10.1109/TPAMI.2008.275</mixed-citation></ref><ref id="hanspub.16795-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">高洪波, 王洪玉, 刘晓凯. 一种基于分层学习的关键点匹配算法[J]. 电子与信息报, 2013(11): 2751-2757.</mixed-citation></ref><ref id="hanspub.16795-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">高洪波. 智能视频监控中的行人跟踪算法研究[D]. 大连: 大连理工大学, 2013.</mixed-citation></ref><ref id="hanspub.16795-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Calonder, M., Lepetit, V., Ozuysal, M., et al. (2010) BRIEF: Computing a Local Binary Descriptor Very Fast. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34, 1281-1298.</mixed-citation></ref><ref id="hanspub.16795-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Leutenegger, S., Chli, M. and Siegwart, R.Y. (2011) BRISK: Binary Robust Invariant Scalable Keypoints. IEEE International Conference on Computer Vision (ICCV), 58, 2548-2555. &lt;br&gt;http://dx.doi.org/10.1109/ICCV.2011.6126542</mixed-citation></ref><ref id="hanspub.16795-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Rublee, E., Rabaud, V., Konolige, K., et al. (2011) ORB: An Efficient Alternative to SIFT or SURF. International Conference on Computer Vision, Barcelona, 2564-2571.</mixed-citation></ref><ref id="hanspub.16795-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Mair, E., Hager, G.D., Burschka, D., Suppa, M. and Hirzinger, G. (2010) Adaptive and Generic Corner Detection Based on the Accelerated Segment Test. European Conference on Computer Vision, 183-196.</mixed-citation></ref><ref id="hanspub.16795-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Alahi, A., Ortiz, R. and Vandergheynst, P. (2012) FREAK: Fast Retina Keypoint. IEEE Conference on Computer Vision and Pattern Recognition, 157, 510-517. &lt;br&gt;http://dx.doi.org/10.1109/CVPR.2012.6247715</mixed-citation></ref><ref id="hanspub.16795-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Muja, M. and Lowe, D.G. (2009) Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration. International Conference on Computer Vision Theory and Application, 331-340.</mixed-citation></ref><ref id="hanspub.16795-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">张东兴, 祝明波, 等. 基于相似三角形的SIFT错误匹配点剔除算法研究[J]. 计算机工程与科学, 2012(4): 66-70.</mixed-citation></ref></ref-list></back></article>