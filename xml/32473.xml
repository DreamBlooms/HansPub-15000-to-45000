<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2019.910208</article-id><article-id pub-id-type="publisher-id">CSA-32473</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20191000000_23536644.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  MPI + OpenMPI混合编程的实现与性能分析
  Implementation and Performance Analysis of MPI + Open MPI Hybrid Programming
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>范</surname><given-names>培勤</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>林</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>唐</surname><given-names>帅</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>敬一</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>海军潜艇学院，山东 青岛</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>30</day><month>09</month><year>2019</year></pub-date><volume>09</volume><issue>10</issue><fpage>1859</fpage><lpage>1866</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   为充分发挥SMP集群的计算能力，结合SMP集群并行系统体系架构的特点，采用MPI + OpenMP混合并行编程模式，研究了基于SMP集群的多级混合并行程序的设计和实现方法。通过节点间使用MPI消息传递接口实现任务的粗粒度划分，节点内使用OpenMP实现细粒度任务并行划分的方法，开发了混合并行编程性能测试用例，并对其进行了测试和分析。测试结果表明，MPI + OpenMP混合编程模式，能够有效利用SMP集群节点间和节点内多级并行机制，充分发挥消息传递编程模型和共享内存编程模型各自的优势，可有效提升SMP集群的计算效率。 In order to give full play to the computing power of SMP cluster, combined with the characteristics of SMP cluster parallel system architecture, the design and implementation of multilevel hybrid parallel program based on SMP cluster are studied by using MPI + OpenMP hybrid parallel programming mode. The performance test case of hybrid parallel programming is developed by using MPI to realize the rough degree partition of tasks between nodes and OpenMP to realize the fine degree partition of tasks in nodes. The test result shows that MPI + OpenMP hybrid programming mode can effectively make use of the multi-level parallel mechanism of SMP cluster, and give full play to the advantages of the two programming models and effectively improve the computing efficiency of SMP cluster. 
  
 
</p></abstract><kwd-group><kwd>集群，混合编程，并行计算，性能分析, Cluster</kwd><kwd> Hybrid Programming</kwd><kwd> Parallel Computation</kwd><kwd> Performance Analysis</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>MPI + OpenMPI混合编程的实现与性能分析<sup> </sup></title><p>范培勤，张林，唐帅，刘敬一</p><p>海军潜艇学院，山东 青岛</p><p><img src="//html.hanspub.org/file/6-1541524x1_hanspub.png" /></p><p>收稿日期：2019年9月19日；录用日期：2019年10月4日；发布日期：2019年10月11日</p><disp-formula id="hanspub.32473-formula67"><graphic xlink:href="//html.hanspub.org/file/6-1541524x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>为充分发挥SMP集群的计算能力，结合SMP集群并行系统体系架构的特点，采用MPI + OpenMP混合并行编程模式，研究了基于SMP集群的多级混合并行程序的设计和实现方法。通过节点间使用MPI消息传递接口实现任务的粗粒度划分，节点内使用OpenMP实现细粒度任务并行划分的方法，开发了混合并行编程性能测试用例，并对其进行了测试和分析。测试结果表明，MPI + OpenMP混合编程模式，能够有效利用SMP集群节点间和节点内多级并行机制，充分发挥消息传递编程模型和共享内存编程模型各自的优势，可有效提升SMP集群的计算效率。</p><p>关键词 :集群，混合编程，并行计算，性能分析</p><disp-formula id="hanspub.32473-formula68"><graphic xlink:href="//html.hanspub.org/file/6-1541524x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/6-1541524x7_hanspub.png" /> <img src="//html.hanspub.org/file/6-1541524x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>SMP集群作为高性能计算系统成为目前应用最为广泛的高性能并行计算平台。并行程序设计模型通常可以分为共享内存和消息传递两类，MPI和OpenMP分别是这两类并行程序设计模型的代表，也是目前应用最为广泛的并行程序开发环境。MPI基于大粒度的进程级并行，通过消息传递实现进程间的数据交互和协同控制，具有扩展性强、便于移植等优点，但实现难度较大；OpenMP基于线程级细粒度并行，通过内存共享，使用显示指导语句，实现应用问题的并行求解，具有实现难度小的优点，但可扩展性差 [<xref ref-type="bibr" rid="hanspub.32473-ref1">1</xref>] 。笔者在实际的科研和教学活动中发现，MPI并行编程环境在并行求解计算量小、通信量大的应用问题时常常表现出较差的可扩展性，这是由于随着进程规模的增加，并行计算部分花费的时间虽然逐渐减小，但进程间通信所花费的时间却在不断地增加，并逐渐在整个计算时间中占据主导地位。针对这个问题，本文结合SMP集群的体系结构特点，开展了基于MPI + OpenMP混合并行编程方法的研究，充分发挥MPI和OpenMP并行编程环境的优点，提高并行程序的效率，并对比分析了MPI与MPI + OpenMP混合程序的性能优劣，总结了造成性能差异的主要原因。</p></sec><sec id="s4"><title>2. 并行编程模式</title><sec id="s4_1"><title>2.1. OpenMP (Open Multi-Processing)</title><p>OpenMP支持多线程并发编程和C、C++、FORTRAN等多种编程语言，是一种共享存储方式的并行编程标准，可在Linux、UNIX等操作系统中运行。OpenMP采用标准的Fork/Join式并行执行模型。OpenMP通过在代码中加入指导语句来控制代码的并行处理，实际并行过程由底层支持库来实现。OpenMP基本的结构包括：parallel、工作共享、master和同步、任务等结构，其中parallel结构是最基本的指导语句，所有的OpenMP程序都会用到 [<xref ref-type="bibr" rid="hanspub.32473-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.32473-ref2">2</xref>] 。</p></sec><sec id="s4_2"><title>2.2. MPI (Message Passing Interface)</title><p>MPI是由工业、科研和政府部门等联合建立的消息传递并行编程标准 [<xref ref-type="bibr" rid="hanspub.32473-ref1">1</xref>] 。MPI凭借其标准化、可移植、可扩展等优点，成为目前最通用的并行编程标准。MPI1.0版于1994年发布，1998年MPI2.0版本发布，在MPI2.0中，共有287个调用接口。</p></sec><sec id="s4_3"><title>2.3. MPI + OpenMP</title><p>MPI + OpenMP混合编程模型通过在节点内使用共享存储模型，节点外采用消息传递模型实现具体问题的多级并行求解。该模型将OpenMP和MPI模型有机融合到一起，通过充分发挥两种模型的各自的优点，从而获得更高的计算性能和可扩展性 [<xref ref-type="bibr" rid="hanspub.32473-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.32473-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.32473-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.32473-ref6">6</xref>] 。具体来讲：节点外采用消息传递，解决了节点间OpenMP模型计算结果无法交互的问题；节点内采用共享存储，减少进程的数量，降低了进程通信所花费的时间。经测试，该模型对大规模循环问题的并行处理具有较高的并行效率和可扩展性。</p></sec></sec><sec id="s5"><title>3. MPI + OpenMP混合并行编程的实现</title><sec id="s5_1"><title>3.1. 算法设计</title><p>本文选用圆周率π数值积分并行求解算法来分析MPI + OpenMP混合编程的具体实现。π数值求解过程物理意义清晰，其计算规模灵活可调，虽然其并行实现过程虽相对简单，仍不失为用于并行程序计算性能测试分析的良好测例。</p><p>求解主要围绕着循环计算各部分梯形面积展开，其算法设计也主要针对循环过程的二级并行划分开展。具体如下：</p><p>(1) 节点间任务划分</p><p>采用均匀划分或交叉划分方法，将计算区间S划分为np个小区间 s i , S = ∪ s i , i = 1 , 2 , ⋯ , n p ，若采用交叉划分，每个区间所含的循环次数尽量为节点内核心数的整数倍。</p><p>(2) 节点内任务划分</p><p>在每个计算节点内调用OpenMP for并行制导语句，将所分配的循环分配给不同的线程，期间需要调整每个线程分配的循环次数，尽可能做到负载均衡，求解流程如图1所示。</p><p>图1. π值MPI + OpenMP混合并行求解流程图</p></sec><sec id="s5_2"><title>3.2. 算法实现</title><p>算法的具体实现过程分为以下几步：</p><p>(1) 设定线程数</p><p>根据计算节点CPU配置，指定每个节点内部的线程数，本文中为24个。</p><p>(2) 计算区间划分(第一级并行MPI)</p><p>采用均匀划分方法，将循环分配给不同的进程(第一级并行处理)。如下所示：</p><p>n=N/np;//N为总循环次数，np为进程数</p><p>for (i=myid*n;i</p><p>{</p><p>#pragma omp parallel private(tid)</p><p>{</p><p>第二级并行;</p><p>}</p><p>}</p><p>(3)计算任务划分(第二级并行OpenMP)</p><p>调用OpenMP for并行制导语句，将分配的计算区间内的计算任务分配给不同的线程(第二级并行处理)。如下所示：</p><p>#pragma omp parallel private(tid)</p><p>{</p><p>tid=omp_get_thread_num();</p><p>#pragma omp for</p><p>for(j=0;j</p><p>{</p><p>s j = f ( j ) ;</p><p>S i = S i + s j ;</p><p>}</p><p>}</p><p>(4) 数据收集和输出</p><p>每个节点分配的计算任务完成后，调用MPI_REDUCE()，将计算结果收集到进程0，并将计算结果输出。</p></sec></sec><sec id="s6"><title>4. 性能测试和分析</title><sec id="s6_1"><title>4.1. 测试环境</title><p>本文利用SMP集群对程序进行了测试，该集群共314个计算节点，节点内2颗Intel Xeon E5-2680 v3 12核CPU，集群理论峰值计算能力310TFlops，Linback实测计算能力210TFlops。</p></sec><sec id="s6_2"><title>4.2. 测试结果</title><p>测试分为节点内测试、节点间测试两部分，节点内测试主要开展节点内MPI、OpenMP并行程序性能测试，节点间测试主要用于开展跨节点MPI、MPI + OpenMP并行程序性能测试。</p><sec id="s6_2_1"><title>4.2.1. 节点内测试</title><p>表1给出了使用一个计算节点，分别用3、6、12、24个线程/进程运行OpenMP、MPI并行计算程序时间统计情况(每种情况运行10次，取计算时间最小值，积分次数N = 100,000,000,000)。图2为表1对应的加速比。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Parallel computing time of MPI and OpenMP parallel progra</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >核心数 实现方式</th><th align="center" valign="middle" >3</th><th align="center" valign="middle" >6</th><th align="center" valign="middle" >12</th><th align="center" valign="middle" >24</th></tr></thead><tr><td align="center" valign="middle" >OpenMP</td><td align="center" valign="middle" >94.22</td><td align="center" valign="middle" >47.12</td><td align="center" valign="middle" >23.57</td><td align="center" valign="middle" >11.90</td></tr><tr><td align="center" valign="middle" >MPI</td><td align="center" valign="middle" >94.71</td><td align="center" valign="middle" >47.37</td><td align="center" valign="middle" >24.78</td><td align="center" valign="middle" >13.14</td></tr><tr><td align="center" valign="middle"  colspan="5"  >备注：单核心只运行一个进程或线程</td></tr></tbody></table></table-wrap><p>表1. 节点内OpenMP、MPI并行程序计算时间</p><p>图2. 表1对应的加速比图</p><p>从表1和图2可以看出，单节点内OpenMP与MPI并行程序性能基本一致，都具有比较高的并行加速比，OpenMP并行程序计算性能略高。</p></sec><sec id="s6_2_2"><title>4.2.2. 节点间测试</title><p>表2给出了分别用1、2、4、8、16、32个节点运行π值MPI、MPI + OpenMP并行程序计算运行时间统计情况(积分次数N = 100,000,000,000)。其中：MPI程序每个节点分配24个进程，MPI + OpenMP每个节点分配1个进程，24个线程。对于MPI + OpenMP程序，核心数 = 节点数 * 单节点核心数；MPI程序，核心数 = 进程数。图3为表2对应的加速比。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Parallel computing time of MPI + OpenMP and MPI parallel progra</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >核心数 实现方式</th><th align="center" valign="middle" >24</th><th align="center" valign="middle" >48</th><th align="center" valign="middle" >96</th><th align="center" valign="middle" >192</th><th align="center" valign="middle" >384</th><th align="center" valign="middle" >768</th></tr></thead><tr><td align="center" valign="middle" >MPI + OpenMP</td><td align="center" valign="middle" >11.96</td><td align="center" valign="middle" >6.25</td><td align="center" valign="middle" >3.46</td><td align="center" valign="middle" >2.55</td><td align="center" valign="middle" >1.49</td><td align="center" valign="middle" >1.73</td></tr><tr><td align="center" valign="middle" >MPI</td><td align="center" valign="middle" >13.14</td><td align="center" valign="middle" >7.39</td><td align="center" valign="middle" >4.84</td><td align="center" valign="middle" >4.18</td><td align="center" valign="middle" >4.08</td><td align="center" valign="middle" >7.41</td></tr><tr><td align="center" valign="middle"  colspan="7"  >备注：单核心只运行一个进程或线程</td></tr></tbody></table></table-wrap><p>表2. 节点间MPI + OpenMP、MPI并行程序计算时间</p><p>图3. 表2对应的加速比图</p><p>从表2、图4可以看出，MPI + OpenMP和MPI程序加速比均呈现先增加后减小的趋势，但MPI + OpenMP程序的加速比随计算规模的增加基本保持稳定，且高于MPI程序近4倍。因此，与MPI并行程序相比，MPI + OpenMP并行程序具有更快的计算速度和更好的可扩展性。</p><p>为了进一步分析两种并行程序计算性能差异的原因，对MPI + OpenMP和MPI并行程序执行过程中时间的分布按照：并行计算时间、通信时间两部分进行统计，表3、表4分别给出了MPI和MPI + OpenMP并行程序执行过程中不同部分消耗时间的统计表。图4、图5分别为表3、表4对应的时间分布表。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Parallel computing distribute time of MPI parallel progra</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >进程数 时间分布</th><th align="center" valign="middle" >24</th><th align="center" valign="middle" >48</th><th align="center" valign="middle" >96</th><th align="center" valign="middle" >192</th><th align="center" valign="middle" >384</th><th align="center" valign="middle" >768</th></tr></thead><tr><td align="center" valign="middle" >并行计算时间</td><td align="center" valign="middle" >11.95</td><td align="center" valign="middle" >6.10</td><td align="center" valign="middle" >3.14</td><td align="center" valign="middle" >2.17</td><td align="center" valign="middle" >1.43</td><td align="center" valign="middle" >0.64</td></tr><tr><td align="center" valign="middle" >进程通信时间</td><td align="center" valign="middle" >1.09</td><td align="center" valign="middle" >1.29</td><td align="center" valign="middle" >1.70</td><td align="center" valign="middle" >2.01</td><td align="center" valign="middle" >2.65</td><td align="center" valign="middle" >6.77</td></tr><tr><td align="center" valign="middle" >合计</td><td align="center" valign="middle" >13.14</td><td align="center" valign="middle" >7.39</td><td align="center" valign="middle" >4.84</td><td align="center" valign="middle" >4.18</td><td align="center" valign="middle" >4.08</td><td align="center" valign="middle" >7.41</td></tr><tr><td align="center" valign="middle"  colspan="7"  >备注：单节点启动24个进程</td></tr></tbody></table></table-wrap><p>表3. MPI并行程序计算时间分配表</p><p>从表3可以看出，随着计算规模的增加，MPI程序并行计算部分花费时间由11.95 s减少为0.64 s，降低近19倍，占总计算时间的百分比由91.0%减小为8.6%；进程间通信消耗的时间由1.09 s增加至6.77 s，提高6.2倍，占总计算时间的9.0%提高至91.4%。进程间通信开销远高于并行计算所花费的时间，极大的制约了并行效率的提高。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Parallel computing distribute time of MPI + OpenMP parallel progra</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >进程数 时间分布</th><th align="center" valign="middle" >1</th><th align="center" valign="middle" >2</th><th align="center" valign="middle" >4</th><th align="center" valign="middle" >8</th><th align="center" valign="middle" >16</th><th align="center" valign="middle" >32</th></tr></thead><tr><td align="center" valign="middle" >并行计算时间</td><td align="center" valign="middle" >11.92</td><td align="center" valign="middle" >6.10</td><td align="center" valign="middle" >3.32</td><td align="center" valign="middle" >2.39</td><td align="center" valign="middle" >1.28</td><td align="center" valign="middle" >1.27</td></tr><tr><td align="center" valign="middle" >进程通信时间</td><td align="center" valign="middle" >0.04</td><td align="center" valign="middle" >0.15</td><td align="center" valign="middle" >0.14</td><td align="center" valign="middle" >0.16</td><td align="center" valign="middle" >0.21</td><td align="center" valign="middle" >0.46</td></tr><tr><td align="center" valign="middle" >合计</td><td align="center" valign="middle" >11.96</td><td align="center" valign="middle" >6.25</td><td align="center" valign="middle" >3.46</td><td align="center" valign="middle" >2.55</td><td align="center" valign="middle" >1.49</td><td align="center" valign="middle" >1.73</td></tr><tr><td align="center" valign="middle"  colspan="7"  >备注：单节点启动1个进程，24个线程</td></tr></tbody></table></table-wrap><p>表4. MPI + OpenMP并行程序计算时间分配</p><p>从表4可以看出，在同样的计算规模下，MPI + OpenMP程序并行部分计算花费时间由11.92 s减少为1.27秒，占总计算时间的百分比由99.7%减少为73.4%；进程通信部分消耗的时间由0.04 s增加为0.46 s，占总计算时间的百分比由0.3%提高至26.6%。采用MPI + OpenMP混合并行程序方法，有效降低了进程间通信的时间开销，大大提高了程序的并行执行效率和可扩展性。</p><p>图4. 表3对应的时间分布图</p><p>图5. 表4对应的时间分布图</p><p>从表3、表4和图4、图5可以见，随着进程数的增加，MPI + OpenMP两种并行编程方式下，应用问题并行部分的计算时间均逐渐减小，但MPI + OpenMP并行程序的通信消耗时间只占MPI并行程序的6.8%。由此可见，MPI + OpenMP混合并行程序在很好地继承了两种并行编程环境的优点的同时，可以有效克服两种并行编程环境的缺点，可以有效提高并行程序的效率。</p></sec></sec></sec><sec id="s7"><title>5. 结束语</title><p>MPI + OpenMP混合编程模型通过节点间消息传递，节点内数据共享的方式，将分布式存储系统和共享式存储系统的优点互相结合，既能解决MPI并行程序性能随进程数增加而降低的问题，同时又克服了OpenMP并行程序扩展性差的问题，可有效地发挥SMP集群的计算性能。并行程序的性能受并行机体系结构、编程环境、问题自身结构、并行程序的优化、编程人员的习惯等多重因素制约，很难给出统一的编程和性能评价标准，需结合具体的应用问题，合理地采取编程和优化方式，系统地进行设计。</p></sec><sec id="s8"><title>基金项目</title><p>本文基金资助项目为国防科技创新特区项目(18-H863-05-ZT- 001-012-06) ；水中军用目标特性国防科技重点实验室基金(614240704040317)。</p></sec><sec id="s9"><title>文章引用</title><p>范培勤,张 林,唐 帅,刘敬一. MPI + OpenMPI混合编程的实现与性能分析Implementation and Performance Analysis of MPI + Open MPI Hybrid Programming[J]. 计算机科学与应用, 2019, 09(10): 1859-1866. https://doi.org/10.12677/CSA.2019.910208</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.32473-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">迟学斌, 王彦棡, 王钰, 等. 并行计算与实现技术[M]. 北京: 科学出版社, 2015.</mixed-citation></ref><ref id="hanspub.32473-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">罗秋明, 明仲, 刘刚, 等. OpenMP编译原理及实现技术[M]. 北京: 清华大学出版社, 2012.</mixed-citation></ref><ref id="hanspub.32473-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">朱昶胜, 邓新, 冯力, 李浩, 等. MPI + OpenMP环境下的二元合金三维相场模型的并行方法[J]. 兰州理工大学学报, 2017, 43(4): 16-22.</mixed-citation></ref><ref id="hanspub.32473-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">杨书博, 乔先孝, 车小花. 基于MPI + OpenMP的三维弹性波方程混合并行有限差分算法[J]. 应用声学, 2018, 31(1): 75-83.</mixed-citation></ref><ref id="hanspub.32473-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">唐兵, Laurent BOBELIN, 贺海武. 基于MPI和OpenMP混合编程的非负矩阵分解并行算法[J]. 计算机科学, 2017, 44(3): 51-55.</mixed-citation></ref><ref id="hanspub.32473-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">刘超, 祝永志. 多核SMP集群混合并行编程技术的研究[J]. 微型机与应用, 2017, 36(4): 18-21.</mixed-citation></ref></ref-list></back></article>