<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.118209</article-id><article-id pub-id-type="publisher-id">CSA-44334</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210800000_39291535.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于循环生成对抗网络的图像转换
  Image Conversion Based on Cyclic Generative Confrontation Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>古</surname><given-names>彭</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>宝镇</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>戚</surname><given-names>馨文</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>怀诚</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>嘉帆</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>西藏大学，西藏 拉萨</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>02</day><month>08</month><year>2021</year></pub-date><volume>11</volume><issue>08</issue><fpage>2042</fpage><lpage>2050</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   在无监督学习领域中生成对抗网络是近几年发展速度较快的一个研究方向，其主要特征是用一种概率估计的方式去逼近未知分布的模型。运用这种模型可以避免复杂的运算和编码，特别是在计算机视觉方面，可以生成质量较好的图片。本文基于循环生成对抗网络，训练一个风格迁移的神经网络，通过输入一张采样图片转化为输出一张与采样图片不同风格的图片。为防止生成器学习到具有欺骗性的虚假数据，本文采用加入一个新的生成器，把第一个生成器的输出当作输入再次使用，使生成器的输出和原图具有较高的相似性，同时不丢失原图片的特征并且确保输出一个和原始输入相似的图片。实验仿真数据集选取真实校园作为场景，在训练初期并不能较好的还原回原始图片，这意味着生成器使用了虚假的输出结果，当训练次数达到10,000次以上，结果显示可以较好的还原回原始图片，证明第一个生成器的输出保留了大量原始图片特征，输出结果较为可靠。 In the field of unsupervised learning, generative confrontation network is a research direction that has developed rapidly in recent years. Its main feature is to use a probability estimation method to approximate an unknown distribution model. Using this model can avoid complex calculations and coding, especially in computer vision, can generate better quality pictures. This article is based on the cyclic generative confrontation network, training a style transfer neural network, inputting a picture and outputting a picture of its different styles. In order to prevent the generator from learning deceptive false data, this article adopts adding a new generator, and uses the output of the first generator as input again, so that the output of the generator has a higher similarity with the original image, while not losing the characteristics of the original picture and ensuring that a picture similar to the original input is output. The experimental simulation data set selects the real campus as the scene, and the original picture cannot be restored well in the initial training stage. This means that the generator uses false output results. When the number of training times reaches 10,000 or more, the results show that it can be restored well. Going back to the original picture, it is proved that the output of the first generator retains a large number of original picture features, and the output result is relatively reliable. 
  
 
</p></abstract><kwd-group><kwd>无监督学习，计算机视觉，循环生成对抗网络, Unsupervised Learning</kwd><kwd> Computer Vision</kwd><kwd> Cyclic Generative Confrontation Network</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>在无监督学习领域中生成对抗网络是近几年发展速度较快的一个研究方向，其主要特征是用一种概率估计的方式去逼近未知分布的模型。运用这种模型可以避免复杂的运算和编码，特别是在计算机视觉方面，可以生成质量较好的图片。本文基于循环生成对抗网络，训练一个风格迁移的神经网络，通过输入一张采样图片转化为输出一张与采样图片不同风格的图片。为防止生成器学习到具有欺骗性的虚假数据，本文采用加入一个新的生成器，把第一个生成器的输出当作输入再次使用，使生成器的输出和原图具有较高的相似性，同时不丢失原图片的特征并且确保输出一个和原始输入相似的图片。实验仿真数据集选取真实校园作为场景，在训练初期并不能较好的还原回原始图片，这意味着生成器使用了虚假的输出结果，当训练次数达到10,000次以上，结果显示可以较好的还原回原始图片，证明第一个生成器的输出保留了大量原始图片特征，输出结果较为可靠。</p></sec><sec id="s2"><title>关键词</title><p>无监督学习，计算机视觉，循环生成对抗网络</p></sec><sec id="s3"><title>Image Conversion Based on Cyclic Generative Confrontation Network<sup> </sup></title><p>Peng Gu<sup>*</sup>, Baozhen Liu<sup>*</sup>, Xinwen Qi, Huaicheng Li<sup>#</sup>, Jiafan Liu</p><p>Tibet University, Lhasa Tibet</p><p><img src="//html.hanspub.org/file/3-1542242x5_hanspub.png?20210809132522623" /></p><p>Received: Jul. 2<sup>nd</sup>, 2021; accepted: Jul. 28<sup>th</sup>, 2021; published: Aug. 4<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/3-1542242x6_hanspub.png?20210809132522623" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In the field of unsupervised learning, generative confrontation network is a research direction that has developed rapidly in recent years. Its main feature is to use a probability estimation method to approximate an unknown distribution model. Using this model can avoid complex calculations and coding, especially in computer vision, can generate better quality pictures. This article is based on the cyclic generative confrontation network, training a style transfer neural network, inputting a picture and outputting a picture of its different styles. In order to prevent the generator from learning deceptive false data, this article adopts adding a new generator, and uses the output of the first generator as input again, so that the output of the generator has a higher similarity with the original image, while not losing the characteristics of the original picture and ensuring that a picture similar to the original input is output. The experimental simulation data set selects the real campus as the scene, and the original picture cannot be restored well in the initial training stage. This means that the generator uses false output results. When the number of training times reaches 10,000 or more, the results show that it can be restored well. Going back to the original picture, it is proved that the output of the first generator retains a large number of original picture features, and the output result is relatively reliable.</p><p>Keywords:Unsupervised Learning, Computer Vision, Cyclic Generative Confrontation Network</p><disp-formula id="hanspub.44334-formula85"><graphic xlink:href="//html.hanspub.org/file/3-1542242x7_hanspub.png?20210809132522623"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-1542242x8_hanspub.png?20210809132522623" /> <img src="//html.hanspub.org/file/3-1542242x9_hanspub.png?20210809132522623" /></p></sec><sec id="s5"><title>1. 研究背景与内容</title><p>随着计算机图形、图像处理和计算机视觉领域的发展 [<xref ref-type="bibr" rid="hanspub.44334-ref1">1</xref>]，越来越多的问题都可以转换为输入图像和输出图像的形式。很多实际场景都可以用红绿蓝三原色，梯度域，边缘图来渲染。相比于自然语言自动翻译，图像到图像的转换问题可以定义为：在给定足够图像数据训练的条件下，将一种存在的场景转换为具有多种可能性的场景。计算机视觉和计算机图形学面临着多对一和一对多的映射问题。本质上的任务是从一个像素点预测另一个像素点。</p><p>卷积神经网络已逐渐成为解决大多数图像预测问题的重要手段。卷积神经网络是一种通过学习使损失函数最小化的方法 [<xref ref-type="bibr" rid="hanspub.44334-ref2">2</xref>]，损失函数的目标是评价结果的质量。虽然计算机在学习过程中是自我学习的，但它仍然需要投入大量的人力和计算能力来设计和计算损失函数。换句话说，最重要的目标就是使卷积神经网络明确最小化的方向。然而在不成熟方面，需要使用卷积神经网络来最小化预测图像与真实图像之间的欧氏距离，最终这种方法会使结果模糊。原因在于欧式距离是通过将所有输出平均来最小化，产生模糊结果显而易见。为了使卷积神经网络输出的结果更加真实，损失函数的设计变得越来越重要。本文使用循环生成对抗网络以学校的教学楼图片为训练图片训练网络参数，最终达到生成图片在整体视觉效果上和原图片相似，但在细节方面有一些区别。</p></sec><sec id="s6"><title>2. GAN网络的基本框架</title><p>生成对抗网络是通过对抗过程来估计生成模型的新框架，并使用随机梯度下降算法实现优化。这避免了反复应用马尔可夫链学习机制带来的分配函数计算 [<xref ref-type="bibr" rid="hanspub.44334-ref3">3</xref>]，降低了运算的复杂度。生成对抗网络需要同时训练两个模型：生成模型G寻找合理生成分布和鉴别器模型D来估计训练数据不生成模型数据的概率 [<xref ref-type="bibr" rid="hanspub.44334-ref4">4</xref>]。生成器网络模型可以学习真实样本的数据分布，生成新的数据样本。鉴别器网络模型是一种通用的二分类器。鉴别器网络模型的输入是真实数据和生成的样本数据，然后判断数据的来源，是真实数据还是虚假数据。为了能在博弈中胜出，两个模型需不断提高自身的生成能力和判别能力。生成模型和判别模型都可以是类似深度神经网络的结构 [<xref ref-type="bibr" rid="hanspub.44334-ref5">5</xref>]。生成对抗网络GAN的优化的期望是能够达到“纳什均衡”的状态，也就是说其优化过程是一个极大极小博弈(Minimax game)问题，生成网络模型中的生成器可以很好地学习数据样本的分布，从而生成所需的数据样本。生成对抗网络框架如图1所示：</p><p>图1. 生成对抗网络框架</p><p>在训练过程中，生成网络G的目的是尽可能多地生成真实的图像来欺骗和辨别网络D。D的目标是尽可能地将G的图像从真实图像中分离出来。目标函数是：</p><p>min G max D V ( D , G ) = E x ∼ p d a t a ( x ) [ log D ( x ) ] + E z ∼ p z ( z ) [ log ( 1 − D ( G ( z ) ) ) ]</p><p>整个公式由两项组成。X是真实图像，Z是输入到G网络的噪声，G(Z)是G网络生成的图像。D(x)是D网络判断真实图片是否真实的概率(因为x是真实的，所以对于D，值越接近1越好)。D(g(z))是D网络判断g生成的图像是否真实的概率。G的目的：如上所述，D(G(z))是D网络判断G生成的图像是否真实的概率。生成器希望G生成的图像“越接近真实越好” [<xref ref-type="bibr" rid="hanspub.44334-ref6">6</xref>]。换句话说，生成器希望D(G(z))尽可能得大，这时V(D, G)会变小。因此目标函数最前面的记号是 min G 。判别器的主要作用为：D的判别效果越强，D(x)应该越大，D(G(x))应该越小。这时V(D, G)会变大。因此式子对于D来说是求最大 max D 。</p><sec id="s6_1"><title>2.1. 条件生成对抗网络</title><p>传统的生成对抗网络是通过随机向量Z生成图像Y。但这样的初始化数据在训练过程中较为复杂，有的时候训练找不到纳什均衡点，最终导致训练不稳定。</p><p>条件生成对抗网络的损失函数和普通的生成对抗网络有一定的区别也有一些联系。条件生成对抗网络增加了有辅助特征，并且多了L1损失函数，这使得源域与目标域之间的图像距离尽可能的靠近。</p><p>L c G A N ( G , D ) = E x , y [ log D ( x , y ) ] + E x , z [ log ( 1 − D ( x , G ( x , z ) ) ) ] ,     arg min G max D L c G A N ( G , D )</p><p>和GAN网络的目标函数类似，但是GAN网络是根据随机噪声z生成图像，而条件生成对抗网络是根据随机噪声Z和输入图像X去生成。此时公式意义有一定的调整：首先整个式子由两项构成。x表示</p><p>源域图像，y表示真实图片，z表示输入生成器网络的噪声，而 G ( x , z ) 表示生成器网络根据源域图像和随机噪声生成的目标域图片。其次 D ( x , y ) 表示判别器网络判断真实图片是否真实的概率 [<xref ref-type="bibr" rid="hanspub.44334-ref7">7</xref>]，真实图片用y表示，生成器网络中加入输入噪声z，而 G ( x , z ) 表示生成器网络根据源域图像和随机噪声生成的目标域图片。其次 D ( x , y ) 表示判别器网络判断输入一个批次的图片真实性的概率，其中Y是真实图片，对于判别器得到的最终结果越靠近1说明图片效果较好。而 D ( x , G ( x , z ) ) 是判别器网络判断生成器生成的图片的是否真实的概率。再其次生成器网络的目的是 D ( x , G ( x , z ) ) 是判别器网络判断生成器生成的图片是否真实的概率，生成器最终的方向是拟合真实图片的概率。也就是说，生成器希望 D ( x , G ( x , z ) ) 尽可能得大，这时 1 − D ( x , G ( x , z ) ) 会变小。因此式子的最前面的记号是 min G 。判别器的目的：判别器的能力越强， D ( x , y ) 应该越大， D ( x , G ( x , z ) ) 应该越小。这时 1 − D ( x , G ( x , z ) ) 会变大。因此式子对于判别器来说是求最大( max D )。条件生成对抗网络的生成器作用除了生成可以愚弄判别器的图像之外，还需要尽量接近目标域的图像Y：</p><p>L L 1 ( G ) = E x , y , z [ ‖ y − G ( x , z ) ‖ 1 ]</p><p>关于损失函数的选取，选用L1损失函数而不用L2损失函数从而保证更少的模糊。对于一维的数据，最小化L2损失函数是获得算术平均数；最小化L1值得到的是中位数。最终的损失函数为：</p><p>G * = arg min G max D L c G A N ( G , D ) + λ L L 1 ( G )</p></sec><sec id="s6_2"><title>2.2. 循环生成对抗网络损失函数</title><p>循环对抗网络中有两个生成器，分别为D和F表示，生成器有X和Y两个定义域。生成器G用来基于X域的图像生成Y域的图像(X &gt; Y)，生成器F用来基于Y域的图像生成X域的图像(Y &gt; X)，这两个生成器的定位是相反的过程，通过损失函数 L ( G , F , D X , D Y ) 进行约束。同时循环生成对抗网络中有2个判别器，分别用D(X)和D(Y)表示，它用于判断输入图像在X域或Y域中是真是假。因此，循环生成对抗网络可视为两种对抗网络的融合。一种对抗网络由发生器G和鉴别器D(y)组成，实现了从X域到y域的图像生成和识别；另一种对抗网络由生成器F和鉴别器D(x)组成，实现了从y域到x域的图像生成和识别。</p><p>L ( G , F , D X , D Y ) = L G A N ( G , D Y , X , Y ) + L G A N ( F , D X , Y , X ) + λ L c y c ( G , F )</p><p>循环生成对抗网络的整体优化目标包括三个部分：第一部分 L G A N ( G , D Y , X , Y ) 表示生成器G和判别器D<sub>Y</sub>的优化目标；第二部分为了改善生成器F和判别器D<sub>X</sub>采用函数 L G A N ( F , D X , Y , X ) 作为优化目标，二部分的目的是生成对抗网络算法本身的优化目标。第三部分采用 L c y c ( G , F ) 来提升从变换域再变换到原域时图像的一致的能力。</p><p>L G A N ( G , D Y , X , Y ) 如公式所示，采用一般生成对抗的优化目标，优化目标的最大化采用判别器D<sub>Y</sub>，生成器G要最小化公式中的后半部分。 L G A N ( G , D Y , X , Y ) 也是同理。</p><p>L G A N ( G , D Y , X , Y ) = E y ∼ p d a t a ( y ) [ log D Y ( y ) ] + E x ∼ p d a t a ( x ) [ log ( 1 − D Y ( G ( x ) ) ) ]</p><p>为了使训练过程更加稳定，实际训练是基于最小二乘损失的。例如，鉴别器D的优化目标变为最小</p><p>化 E x → p d a t a ( y ) [ ( D ( y ) − 1 ) 2 ] + E x → p d a t a ( x ) [ D ( G ( x ) ) 2 ]</p><p>优化目标的第一部分是期望D(y)尽可能接近1，也就是说，当输入真实图像y时，鉴别器D具有相信输入是真实的最大概率。第二部分是期望D(g(x))尽可能接近0，也就是说，当输入假图像g(x)时，鉴别器D相信输入是真的概率最低。生成器G的优化目标变成最小化 E → p d a t a ( x ) [ ( D ( G ( x ) ) − 1 ) 2 ] ，这个优化目标是使D(G(x))的期望值更接近1，而D(G(x))越接近1表示判别器D越相信生成器G生成的图像G(x)是真实图像。</p><p>L c y c ( G , F ) 如下公式所示，其中G(x)表示从X域到Y域的变化，F(G(X))表示再从Y域变换到X域，模型训练的目标当然是希望F(G(X))尽可能和X接近，因此用其距离来约束，也就是 x → G ( x ) → F ( G ( x ) ) ≈ x ，将这个称之为正向循环一致性；同理对于 y → F ( y ) → G ( F ( y ) ) ≈ y 称之为后向循环一致性。</p><p>L c y c ( G , F ) = E x ∼ p d a t a ( x ) [ ‖ F ( G ( x ) ) − x ‖ ] + E y ∼ p d a t a ( y ) [ ‖ G ( F ( y ) ) − y ‖ ]</p></sec></sec><sec id="s7"><title>3. 实验设计</title><sec id="s7_1"><title>3.1. 实验设计</title><p>操作系统使用Windows10系统，安装Anaconda3 (64bit)和PyCharm2020 (64bit)集成编译环境，使用python3.6.8版本编译和tensorflow2.0。需要的package：matplotlib.pyplot、keras2.2.4、time、os、tensorflow.keras、numpy等。</p></sec><sec id="s7_2"><title>3.2. 网络结构</title><p>为了保证生成器生成的图片是基于输入图片，而不是在发现判别器的“漏洞”，循环生成对抗网络引入了与目标生成器和判别器对称的另一组生成器和判别器。如果目标网络寻找的是x到y的映射，这组另外添加的网络就是寻找y到x的映射。如图2所示，目标生成器的输出可以作为额外添加生成器的输入，输入图片连续经过两个生成器，最终结果与输入图片的差别可作为网络是否成功训练的依据。</p><p>图2. 循环生成对抗网络</p></sec><sec id="s7_3"><title>3.3. U-Net为生成器主要的网络结构</title><p>生成器网络为U型结构，整体可分为两个部分：特征通道逐渐变小，通道数逐渐变多的“下采样”过程；特征通道逐渐变大，通道数逐渐变少的“上采样”过程。上图的左半部分为生成器网络的整体结构。上图的右半部分为“上采样”过程中Block的具体实现方法。一个卷积层、一个归一化层和激励函数组成每个Block块。与“下采样”过程的区别是：“上采样”中残差和上一个Block的输出同时作为此Block的输入，卷积层为反卷积；“下采样”中输出在传递到下一个Block的同时也作为残差传递到相应的Block。如图3所示。</p><p>图3. 生成器网络结构</p></sec><sec id="s7_4"><title>3.4. 判别器网络结构</title><p>判别器网络与传统的卷积神经网络很相似，区别主要是没有在最后添加全连接层。上图为判别器网络的结构。判别器网络可以看作另外的生成器，它的输入为生成器的输入和输出图像，或者是数据集中的两种对应图片，而它的输出是一张30 &#215; 30的小图片，通过统计判别器生成的图片中的像素整体情况来判断输入图片间的关系。如图4所示：</p><p>图4. 判别器网络框架</p></sec></sec><sec id="s8"><title>4. 结果分析</title><p>第一组实验如图5、图6所示，输入都是标准图像，左侧为输入，右侧为输出。观察第一组的实验结果可以发现，由简笔画生成的建筑图片色调上有明显的差别，建筑的结构只在图片的边缘处出现错位等情况；而建筑图片生成的简笔画总体上具有建筑的结构，但其中会出现一些没有意义的色块。</p><p>第二组实验如图7所示，输入标准的建筑图像，得到输出的简笔画图像后，再将该简笔画图像输入，最后得到生成的建筑图像。通过研究第二组的实验结果可以发现，生成对抗网络从简笔画图像中还原出了过于完整的细节，而在简笔画图像中不能看到这部分的细节。这是循环生成对抗网络中的经典问题，由于在损失中加入了生成图像与原图像的区别，生成对抗网络会将信息隐藏在生成的简笔画图像中，在最终的生成结果中将这些信息还原出来。</p><p>图5. 建筑物一标准图到简笔画</p><p>图6. 建筑物二标准图到简笔画</p><p>图7. 简笔画到生成图片</p></sec><sec id="s9"><title>5. 展望</title><p>针对图片生成图片的研究，本文运用循环生成对抗网络，无须使用成对的图片进行训练，可以将图片从一个领域变换到另一个领域，训练学习两种映射的生成网络。循环生成对抗网络存在一种情况是训练稳定后某些部分会被隐藏起来，由于服务器问题本次实验只运行了5000代，在清晰度方面还存在一定问题。未来研究方向可以结合styleGAN网络框架，可以有效的调整细节方面的需求，改善生成图片的视觉效果和解码能力。</p></sec><sec id="s10"><title>文章引用</title><p>古 彭,刘宝镇,戚馨文,李怀诚,刘嘉帆. 基于循环生成对抗网络的图像转换Image Conversion Based on Cyclic Generative Confrontation Network[J]. 计算机科学与应用, 2021, 11(08): 2042-2050. https://doi.org/10.12677/CSA.2021.118209</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.44334-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">朱秀昌, 唐贵进. 生成对抗网络图像处理综述[J]. 南京邮电大学学报(自然科学版), 2019, 39(3): 1-12.</mixed-citation></ref><ref id="hanspub.44334-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">黄宇, 韩璞, 王东风, 张婧. 基于BP神经网络整定的PID控制在过热汽温系统中的应用[J]. 仪器仪表学报, 2006(S3): 1980-1981.</mixed-citation></ref><ref id="hanspub.44334-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">冯杰, 班彪华. 生成对抗网络模型的基本介绍和应用综述[J]. 现代计算机(专业版), 2019(4): 34-39.</mixed-citation></ref><ref id="hanspub.44334-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">卢庆林, 叶伟, 李国靖. 基于DCGAN的SAR虚假目标图像仿真[J]. 电子信息对抗技术, 2020, 35(2): 57-61+65.</mixed-citation></ref><ref id="hanspub.44334-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">时澄, 潘斌, 郭小明, 李芹芹, 张露月, 钟凡. 生成式对抗网络在图像补全中的应用[J]. 计算机科学与探索, 2019, 13(8): 1402-1410.</mixed-citation></ref><ref id="hanspub.44334-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">朱良宽, 晏铭, 黄建平. 一种新型卷积神经网络植物叶片识别方法[J]. 东北林业大学学报, 2020, 48(4): 50-53.</mixed-citation></ref><ref id="hanspub.44334-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">毕松, 刁奇, 孙贵宾, 韩存武. 交通场景物体检测模型研究[J]. 计算机仿真, 2018, 35(10): 193-197.</mixed-citation></ref></ref-list></back></article>