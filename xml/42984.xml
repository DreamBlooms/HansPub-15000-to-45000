<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">PM</journal-id><journal-title-group><journal-title>Pure  Mathematics</journal-title></journal-title-group><issn pub-type="epub">2160-7583</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/PM.2021.116114</article-id><article-id pub-id-type="publisher-id">PM-42984</article-id><article-categories><subj-group subj-group-type="heading"><subject>PM20210600000_88798661.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于加权Schatten-1/2范数的低秩矩阵近似算法
  Weighted Schatten-1/2 Norm Minimization for Low-Rank Matrix Approximation
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>素</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>顾</surname><given-names>颖菁</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>袁</surname><given-names>泉</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>南京航空航天大学理学院数学系，江苏 南京</addr-line></aff><aff id="aff4"><addr-line>南京航空航天大学理学院数学系，江苏 南京；飞行器数学建模与高性能计算工业和信息化部重点实验室(南京航空航天大学)，江苏 南京</addr-line></aff><aff id="aff3"><addr-line>南京晓庄学院商学院，江苏 南京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>03</day><month>06</month><year>2021</year></pub-date><volume>11</volume><issue>06</issue><fpage>998</fpage><lpage>1009</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    本文提出加权的Schatten-1/2拟范数求解低秩矩阵近似问题，该模型以加权的Schatten-1/2拟范数为目标函数，观测矩阵为约束。通过基于阈值的加权不动点迭代算法求解。该方法通过分配不同权值体现奇异值的重要性可更好地近似原来的低秩假设。另一方面，针对奇异值计算量大的问题引入约化奇异值分解。数值实验结果表明，该方法具有较快的收敛速度。
    In this paper, the low-rank matrix approximation problem is discussed with a weighted Schatten quasi-norm as the objective function, constrained by partial obtained data. The weights are in-troduced to measure the importance of different rank components. A weighted fixed point iterative thresholding algorithm is proposed based on the fixed point representation theory. The con-vergence analysis of the algorithm is provided. Numerical examples illustrate the effciency of our method. 
  
 
</p></abstract><kwd-group><kwd>加权Schatten-1/2拟范数，低秩矩阵近似，不动点迭代算法，约化奇异值分解，非凸正则化, The Weighted Schatten-1/2 Norm</kwd><kwd> Low-Rank Matrix Approximation</kwd><kwd> Fixed Point Iterative 
Algorithm</kwd><kwd> Reduced Singular Value Decomposition</kwd><kwd> Non-Convex Regularization</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>本文提出加权的Schatten- 1 2 拟范数求解低秩矩阵近似问题，该模型以加权的Schatten- 1 2 拟范数为目标函数，观测矩阵为约束。通过基于阈值的加权不动点迭代算法求解。该方法通过分配不同权值体现奇异值的重要性可更好地近似原来的低秩假设。另一方面，针对奇异值计算量大的问题引入约化奇异值分解。数值实验结果表明，该方法具有较快的收敛速度。</p></sec><sec id="s2"><title>关键词</title><p>加权Schatten- 1 2 拟范数，低秩矩阵近似，不动点迭代算法，约化奇异值分解，非凸正则化</p><p><img src="//html.hanspub.org/file/3-1251299x7_hanspub.png" /></p></sec><sec id="s3"><title>Weighted Schatten- 1 2 Norm Minimization for Low-Rank Matrix Approximation</title><p>Su Wang<sup>1</sup>, Yingjing Gu<sup>2</sup>, Quan Yuan<sup>1,3</sup></p><p><sup>1</sup>Department of Mathematics, College of Science, Nanjing University of Aeronautics and Astronautics, Nanjing Jiangsu</p><p><sup>2</sup>Business College, Nanjing Xiaozhuang University, Nanjing Jiangsu</p><p><sup>3</sup>Key Laboratory of Mathematical Modelling and High Performance Computing of Air Vehicles (NUAA), MIIT, Nanjing Jiangsu</p><p><img src="//html.hanspub.org/file/3-1251299x9_hanspub.png" /></p><p>Received: Apr. 28<sup>th</sup>, 2021; accepted: May 31<sup>st</sup>, 2021; published: Jun. 8<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/3-1251299x10_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In this paper, the low-rank matrix approximation problem is discussed with a weighted Schatten quasi-norm as the objective function, constrained by partial obtained data. The weights are introduced to measure the importance of different rank components. A weighted fixed point iterative thresholding algorithm is proposed based on the fixed point representation theory. The convergence analysis of the algorithm is provided. Numerical examples illustrate the effciency of our method.</p><p>Keywords:The Weighted Schatten- 1 2 Norm, Low-Rank Matrix Approximation, Fixed Point Iterative Algorithm, Reduced Singular Value Decomposition, Non-Convex Regularization</p><disp-formula id="hanspub.42984-formula26"><graphic xlink:href="//html.hanspub.org/file/3-1251299x12_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-1251299x13_hanspub.png" /> <img src="//html.hanspub.org/file/3-1251299x14_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>低秩矩阵近似的目的是提取关键信息代替或恢复原始数据。随着其在大数据处理中的明显优势以及理论的深入研究，低秩矩阵近似技术已广泛地应用于解决图像恢复 [<xref ref-type="bibr" rid="hanspub.42984-ref1">1</xref>]、遥感技术 [<xref ref-type="bibr" rid="hanspub.42984-ref2">2</xref>]、推荐系统 [<xref ref-type="bibr" rid="hanspub.42984-ref3">3</xref>] 等各类实际问题。例如，在图像恢复中，灰度图像可以用低秩矩阵或近似低秩矩阵表示。但某些特殊情况可能导致图像的一些灰度值丢失，如破损照片中的划痕和污迹，图像中的文字覆盖等。因此有必要对丢失的灰度值进行还原，以恢复原始图像。Candės [<xref ref-type="bibr" rid="hanspub.42984-ref4">4</xref>] 将低秩矩阵近似问题描述为以下优化问题</p><p>min     r a n k ( X ) s . t .   X i , j = M i , j , ∀ ( i , j ) ∈ Ω (1.1)</p><p>X ∈ R m &#215; n ， Ω 表示待恢复矩阵M中可观测元素下标的集合。Candės分析了问题(1.1)的计算复杂度并证明其为NP-hard问题。随后，作为rank函数的凸包络，核范数被提出用来逼近问题(1.1)的目标函数</p><p>min X       ‖ X ‖ * s . t .   X i , j = M i , j , ∀ ( i , j ) ∈ Ω (1.2)</p><p>其中 ‖ X ‖ * = Σ k σ k ， σ k 为X的奇异值， k = 1 , ⋯ , r , r = min { m , n } 。相关理论分析表明 [<xref ref-type="bibr" rid="hanspub.42984-ref4">4</xref>]，相比于问题(1.1)，凸优化问题(1.2)更容易解决。</p><p>针对问题(1.2)出现了一些突破性的方法和成果 [<xref ref-type="bibr" rid="hanspub.42984-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref8">8</xref>]，然而，核范数需要最小化所有奇异值的和，使得近似效果不太理想 [<xref ref-type="bibr" rid="hanspub.42984-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref11">11</xref>]。一般来说，较大的奇异值包含了数据矩阵的主要信息。图像灰度矩阵的较大奇异值包含了主要的边缘和纹理信息，因此处理奇异值时应小幅度收缩较大的奇异值，同时尽可能多的收缩较小的奇异值 [<xref ref-type="bibr" rid="hanspub.42984-ref12">12</xref>]。为了更合理地度量不同奇异值的重要性，Gu [<xref ref-type="bibr" rid="hanspub.42984-ref13">13</xref>] 提出了加权核范数模型解决问题(1.2)。加权核范数定义为 ‖ X ‖ W , * = | Σ k ω k σ k | ，其中 W = ( ω 1 , ω 2 , ⋯ , ω r ) T ， ω k ≥ 0 为奇异值 σ k 的非负权值， k = 1 , 2 , ⋯ , r 。</p><p>在实际应用中，测量噪声普遍存在，以上方法的优化性能会下降，甚至会严重偏离初始问题(1.1)。为了更准确地逼近，非凸函数方法如 S c h a t t e n - p ( 0 &lt; p &lt; 1 ) ( S p )拟范数被用来近似问题(1.1)中的目标函数</p><p>min X       ‖ X ‖ p p s . t .   X i , j = M i , j , ∀ ( i , j ) ∈ Ω (1.3)</p><p>‖ X ‖ p = ( ∑ k σ k p ) 1 p 。问题(1.3)可由MM (Majorization-Minimization)迭代算法 [<xref ref-type="bibr" rid="hanspub.42984-ref14">14</xref>] 计算数值解。然而，由于目标函数非凸、非光滑、非Lipschitz连续的性质，使得解析解难以求解 [<xref ref-type="bibr" rid="hanspub.42984-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref15">15</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref16">16</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref17">17</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref18">18</xref>]。另一方面，参数p的变化也会对问题(1.3)产生很大影响。文献 [<xref ref-type="bibr" rid="hanspub.42984-ref19">19</xref>] 指出当 p ∈ [ 1 2 , 1 ) ，近似结果相对理想，当 p ∈ [ 0 , 1 2 )</p><p>时，结果基本不具参考性。Ding [<xref ref-type="bibr" rid="hanspub.42984-ref14">14</xref>] 提出了问题(1.3)的一阶必要性条件，并通过解析阈值的不动点算法求解其迭代式。</p><p>基于加权核范数模型和 S p 拟范数，本文主要研究低秩矩阵近似模型</p><p>min       ‖ X ‖ W , S p p s . t .   X i , j = M i , j , ∀ ( i , j ) ∈ Ω (1.4)</p><p>其中 ‖ X ‖ W , S p p = ∑ k ω k σ k p 。当 ω i = 1 且 p = 1 时，问题(1.4)退化为问题(1.2)。</p><p>论文的其他部分安排如下。在第2部分中，根据奇异值的重要性，构造相对应的权值以更好的逼近rank函数。同时指出权值对不动点迭代算法效果的影响，即当权值越小，算法的综合性能越好。第3节使用约化的SVD(singular value decomposition)以减少奇异值计算量大的问题，并给出了基于阈值的不动点迭代算法及其收敛性。第4节，对比实验验证了算法的有效性。</p><p>下面给出本文的符号说明。不失一般性，假设 m ≥ n 。对于给定的矩阵 X , Y ∈ R m &#215; n ，</p><p>〈 X , Y 〉 = t r a c e ( Y T X ) ， t r a c e ( X ) = ∑ i X i , i ， ‖ X ‖ F = ( ∑ i , j X i , j 2 ) 1 2 = ( ∑ i = 1 n σ i 2 ) 1 2 。向量 x , y ∈ R n ， ‖ x ‖ 2 = ( ∑ i = 1 n x i 2 ) 1 2 ，</p><p>〈 x , y 〉 = x T y 。</p></sec><sec id="s6"><title>2. 全局必要最优条件</title><p>为更好地刻画矩阵的低秩结构，文献 [<xref ref-type="bibr" rid="hanspub.42984-ref19">19</xref>] 指出 S 1 2 拟范数正则化具有低秩无偏理论的特点。在本节中，</p><p>取 p = 1 2 ，给出加权 S 1 2 拟范数正则化模型和相应的不动点迭代算法。</p>阈值和加权处理<p>假设 M ∈ R m &#215; n 是一个秩为r的实数矩阵， Y ∈ R m &#215; n 是由M的可观测元素构成的矩阵，问题(1.4)的目的是找到最小秩矩阵 X ∈ R m &#215; n 近似M。Ding [<xref ref-type="bibr" rid="hanspub.42984-ref19">19</xref>] 通过引入Tikhonov正则项，将问题(1.4)描述为以下形式</p><p>min X 1 2 ‖ P Ω ( X ) − P Ω ( Y ) ‖ F 2 + ‖ X ‖ W , S 1 2 1 2 (2.1)</p><p>其中 ‖ X ‖ W , S 1 2 2 = ∑ i = 1 min { m , n } ω i δ i ， ω i 为权值， δ i 为奇异值， P Ω ( &#183; ) 为集合 Ω 的投影。</p><p>问题(2.1)的非凸和非光滑性使得问题不易求解。利用以下结论可以将问题(2.1)转化为可分离的问题进行求解。</p><p>引理1 [<xref ref-type="bibr" rid="hanspub.42984-ref20">20</xref>] 假设 A , B ∈ R m &#215; n ， σ ( A ) = [ σ 1 ( A ) , ⋯ , σ r ( A ) ] T , σ ( B ) = [ σ 1 ( B ) , ⋯ , σ r ( B ) ] T 分别为矩阵 A , B 的奇异值，并且 σ 1 ( A ) ≥ ⋯ ≥ σ r ( A ) , σ 1 ( B ) ≥ ⋯ ≥ σ r ( B ) , r = min { m , n } 。</p><p>则不等式 t r ( A T B ) ≤ t r ( σ ( A ) T σ ( B ) ) 成立当且仅当存在正交矩阵 U ∈ R m &#215; r , V ∈ R n &#215; r ，使得</p><p>A = U ∑ A V , B = U ∑ B V (2.2)</p><p>Σ A , Σ B 分别为矩阵 A , B 的奇异值矩阵。</p><p>引理2 [<xref ref-type="bibr" rid="hanspub.42984-ref21">21</xref>] 假设 Y ∈ R m &#215; n ，且有 Y = U ∑ V T ， Σ = d i a g ( σ 1 , ⋯ , σ r ) , σ 1 ≥ σ 2 ≥ ⋯ ≥ σ r , r = min { m , n } 。问题(2.1)最优解的SVD为 X = U Δ V T ，其中 Δ = d i a g ( δ 1 , ⋯ , δ r ) ， δ i ≥ δ j ， i ≥ j 。 δ i 是如下优化问题的解</p><p>min δ 1 , ⋯ , δ r     ∑ i r [ ( δ i − σ i ) 2 + ω i δ i 1 2 ] s . t .   δ i ≥ 0 , δ i ≥ δ j , i ≤ j , i = 1 , ⋯ , r (2.3)</p><p>引理1和引理2表明问题(2.1)可以转化为可分离问题(2.3)求解。对于问题(2.3)，讨论其一般项的求解。为文章的完整性起见，将文献 [<xref ref-type="bibr" rid="hanspub.42984-ref19">19</xref>] 的证明整理如下。</p><p>引理3假设 y ∈ R n , λ &gt; 0 , 令 f λ ( x ) : = ( x − y ) 2 + λ x 1 2 。问题</p><p>h λ ( y ) = arg min x ≥ 0 { f λ ( x ) } (2.4)</p><p>的解可以表示为</p><p>h λ ( y ) = { h λ ( y ) , y &gt; 3 4 λ 2 3         0 , y &gt; 3 4 λ 2 3 (2.5)</p><p>其中</p><p>h λ ( y ) = 2 3 y ( 1 + cos ( 2 π 3 − 2 3 Φ λ ( y ) ) ) (2.6)</p><p>Φ λ ( y ) = arccos ( λ 8 ( y 3 ) − 3 2 ) (2.7)</p><p>证明： ∀ y ∈ R n ， ‖ y ‖ 1 2 1 2 是连续可微的。令 f λ ( x ) 的一阶导数为零</p><p>y i − x i + λ s i g n ( y i ) 4 | y i | = 0 (2.8)</p><p>当且仅当 x &gt; 0 时，(2.8)有非负解， x ≤ 0 时， f λ ( x ) 有唯一解 x = 0 。因此，仅考虑 x &gt; 0 的情况。</p><p>令 | y i | = η , y i = η 2 。当 x i &gt; 3 4 λ 2 3 时，有</p><p>η 3 − x i η + λ 4 = 0 (2.9)</p><p>假设 r = | x i | 3 , q = λ 8 , Φ = arccos ( q 3 ) , (2.9)的三个解可以表示为： η 1 = − 2 r cos ( Φ 3 ) ,</p><p>η 2 = 2 r cos ( π 3 + Φ 3 ) , η 3 = 2 r cos ( π 3 − Φ 3 ) [<xref ref-type="bibr" rid="hanspub.42984-ref11">11</xref>]。经验证， η 3 = 2 r cos ( π 3 − Φ 3 ) 是问题(2.4)的最优解，且解</p><p>的形式为</p><p>y i = 2 3 x i ( 1 + cos ( 2 π 3 − 2 3 λ Φ λ ( x i ) ) ) (2.10)</p><p>当 x i &gt; 3 4 λ 2 3 时，证明与上述类似。</p><p>定义1 ∀ λ &gt; 0 , 向量阈值函数定义为</p><p>H λ ( x ) : = ( h λ ( x 1 ) , h λ ( x 2 ) , ⋯ , h λ ( x n ) ) , ∀ x = ( x 1 , x 2 , ⋯ , x n ) ∈ R n (2.11)</p><p>h λ ( x ) 的形式如(2.5)所示。</p><p>假设秩为 r 的矩阵 Y ∈ R m &#215; n 的SVD为 Y = U ∑ V T ， U ∈ R m &#215; r 、 V ∈ R n &#215; r 均为列正交矩阵， Σ ∈ R r &#215; r 为奇异值非增的对角矩阵。</p><p>对 ∀ λ &gt; 0 ，由向量阈值函数可以定义矩阵阈值函数</p><p>Η λ ( Y ) : = U ∑ H λ V T (2.12)</p><p>Σ H λ = d i a g ( H λ ( σ 1 ) , ⋯ , H λ ( σ r ) ) 。因此，问题(2.1)可以由矩阵阈值函数求解。本节的后续内容将会介绍由(2.12)表示的问题(2.1)解的形式以及权重对奇异值的影响。</p><p>引理3给出了当矩阵维数为1时的解的情况，现在考虑矩阵 Y ∈ R m &#215; n 。</p><p>定理1当且仅当权重满足 0 ≤ ω 1 ≤ ⋯ ≤ ω r 时，问题(2.1)最优解的奇异值满足 δ 1 ≥ ⋯ ≥ δ r 。</p><p>证明：由引理2和引理3，问题(2.1)可分离出子问题</p><p>f δ ( x ) : = ( σ − δ ) 2 + ω δ 1 2 (2.13)</p><p>文献 [<xref ref-type="bibr" rid="hanspub.42984-ref19">19</xref>] 指出， f δ ( x ) 有唯一解 h ω ( δ ) 。</p><p>首先，证明当 ω i &lt; ω j 时，有 h ω i ≥ h ω j 。</p><p>当 y &lt; 3 4 ω i 2 3 且 y &lt; 3 4 ω j 2 3 时，由(2.5)，有 h ω i = h ω j = 0 。结论 h ω i ≥ h ω j 成立。</p><p>当 y &gt; 3 4 ω i 2 3 且 y &lt; 3 4 ω j 2 3 时，显然有 h ω j = 0 。另一方面，由引理3的证明，当 y &gt; 0 时有 h ω i &gt; 0 。因此，结论 h ω i ≥ h ω j 成立。</p><p>当 y &gt; 3 4 ω i 2 3 且 y &gt; 3 4 ω j 2 3 时，通过函数 h ω ( δ ) 的单调性证明结论。因为 y &gt; 3 4 ω 2 3 ，且 ω 8 ( x 3 ) − 2 3 ∈ ( 0 , 1 ) ，则由(2.7)可得 Φ ω ( y ) ∈ ( 0 , π 2 ) ，由此可得函数 cos ( 2 3 − 2 3 Φ ω ( y ) ) 单调递减。因此， h ω i ≥ h ω j 成立。</p><p>然后，对给定 ω &gt; 0 ，以下不等式显然成立</p><p>2 3 y i ( 1 + cos ( 2 π 3 − 2 3 Φ ) ) &gt; 2 3 y j ( 1 + cos ( 2 π 3 − 2 3 Φ ) ) , ∀ y i &gt; y j .</p><p>综合两方面的证明，可以得到结论</p><p>h ω i ( y i ) ≥ h ω j ( y j ) , ∀ ω i &lt; ω j , y i &gt; y j .</p><p>证毕。</p><p>文献 [<xref ref-type="bibr" rid="hanspub.42984-ref14">14</xref>] [<xref ref-type="bibr" rid="hanspub.42984-ref22">22</xref>] 将矩阵问题转化为易于求解的向量问题，同时表明非凸问题可以用阈值法解决。</p><p>引理4 [<xref ref-type="bibr" rid="hanspub.42984-ref14">14</xref>] 令 λ &gt; 0 ，假设秩为 r 的矩阵 Y ∈ R m &#215; n 的SVD为 Y = U ∑ Y V T ，对应的矩阵阈值函数为</p><p>Η λ ( Y ) : = U ∑ H λ ( σ ) V T (2.14)</p><p>则</p><p>Η λ ( Y ) : = arg min X ∈ R m &#215; n { ‖ X − Y ‖ F 2 + λ ‖ X ‖ 1 2 1 2 } (2.15)</p></sec><sec id="s7"><title>3. 最优算法及其收敛性分析</title><p>本节将给出非凸、非光滑和非Lipschitz连续的 S 1 2 拟范数正则化问题全局最优解的不动点表示理论。 ∀ μ &gt; 0 , Z ∈ R m &#215; n ，定义</p><p>C W ( X ) : = ‖ P Ω ( X ) − P Ω ( Y ) ‖ F 2 + ‖ X ‖ W , 1 2 1 2 C W , μ : = μ ( C W ( X ) − ‖ P Ω ( X ) − P Ω ( Z ) ‖ F 2 ) + ‖ X − Z ‖ F 2 . B μ ( Z ) : = Z + μ ( P Ω ( Y ) − P Ω ( X * ) ) (3.1)</p><p>定理2对 ∀ ω i , μ &gt; 0 ，假设正则化模型(2.1)的最优解为 X * 。令</p><p>B μ ( X * ) : = X * + μ ( P Ω ( Y ) − P Ω ( X * ) )</p><p>且 B μ ( X * ) 的SVD为</p><p>U * ∑ B μ ( X * ) ( V * ) T</p><p>则</p><p>X * ∈ Η W ( B μ ( X * ) ) (3.2)</p><p>因此，最优解的第i个奇异值为</p><p>σ i ( X * ) = { h ω i μ , 1 2 ( σ i ( B μ ( X * ) ) ) , σ i ( B μ ( X * ) ) &gt; 54 3 4 ( ω i μ ) 2 3               0 , σ i ( B μ ( X * ) ) ≤ 54 3 4 ( ω i μ ) 2 3 (3.3)</p><p>其中</p><p>h ω i ( y ) = { h ω i μ , 1 2 ( σ i ) ， σ i &gt; 54 3 4 ( ω i μ ) 2 3         0 ， σ i ≤ 54 3 4 ( ω i μ ) 2 3 (3.4)</p><p>h ω i ， 1 2 ( σ i ) = 2 3 y ( 1 + cos ( 2 π 3 − 2 3 Φ ω i ) ) (3.5)</p><p>Φ ω i ( y ) = arccos ( ω i μ 8 ( y 54 3 ) − 2 3 ) (3.6)</p><p>证明：</p><p>C W , μ ( X , Z ) : = μ ( C W ( X ) − ‖ P Ω ( X ) − P Ω ( Z ) ‖ F 2 ) + ‖ X − Z ‖ F 2 = ‖ X ‖ − 2 〈 X , Z + μ ( P Ω ( Y ) − P Ω ( Z ) ) 〉 + μ ‖ X ‖ W , S 1 2 1 2 + ‖ Z ‖ + μ ‖ P Ω ( Y ) ‖ F 2 − μ ‖ P Ω ( Z ) ‖ F 2</p><p>= ‖ X ‖ − 2 〈 X , B μ ( Z ) 〉 + μ ‖ X ‖ W , S 1 2 1 2 + ‖ Z ‖ + μ ‖ P Ω ( Y ) ‖ F 2 − μ ‖ P Ω ( Z ) ‖ F 2 = ‖ X − B μ ( Z ) ‖ F 2 + μ ‖ X ‖ W , S 1 2 1 2 + ‖ Z ‖ + μ ‖ P Ω ( Y ) ‖ F 2 − μ ‖ P Ω ( Z ) ‖ F 2 − ‖ B μ ( Z ) ‖ F 2 (3.7)</p><p>(3.7)的最后一个等式表明，最小化 C W , μ ( X , Z ) 相当于求解问题</p><p>min X ∈ R m &#215; n { ‖ X − B μ ( Z ) ‖ F 2 + μ ‖ X ‖ W , S 1 2 1 2 } . (3.8)</p><p>假设 X * 是 C W ( X ) 的全局最优解，由文献 [<xref ref-type="bibr" rid="hanspub.42984-ref11">11</xref>] 可知， X * 也是 C W , μ ( X , X * ) 的全局最优解。显然(3.2)成立。</p><p>类似于问题(2.3)的求解方法，问题(3.8)可分离的求解</p><p>min { x i 2 − 2 x i [ B μ ( Z ) ] i + μ ω i | x i | 2 } (3.9)</p><p>令(3.9)函数的一阶导数为0</p><p>x i 2 − [ B μ ( Z ) ] i + μ ω i sign ( x i ) 4 | x i | = 0 (3.10)</p><p>由引理3的证明， [ B μ ( Z ) ] i 满足不等式 [ B μ ( Z ) ] i &gt; 3 4 ( ω i μ ) 2 3 。文献 [<xref ref-type="bibr" rid="hanspub.42984-ref11">11</xref>] 表明只需比较 f ω i ( h ω i , 1 2 ) 和 f ω i ( 0 ) 即可得到原问题的最优解。经验证，当且仅当 | [ B μ ( Z ) ] i | ≤ 54 3 4 ( ω i μ ) 2 3 时有 f ω i ( h ω i , 1 2 ) ≤ f ω i ( 0 ) 。后</p><p>续证明与引理3类似。</p><p>证毕。</p><p>定理2给出了问题(1.4)最优解的形式。以上证明表示可以用阈值不动点迭代算法求解非凸、非光滑、非Lipschitz问题(1.4)。不动点算法的基本框架描述如下。</p><p>优化问题的结果取决于正则化参数的选择，即权重向量。将第k次迭代的 ω i 取为</p><p>ω i = 96 9 μ 0 ( [ σ ( X k ) ] r k ) 3 2 (3.11)</p><p>r k 表示矩阵 X k 的秩。为延续 ω k 的取值，将向量 ω k 更新为</p><p>ω k + 1 = max { λ &#175; , min { η ω k , 96 9 μ 0 ( [ σ ( X k ) ] r k ) 3 2 } } (3.12)</p><p>其中 η ∈ ( 0 , 1 ) 为常数， λ &#175; 为足够小的正实数。显然序列 { ω k } 单调减小且收敛。采用延续技术改进的不动点算法如算法2所示。</p><p>在算法2中，主要的计算量来自奇异值分解。Drineas [<xref ref-type="bibr" rid="hanspub.42984-ref23">23</xref>] 提出了一种近似的SVD算法代替传统的SVD以减少计算成本，完整的阈值不动点迭代算法如算法3所示。</p><p>下面给出算法3的收敛性分析。</p><p>引理5 [<xref ref-type="bibr" rid="hanspub.42984-ref19">19</xref>] 给定 λ &gt; 0 , μ ∈ ( 0 , 1 ] ， { X k } 为(3.2)生成的序列，则</p><p>(1) 假设 X * 为 { X k } 的任一聚点，则 C λ ( X k ) 单调递减收敛于 C λ ( X * ) ，</p><p>(2) X k 是渐进正则的，即 lim k → ∞ ‖ X k + 1 − X k ‖ = 0 ，</p><p>(3) { X k } 的任一聚点均为问题(1.4)的全局最优解。</p></sec><sec id="s8"><title>4. 实验结果与分析</title><p>在本节中，WHFPA的有效性将通过一些实验来说明。</p><p>在WHFPA和HFPA (基于阈值的不动点迭代算法)中，终止准则取为</p><p>‖ X k + 1 − X k ‖ F max { 1 , ‖ X k ‖ F } &lt; x t o l ,</p><p>评估WHPFA和HFPA结果X与原始矩阵M之间的接近度</p><p>r e l : = ‖ M − X &#175; ‖ F ‖ M ‖ F .</p><p>在约化SVD算法中，设置样本cs的个数随矩阵的秩而变化。另外，初始矩阵 X 0 由矩阵可观测元素组成，初始权值 W 0 设为初始矩阵 X 0 的奇异值。生成随机矩阵的方法如下。随机生成秩为r的矩阵</p><p>M L ∈ R m &#215; r ， M R ∈ R n &#215; r ，则 M = M L M R T 。 S R = p m n 为采样比，p是采样数。此外，对于矩阵类型的定义 [<xref ref-type="bibr" rid="hanspub.42984-ref19">19</xref>] 如下。一个矩阵称为“简单”的满足： p r ( m + n − r ) &#215; S R &gt; 0.5 ， p r ( m + n − r ) &gt; 2.6 ，“复杂”矩阵定义为 p r ( m + n − r ) &#215; S R ≤ 0.5 ， p r ( m + n − r ) ≤ 2.6 。</p><p>由于 S 1 2 阈值不动点迭代算法比SVP (Singular Value Projection) [<xref ref-type="bibr" rid="hanspub.42984-ref24">24</xref>]，MSS (Muti-Schatten p norm Surrogate) [<xref ref-type="bibr" rid="hanspub.42984-ref25">25</xref>]，SVT [<xref ref-type="bibr" rid="hanspub.42984-ref26">26</xref>] (Singular Vaule Thresholding)等方法更有效 [<xref ref-type="bibr" rid="hanspub.42984-ref19">19</xref>]，其中SVT解决的是秩最小化问题，SVP用于Tikhonov正则化问题，MSS用于解决Schatten-p正则化问题。本文只比较HFPA和WHFPA两种方法，与其他方方法的比较不再赘述。在相同的测试环境下，算法时间越短，准确率越高，效果越好。给出了各矩阵在维数、秩和抽样比上结果的差异。</p><sec id="s8_1"><title>4.1. 相同的尺寸，不同的抽样比和等级</title><p>取m = n = 100，设xtol = 10<sup>−</sup><sup>6</sup>，矩阵M的秩r从8增加到20，采样比SR分别为0.307、0.451、0.589、0.720。</p><p>对于每个子问题，随机生成100个矩阵进行测试，结果如表1所示。在约化SVD算法中设置cs = 35，μ = 0.9。实验结果表明，HFPA和WHFPA的精度相似，在10<sup>−6</sup>左右。在时间上，WHFPA将会比HFPA整体短一些。一般来说，两种方法相比，在维数小且矩阵为“困难”的情况下，WHFPA比HFPA更有效。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison of HFPA and WHFPA for randomly created small but hard matrices (m = n = 100, r = 8:4:20, xtol = 10−6</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >r</th><th align="center" valign="middle" >SR</th><th align="center" valign="middle" >solver</th><th align="center" valign="middle" >time</th><th align="center" valign="middle" >rel</th><th align="center" valign="middle" >cs</th><th align="center" valign="middle" >μ</th></tr></thead><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >0.307</td><td align="center" valign="middle" >HFPA</td><td align="center" valign="middle" >0.6036</td><td align="center" valign="middle" >6.2431e−5</td><td align="center" valign="middle" >35</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >WHFPA</td><td align="center" valign="middle" >0.4715</td><td align="center" valign="middle" >1.2672e−5</td><td align="center" valign="middle" >35</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" >12</td><td align="center" valign="middle" >0.4451</td><td align="center" valign="middle" >HFPA</td><td align="center" valign="middle" >0.7113</td><td align="center" valign="middle" >7.2465e−6</td><td align="center" valign="middle" >35</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >WHFPA</td><td align="center" valign="middle" >0.5980</td><td align="center" valign="middle" >6.9496e−6</td><td align="center" valign="middle" >35</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >0.589</td><td align="center" valign="middle" >HFPA</td><td align="center" valign="middle" >0.7417</td><td align="center" valign="middle" >2.7190e−6</td><td align="center" valign="middle" >35</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >WHFPA</td><td align="center" valign="middle" >0.6434</td><td align="center" valign="middle" >3.2415e−6</td><td align="center" valign="middle" >35</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >0.720</td><td align="center" valign="middle" >HFPA</td><td align="center" valign="middle" >0.7437</td><td align="center" valign="middle" >8.4472e−6</td><td align="center" valign="middle" >35</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >WHFPA</td><td align="center" valign="middle" >0.6528</td><td align="center" valign="middle" >6.7380e−6</td><td align="center" valign="middle" >35</td><td align="center" valign="middle" >0.9</td></tr></tbody></table></table-wrap><p>表1. HFPA和WHFPA关于随机矩阵的比较m = n = 100, r = 8:4:20, xtol = 10<sup>−6</sup>)</p></sec><sec id="s8_2"><title>4.2. 相同的采样比，不同的维度和等级</title><p>我们将维数从500增加到2000，采样比为0.570，取xtol = 10<sup>−</sup><sup>4</sup>。</p><p>随机生成100个矩阵进行测试，最终得到如下结果，详见表2。在表2中，设置μ = 0.24，在约化SVD中设置可变参数cs。HFPA和WHFPA的精度相似，在10<sup>−4</sup>左右。在时间上，维数越大，WHFPA与HFPA差距越明显。两种方法相比，在维数大且矩阵为“困难”的情况下，WHFPA比HFPA更有效。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of HFPA and WHFPA for randomly created large but hard matrices (SR = 0.570, xtol = 10−4</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >m = n</th><th align="center" valign="middle" >r</th><th align="center" valign="middle" >solver</th><th align="center" valign="middle" >time</th><th align="center" valign="middle" >rel</th><th align="center" valign="middle" >cs</th><th align="center" valign="middle" >μ</th></tr></thead><tr><td align="center" valign="middle" >500</td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >HFPA</td><td align="center" valign="middle" >3.8010</td><td align="center" valign="middle" >7.5474e−4</td><td align="center" valign="middle" >90</td><td align="center" valign="middle" >0.24</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >WHFPA</td><td align="center" valign="middle" >3.4258</td><td align="center" valign="middle" >8.0933e−4</td><td align="center" valign="middle" >90</td><td align="center" valign="middle" >0.24</td></tr><tr><td align="center" valign="middle" >800</td><td align="center" valign="middle" >80</td><td align="center" valign="middle" >HFPA</td><td align="center" valign="middle" >10.9986</td><td align="center" valign="middle" >6.0489e−4</td><td align="center" valign="middle" >120</td><td align="center" valign="middle" >0.24</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >WHFPA</td><td align="center" valign="middle" >10.2138</td><td align="center" valign="middle" >5.8972e−4</td><td align="center" valign="middle" >120</td><td align="center" valign="middle" >0.24</td></tr><tr><td align="center" valign="middle" >1000</td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >HFPA</td><td align="center" valign="middle" >12.5033</td><td align="center" valign="middle" >4.3196e−4</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.24</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >WHFPA</td><td align="center" valign="middle" >11.4922</td><td align="center" valign="middle" >8.0777e−4</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.24</td></tr><tr><td align="center" valign="middle" >2000</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >HFPA</td><td align="center" valign="middle" >53.7574</td><td align="center" valign="middle" >8.7843e−4</td><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.24</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >WHFPA</td><td align="center" valign="middle" >48.8033</td><td align="center" valign="middle" >9.1057e−4</td><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.24</td></tr></tbody></table></table-wrap><p>表2. HFPA和WHFPA关于随机矩阵的比较(SR = 0.570, xtol = 10<sup>−4</sup>)</p></sec><sec id="s8_3"><title>4.3. 有噪声的随机矩阵</title><p>假设带噪声的矩阵定义为</p><p>B i j = M i j + Z i j</p><p>其中，矩阵 Z ∈ R m &#215; n 是方差为 σ ，零均值的高斯矩阵。设置μ = 0.9，分别在方差10<sup>−</sup><sup>2</sup>和10<sup>−</sup><sup>1</sup>的噪声矩阵下进行试验，矩阵维数设置为m = n = 1000，取xtol = 10<sup>−</sup><sup>4</sup>。实验数据详见表3。WHFPA比HFPA的精度好，在10<sup>−4</sup>左右。在时间上，HFPA是WHFPA的2倍。两种方法相比，在有噪声的情况下，WHFPA比HFPA更有效。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Comparison of HFPA and WHFPA for randomly created noise disturbance matrices (m = n = 1000, xtol = 10−4</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >noise σ</th><th align="center" valign="middle"  rowspan="2"  >m = n</th><th align="center" valign="middle"  rowspan="2"  >r</th><th align="center" valign="middle"  rowspan="2"  >cs</th><th align="center" valign="middle"  rowspan="2"  >μ</th><th align="center" valign="middle"  rowspan="2"  >SR</th><th align="center" valign="middle"  colspan="2"  >HFPA</th><th align="center" valign="middle"  colspan="2"  >WHFPA</th></tr></thead><tr><td align="center" valign="middle" >time</td><td align="center" valign="middle" >rel</td><td align="center" valign="middle" >time</td><td align="center" valign="middle" >rel</td></tr><tr><td align="center" valign="middle" >10<sup>−</sup><sup>2</sup></td><td align="center" valign="middle" >1000</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >0.119</td><td align="center" valign="middle" >28.9859</td><td align="center" valign="middle" >2.00e−03</td><td align="center" valign="middle" >13.0047</td><td align="center" valign="middle" >1.90e−03</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >0.390</td><td align="center" valign="middle" >44.3209</td><td align="center" valign="middle" >1.94e−03</td><td align="center" valign="middle" >20.3113</td><td align="center" valign="middle" >1.95e−03</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >150</td><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >0.570</td><td align="center" valign="middle" >67.2349</td><td align="center" valign="middle" >1.52e−03</td><td align="center" valign="middle" >33.6021</td><td align="center" valign="middle" >1.51e−03</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >200</td><td align="center" valign="middle" >300</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >0.555</td><td align="center" valign="middle" >82.6623</td><td align="center" valign="middle" >1.46e−03</td><td align="center" valign="middle" >37.6521</td><td align="center" valign="middle" >1.42e−03</td></tr><tr><td align="center" valign="middle" >10<sup>−</sup><sup>1</sup></td><td align="center" valign="middle" >1000</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >0.119</td><td align="center" valign="middle" >26.4703</td><td align="center" valign="middle" >2.00e−02</td><td align="center" valign="middle" >11.9425</td><td align="center" valign="middle" >1.90e−02</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >0.390</td><td align="center" valign="middle" >44.4525</td><td align="center" valign="middle" >1.87e−02</td><td align="center" valign="middle" >20.3280</td><td align="center" valign="middle" >1.89e−02</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >150</td><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >0.570</td><td align="center" valign="middle" >67.2419</td><td align="center" valign="middle" >1.49e−02</td><td align="center" valign="middle" >30.7486</td><td align="center" valign="middle" >1.50e−02</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >200</td><td align="center" valign="middle" >300</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >0.555</td><td align="center" valign="middle" >82.7340</td><td align="center" valign="middle" >1.38e−02</td><td align="center" valign="middle" >41.0454</td><td align="center" valign="middle" >1.38e−02</td></tr></tbody></table></table-wrap><p>表3. HFPA和WHFPA关于有噪声随机矩阵的比较(m = n = 1000, xtol = 10<sup>−4</sup>)</p></sec></sec><sec id="s9"><title>5. 结论</title><p>本文利用 S 1 2 拟范数正则化方法，提出了一种基于奇异值贡献度的权重处理方法。针对非凸、非光滑、非Lipschitz优化问题，首先给出了阈值不动点迭代算法中权值对奇异值的影响，然后给出了经过延续处理以及约化奇异值分解的改进算法。实验结果表明，WHFPA性能优于HFPA，说明了算法的有效性。</p></sec><sec id="s10"><title>文章引用</title><p>王 素,顾颖菁,袁 泉. 基于加权Schatten-1/2范数的低秩矩阵近似算法Weighted Schatten-1/2 Norm Minimization for Low-Rank Matrix Approximation[J]. 理论数学, 2021, 11(06): 998-1009. https://doi.org/10.12677/PM.2021.116114</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.42984-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Li, W., Zhao, L. and Lin, Z. (2015) Non-Local Image Inpainting Using Low-Rank Matrix Completion. Computer Graphics Forum, 34, 111-122. &lt;br&gt;https://doi.org/10.1111/cgf.12521</mixed-citation></ref><ref id="hanspub.42984-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Candes, E.J. and Recht, B. (2009) Exact Matrix Completion via Convex Optimization. Foundations of Computational Mathematics, 9, 717-772. &lt;br&gt;https://doi.org/10.1007/s10208-009-9045-5</mixed-citation></ref><ref id="hanspub.42984-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Gogna, A. and Majumdar, A. (2015) Matrix Completion Incor-porating Auxiliary Information for Recommender System Design. Expert Systems with Applications, 42, 5789-5799. &lt;br&gt;https://doi.org/10.1016/j.eswa.2015.04.012</mixed-citation></ref><ref id="hanspub.42984-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Candes, E.J. and Emmanuel, J. (2010) The Power of Convex Relaxation: Near-Optimal Matrix Completion. IEEE Transactions on Information Theory, 56, 2053-2080. &lt;br&gt;https://doi.org/10.1109/TIT.2010.2044061</mixed-citation></ref><ref id="hanspub.42984-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Candes, E.J. and Recht, B. (2009) Exact Matrix Completion via Convex Optimization. Foundations of Computational Mathematics, 9, 717-772. &lt;br&gt;https://doi.org/10.1007/s10208-009-9045-5</mixed-citation></ref><ref id="hanspub.42984-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Recht, B., Fazel, M. and Parrilo, P.A. (2010) Guaranteed Mini-mum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization. SIAM Review, 52, 471-501. &lt;br&gt;https://doi.org/10.1137/070697835</mixed-citation></ref><ref id="hanspub.42984-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Hu, Y., Zhang, D. and Ye, J. (2013) Fast and Accurate Matrix Completion via Truncated Nuclear Norm Regularization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 2117-2130.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2012.271</mixed-citation></ref><ref id="hanspub.42984-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, D., Hu, Y. and Ye, J. (2013) Matrix Completion by Trun-cated Nuclear Norm Regularization. IEEE Conference on Computer Vision and Pattern Recognition, Vol. 35, 2192-2199.</mixed-citation></ref><ref id="hanspub.42984-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Candes, E.J. and Plan, Y. (2010) Matrix Completion with Noise. Proceedings of the IEEE, 98, 925-936.  
&lt;br&gt;https://doi.org/10.1109/JPROC.2009.2035722</mixed-citation></ref><ref id="hanspub.42984-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Koltchinskii, V., Lounici, K. and Tsybakov, A.B. (2011) Nu-clear-Norm Penalization and Optimal Rates for Noisy Low-Rank Matrix Completion. The Annals of Statistics, 39, 2302-2329. &lt;br&gt;https://doi.org/10.1214/11-AOS894</mixed-citation></ref><ref id="hanspub.42984-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Xu, Z., Chang, X., Xu, F. and Zhang, H. (2012)   Regu-larization: A Thresholding Representation Theory and a Fast Solver. IEEE Transactions on Neural Networks and Learning Systems, 23, 1013-1027.  
&lt;br&gt;https://doi.org/10.1109/TNNLS.2012.2197412</mixed-citation></ref><ref id="hanspub.42984-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Gu, S., Zhang, L. and Zuo, W. (2014) Weighted Nuclear Norm Minimization with Application to Image Denoising. 2014 IEEE Conference on Computer Vision and Pattern Recognition, Vol. 1, 2862-2869.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2014.366</mixed-citation></ref><ref id="hanspub.42984-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Oliveira, J.P., Bioucas-Dias, J.M. and Figueiredo, M.A.T. (2009) Adaptive Total Variation Image Deblurring: A Majorization Minimization Approach. Signal Processing, 89, 1683-1693. &lt;br&gt;https://doi.org/10.1016/j.sigpro.2009.03.018</mixed-citation></ref><ref id="hanspub.42984-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Lu, Z., Zhang, Y. and Liu, X. (2015) Penalty Decomposition Methods for Rank Minimization. Optimization Methods and Software, 30, 531-558. &lt;br&gt;https://doi.org/10.1080/10556788.2014.936438</mixed-citation></ref><ref id="hanspub.42984-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Lai, M., Xu, Y. and Yin, W. (2013) Improved Iteratively Reweighted Least Squares for Unconstrained Smoothed   Minimization. SIAM Journal on Numerical Analysis, 51, 927-957. &lt;br&gt;https://doi.org/10.1137/110840364</mixed-citation></ref><ref id="hanspub.42984-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Mohan, K. and Fazel, M. (2012) Iterative Reweighted Algo-rithms for Matrix Rank Minimization. Journal of Machine Learning Research, 13, 3441-3473.</mixed-citation></ref><ref id="hanspub.42984-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Rao, G., Peng, Y. and Xu, Z. (2013) Robust Sparse and Low-Rank Matrix Decomposition Based on the   Modeling. Science Chi-na-Information Sciences, 43, 733-748.</mixed-citation></ref><ref id="hanspub.42984-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Xu, Z.B., Guo, H., Wang, Y. and Zhang, H. (2012) The Representation of   Regularizer among   Regularizer: An Experimental Study Based on Phase Diagram. Acta Automatica Sinica, 38, 1225-1228.  
&lt;br&gt;https://doi.org/10.3724/SP.J.1004.2012.01225</mixed-citation></ref><ref id="hanspub.42984-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Peng, D., Xiu, N. and Yu, J. (2017)   Regularization Methods and Fixed Point Algorithms for Affine Rank Minimization Problems. Computational Optimization and Ap-plications, 67, 543-569.  
&lt;br&gt;https://doi.org/10.1007/s10589-017-9898-5</mixed-citation></ref><ref id="hanspub.42984-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Mirsky, L. (1975) A Trace Inequality of John von Neumann. Monatshefte für Mathematik, 79, 303-306.  
&lt;br&gt;https://doi.org/10.1007/BF01647331</mixed-citation></ref><ref id="hanspub.42984-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Xie, Y., Gu, S. and Liu, Y. (2016) Weighted Schatten p-Norm Mini-mization for Image Denoising and Background Subtraction. IEEE Transactions on Image Processing, 25, 4842-4857. &lt;br&gt;https://doi.org/10.1109/TIP.2016.2599290</mixed-citation></ref><ref id="hanspub.42984-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Ma, S., Goldfarb, D. and Chen, L. (2011) Fixed Point and Bregman Iterative Methods for Matrix Rank Minimization. Mathematical Programming, 128, 321-353. &lt;br&gt;https://doi.org/10.1007/s10107-009-0306-5</mixed-citation></ref><ref id="hanspub.42984-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Drineas, P., Kannan, R. and Mahoney, M.W. (2006) Fast Monte Carlo Algorithms for Matrices q: Computing Low-Rank Approximations to a Matrix. SIAM Journal on Computing, 36, 158-183. &lt;br&gt;https://doi.org/10.1137/S0097539704442696</mixed-citation></ref><ref id="hanspub.42984-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Jain, P., Meka, R. and Dhillon, I. (2010) Guaranteed Rank Minimization via Singular Value Projection. Proceedings of the Advances in Neural Information Processing Systems, Vol. 1, 937-945.</mixed-citation></ref><ref id="hanspub.42984-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Cai, J.F., Candes, E.J. and Shen, Z.W. (2010) A Singular Value Thresholding Algo-rithm for Matrix Completion. SIAM Journal on Optimization, 20, 1956-1982. &lt;br&gt;https://doi.org/10.1137/080738970</mixed-citation></ref><ref id="hanspub.42984-ref26"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Xu, C., Lin, Z.C. and Zha, H.B. (2017) A Unified Convex Surrogate for the Schatten-p Norm. Proceedings of the Thirty- First AAAI Conference on Artificial Intelligence, Vol. 25, 926-932.</mixed-citation></ref></ref-list></back></article>