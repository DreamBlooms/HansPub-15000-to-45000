<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2019.92040</article-id><article-id pub-id-type="publisher-id">CSA-28870</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20190200000_38564815.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  Ada_Nesterov动量法——一种具有自适应学习率的Nesterov动量法
  Ada_Nesterov Momentum Algorithm—The Nesterov Momentum Algorithm with Adaptive Learning Rate
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>贾</surname><given-names>熹滨</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>史</surname><given-names>佳帅</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>北京工业大学信息学部，北京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>29</day><month>01</month><year>2019</year></pub-date><volume>09</volume><issue>02</issue><fpage>351</fpage><lpage>358</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   Nesterov动量法可以很好地改进梯度下降方向，但是其所有参数都具有相同的学习率，并且学习率需要人为设定。Adadelta算法可以自适应学习率，并且每维参数具有独立的学习率。因此，本文首先基于Adadelta算法推导出每一维的学习率公式，其次将其带入Nesterov动量法中，得到了Ada_Nesterov动量法。为了验证提出的Ada_Nesterov动量法，本文设计了两个实验。实验结果表明：动量参数0.5时，Ada_Nesterov动量法在VggNet_16神经网络架构上，基于CIFAR_100数据集的验证准确率最高，损失最小，收敛速度最快。即Ada_Nesterov动量法改进了Nesterov动量法，具有自适应学习率。 The Nesterov momentum algorithm can efficiently improve the gradient descent direction. However, all parameters of the Nesterov momentum algorithm share the same learning rate, and the learning rate needs to be set by workers. Adadelta algorithm has the characteristic of adaptive learning rate, and each dimension parameter has independent learning rate. Therefore, we firstly deduced the learning rate formula of each dimension, which was based on Adadelta algorithm. Secondly we introduced it into the Nesterov momentum algorithm. And finally, Ada_Nesterov momentum algorithm was proposed. To verify Ada_Nesterov momentum algorithm, two experiments were designed, which indicated that with momentum parameter 0.5, and VggNet_16, the CIFAR_100 dataset, Ada_Nesterov momentum algorithm achieved the highest test and evaluation accuracy, the lowest valuation loss, and fastest rate of convergence. Therefore, Ada_Nesterov momentum algorithm improved Nesterov momentum algorithm with adaptive learning rate. 
  
 
</p></abstract><kwd-group><kwd>Nesterov动量法，Adadelta算法，Ada_Nesterov动量法，自适应学习率, Nesterov Momentum Algorithm</kwd><kwd> Adadelta Algorithm</kwd><kwd> Ada_Nesterov Momentum Algorithm</kwd><kwd> Adaptive Learning Rate</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>Ada_Nesterov动量法<sup> </sup></title><p>——一种具有自适应学习率的Nesterov动量法</p><p>贾熹滨，史佳帅</p><p>北京工业大学信息学部，北京</p><p><img src="//html.hanspub.org/file/19-1541246x1_hanspub.png" /></p><p>收稿日期：2019年1月29日；录用日期：2019年2月8日；发布日期：2019年2月15日</p><disp-formula id="hanspub.28870-formula13"><graphic xlink:href="//html.hanspub.org/file/19-1541246x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>Nesterov动量法可以很好地改进梯度下降方向，但是其所有参数都具有相同的学习率，并且学习率需要人为设定。Adadelta算法可以自适应学习率，并且每维参数具有独立的学习率。因此，本文首先基于Adadelta算法推导出每一维的学习率公式，其次将其带入Nesterov动量法中，得到了Ada_Nesterov动量法。为了验证提出的Ada_Nesterov动量法，本文设计了两个实验。实验结果表明：动量参数0.5时，Ada_Nesterov动量法在VggNet_16神经网络架构上，基于CIFAR_100数据集的验证准确率最高，损失最小，收敛速度最快。即Ada_Nesterov动量法改进了Nesterov动量法，具有自适应学习率。</p><p>关键词 :Nesterov动量法，Adadelta算法，Ada_Nesterov动量法，自适应学习率</p><disp-formula id="hanspub.28870-formula14"><graphic xlink:href="//html.hanspub.org/file/19-1541246x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/19-1541246x7_hanspub.png" /> <img src="//html.hanspub.org/file/19-1541246x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>优化算法是深度学习的一个重要组成部分，近年来深度学习能够取得瞩目成绩的一个原因为其优化算法的改进。在求解深度学习网络时，优化算法按照反向求导梯度的阶数可以分为一阶方法(梯度下降法 [<xref ref-type="bibr" rid="hanspub.28870-ref1">1</xref>] 、随机梯度下降法 [<xref ref-type="bibr" rid="hanspub.28870-ref2">2</xref>] 、动量法 [<xref ref-type="bibr" rid="hanspub.28870-ref3">3</xref>] 等)和二阶近似法(牛顿法 [<xref ref-type="bibr" rid="hanspub.28870-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.28870-ref5">5</xref>] 、共轭梯度法 [<xref ref-type="bibr" rid="hanspub.28870-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.28870-ref7">7</xref>] 和存储受限的BFGS(L-BFGS)算法等)。但是由于二阶近似方法计算量很大，对于高维度的数据的优化效果不好，所以本文主要研究一阶方法。</p><p>深度学习中一阶梯度优化算法的改进均为以梯度下降法为基础，对其梯度下降方向或者学习率进行改进。Nesterov动量法可以很好地改进梯度下降方向，但是其所有参数都具有相同的学习率，并且学习率需要人为设定。Adadelta算法具有自适应学习率的功能，但是在训练后期存在梯度下降方向不准确的现象。因此，本文结合Nesterov动量法的梯度下降策略和Adadelta算法的自适应学习率策略，提出了具有自适应学习率，且梯度下降方向准确的Ada_Nesterov动量法。</p></sec><sec id="s4"><title>2. 深度学习优化算法比较</title><sec id="s4_1"><title>2.1. 梯度下降法</title><p>深度学习优化算法求解损失函数最小化的问题，其基础的算法为梯度下降法(Gradient Descent, GD) [<xref ref-type="bibr" rid="hanspub.28870-ref1">1</xref>] 。在深度学习领域，梯度下降法又被称为批量梯度下降法(BGD)。在批量梯度下降法中，学习率可以理解为梯度下降法的步长，梯度可以理解为梯度下降法的下降方向，学习率决定以多大的步长更新参数。批量梯度下降法在每次迭代的时候，通过计算数据集上所有数据来完成一次迭代，该算法可以得到较准确的局部最优解或者全局最优解，但是随着数据集的增多，算法速度会越来越低。同时，该算法不能在训练过程中添加数据。</p></sec><sec id="s4_2"><title>2.2. 随机梯度下降法</title><p>为了解决批量梯度下降法的求解速度问题，Bottou [<xref ref-type="bibr" rid="hanspub.28870-ref2">2</xref>] 于1998年提出了随机梯度下降法。与批量梯度下降法不同的是随机梯度下降法首先把数据集分成m个小批量样本，然后通过计算他们的梯度均值，计算梯度无偏估计，最后进行参数迭代。当m = 1时，称为在线梯度下降法；当m &gt; 1时，称为小批量随机梯度下降法。</p><p>随机梯度下降法的优点为收敛速度快，其计算时间不取决于数据集的大小。对于大容量的数据集，往往在没有全部计算完数据之前，其收敛精度已经达到规定的范围。随机梯度下降法的缺点为需要人为调参。由于在随机梯度下降法随机采样m个训练样本，引入了梯度估计的噪声源(在极点处不消失)，因此随机梯度下降法不能像批量梯度下降法固定学习率的数值，而是要根据实验结果人为的调整学习率的大小。</p></sec><sec id="s4_3"><title>2.3. 动量法</title><p>随机梯度下降法虽然比梯度下降法的收敛速度提高，但是在接近最优点时有时很难通过陡谷，即在一个维度上的表面弯曲程度远大于其他维度的区域 [<xref ref-type="bibr" rid="hanspub.28870-ref3">3</xref>] 。为了提高随机梯度下降法的收敛速度Polyak等人 [<xref ref-type="bibr" rid="hanspub.28870-ref8">8</xref>] 提出了动量法。</p><p>与随机梯度下降法不同的是动量法在速度向量上乘以了一个超参数α ϵ [0,1)。和学习率一样，超参数也需要不断调整。对于梯度点处具有相同方向的维度时，超参数增大；相反，超参数减小。从而，通过减少摇摆，提高收敛速度 [<xref ref-type="bibr" rid="hanspub.28870-ref9">9</xref>] 。</p></sec><sec id="s4_4"><title>2.4. Nesterov动量法</title><p>动量法梯度下降改进了梯度的方向，但是该梯度方向具有一定的盲目性，不能预测以后的梯度方向是上升还是下降。受Nesterov [<xref ref-type="bibr" rid="hanspub.28870-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.28870-ref11">11</xref>] 影响，Sutskever等人 [<xref ref-type="bibr" rid="hanspub.28870-ref12">12</xref>] 提出了动量算法的一个变种——Nesterov动量法。Nesterov动量法在动量法的梯度计算之前，对参θ进行了校正，其更新规则如(公式(1)、公式(2))所示：</p><p>θ ← θ + Δ θ (1)</p><p>Δ θ = ∂ θ − η ∇ θ ( 1 m ∑ i = 1 m L ( f ( x ( i ) ; θ ) , y ( i ) ) ) (2)</p><p>实验表明在凸批量梯度的情况下，Nesterov动量法将额外误差收敛率从O(1/k) (k步后)改进到(1/k<sup>2</sup>) [<xref ref-type="bibr" rid="hanspub.28870-ref12">12</xref>] 。但是，在随机梯度的情况下，Nesterov动量法没有改进收敛率。</p></sec><sec id="s4_5"><title>2.5. Adagrad算法</title><p>由于损失通常敏感于参数空间的某些方向，动量法可以改变梯度下降的方向，但是却引入了超参数。基于Delta-bar-delta算法 [<xref ref-type="bibr" rid="hanspub.28870-ref13">13</xref>] 中自动适应模型参数各自学习率的方法，Duchi等人 [<xref ref-type="bibr" rid="hanspub.28870-ref14">14</xref>] 提出了Adagrad算法。该算法基于一种很简单的思想：具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降，其更新规则如公式(3)~公式(6)所示：</p><p>计算梯度：</p><p>g ← ∇ θ ( 1 m ∑ i = 1 m L ( f ( x ( i ) ; θ ) , y ( i ) ) ) (3)</p><p>累积平方梯度：</p><p>r ← r + g ⊙ g (4)</p><p>通过逐元素地应用除和求平方根，计算θ更新：</p><p>Δ θ ← − ε ξ + r ⊙ g (5)</p><p>应用更新：</p><p>θ ← θ + Δ θ (6)</p><p>Adagrad算法具有良好的理论依据，但是由于其在分母中累加了梯度的平方根，在训练过程中累加会持续增长，导致学习率变小。在学习率无限小时，Adagrad算法将无法获得额外的信息。Adagrad算法在某些模型 [<xref ref-type="bibr" rid="hanspub.28870-ref15">15</xref>] [<xref ref-type="bibr" rid="hanspub.28870-ref16">16</xref>] 上可以得到很好的收敛，但不是全部。为了解决批量梯度下降法的求解速度问题，Bottou [<xref ref-type="bibr" rid="hanspub.28870-ref2">2</xref>] 于1998年提出了随机梯度下降法。与批量梯度下降法不同的是随机梯度下降法首先把数据集分成m个小批量样本，然后通过计算他们的梯度均值，计算梯度无偏估计，最后进行参数迭代(公式(3))。当m = 1时，称为在线梯度下降法；当m &gt; 1时，称为小批量随机梯度下降法。</p><p>随机梯度下降法的优点为收敛速度快，其计算时间不取决于数据集的大小。对于大容量的数据集，往往在没有全部计算完数据之前，其收敛精度已经达到规定的范围。随机梯度下降法的缺点为需要人为调参。由于在随机梯度下降法随机采样m个训练样本，引入了梯度估计的噪声源(在极点处不消失)，因此随机梯度下降法不能像批量梯度下降法固定学习率的数值，而是要根据实验结果人为的调整学习率的大小。</p></sec><sec id="s4_6"><title>2.6. Adadelta算法</title><p>Adadelta算法 [<xref ref-type="bibr" rid="hanspub.28870-ref17">17</xref>] 是Adagrad算法的扩展，较Adagrad算法的改进有以下几点：a) 将梯度的平方递归地表示成所有历史梯度平方的均值，而不是计算所有的梯度的平方(公式(7))；</p><p>E [ g 2 ] t = γ E [ g 2 ] t − 1 + ( 1 − γ ) g t 2 (7)</p><p>b) 为了解决非凸优化问题，Adadelta算法更新规则中设计参数具有相同的假设单位，首次定义了另一个指数衰减均值，不是梯度平方，而是参数平方的更新(公式(8))；</p><p>E [ Δ θ 2 ] t = γ E [ Δ θ 2 ] t − 1 + ( 1 − γ ) Δ θ t 2 (8)</p><p>改进后的Adadelta算法的更新规格为：</p><p>Δ θ t = − R M S [ Δ θ ] t − 1 R M S [ Δ g ] t ⋅ g t (9)</p><p>Adadelta算法只需要小的计算量，并且一直是一阶梯度计算。此外，Adadelta算法不需要调节学习率。在模型训练初期和中期，Adadelta算法具有很好的收敛速度，但是在训练后期，Adadelta算法会反复在局部最小值附近抖动，很难脱离局部最小值吸引盆。这时如果换Adadelta算法为Nesterov动量法，就会发现模型测试准确率将提高5%左右，说明Adadelta算法在局部最小值附近时，梯度下降方向不准确。</p><p>此外，为了使Adam算法和AdaGrad算法有更快的收敛速度，张慧 [<xref ref-type="bibr" rid="hanspub.28870-ref18">18</xref>] 在Adam算法和AdaGrad算法中引入了动量的思想，提出了带动量的Adam算法和Adarad算法：AMM算法。</p></sec></sec><sec id="s5"><title>3. Ada_Nesterov动量法定义</title><p>Ada_Nesterov动量法在Nesterov动量法的基础上添加了Adadelta算法的自适应学习率策略。Adadelta算法在Adagrad算法基础上做出来两个方面的改进，首先，为解决Adagrad算法在训练后期学习率非常小的问题，将梯度的平方递归地表示成所有历史梯度平方的均值，而不是计算所有的梯度的平方(公式(8))，得到公式(10)：</p><p>Δ θ t = − η R M S [ Δ g ] t ⋅ g t (10)</p><p>其次，AdaDelta算法通过计算指数衰减的累积梯度平方和(公式(8))，解决了SGD算法、Adagrad算法、动量法等算法在参数更新时单位不一致的问题，得到梯度下降的公式(9)。比较公式(10)和公式(9)，得出每一维的学习率为指数衰减均值的均方根(公式(11))：</p><p>η = R M S [ Δ θ ] t − 1 (11)</p><p>把公式(11)带入Nesterov动量法的梯度下降公式(2)，得Ada_Nesterov动量法的梯度下降公式(12)：</p><p>Δ θ = ρ [ Δ θ ] t − 1 − R M S [ Δ θ ] t − 1 ∇ θ ( 1 m ∑ i = 1 m L ( f ( x ( i ) ; θ ) , y ( i ) ) ) (12)</p><p>Ada_Nesterov动量法的具体更新规则如算法1所示：</p><disp-formula id="hanspub.28870-formula15"><graphic xlink:href="//html.hanspub.org/file/19-1541246x21_hanspub.png"  xlink:type="simple"/></disp-formula><p>步骤1：设置动量因子ρ，常数项，衰减系数α；初始化假设函数变量θ，初始化梯度下降的初始期望的平方为零。</p><p>步骤2：在设定的循环次数内，计算指数衰减均值，并对其求平方根，更新假设函数变量θ，并计算下一步梯度下降参数。</p><p>步骤3：判断循环是否达到终止条件，如果未达到终止条件，则循环步骤2，如果达到了终止条件，则算法终止。</p></sec><sec id="s6"><title>4. 实验验证</title><p>本文对Ada_Nesterov动量法进行了两个实验，实验一构建了一个简单的卷积神经网络的网络训练，以确定Ada_Nesterov动量法的最优动量因子。实验二构建了一个神经网络，并训练了CIFAR-100数据集，比较Nesterov动量法，Adadelta算法和Ada_Nesterov动量法的性能。</p><sec id="s6_1"><title>4.1. 实验一</title><p>为了确定Ada_Nesterov动量法的最优动量因子，本文构建了一个具有两个卷积层，两个全连接层的简单卷积神经网络。此神经网络卷积层的卷积核大小为5 &#215; 5，第二个卷积成后进行dropout操作，激活函数为ReLU [<xref ref-type="bibr" rid="hanspub.28870-ref19">19</xref>] 。训练和测试数据集为MNIST数据集。试验平台为OS X EI Capitan系统，Intel Core i5主板，8G内存，tensorflow为1.2.1 CPU版本。</p><p>Ada_Nesterov动量法的常数项为0.00001，衰减系数α为0.001。在经过1个epoch后，不同的动量因子，得到的测试正确率和损失结果如表1所示：</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The results of Ada_Nesterov momentum algorithm with different parameter</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >动量因子</th><th align="center" valign="middle" >正确率</th><th align="center" valign="middle" >损失</th></tr></thead><tr><td align="center" valign="middle" >0.1</td><td align="center" valign="middle" >94%</td><td align="center" valign="middle" >0.2187</td></tr><tr><td align="center" valign="middle" >0.2</td><td align="center" valign="middle" >95%</td><td align="center" valign="middle" >0.1898</td></tr><tr><td align="center" valign="middle" >0.3</td><td align="center" valign="middle" >95%</td><td align="center" valign="middle" >0.1849</td></tr><tr><td align="center" valign="middle" >0.4</td><td align="center" valign="middle" >96%</td><td align="center" valign="middle" >0.1502</td></tr><tr><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >97%</td><td align="center" valign="middle" >0.1378</td></tr><tr><td align="center" valign="middle" >0.6</td><td align="center" valign="middle" >96%</td><td align="center" valign="middle" >0.1748</td></tr><tr><td align="center" valign="middle" >0.7</td><td align="center" valign="middle" >95%</td><td align="center" valign="middle" >0.1833</td></tr><tr><td align="center" valign="middle" >0.8</td><td align="center" valign="middle" >88%</td><td align="center" valign="middle" >0.4458</td></tr><tr><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" >10%</td><td align="center" valign="middle" >2.3045</td></tr></tbody></table></table-wrap><p>表1. Ada_Nesterov动量法在不同的动量因子下的测试结果</p><p>由表1可以看出，在影响因子0.5时，Ada_Nesterov动量法的正确率最高(97%)，损失最小(0.1378)；在影响因子0.9时，Ada_Nesterov动量法的正确率最低(10%)，损失最大(2.3.45)。</p><p>也可以看到随着动量因子在0.5附近其正确率和损失大小比较稳定，在动量因子减小或者增大时，正确率均会降低，损失均增加。同时，动量因子减小正确率缓慢降低，损失缓慢增加；但是，动量因子增大正确率急剧降低，损失急剧增加。</p><p>综上所述，动量因子在0.5时，Ada_Nesterov动量法的正确率最高，损失最小。本文在进行实验二时，对动量因子0.5的Ada_Nesterov动量法进行验证。</p></sec><sec id="s6_2"><title>4.2. 实验二</title><p>为了验证训练了Ada_Nesterov动量法，Nesterov动量法和Adadelta算法的性能，本文基于VggNet-16网络架构，训练了CIFAR-100数据集。三种算法均设置64的批处理量，训练6000步。图1和图2分别为三种算法的验证正确率曲线和验证损失曲线。</p><p>图1. 三种优化算法在CIFAR-100数据集上的验证准确率</p><p>由图1可知，Ada_Nesterov动量法的验证准确率曲线基本上处于Adadelta算法和Nesterov动量法之上，即Ada_Nesterov动量法的验证准确率整体上高于Adadelta算法和Nesterov动量法。在训练6000步时，三种算法的准确率均为最高，Ada_Nesterov动量法，Adadelta算法和Nesterov动量法的验证准确率分别为：82.8%，78.1%和68.8%。</p><p>图2. 三种优化算法在CIFAR-100数据集上的验证损失</p><p>由图2可知，Ada_Nesterov动量法的验证损失曲线基本上处于Adadelta算法和Nesterov动量法之下，即Ada_Nesterov动量法的验证损失整体上高于Adadelta算法和Nesterov动量法。在训练6000步时，三种算法的损失均为最低，Ada_Nesterov动量法，Adadelta算法和Nesterov动量法的验证损失分别为：0.72、0.94和0.91。</p><p>本文对训练得到的各神经网络模型，基于CIFAR-100测试数据集进行了测试，测试结果如表2所示。由表2可知，基于Nesterov动量法、Adadelta算法和Ada_Nesterov动量法训练得到的模型的测试正确率分别为：78.5%、75.2%和61.7%；测试损失分别为：1.13、1.58和2.17。</p><p>此外，从三种优化算法的运行时间分析，运行6000时，Ada_Nesterov动量法，Adadelta算法和Nesterov动量法耗时分别约为16小时，19小时和10小时。可见，Adadelta算法复杂度最高，Nesterov动量法复杂度最低，Ada_Nesterov动量法复杂度介于Adadelta算法和Nesterov动量法之间。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> System resulting data of standard experimen</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >测试正确率</th><th align="center" valign="middle" >测试损失</th></tr></thead><tr><td align="center" valign="middle" >Nesterov动量法</td><td align="center" valign="middle" >61.7%</td><td align="center" valign="middle" >2.17</td></tr><tr><td align="center" valign="middle" >Adadelta算法</td><td align="center" valign="middle" >75.2%</td><td align="center" valign="middle" >1.58</td></tr><tr><td align="center" valign="middle" >Ada_Nesterov动量法</td><td align="center" valign="middle" >78.5%</td><td align="center" valign="middle" >1.13</td></tr></tbody></table></table-wrap><p>表2. 各算法测试正确率和测试损失</p><p>综上所述，动量因子0.5的Ada_Nesterov动量法可以给每一维数据一个自适应学习率，算法复杂度适中，具有验证准确率高，损失小，收敛速度快，是一个综合性能较好的优化算法。</p></sec></sec><sec id="s7"><title>5. 结论</title><p>Nesterov动量法可以很好的改变动量法上梯度的下降方向，但是Nesterov动量法的每一维度都具有相同的学习率，本文借助Adadelta算法的自适应学习率的策略，提出了自适应学习率的Ada_Nesterov动量法。为了验证Ada_Nesterov动量法的性能，本文设计了两个实验。实验一为确定Ada_Nesterov动量法的最优的动量因子ρ。我们分别在动量因子0.1~0.9的情况下，基于MNIST数据集训练了一个简单的卷积神经网络。对神经网络模型进行测试的结果表明：动量因子0.5时Ada_Nesterov动量法具有最高的测试正确率和损失。实验二为Ada_Nesterov动量法(动量因子0.5)与Adadelta算法和Nesterov动量法的性能比较。我们在VggNet_16神经网络架构上，基于CIFAR_100数据集，分别对三种算法进行了训练，实验结果表明Ada_Nesterov动量法的准确率最高，损失最小，收敛速度最快。即Ada_Nesterov动量法改进了Nesterov动量法的自适应学习率。</p></sec><sec id="s8"><title>基金项目</title><p>北京工业大学国际科研合作种子基金项目资助(项目编号：2018A02)。</p></sec><sec id="s9"><title>文章引用</title><p>贾熹滨,史佳帅. Ada_Nesterov动量法——一种具有自适应学习率的Nesterov动量法Ada_Nesterov Momentum Algorithm—The Nesterov Momentum Algorithm with Adaptive Learning Rate[J]. 计算机科学与应用, 2019, 09(02): 351-358. https://doi.org/10.12677/CSA.2019.92040</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.28870-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Cauchy, A. (1847) Méthode générale pour la résolution des systemes d’équations simultanées. Comp. Rend. Sci. Paris, 25, 536-538.</mixed-citation></ref><ref id="hanspub.28870-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Bottou, L. (1998) Online Learning and Stochastic Approximations. On-Line Learning in Neural Net-works, 17, 142.</mixed-citation></ref><ref id="hanspub.28870-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Sutton, R.S. (1986) Two Problems with Backpropagation and Other Steepest-Descent Learning Procedures for Networks. In: Proceedings of the 8th Annual Conference of the Cognitive Science Society, Erlbaum, Hillsdale, NJ, 823-831.</mixed-citation></ref><ref id="hanspub.28870-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Levenberg, K. (1944) A Method for the Solution of Certain Non-Linear Problems in Least Squares. Quarterly of Applied Mathematics, 2, 164-168. &lt;br&gt;https://doi.org/10.1090/qam/10666</mixed-citation></ref><ref id="hanspub.28870-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Marquardt, D.W. (1963) An Algorithm for Least-Squares Estimation of Nonlinear Parameters. Journal of the Society for Industrial and Applied Mathematics, 11, 431-441. &lt;br&gt;https://doi.org/10.1137/0111030</mixed-citation></ref><ref id="hanspub.28870-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Møller, M.F. (1993) Efficient Training of Feed-Forward Neural Networks. DAIMI Report Series, 22, No. 464.</mixed-citation></ref><ref id="hanspub.28870-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Le Roux, N., Bengio, Y. and Fitzgibbon, A. (2011) 15. Improving First and Second-Order Methods by Modeling Uncertainty. In: Sra, S., Nowozin, S. and Wright, S.J., Eds., Optimization for Machine Learning, The MIT Press, Cambridge, MA, 403.</mixed-citation></ref><ref id="hanspub.28870-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">LeCun, Y., Boser, B., Denker, J.S., et al. (1989) Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation, 1, 541-551. &lt;br&gt;https://doi.org/10.1162/neco.1989.1.4.541</mixed-citation></ref><ref id="hanspub.28870-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">LeCun, Y.A., Bottou, L., Orr, G.B., et al. (2012) Efficient Backprop. In: Neural Networks: Tricks of the Trade, Springer Berlin Heidelberg, 9-48.</mixed-citation></ref><ref id="hanspub.28870-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Nesterov, Y. (1983) A Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2). Soviet Mathematics Doklady, 27, 372-376.</mixed-citation></ref><ref id="hanspub.28870-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Nesterov, Y. (2013) Introductory Lectures on Convex Optimization: A Basic Course. Springer Science &amp; Business Media.</mixed-citation></ref><ref id="hanspub.28870-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Sutskever, I., Martens, J., Dahl, G., et al. (2013) On the Importance of Initialization and Momen-tum in Deep Learning. Proceedings of the 30th International Conference on Machine Learning, 28, 1139-1147.</mixed-citation></ref><ref id="hanspub.28870-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Jacobs, R.A. (1988) Increased Rates of Convergence through Learning Rate Adaptation. Neural Net-works, 1, 295-307.  
&lt;br&gt;https://doi.org/10.1016/0893-6080(88)90003-2</mixed-citation></ref><ref id="hanspub.28870-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Duchi, J., Hazan, E. and Singer, Y. (2011) Adaptive Subgra-dient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121-2159.</mixed-citation></ref><ref id="hanspub.28870-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Dean, J., Corrado, G., Monga, R., et al. (2012) Large Scale Distributed Deep Networks. Proceedings of the 25th International Conference on Neural Information Processing Systems, 1, 1223-1231.</mixed-citation></ref><ref id="hanspub.28870-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Pennington, J., Socher, R. and Manning, C.D. (2014) Glove: Global Vectors for Word Representation. EMNLP, 14, 1532-1543.</mixed-citation></ref><ref id="hanspub.28870-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Zeiler, M.D. (2012) ADADELTA: An Adaptive Learning Rate Method. arXiv preprint arXiv:1212.5701.</mixed-citation></ref><ref id="hanspub.28870-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">张慧. 深度学习中优化算法的研究与改进[D]: [硕士学位论文]. 北京: 北京邮电大学, 2018.</mixed-citation></ref><ref id="hanspub.28870-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Glorot, X., Bordes, A. and Bengio, Y. (2011) Deep Sparse Rectifier Neural Networks. Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, PMLR 15, 315-323.</mixed-citation></ref></ref-list></back></article>