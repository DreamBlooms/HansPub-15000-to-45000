<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2019.82034</article-id><article-id pub-id-type="publisher-id">AAM-29023</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20190200000_20000264.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  内投影神经网络对
  <span style="font-size:10px;">l<sub>1</sub>-</span>α
  <em>l</em>
  <sub>2</sub>极小化问题的稀疏信号恢复
  An Inertial Projection Neural Network for Sparse Signal Recovery via 
  <span style="font-size:10px;">l<sub>1</sub>-</span>α
  <em>l</em>
  <sub>2</sub>Minimization
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>罗</surname><given-names>晓敏</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>西南大学，数学与统计学院，重庆</addr-line></aff><pub-date pub-type="epub"><day>30</day><month>01</month><year>2019</year></pub-date><volume>08</volume><issue>02</issue><fpage>301</fpage><lpage>308</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    本文旨在开发一种新算法，从少量测量数据中恢复稀疏信号，这是压缩传感领域的一个基本问题。目前，压缩感知倾向于非相干系统，其中任何两个测量值的相关性都尽可能小。然而，在现实中，许多问题是相干的，传统的方法，如
   l
   <sub>1</sub>最小化，处理效果不佳。我们提出了一种新的基于惯性投影神经网络的压缩传感
   l<sub>1</sub>-α
   <em>l</em>
   <sub>2</sub>极小化问题。针对高相干测量矩阵的稀疏信号恢复，提出了
   l
   <sub>1</sub>极小化问题，不同于传统的使用标准凸松弛的
   l<sub>1</sub>-α
   <em>l</em>
   <sub>2</sub>极小化问题。本文详细介绍了如何将惯性投影神经网络应用到压缩传感技术中。此外，还进行了数值实验，证明了稀疏信号恢复算法的有效性和显著的性能。
    This paper aims to develop a new algorithm for recovering a sparse vector from a small number of measurements, which is a fundamental problem in the field of compressive sensing (CS). Currently, CS favors incoherent systems, in which any two measurements are as little correlated as possible. In reality, however, many problems are coherent, conventional methods such as 
   l
   <sub>1</sub> minimization, do not work well. We propose a 
   l<sub>1</sub>-α
   <em>l</em>
   <sub>2</sub> minimization problem for compressed sensing using inertial projection neural network. The 
   l<sub>1</sub>-α
   <em>l</em>
   <sub>2</sub> minimization problem is presented for sparse signal recovery from highly coherent measurement matrices, differing from conventional 
   l
   <sub>1</sub> mini-mization which uses standard convex relaxation. We describe in details how to incorporate inertial projection neural network into compressed sensing. Furthermore, numerical experiments are conducted to support the effectiveness and remarkable performance of the algorithm for sparse signal recovery. 
  
 
</p></abstract><kwd-group><kwd>压缩感知，稀疏信号恢复，内投影神经网络，l<sub>1</sub>-α<em>l</em><sub>2</sub>极小化, Compressed Sensing</kwd><kwd> Sparse Signal Recovery</kwd><kwd> Inertial Projection Neural Network</kwd><kwd> l<sub>1</sub>-α<em>l</em><sub>2</sub> Minimization</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>内投影神经网络对 l 1 − α l 2 极小化问题的 稀疏信号恢复<sup> </sup></title><p>罗晓敏</p><p>西南大学，数学与统计学院，重庆</p><disp-formula id="hanspub.29023-formula27"><graphic xlink:href="//html.hanspub.org/file/16-2620857x11_hanspub.png"  xlink:type="simple"/></disp-formula><p>收稿日期：2019年2月3日；录用日期：2019年2月18日；发布日期：2019年2月26日</p><disp-formula id="hanspub.29023-formula28"><graphic xlink:href="//html.hanspub.org/file/16-2620857x12_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>本文旨在开发一种新算法，从少量测量数据中恢复稀疏信号，这是压缩传感领域的一个基本问题。目前，压缩感知倾向于非相干系统，其中任何两个测量值的相关性都尽可能小。然而，在现实中，许多问题是相干的，传统的方法，如 l 1 最小化，处理效果不佳。我们提出了一种新的基于惯性投影神经网络的压缩传感 l 1 − α l 2 极小化问题。针对高相干测量矩阵的稀疏信号恢复，提出了 l 1 − α l 2 极小化问题，不同于传统的使用标准凸松弛的 l 1 极小化问题。本文详细介绍了如何将惯性投影神经网络应用到压缩传感技术中。此外，还进行了数值实验，证明了稀疏信号恢复算法的有效性和显著的性能。</p><p>关键词 :压缩感知，稀疏信号恢复，内投影神经网络， l 1 − α l 2 极小化</p><disp-formula id="hanspub.29023-formula29"><graphic xlink:href="//html.hanspub.org/file/16-2620857x18_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/16-2620857x19_hanspub.png" /> <img src="//html.hanspub.org/file/16-2620857x20_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>随着大型数据集越来越可用和重要，科学技术的最新发展已经引起了数据处理的革命。为了满足“大数据”时代的需要，压缩传感(CS) [<xref ref-type="bibr" rid="hanspub.29023-ref1">1</xref>] 领域正在迅速发展。CS的过程包括编码和解码。编码过程涉及一组(线性)测量值 b = A x ，其中A是一个大小为 m &#215; n 的矩阵。如果 m &lt; n ，我们称为 x ∈ R n 能被压缩。译码的过程是从b中恢复x，另外假设x是稀疏的。它可以表示为一个优化问题，</p><p>min x ‖ x ‖ 0         s .t .     b = A x (1.1)</p><p>其中 ‖   ⋅   ‖ 0 表示 l 0 范数。因为 l 0 等于非零元素的数目，将 l 0 范数最小化，等于求出最稀疏的解。在CS中最大的障碍之一是解决解码问题式(1.1)，因为 范数极小化问题的求解在数学上是一个NP [<xref ref-type="bibr" rid="hanspub.29023-ref2">2</xref>] 难问题，即在多项式时间内无法有效求解。为了克服上述困难，一种流行的方法是用凸形代替，即 l 0 范数极小化</p><p>min x ‖ x ‖ 1       s .t .     b = A x (1.2)</p><p>其中 ‖ x ‖ 1 = ∑ i = 1 n | x i | 。(1.2)是一个凸优化问题，同时，它可以通过梯度投影法 [<xref ref-type="bibr" rid="hanspub.29023-ref3">3</xref>] ，同源性方法 [<xref ref-type="bibr" rid="hanspub.29023-ref4">4</xref>] 等来求解。最近，将非凸度量作为 l 1 的替代方法的应用不断增加。特别是 [<xref ref-type="bibr" rid="hanspub.29023-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.29023-ref6">6</xref>] 中 p ∈ ( 0 , 1 ) 的非凸度量p可以看作是近似 l 0 随着 p → 0 的连续策略。优化策略包括迭代重加权 [<xref ref-type="bibr" rid="hanspub.29023-ref7">7</xref>] 和半阈值 [<xref ref-type="bibr" rid="hanspub.29023-ref6">6</xref>] 。其他非凸 l 1 变形包括 l 1 − 2 [<xref ref-type="bibr" rid="hanspub.29023-ref8">8</xref>] ，</p><p>min x ‖ x ‖ 1 − ‖ x ‖ 2       s .t .     b = A x (1.3)</p><p>结果表明，当测量矩阵A高度相干时， l 1 − 2 极小化一致优于经典的 l 1 和 l q 极小化，在 [<xref ref-type="bibr" rid="hanspub.29023-ref8">8</xref>] 中理论上，给出了一个RIP类型的充分条件，以确保 l 1 − 2 能精确地恢复稀疏信号。</p><p>众所周知，神经网络在优化问题和许多应用中得到了广泛的应用。但是，可能会遇到高相干性的测量矩阵，开发出更好、高效的优化方法是有必要的。本文通过推广 l 1 − 2 形式，考虑 l 1 − α l 2 ( 0 &lt; α ≤ 1 ) 度量，</p><p>min x ‖ x ‖ 1 − α ‖ x ‖ 2       s .t .     b = A x (1.4)</p><p>显然，这是一个非凸优化问题。应用变量替换以及投影算子 [<xref ref-type="bibr" rid="hanspub.29023-ref9">9</xref>] 的知识，我们提出了一种新的无约束 l 1 − α l 2 极小化问题的神经网络模型。用这种方法不仅能真正解决问题同时也避免了次梯度项的困难。</p></sec><sec id="s4"><title>2. 问题介绍</title><p>通过拉格朗日乘子，我们将问题转换为无约束优化问题</p><p>min u , v ∈ R n 1 2 ‖ A x − b ‖ 2 2 + τ ( ‖ x ‖ 1 − α ‖ x ‖ 2 ) (2.1)</p><p>毫无疑问是不可微的，我们首先将目标函数进行转换，而不是直接寻找将非凸目标函数极小化的方法。假设在问题中，用 x = u − v 来代替未知变量x，其中 u , v ∈ R n 分别为x中所有的正的和负的元素，也就是说 u i = ( x _ i ) + ， u i = ( − x _ i ) + ， ( x _ i ) + = max { x i , 0 } ， i = 1 , ⋯ , n 。有了这样的替换，很容易有</p><p>‖ x ‖ 1 = I n T ( u + v ) = I n T u + I n T v</p><p>‖ x ‖ 2 = ‖ u − v ‖</p><p>A x = A ( u − v )</p><p>其中 I n T = ( 1 , ⋯ , 1 ) T 。因此，问题可以改写为</p><p>min u , v ∈ R n 1 2 ‖ A ( u − v ) − b ‖ 2 2 − τ ‖ u − v ‖ 2 + τ I n T u + I n T v</p><p>Subject to u ≥ 0 , v ≥ 0 (2.2)</p><p>这里 τ &gt; 0 是一个正则化参数。这个问题相当于下面的非凸问题</p><p>min f ( z ) = 1 2 z T B z − τ ( z T z ) 1 2 + c T</p><p>Subject to z ∈ S = { z ∈ R n | z ≥ 0 } (2.3)</p><p>其中目标函数 f ( z ) 是一个非凸连续微分，并且</p><p>z = (uv)</p><p>B = ( A T A − A T A − A T A A T A )</p><p>c = τ I 2 n + ( − A T b A T b )</p></sec><sec id="s5"><title>3. 神经网络模型</title><p>本节介绍了求解CS中 l 1 − α l 2 极小化问题的IPNN方法。基于比例梯度投影，建立了求解非凸问题的神经网络模型。令 z * ∈ S 作为一个最优解，因为 f ( z ) 二次可微，则对所有的 t ∈ [ 0 , 1 ] 和 z ∈ S ，有 z * + t ( z − z * ) ∈ S 。因此函数 q ( t ) = f ( z * + t ( z − z * ) ) 在 ( 0 , 1 ) 上可微，因为 q ( t ) 在 t = 0 处达到最小值，因此 q ′ ( t ) ≥ 0 。则有</p><p>q ′ ( 0 ) = ∇ f ( z ) T ( z − z * ) ≥ 0 ,   ∀ z ∈ S (3.1)</p><p>找到一个最优值是相当于求解变分不等式，它可以被视为一个在科学和工程领域的一个自然的平衡问题.因此我们的下一个任务是寻求一个适当的方法。由于，我们有以下的惯性投影神经网络IPNN模型</p><p>{ z ˙ = w w ˙ = − λ w + [ z − B z + τ z ‖ z ‖ − c ] + − z z ( t 0 ) = z 0 ∈ S (3.2)</p><p>其中 S = { z ∈ R n | z ≥ 0 } ， ( ⋅ ) + 是投影算子：</p><p>( z i ) + = { z i ,     z i ≥ 0. 0 ,         z i &lt; 0.</p><p>该模型属于一个存在于神经动力学优化模型中的双层结构，克服了一些缺点并且由于内点项 w ˙ 的存在加速了收敛过程。</p></sec><sec id="s6"><title>4. 算法</title><p>在这个部分中，优化算法的原则总结在算法1中。该算法基于IPNN模型，提出了重构稀疏信号求解无约束 l 1 − α l 2 最小化问题的迭代步骤。由于问题的非凸性，该算法一般不能保证收敛到全局最小，因此一个好的初始化对该算法的性能是至关重要的。一个很好的启发是利用求解无约束的 l 1 最小化问题的解来作为这个算法的初始值。我们使用 x 0 = ( A T A ) − 1 A T b 来将算法初始化，其中 ( A T A ) − 1 A T b 是A的伪逆，则很容易计算 z 0 通过 x 0 。</p></sec><sec id="s7"><title>5. 数值仿真</title><p>在本节进行数值实验来证明我们提出的算法IPNN l 1 − α l 2 的有效性和信号重构能力。所有的实验均在MATLAB上实现，采用的常微分方程求解器是ode15s。从 [<xref ref-type="bibr" rid="hanspub.29023-ref8">8</xref>] 中测试了稀疏向量的精确重构在矩阵A较弱的条件下，可以保证问题的最小值的稀疏度。为了验证在A是一个高相干矩阵的情况下的稀疏向量精确重构，我们通过创建一个随机采样部分DCT矩阵 A m &#215; n 来产生传感矩阵，其中矩阵 A i = ( cos ( 2 i − 1 ) π ζ / F ) / m 。矩阵A的相干性随着正整数F的增加而增加。 ζ ∈ R n 的元素在[0,1]上独立同分布。接下来，我们将研究F与相干性之间的关系，其中相干性定义为 μ ( A ) = max i ≠ j | A i T A j | ‖ A i ‖ 2 ‖ A j ‖ 2 。</p><p>例1：考虑无约束 l 1 − α l 2 极小化问题</p><p>min u , v ∈ R n 1 2 ‖ A x − b ‖ 2 2 + τ ( ‖ x ‖ 1 − α ‖ x ‖ 2 ) (5.1)</p><p>首先给出了无噪声情况下问题的稀疏信号重构。测试长度为n的稀疏信号 x &#175; 由标准正态分布生成。由A和 x &#175; ，可由 b = A x &#175; 计算得到b。令 m = 64 ， n = 256 ， F = 14 ， τ = 10 − 2 。IPNN l 1 − α l 2 算法的效果可以从图中看出。</p><p>图1. 相干性</p><p>图1绘制了产生的矩阵 A m &#215; n 的相干性结果。很容易看出， μ ( A ) 随着F增加趋向于变大，即A的相干性随着F增大而增大。为了方便起见，在不引起混淆的情况下，我们将使用F来表示相干性。因此，在随后的实验中，我们设置F = 14足以保证高相干性测量矩阵。</p><p>图2. ‖ x ‖ 1 − α ‖ x ‖ 2 等值线图</p><p>图2展示了当 α 取不同值时的 ‖ x ‖ 1 − α ‖ x ‖ 2 等值线图。</p><p>图3. 解</p><p>图3展示了神经网络的解渐近稳定且收敛于局部最优解 x * 。</p><p>图4. IPNN l 1 − α l 2 ( α = 0.6 )算法下得原始信号与重构信号</p><p>从图4可以直观观察重构结果，显示原始信号和重构信号基本上重合。因此，在无噪声情况下，我们提出的算法IPNN l 1 − α l 2 可以有效重构稀疏信号。</p><p>图5展示了50个实验值的平均相对误差， l 1 − α l 2 ( α = 0.6 )与 l 1 − l 2 相比，收敛速度更快，且相对误差更小。</p><p>例2：在这一部分中，我们考虑了噪声情况下问题(5.1)的鲁棒恢复。我们将白高斯噪声加到干净的数据 A x &#175; 中，然后得到测量值 b = a w g n ( A x ; s n r ) ，其中snr对应于以dB测量的信噪比(snr)值。我们</p><p>图5. 相对误差</p><p>设置初始值 x 0 = ( A T A ) − 1 A T b 。重构信号 x * 通过算法IPNN l 1 − α l 2 获得，我们计算重构的信噪比通过 10 log 10 ‖ x &#175; ‖ 2 2 / ‖ x * − x &#175; ‖ 2 2 。</p><p>IPNN是一种通过 l 1 − 2 [<xref ref-type="bibr" rid="hanspub.29023-ref10">10</xref>] 极小化实现稀疏信号恢复的惯性投影神经网络算法。DCA- l 1 − 2 [<xref ref-type="bibr" rid="hanspub.29023-ref8">8</xref>] 是求解 l 1 − 2 极小化问题的典型优化方法。通过嵌入 l p ( p ≥ 0 )范数噪声约束在最近出现的 l 1 − 2 方法中，采用了IR l 1 − l 2 / l p [<xref ref-type="bibr" rid="hanspub.29023-ref11">11</xref>] 方法从高相干测量矩阵中恢复一般噪声信号。在下面的实验中，我们设定 p = 2 。优化算法比如ADMM-lasso [<xref ref-type="bibr" rid="hanspub.29023-ref12">12</xref>] 是解决稀疏问题的重要和流行的经典算法。通过噪声的改变，我们在随机过采样部分DCT矩阵和常用随机高斯矩阵上比较了IPNN l 1 − α l 2 ( α = 0.6 )，IPNN，IR l 1 − l 2 / l 2 ，DCA- l 1 − 2 ，ADMM-lasso。对于随机过采样的部分DCT矩阵，我们设 m = 100 ， n = 2000 。对于高斯矩阵，我们假设 m = 256 ， n = 1024 。每个记录值是50次随机实现的平均值。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Randomly over-sampled partial DCT matri</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >snr(dB)</th><th align="center" valign="middle" >IPNN l 1 − α l 2</th><th align="center" valign="middle" >IPNN</th><th align="center" valign="middle" >IR l 1 − l 2 / l 2</th><th align="center" valign="middle" >DCA- l 1 − 2</th><th align="center" valign="middle" >ADMM-lasso</th></tr></thead><tr><td align="center" valign="middle" >35</td><td align="center" valign="middle" >27.5678</td><td align="center" valign="middle" >25.5423</td><td align="center" valign="middle" >16.8824</td><td align="center" valign="middle" >17.0934</td><td align="center" valign="middle" >12.2916</td></tr><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >17.9147</td><td align="center" valign="middle" >16.6792</td><td align="center" valign="middle" >5.6427</td><td align="center" valign="middle" >3.1806</td><td align="center" valign="middle" >3.0157</td></tr></tbody></table></table-wrap><p>表1. 随机过采样部分DCT矩阵</p><p>表1显示了使用过采样DCT矩阵得到的结果。相比之下，在中等噪声下，我们的IPNN l 1 − α l 2 表现更佳。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Random Gaussian matri</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >snr(dB)</th><th align="center" valign="middle" >IPNN l 1 − α l 2</th><th align="center" valign="middle" >IPNN</th><th align="center" valign="middle" >IR l 1 − l 2 / l 2</th><th align="center" valign="middle" >DCA- l 1 − 2</th><th align="center" valign="middle" >ADMM-lasso</th></tr></thead><tr><td align="center" valign="middle" >40</td><td align="center" valign="middle" >43.6460</td><td align="center" valign="middle" >42.3778</td><td align="center" valign="middle" >28.9745</td><td align="center" valign="middle" >29.2398</td><td align="center" valign="middle" >27.6103</td></tr><tr><td align="center" valign="middle" >30</td><td align="center" valign="middle" >32.8364</td><td align="center" valign="middle" >31.8225</td><td align="center" valign="middle" >18.8502</td><td align="center" valign="middle" >19.5802</td><td align="center" valign="middle" >18.3454</td></tr><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >22.1155</td><td align="center" valign="middle" >21.9312</td><td align="center" valign="middle" >14.0764</td><td align="center" valign="middle" >11.0752</td><td align="center" valign="middle" >9.8646</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >13.2514</td><td align="center" valign="middle" >13.1233</td><td align="center" valign="middle" >5.2451</td><td align="center" valign="middle" >3.6700</td><td align="center" valign="middle" >3.1970</td></tr></tbody></table></table-wrap><p>表2. 随机高斯矩阵</p><p>表2显示了高斯测量下的结果。我们的IPNN l 1 − α l 2 也明显优于其他算法。</p></sec><sec id="s8"><title>6. 结论</title><p>本文介绍了用IPNN l 1 − α l 2 方法通过 l 1 − α l 2 极小化来实现稀疏信号的恢复。为了验证我们方法的有效性和鲁棒性，进行几组对比实验包括噪声和无噪声，以及高斯随机矩阵和高相干矩阵。结果表明，该方法在信号恢复方面优于其它最先进的方法。稀疏信号重构问题其应用广泛而成为一个热点问题，针对稀疏信号重构等相关问题，需要寻找更加实用有效的方法来解决这些非凸问题，结合神经网络，压缩感知将会迎来美好的明天。</p></sec><sec id="s9"><title>基金项目</title><p>中央高校基本科研业务费资助项目(No. XDJK2018C076)。</p></sec><sec id="s10"><title>文章引用</title><p>罗晓敏. 内投影神经网络对l<sub>1</sub>-αl<sub>2</sub>极小化问题的稀疏信号恢复An Inertial Projection Neural Network for Sparse Signal Recovery via l<sub>1</sub>-αl<sub>2</sub>Minimization[J]. 应用数学进展, 2019, 08(02): 301-308. https://doi.org/10.12677/AAM.2019.82034</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.29023-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Candès, E.J. (2006) Compressive Sampling. Marta Sanz Solé, 17, 1433-1452.</mixed-citation></ref><ref id="hanspub.29023-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Natarajan, B. (1995) Sparse Ap-proximate Solutions to Linear Systems. SIAM Journal on Computing, 24, 227-234.  
&lt;br&gt;https://doi.org/10.1137/S0097539792240406</mixed-citation></ref><ref id="hanspub.29023-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Figueiredo, M.A.T., Nowak, R.D. and Wright, S.J. (2008) Gradient Projection for Sparse Reconstruction: Application to Compressed Sensing and Other Inverse Problems. IEEE Journal of Selected Topics in Signal Processing, 1, 586-597.  
&lt;br&gt;https://doi.org/10.1109/JSTSP.2007.910281</mixed-citation></ref><ref id="hanspub.29023-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Donoho, D.L. and Tsaig, Y. (2008) Fast Solution of l1-Norm Minimization Problems When the Solution May Be Sparse. IEEE Transactions on Information Theory, 54, 4789-4812. &lt;br&gt;https://doi.org/10.1109/TIT.2008.929958</mixed-citation></ref><ref id="hanspub.29023-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Wen, J.M., Li, D.F. and Zhu, F.M. (2015) Stable Recovery of Sparse Signals via lp Minimization. Applied and Computational Harmonic Analysis, 38, 161-176. &lt;br&gt;https://doi.org/10.1016/j.acha.2014.06.003</mixed-citation></ref><ref id="hanspub.29023-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Xu, Z., Chang, X., Xu, F. and Zhang, H. (2012) l1/2 Regularization: A Thresholding Representation Theory and a Fast Solver. IEEE Transactions on Neural Networks and Learning Systems, 23, 1013-1027.  
&lt;br&gt;https://doi.org/10.1109/TNNLS.2012.2197412</mixed-citation></ref><ref id="hanspub.29023-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Lai, M.J., Xu, Y. and Yin, W. (2013) Improved Iteratively Reweighted Least Squares for Unconstrained Smoothed lq Minimization. SIAM Journal on Numerical Analysis, 51, 927-957. &lt;br&gt;https://doi.org/10.1137/110840364</mixed-citation></ref><ref id="hanspub.29023-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Yin, P., Lou, Y., He, Q. and Xin, J. (2015) Minimization of l1−2 for Compressed Sensing. SIAM Journal on Scientific Computing, 37, 536-563. &lt;br&gt;https://doi.org/10.1137/140952363</mixed-citation></ref><ref id="hanspub.29023-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">He, X., Huang, T., Yu, J., Li, C. and Li, C. (2016) An Inertial Projection Neural Network for Solving Variational Inequalities. IEEE Transactions on Cybernetics, 47, 809-814.</mixed-citation></ref><ref id="hanspub.29023-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, L.J., Wang, J.J., He, X. and Zhao, Y. (2018) An Inertial Projection Neural Network for Sparse Signal Reconstruction via l1−2 Minimization. Neurocomputing, 315, 89-95. &lt;br&gt;https://doi.org/10.1016/j.neucom.2018.06.050</mixed-citation></ref><ref id="hanspub.29023-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Wang, W.D., Wang, J.J. and Zhang, Z.L. (2017) Robust Signal Recovery with Highly Coherent Measurement Matrices. IEEE Signal Processing Letters, 24, 304-308. &lt;br&gt;https://doi.org/10.1109/LSP.2016.2626308</mixed-citation></ref><ref id="hanspub.29023-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Boyd, S., Parikh, N., Chu, E., Peleato, B. and Eckstein, J. (2011) Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning, 3, 1-122.  
&lt;br&gt;https://doi.org/10.1561/2200000016</mixed-citation></ref></ref-list></back></article>