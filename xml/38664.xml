<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.1011206</article-id><article-id pub-id-type="publisher-id">CSA-38664</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20201100000_49755598.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于联合粒度属性约简信息损失的研究
  Research on Information Loss of Attribute Reduction Based on Joint Granularity
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>薛</surname><given-names>欢欢</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郭</surname><given-names>步</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>隋</surname><given-names>龙飞</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>群笑</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>嘉兴职业技术学院，浙江 嘉兴</addr-line></aff><aff id="aff2"><addr-line>嘉兴学院南湖学院，浙江 嘉兴</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>11</month><year>2020</year></pub-date><volume>10</volume><issue>11</issue><fpage>1952</fpage><lpage>1961</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   随着互联网技术的迅速发展，社会进入了大数据时代。数据不仅类型多种多样，结构错综复杂还具有动态变化的特点。如何从海量数据中快速获取有价值的信息是当前亟待解决的问题。粗糙集是一种处理数据不确定性的数据评价方法。属性约简是粗糙集理论的一个重要核心应用。本文将围绕属性约简后信息损失量进行研究，从而找寻一种属性约简算法，在约简后既能保持数据分类准确率较高且信息损失较少。本文借助知识粒度的概念和约简算法，引入联合粒度，并将其运用到属性约简过程，进一步得出基于联合粒度属性约简算法。然后运用其算法对决策表系统进行约简，得出该算法在保持分类准确率不变的情况下，其信息损失量降至较低。最后通过UCI数据集进行仿真实验探究，从而验证了该方法的准确性和有效性。 With the rapid development of Internet technology, the society has entered the era of big data. The data is not only of various types and structures, but also of dynamic change. How to quickly obtain valuable information from massive data is an urgent problem to be solved. Rough set is a data evaluation method to deal with data uncertainty. Attribute reduction is an important core application of rough set theory. This paper will focus on the amount of information loss after attribute reduction, so as to find an attribute reduction algorithm, which can keep the data classification accuracy higher and information loss less after reduction. In this paper, the concept of knowledge granularity and reduction algorithm, the introduction of joint granularity, and its application to the process of attribute reduction, further get the attribute reduction algorithm based on joint granularity. Then the algorithm is used to reduce the decision table system. It is concluded that the information loss of the algorithm is reduced to a low level while the classification accuracy remains unchanged. Finally, the accuracy and effectiveness of this method are verified by the simulation experiment of UCI data set. 
  
 
</p></abstract><kwd-group><kwd>属性约简，知识粒度，联合粒度，信息损失, Attribute Reduction</kwd><kwd> Knowledge Granularity</kwd><kwd> Union Granularity</kwd><kwd> Information Loss</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>随着互联网技术的迅速发展，社会进入了大数据时代。数据不仅类型多种多样，结构错综复杂还具有动态变化的特点。如何从海量数据中快速获取有价值的信息是当前亟待解决的问题。粗糙集是一种处理数据不确定性的数据评价方法。属性约简是粗糙集理论的一个重要核心应用。本文将围绕属性约简后信息损失量进行研究，从而找寻一种属性约简算法，在约简后既能保持数据分类准确率较高且信息损失较少。本文借助知识粒度的概念和约简算法，引入联合粒度，并将其运用到属性约简过程，进一步得出基于联合粒度属性约简算法。然后运用其算法对决策表系统进行约简，得出该算法在保持分类准确率不变的情况下，其信息损失量降至较低。最后通过UCI数据集进行仿真实验探究，从而验证了该方法的准确性和有效性。</p></sec><sec id="s2"><title>关键词</title><p>属性约简，知识粒度，联合粒度，信息损失</p></sec><sec id="s3"><title>Research on Information Loss of Attribute Reduction Based on Joint Granularity<sup> </sup></title><p>Huanhuan Xue<sup>1</sup>, Bu Guo<sup>1</sup>, Longfei Sui<sup>2</sup>, Qunxiao Wang<sup>1</sup></p><p><sup>1</sup>Nanhu College of Jiaxing University, Jiaxing Zhejiang</p><p><sup>2</sup>Jiaxing Vocational and Technical College, Jiaxing Zhejiang</p><p><img src="//html.hanspub.org/file/4-1541899x4_hanspub.png" /></p><p>Received: Oct. 27<sup>th</sup>, 2020; accepted: Nov. 11<sup>th</sup>, 2020; published: Nov. 18<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/4-1541899x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>With the rapid development of Internet technology, the society has entered the era of big data. The data is not only of various types and structures, but also of dynamic change. How to quickly obtain valuable information from massive data is an urgent problem to be solved. Rough set is a data evaluation method to deal with data uncertainty. Attribute reduction is an important core application of rough set theory. This paper will focus on the amount of information loss after attribute reduction, so as to find an attribute reduction algorithm, which can keep the data classification accuracy higher and information loss less after reduction. In this paper, the concept of knowledge granularity and reduction algorithm, the introduction of joint granularity, and its application to the process of attribute reduction, further get the attribute reduction algorithm based on joint granularity. Then the algorithm is used to reduce the decision table system. It is concluded that the information loss of the algorithm is reduced to a low level while the classification accuracy remains unchanged. Finally, the accuracy and effectiveness of this method are verified by the simulation experiment of UCI data set.</p><p>Keywords:Attribute Reduction, Knowledge Granularity, Union Granularity, Information Loss</p><disp-formula id="hanspub.38664-formula23"><graphic xlink:href="//html.hanspub.org/file/4-1541899x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/4-1541899x7_hanspub.png" /> <img src="//html.hanspub.org/file/4-1541899x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>粗糙集理论 [<xref ref-type="bibr" rid="hanspub.38664-ref1">1</xref>] 作为一种数据分析方法，在定性和定量分析的基础上用来描述决策系统中的不确定性。属性约简 [<xref ref-type="bibr" rid="hanspub.38664-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.38664-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.38664-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.38664-ref5">5</xref>]，也称特征选择，是粗糙集理论的重要核心应用。它是从信息系统的所有属性中选择部分重要度较高的属性，同时不减少数据分类的准确性。属性约简已被广泛应用于许多研究领域，如模式识别、数据挖掘与机器学习等。在数据挖掘 [<xref ref-type="bibr" rid="hanspub.38664-ref6">6</xref>] 领域中，属性约简是一种常用的数据处理工具，主要对数据中的冗余属性进行删除操作，使其达到更高的分类性能且不损失数据表的任何信息。</p><p>前期学者们提出了许多属性约简准则和约简算法，几乎所有的约简准则都宣称保持分类不变、信息无损失。然而，本文从信息熵角度 [<xref ref-type="bibr" rid="hanspub.38664-ref6">6</xref>] 分析，定量分析删除的冗余属性可能含有一定的信息量，可以得出属性约简可能会导致信息的损失。为了探究不同约简准则信息损失量的差异，提出联合粒度属性约简算法，同时对比了正区域约简、知识粒度约简和该算法的约简准则对数据分类、信息损失量的影响，从而验证此算法的有效性。</p></sec><sec id="s6"><title>2. 基础知识</title><p>给定一个信息系统 [<xref ref-type="bibr" rid="hanspub.38664-ref1">1</xref>] I S = ( U , A ) 中，U是论域，A是论域U上的条件属性集。在条件属性集中任取条件属性 m ∈ A 都存在一个函数 m : U → F m ，与其相对应， F m 为属性m的值域。U中每个元素称为个体、对象或行。</p><p>定义2.1 [<xref ref-type="bibr" rid="hanspub.38664-ref5">5</xref>] 对于任意的属性子集 B ⊆ A 和任何个体 x ∈ A 都对应着如下的信息函数：</p><p>I n f B ( x ) = { ( a , a ( x ) ) : a ∈ B }</p><p>B-不分明关系(或称为不可区分关系)定义为:</p><p>I N D ( B ) = { ( x , y ) : I n f B ( x ) = I n f B ( y ) }</p><p>任何满足 I N D ( B ) 的2个元素 x , y 都不能由B的任何子集区分， [ x ] B 表示由x引导的 I N D ( B ) 等价类。</p><sec id="s6_1"><title>2.1. 属性约简</title><p>定义2.2 [<xref ref-type="bibr" rid="hanspub.38664-ref6">6</xref>] 在决策系统 D S = ( U , C , d ) 中， B ⊆ C 是DS的基于正区域的相对约简当且仅当 B ⊆ C 满足以下两个条件：</p><p>(1) P O S B ( d ) = P O S C ( d ) ；</p><p>(2) 对于任意的 a ∈ B ，都有 P O S B − { a } ( d ) ≠ P O S C ( d ) .</p><p>定义2.3 [<xref ref-type="bibr" rid="hanspub.38664-ref6">6</xref>] 给定一个决策系统 D S = ( U , A , d ) ， B ⊆ A 称为该决策系统DS的一个基于条件熵的相对约简当且仅当 B ⊆ A 满足如下两个条件：</p><p>(1) H ( D S , { d } | B ) = H ( D S , { d } | A ) ；</p><p>(2) 对任意的 S ⊂ B ，均都有 H ( D S , { d } | S ) ≠ H ( D S , { d } | A ) 。</p></sec><sec id="s6_2"><title>2.2. 知识粒度的基本概念</title><p>定义2.4 [<xref ref-type="bibr" rid="hanspub.38664-ref7">7</xref>] 给定一个信息系统 I S = ( U , A , V , f ) ，对于任意属性子集 M ⊆ A 导出的划分 U / M = { R 1 , R 2 , R 3 , ⋯ , R n } ，则该信息系统对于属性子集M的知识粒度的定义为</p><p>G D ( M ) = ∑ i = 1 n | R i | 2 | U | 2</p><p>定义2.5 [<xref ref-type="bibr" rid="hanspub.38664-ref7">7</xref>] 设给定一个信息系统 I S = ( U , A , V , f ) ，如果存在属性子集 R 1 , R 2 ⊆ A ，则有属性子集 R 1 相对于子集 R 2 的知识粒度的定义为</p><p>G D ( R 1 | R 2 ) = G K ( R 2 ) − G K ( R 1 ∪ R 2 )</p><p>定义2.6 [<xref ref-type="bibr" rid="hanspub.38664-ref7">7</xref>] 对于给定决策系统 D S = ( U , A ∪ D , V , f ) ，若属性子集 R ⊆ A 是当且仅当满足以下两个条件：</p><p>(1) G D ( D | A ) = G K ( D | R ) ；</p><p>(2) 对于任意的 m ∈ R ， G K ( D | R − { m } ) ≠ G K ( D | R ) 。</p><p>称 R ⊆ A 为该决策系统的一个知识属性约简。</p><p>定义2.7 [<xref ref-type="bibr" rid="hanspub.38664-ref8">8</xref>] 在给定的信息系统 I S = ( U , A ∪ D , V , f ) 中，若存在属性子集 c ∈ A ，则属性 c ∈ A 与决策属性D的相似度定义为：</p><p>s ( c , D ) = | I N D ( D ∪ { c } ) | | I N D ( c ) | ⋅ | I N D ( D ) |</p></sec><sec id="s6_3"><title>2.3. 知识粒度的启发式属性约简算法 [<xref ref-type="bibr" rid="hanspub.38664-ref8">8</xref>]</title><p>根据知识属性约简的定义可得出一个属性约简算法，算法具体步骤如表1。</p><p>目前很多约简算法均要先求核属性，但该算法不需要。此算法在一定程度上减少了求核属性的工作量，同时时间复杂度相对缩减。</p></sec></sec><sec id="s7"><title>3. 联合粒度属性约简算法</title><p>受表1算法的启发，以知识粒度 [<xref ref-type="bibr" rid="hanspub.38664-ref7">7</xref>] 作为启发式函数进行遍历搜索，选取知识粒度值的大小进行比较，得出一个新算法—基于联合粒度属性约简算法。算法的具体描述如表2所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Heuristic attribute reduction algorithm based on knowledge granularity [8</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >输入：一个简单决策表 D T = ( U , C ∪ D , V , f ) 。 输出：条件属性C相对于决策属性D的一个相对约简 B ∈ R E D D ( C ) 。</th></tr></thead><tr><td align="center" valign="middle" >1. 计算每个条件属性与决策属性的相似度 s ( c i , D ) 。 2. 然后根据每个条件属性与决策属性的相似度的值的大小将条件属性集进行降序排列 C = { c 1 , c 2 , ⋯ , c n } ,其中n是条件属性的个数。 3. 对属性约简 R E D D ( C ) 赋初始值 R E D D ( C ) = C ，冗余属性 C P = ∅ ，对此，检验 C 1 = ∅ ，然后对集合C中的每个条件属性属性 c i ，令 C m = R E D D ( C ) − c i ，执行下述操作： 3.1. 计算每个条件属性属性 c i 与集合 C m 中条件属性 c j 的相似度 s ( c i , c j ) ，然后与前面的 s ( c i , D ) 进行比较，若 s ( c i , D ) ≤ s ( c i , c j ) ，则将条件属性 c j 赋值给 C P ，将集合 R E D D ( C ) − c j 赋值给 C 1 ； 3.2. 接下来计算 s ( C 1 , D ) 是否与 s ( C , D ) 相等，如果相等，则删除条件属性 c j ，将集合 C 1 赋给 R E D D ( C ) ，反之将保留 c j ， R E D D ( C ) 保持不变； 3.3. 如果 C m 中的条件属性没有完成遍历，则直接转至步骤3.1，若完成遍历，则令 i = i + 1 ， C m = R E D D ( C ) − c i ，如果 C m = ∅ ，转步骤4，否则，转步骤3.1； 4. 算法结束，输出相对属性约简集合 R E D D ( C ) 。</td></tr></tbody></table></table-wrap><p>表1. 基于知识粒度的启发式属性约简算法 [<xref ref-type="bibr" rid="hanspub.38664-ref8">8</xref>]</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Attribute Reduction Algorithm Based on Joint Granularit</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >输入：一个完备的决策信息系统 D T = ( U , A ∪ { d } , V , f ) 输出：属性约简结果 P = R E D A</th></tr></thead><tr><td align="center" valign="middle" >1. 初始化 R E D A = ∅ ； 2. 根据知识粒度的定义，计算所有剩余属性的知识粒度值 G K ( A i ) ； 3. If G K ( A i ) &lt; 1 then 4. 寻找第2步中的知识粒度最小的属性，记为 min ( A i ) ； 5. R E D A = R E D A ∪ min ( A i ) ； 6. 跳入第2步； 7. Else跳转步骤10； 8. Endif 9. 返回属性约简 R E D A 。算法结束。</td></tr></tbody></table></table-wrap><p>表2. 基于联合粒度属性约简算法</p><p>基于联合粒度属性约简算法不同于上述表1，它主要从知识属性、粒计算 [<xref ref-type="bibr" rid="hanspub.38664-ref9">9</xref>] 角度进行约简，可以直接对不同知识属性进行粒度和重要度的比较，同时使得约简过程更加精细，约简结果更加精确。</p>属性约简信息损失 [<xref ref-type="bibr" rid="hanspub.38664-ref9">9</xref>]<p>定义3.1 [<xref ref-type="bibr" rid="hanspub.38664-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.38664-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.38664-ref11">11</xref>] 在一个给定的决策系统 D S = ( U , A , d ) 中，设A和 { d } 在论域U上导出的划分分别为X和Y，其中 X = U / A = { X 1 , X 2 , ⋯ , X N } ， Y = U / { d } = { Y 1 , Y 2 , ⋯ , Y M } ， p ( X i ) = | X i | | U | ， p ( Y j ) = | Y j | | U | ， p ( X i , Y j ) = | X i ∩ Y j | | U | ， p ( Y j | X i ) = | X i ∩ Y j | | X i | ， i = 1 , 2 , ⋯ , N ， j = 1 , 2 , ⋯ , M . A和 { d } 的信息熵 [<xref ref-type="bibr" rid="hanspub.38664-ref12">12</xref>] 分别可</p><p>定义为：</p><p>H ( D S , A ) = − ∑ i = 1 N p ( X i ) l b p ( X i )</p><p>{ d } 与A的联合熵 [<xref ref-type="bibr" rid="hanspub.38664-ref13">13</xref>] 即可定义为：</p><p>H ( D S , A , { d } ) = − ∑ i = 1 N ∑ i = 1 M p ( X i , Y j ) l b p ( X i , Y j )</p><p>定义3.2 [<xref ref-type="bibr" rid="hanspub.38664-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.38664-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.38664-ref11">11</xref>] 给定 D S = ( U , A , d ) 是一个决策系统， B ⊆ A 是该决策系统的一个约简，则 B ⊆ A 属性约简的信息损失定义如下：</p><p>Δ ( B ) = H ( D S , A ) − H ( D S , B )</p><p>B ⊆ A 属性约简的信息损失率可定义如下：</p><p>s ( B ) = Δ ( B ) H ( D S , A ) &#215; 100 % = 1 − H ( D S , B ) H ( D S , A ) &#215; 100 %</p><p>命题1 给定一个决策系统 D S = ( U , A , d ) ，若 B 1 ⊆ A 是基于正区域的相对约简， B 2 ⊆ A 是基于知识粒度的属性约简， B 3 ⊆ A 是基于联合粒度的属性约简，则有 Δ ( B 1 ) ≥ Δ ( B 2 ) ≥ Δ ( B 3 ) .</p><p>证明：根据知识粒度约简、联合粒度约简及正区域相对约简可证。</p><p>例1下表所示 D S 1 = ( U , C ∪ D , V , f ) 是一个决策表，如表3所示。U为论域，其中 U = { x 1 , x 2 , x 3 , x 4 , x 5 , x 6 , x 7 , x 8 } ， C = { e 1 , e 2 , e 3 , e 4 } 是条件属性， D = { d } 是决策属性集。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Decision System D S 1 = ( U , A , d </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >U</th><th align="center" valign="middle" >e<sub>1</sub></th><th align="center" valign="middle" >e<sub>2</sub></th><th align="center" valign="middle" >e<sub>3</sub></th><th align="center" valign="middle" >e<sub>4</sub></th><th align="center" valign="middle" >d</th></tr></thead><tr><td align="center" valign="middle" >x<sub>1</sub></td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >x<sub>2</sub></td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >x<sub>3</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >x<sub>4</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >x<sub>5</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >x<sub>6</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >x<sub>7</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >x<sub>8</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr></tbody></table></table-wrap><p>表3. 决策系统 D S 1 = ( U , A , d )</p><p>U / C = { { x 1 } , { x 2 } , { x 3 , x 4 } , { x 5 , x 6 } , { x 7 } , { x 8 } }</p><p>U / { d } = { { x 1 , x 3 , x 5 , x 7 } , { x 2 , x 4 , x 6 , x 8 } }</p><p>由正区域的定义可得： P O S C ( d ) = { x 1 , x 2 , x 7 , x 8 } 。</p><p>根据条件属性依赖度的计算公式可得： γ C ( D ) ≠ 1 ，所以该决策表是不相容决策表。</p><p>(1) 根据知识粒度的属性约简算法可得约简后为： R 1 = R E D C = { e 2 , e 3 , e 4 } .</p><p>根据属性约简的信息损失的计算方法可得：</p><p>H ( D S 1 , C ) = − ∑ p ( X ) l b p ( X ) = 2.5</p><p>H ( D S 1 , R 1 ) = − ∑ p ( X ) l b p ( X ) = 2.5</p><p>故 R 1 的信息损失量为： Δ ( R 1 ) = 0 。</p><p>(2) 由正区域约简算法可得： R 2 = { e 2 , e 3 , e 4 } ， R 3 = { e 1 , e 3 , e 4 }</p><p>H ( D S 1 , R 3 ) = − ∑ p ( X ) l b p ( X ) = 2</p><p>故 R 2 的信息损失量为0。</p><p>R 3 的信息损失量为 Δ ( R 3 ) = 0.5 。</p><p>(3) 根据联合粒度属性约简算法可得： R 4 = { e 2 , e 3 , e 4 }</p><p>故 R 4 的信息损失量为0。</p><p>例2下表所示为一个决策表， D S 2 = ( U , A , d ) 是一个决策系统，如表4所示。U为论域， A = { α 1 , α 2 , α 3 , α 4 } 是条件属性， { d } 是决策属性集。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Decision system D S 2 = ( U , A , d </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >U</th><th align="center" valign="middle" >a</th><th align="center" valign="middle" >b</th><th align="center" valign="middle" >c</th><th align="center" valign="middle" >e</th><th align="center" valign="middle" >d</th></tr></thead><tr><td align="center" valign="middle" >e<sub>1</sub></td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >e<sub>2</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >e<sub>3</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >e<sub>4</sub></td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >e<sub>5</sub></td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >e<sub>6</sub></td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr></tbody></table></table-wrap><p>表4. 决策系统 D S 2 = ( U , A , d )</p><p>U / A = { { e 1 } , { e 2 } , { e 3 } , { e 4 } , { e 5 } , { e 6 } } ， U / { d } = { { e 1 , e 4 , e 6 } , { e 2 , e 3 , e 5 } } ，</p><p>由正区域的定义可得： P O S A ( d ) = { e 1 , e 2 , e 3 , e 4 , e 5 , e 6 } 。</p><p>根据条件属性依赖度计算公式得： γ A ( D ) = 1 ，所以该决策表是相容决策表。</p><p>H ( D S 2 , A ) = − ∑ p ( X ) l b p ( X ) = 2.585</p><p>(1) 基于正区域的相对约简： S 1 = { a , b }</p><p>H ( D S 2 , S 1 ) = − ∑ p ( X ) l b p ( X ) = 1.918</p><p>故 S 1 的信息损失量为： Δ ( S 1 ) = 0.667 。</p><p>(2) 基于知识粒度的属性约简： S 2 = R E D A = { a , b , e }</p><p>H ( D S 2 , S 2 ) = − ∑ p ( X ) l b p ( X ) = 2.252</p><p>故 S 2 的信息损失量为： Δ ( S 2 ) = 0.333 。</p><p>(3)基于联合粒度的属性约简： S 3 = { a , b , e } , S 4 = { a , b , c }</p><p>H ( D S 2 , S 4 ) = − ∑ p ( X ) l b p ( X ) = 2.252</p><p>故 S 3 , S 4 的信息损失量为： Δ ( S 3 ) = Δ ( S 4 ) = 0.333 。</p></sec><sec id="s8"><title>4. 仿真实验分析</title><p>本节将采用一系列数据集进行验证本文所提出的算法的高效性和准确性。首先将本文提出的联合粒度属性约简算法与知识粒度属性约简算法、正区域相对约简算法 [<xref ref-type="bibr" rid="hanspub.38664-ref14">14</xref>] 对同一组UCI数据集进行离散化处理，然后比较三种算法属性约简后的信息损失量从而验证本文算法的有效性。其实验平台的硬件环境为CPU Intel core i5和4 GB内存，操作系统为Windows 10专业版，实验所运用的编程工具为VC++6.0。</p><p>本次实验数据取自UCI数据集中的四组典型数据如表5所示。首先将每组数据集分成10等份，选择1份作为测试集，其他9份作为训练集，对这9份训练集进行联合粒度、知识粒度和正区域约简，然后比较它们约简后的信息损失量。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Data set descriptio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >序号</th><th align="center" valign="middle" >数据集名称</th><th align="center" valign="middle" >属性</th><th align="center" valign="middle" >对象</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >Dermatology</td><td align="center" valign="middle" >34</td><td align="center" valign="middle" >366</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Contraceptive Method Choice</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >1473</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >Mushroom</td><td align="center" valign="middle" >22</td><td align="center" valign="middle" >8124</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >Letter Recognition</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >20000</td></tr></tbody></table></table-wrap><p>表5. 数据集描述</p><p>文中采用重庆邮电大学开发的RIDAS系统进行属性约简。使用正区域约简算法、知识粒度约简算法和联合粒度约简算法求取不同属性约简集合。根据信息熵的计算公式，根据已有计算信息熵的程序代码 [<xref ref-type="bibr" rid="hanspub.38664-ref9">9</xref>]，计算给定信息系统的条件属性集合的信息熵及每个约简后的信息熵，可得出不同约简准则的信息损失量的大小以及信息熵与信息损失率之间的关系。</p><p>图1~4分别表示四组数据集的约简准则与信息损失量仿真实验结果图，其中横坐标表示属性约简结果，纵坐标表示信息损失量大小。图5~8分别表示四组数据集约简后集合的信息熵与信息损失率仿真实验图，其中横坐标表示约简后信息熵，纵坐标表示信息损失率大小。</p><p>根据上述8张图示可以清楚地看出，本文提出的基于联合粒度属性约简的信息损失远小于基于正区域、基于知识粒度属性约简信息损失，从而说明本文提出的基于联合粒度属性约简算法的有效性，可以有效地解决大数据时代下海量数据的约简，减少约简导致的分类准确率较低、信息损失量较大的问题。</p><p>图1. Dermatology比较</p><p>图2. Contraceptive Method Choice比较</p><p>图3. Mushroom比较</p><p>图4. Letter Recognition比较</p><p>图5. Dermatology分析</p><p>图6. Contraceptive Method Choice分析</p><p>图7. Mushroom分析</p><p>图8. Letter Recognition分析</p></sec><sec id="s9"><title>5. 结论</title><p>本文通过引入联合粒度的概念，并将其运用到粗糙集理论的属性约简 [<xref ref-type="bibr" rid="hanspub.38664-ref15">15</xref>] 中，提出了基于联合粒度属性约简算法，然后分析比较不同的属性约简产生的信息损失，最终得到本文的基于联合粒度的属性约简不仅能保持数据分类的准确性，同时也能维持约简后的信息损失较低的结论。后续我们将继续研究该算法对带权决策表的约简信息损失的影响及进一步的优化。</p></sec><sec id="s10"><title>文章引用</title><p>薛欢欢,郭 步,隋龙飞,王群笑. 基于联合粒度属性约简信息损失的研究Research on Information Loss of Attribute Reduction Based on Joint Granularity[J]. 计算机科学与应用, 2020, 10(11): 1952-1961. https://doi.org/10.12677/CSA.2020.1011206</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.38664-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Pawlak, Z. (1982) Rough Sets. International Journal of Computer and Information Sciences, 11, 341-356.  
&lt;br&gt;https://doi.org/10.1007/BF01001956</mixed-citation></ref><ref id="hanspub.38664-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Hobbs, J.R. (1985) Granularity. Proceedings of the Ninth International Joint Conference on Artificial Intelligence, Los Angeles, 432-435.</mixed-citation></ref><ref id="hanspub.38664-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Lin, T.Y. (1997) Granular Computing. An-nouncement of the BASIC Special Interest Group on Granular Computing.</mixed-citation></ref><ref id="hanspub.38664-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, C.C. (2020) Knowledge Granu-larity Based Incremental Attribute Reduction for Incomplete Decision Systems. International Journal of Machine Learn-ing and Cybernetics, 11, 1141-1157.  
&lt;br&gt;https://doi.org/10.1007/s13042-020-01089-4</mixed-citation></ref><ref id="hanspub.38664-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">李旭, 等. 带权决策表的属性约简[J]. 计算机工程与应用, 2020, 56(12): 54-59.</mixed-citation></ref><ref id="hanspub.38664-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">大数据背景下粗糙集属性约简研究进展[J]. 计算机工程与应用, 2019, 55(6): 31-38.</mixed-citation></ref><ref id="hanspub.38664-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">基于知识粒化的信息系统增量式属性约简[J]. 模式识别与人工智能, 2019, 38(8): 31-38.</mixed-citation></ref><ref id="hanspub.38664-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">一种基于知识粒度的启发式属性约简算法[J]. 计算机工程与应用, 2012, 48(36): 31-38.</mixed-citation></ref><ref id="hanspub.38664-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">邓大勇, 薛欢欢, 苗夺谦, 卢克文. 属性约简准则与约简信息损失的研究[J]. 电子学报, 2017, 45(2): 401-407.</mixed-citation></ref><ref id="hanspub.38664-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">王国胤. Rough集理论与知识获取[M]. 西安: 西安交通大学出版社, 2001.</mixed-citation></ref><ref id="hanspub.38664-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">腾书华. 基于粗糙集理论的不确定性度量和属性约简方法研究[D]: [博士学位论文]. 长沙: 国防科学技术大学, 2010.</mixed-citation></ref><ref id="hanspub.38664-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">桑妍丽, 钱宇华. 多粒度决策粗糙集中的粒度约简方法[J]. 计算机科学, 2017, 44(5): 199-205.</mixed-citation></ref><ref id="hanspub.38664-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">桑妍丽, 钱宇华. 一种悲观多粒度粗糙集中的粒度约简算法[J]. 模式识别与人工智能, 2012, 25(3): 361-366.</mixed-citation></ref><ref id="hanspub.38664-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">邓大勇, 黄厚宽. 多粒度粗糙集的双层绝对约简[J]. 模式识别与人工智能, 2016, 29(11): 969-975.</mixed-citation></ref><ref id="hanspub.38664-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">苗夺谦, 李道国. 粗糙集理论、算法与应用[M]. 北京: 清华大学出版社, 2008: 4.</mixed-citation></ref></ref-list></back></article>