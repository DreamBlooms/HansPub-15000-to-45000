<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">SA</journal-id><journal-title-group><journal-title>Statistics and Application</journal-title></journal-title-group><issn pub-type="epub">2325-2251</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/SA.2021.101007</article-id><article-id pub-id-type="publisher-id">SA-40471</article-id><article-categories><subj-group subj-group-type="heading"><subject>SA20210100000_86131992.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于C-Vine Copula理论的监督学习分类器的优化
  Optimization for C-Vine Copula-Based Supervised Learning Classification
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>蕾</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>杨</surname><given-names>光</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>付</surname><given-names>志慧</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff3"><addr-line>闽南师范大学，数学与统计学院，福建 漳州</addr-line></aff><aff id="aff2"><addr-line>沈阳师范大学，数学与系统科学学院，辽宁 沈阳</addr-line></aff><pub-date pub-type="epub"><day>04</day><month>02</month><year>2021</year></pub-date><volume>10</volume><issue>01</issue><fpage>70</fpage><lpage>76</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    由于朴素贝叶斯分类器对特征变量作了独立性假设，忽略了相关性，导致在某些特征相关的情况下分类效果很差。为了提高分类效果，本文对有缺失的数据集利用C-Vine Copula理论进行填补从而得到完整的数据集，并结合Copula函数研究特征变量之间的相关性优化问题，用C-Vine Copula分类器对完整数据集做分类。结果表明，基于C-Vine Copula理论的监督学习分类器具备良好的分类性能。
    Because of the feature independence assumption, the correlation between variables is ignored, causing that the Naive Bayes works poorly in classification for some cases when the features are correlated. In this paper, for improving the classification effect, the missing datasets are filled by using C-Vine Copula theory. As a result, the complete datasets are got after imputation. By combining the copula function and investigating on the correlation between features, C-vine copula classifier is used to classify complete datasets. The obtained results show that the supervised learning classifier based on the C-Vine Copula theory has better performance. 
  
 
</p></abstract><kwd-group><kwd>缺失数据，C-Vine Copula，监督学习分类器，贝叶斯决策, Missing Data</kwd><kwd> C-Vine Copula</kwd><kwd> Supervised Learning Classification</kwd><kwd> Bayesian Decision Theory</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>由于朴素贝叶斯分类器对特征变量作了独立性假设，忽略了相关性，导致在某些特征相关的情况下分类效果很差。为了提高分类效果，本文对有缺失的数据集利用C-Vine Copula理论进行填补从而得到完整的数据集，并结合Copula函数研究特征变量之间的相关性优化问题，用C-Vine Copula分类器对完整数据集做分类。结果表明，基于C-Vine Copula理论的监督学习分类器具备良好的分类性能。</p></sec><sec id="s2"><title>关键词</title><p>缺失数据，C-Vine Copula，监督学习分类器，贝叶斯决策</p></sec><sec id="s3"><title>Optimization for C-Vine Copula-Based Supervised Learning Classification</title><p>Lei Wang<sup>1</sup>, Guang Yang<sup>1</sup>, Zhihui Fu<sup>2</sup></p><p><sup>1</sup>Collage of Mathematics and Systems Science, Shenyang Normal University, Shenyang Liaoning</p><p><sup>2</sup>Collage of Mathematics and Statistics, Minnan Normal University, Zhangzhou Fujian</p><p><img src="//html.hanspub.org/file/7-2580704x4_hanspub.png" /></p><p>Received: Jan. 11<sup>th</sup>, 2021; accepted: Feb. 15<sup>th</sup>, 2021; published: Feb. 22<sup>nd</sup>, 2021</p><p><img src="//html.hanspub.org/file/7-2580704x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Because of the feature independence assumption, the correlation between variables is ignored, causing that the Naive Bayes works poorly in classification for some cases when the features are correlated. In this paper, for improving the classification effect, the missing datasets are filled by using C-Vine Copula theory. As a result, the complete datasets are got after imputation. By combining the copula function and investigating on the correlation between features, C-vine copula classifier is used to classify complete datasets. The obtained results show that the supervised learning classifier based on the C-Vine Copula theory has better performance.</p><p>Keywords:Missing Data, C-Vine Copula, Supervised Learning Classification, Bayesian Decision Theory</p><disp-formula id="hanspub.40471-formula17"><graphic xlink:href="//html.hanspub.org/file/7-2580704x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/7-2580704x7_hanspub.png" /> <img src="//html.hanspub.org/file/7-2580704x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>随着信息技术与网络技术的高速发展，各个行业每天都会产生大量数据。然而现实世界中的数据集通常有质量问题，如存在一些错误数据、缺失数据、不确定数据。在数据质量问题中，缺失数据现象尤其常见。</p><p>朴素贝叶斯分类是机器学习和数据挖掘中最流行的学习算法之一，主要用于给定特征变量的分类问题。朴素贝叶斯分类器对分类对象的特征变量作了条件独立性假设，该假设忽略了特征变量之间的相关性，从而忽略了实际数据中的相关性 [<xref ref-type="bibr" rid="hanspub.40471-ref1">1</xref>]。</p><p>Sklar (1959) [<xref ref-type="bibr" rid="hanspub.40471-ref2">2</xref>] 提出的Copula理论指出，一个多元联合分布可以分解为k个边缘分布和一个Copula函数，这个Copula函数描述了变量间的相关性。Nelsen (1999) [<xref ref-type="bibr" rid="hanspub.40471-ref3">3</xref>] 较为系统地介绍了Copula函数的定义和构建方法，使得Copula理论成为构造多元变量联合分布及描述随机变量间相依结构的重要工具。Joe (1996) [<xref ref-type="bibr" rid="hanspub.40471-ref4">4</xref>] 首次提出了Pair Copula的理念，Bedford和Cooke (2001) [<xref ref-type="bibr" rid="hanspub.40471-ref5">5</xref>] 基于Joe的研究提出了Pair Copula构建(PCC)方法，利用图论中藤(Vine)描述结构，将高维Copula函数仿照树藤结构的分解形式分解为一系列二元成对Copula函数，称为Vine Copula模型。Aas (2009) [<xref ref-type="bibr" rid="hanspub.40471-ref6">6</xref>] 等进一步深入研究Vine Copula模型，详细论述了Vine Copula模型的参数估计和数值模拟的方法。</p><p>针对有缺失的数据集，本文设计并提出基于C-Vine Copula理论的贝叶斯分类器，进一步优化贝叶斯分类器。首先，确定需要归因的特征变量，通过C-Vine Copula理论将数据集的联合概率分布分解为一系列二元Copula函数与边缘概率密度函数乘积的形式；然后，将归因特征作为目标特征，对其他特征变量根据特征样本间的相关性，选择适当的二元Copula函数，计算条件分布函数和相应的逆函数，从而提出C-Vine Copula Imputation算法(CVI)，用该算法得到的预测值替换缺失值得到新的数据集，对新的数据集构建贝叶斯分类器中的条件概率密度函数；最后，将优化后的分类器应用到实际分类问题中，对模型进行分析验证。与样本丢弃法(NADel)、均值归因法(Mean)、预测平均匹配插补法(PMM)、贝叶斯线性回归归因法(Norm)、分类回归树归因法(Cart)和在观测值中随机抽样归因法(Sample)比较，本文提出的分类器具备良好的性能，能够为有缺失的数据集的分类提供新的实现途径。</p></sec><sec id="s6"><title>2. C-vine Copula贝叶斯分类器</title><sec id="s6_1"><title>2.1. Copula函数</title><p>根据Sklar定理可知，一组n维随机向量，其联合概率分布可以分解为n个一元的边缘分布函数与一个n维Copula函数的乘积。设 X = ( X 1 , ⋯ , X n ) 为一组n维随机变量，其联合概率分布 F ( x 1 , ⋯ , x n ) 与边缘分布 F 1 , ⋯ , F n 的关系可以表示为：</p><p>F ( x 1 , ⋯ , x n ) = C ( F 1 ( x 1 ) , ⋯ , F n ( x n ) ) ，</p><p>如果各边缘分布函数都是连续的，则Copula函数是唯一的。若 F k 的逆函数 F k − 1 存在，则</p><p>C ( u 1 , ⋯ , u n ) = F ( F 1 − 1 ( u 1 ) , ⋯ , F n − 1 ( u n ) ) ，</p><p>其中 u k = F k ( x k ) ∈ ( 0 , 1 ) ， k = 1 , 2 , ⋯ , p 。当Copula函数可微时，可以得到F的联合概率分布</p><p>f ( x 1 , ⋯ , x n ) = ∏ k − 1 p f k ( x k ) ⋅ c ( F 1 ( x 1 ) , ⋯ , F n ( x n ) ) ，</p><p>f k ( x k ) 是 F k 的边缘密度，c是C的Copula密度。</p><p>f ( x 1 , ⋯ , x n ) = f n ( x n ) ⋅ f ( x n − 1 | x n ) ⋅ f ( x n − 2 | x n − 1 , x n ) ⋯ f ( x 1 | x 2 , ⋯ , x n ) (1)</p><p>根据Aas [<xref ref-type="bibr" rid="hanspub.40471-ref6">6</xref>] 等的论述，可以再次分解条件概率密度函数式(1)，可得</p><p>f ( x | ν ) = c x , v j | ν − j ( F ( x | ν − j ) , F ( v k | ν − j ) ) ⋅ f ( x | ν − j ) ，</p><p>其中，条件分布的表达式为</p><p>F ( x | ν ) = ∂ C x , ν j | ν − j ( F ( x | ν − j ) , F ( v j | ν − j ) ) ∂ F ( v j | ν − j ) ，</p><p>ν 是随机变量x去掉 x i 后的 n − 1 维向量； ν − j 是 ν 中去掉 ν j 后的向量； ν j 为 ν 中任意一个成分； c x , ν j | ν − j 为相应条件下的二元Copula条件概率密度函数。</p><p>当 ν − k = Φ ，x和 ν 都是均匀分布时，式(6)可简化得到</p><p>h ( x , ν ) = F ( x | ν ) = ∂ C x , ν ( x , ν ) ∂ ν ， (2)</p><p>其中 h ( ⋅ ) 称为h-函数，是用于生成伪观测值的函数，之后将用它来拟合C-Vine结构的模型 [<xref ref-type="bibr" rid="hanspub.40471-ref7">7</xref>]。</p></sec><sec id="s6_2"><title>2.2. C-Vine Copula Imputation算法</title><p>在构建多维变量的联合概率分布函数时，可以用多种二元Copula结构描述变量之间的相关性，其中C-vine Copula和D-vine Copula是最典型的两种 [<xref ref-type="bibr" rid="hanspub.40471-ref8">8</xref>]。本文采用C-vine Copula结构对随机变量的复杂相关性进行建模。图1为一个4维C-Vine的分解实例。</p><p>图1. 4维C-vine结构</p><p>逆采样方法是较为常见的生成随机数的方法，令F是 ℜ 上连续递增的分布函数，它的逆函数 F − 1 定义为：</p><p>F − 1 : ( 0 , 1 ) → ℜ ， F − 1 ( x ) : = inf { y ∈ ℜ : F ( y ) ≥ x } 。</p><p>如果 U ~ U [ 0 , 1 ] 是在 [ 0 , 1 ] 上的均匀分布随机变量，则 F − 1 ( U ) 有分布函数F。如果X有分布函数F，则 F ( X ) 是 [ 0 , 1 ] 上的均匀分布。</p><p>Bedford和Cooke [<xref ref-type="bibr" rid="hanspub.40471-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.40471-ref9">9</xref>] 以及Kurowicka和Cooke等 [<xref ref-type="bibr" rid="hanspub.40471-ref10">10</xref>] 对Vine抽样算法都有讨论。Aas等 [<xref ref-type="bibr" rid="hanspub.40471-ref6">6</xref>] 给出C-Vine和D-Vine的一般性算法。对于 X = ( X 1 , ⋯ , X n − 1 , X n ) 这组n维随机变量， X n 上有缺失数据。首先，生成与缺失数据数量相等的随机数 v n ；再根据条件分布函数和h-函数可以得到</p><p>F ( x n | x 1 , ⋯ , x n − 1 ) = ∂ n − 1 C n , n − 1 | 1 : n − 2 ( F ( x n | x 1 , ⋯ , x n − 2 ) , F ( x n − 1 | x 1 , ⋯ , x n − 2 ) ) ∂ F ( x n − 1 | x 1 , ⋯ , x n − 2 ) 。</p><p>所以，缺失数据 X n 可以表示为</p><p>X n = F n | 1 : n − 1 − 1 ( V n | X 1 , ⋯ , X n − 1 ) 。</p></sec><sec id="s6_3"><title>2.3. C-Vine Copula分类器</title><p>在n维C-Vine结构中，有 n − 1 颗树 T j ( j = 1 , ⋯ , n − 1 ) ，每棵树由节点和边组成。每条边对应一个二元Copula，树 T j 的边是树 T j + 1 的节点。整个分解由n个边缘分布和 n ( n − 1 ) / 2 个二元Copula的乘积得到的。由此可知，C-vine Copula结构的联合概率分布函数可以分解为</p><p>f ( x 1 , ⋯ , x n ) = ∏ k = 1 n f ( x k ) &#215; ∏ i = 1 n − 1 ∏ j = 1 n − i c i , i + j ; 1 : ( i − 1 ) ( F ( x i | x 1 , ⋯ , x i − 1 ) , F ( x i + j | x 1 , ⋯ , x i − 1 ) ) ， (3)</p><p>其中， F ( ⋅ | ⋅ ) 是条件分布函数，可由式(2)得到； c i , i + j ; 1 : ( i − 1 ) 为二元Copula条件概率密度。</p><p>根据贝叶斯准则，向量 X = ( X 1 , ⋯ , X n ) 是类别 e ∈ E 的概率为</p><p>Pr ( e | x ) ∝ Pr ( x | e ) ⋅ Pr ( e ) ， (4)</p><p>其中E是类向量， E = ( e 1 , ⋯ , e l ) ′ ，l是类别总数。结合联合概率分布函数式(3)，式(4)写作</p><p>Pr ( e | x ) ∝ ∏ k = 1 n f k ( x k | e ) ∏ i = 1 n − 1 ∏ j = 1 n − i c i , i + j ; 1 : ( i − 1 ) ( F ( x i | x 1 , ⋯ , x i − 1 ) , F ( x i + j | x 1 , ⋯ , x i − 1 ) | e ) ︸ f ( x | e ) &#215; Pr ( e ) 。</p><p>根据最大后验决策规则(MAP)，可以得到</p><p>classify ( x ) = { e : arg max f ( x | e ) ⋅ Pr ( e ) } 。</p><p>本文采用核函数方法估计边缘概率密度函数。核函数方法是一种非参数方法， x 1 , ⋯ , x n 是独立同分布的n个样本点，设概率密度函数为f，则测试样本x的概率密度估计值为</p><p>f ^ n ( x ) = 1 n ∑ i = 1 n K n ( x − x i ) = 1 n h ∑ i = 1 n K ( x − x i h ) ，</p><p>其中， K ( ⋅ ) 是高斯核函数； h &gt; 0 称为带宽 [<xref ref-type="bibr" rid="hanspub.40471-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.40471-ref12">12</xref>]。</p></sec></sec><sec id="s7"><title>3. 实证分析</title><p>为验证本文提出方法的有效性和可行性，用UCI数据库中的数据集对比不同填补缺失数据方法下的分类结果。</p><sec id="s7_1"><title>3.1. 实验设计</title><p>首先，对数据集某一列 X n 做完全随机缺失，缺失率分别为5%、10%、20%、50%和70%，将数据集分为完整数据(complete)和缺失数据(missing)。然后，根据CVI得出缺失数据 u n ，并将 u n 与70%的完整数据(complete)放在一起作为新的训练集，剩下的30%完整数据作为测试集，依据新的训练集用C-vine Copula分类器对测试集进行分类。接下来，分别用样本丢弃法(NADel)、均值归因法(Mean)、预测平均匹配插补法(PMM)、贝叶斯线性回归归因法(Norm)、分类回归树归因法(Cart)及从观测值中随机抽样归因法(Sample)填补缺失数据，再在C-Vine Copula分类器上对测试集分类。最后，重复以上步骤100次，得到不同填补算法的平均分类准确性。各个填补算法的分类结果如下。</p></sec><sec id="s7_2"><title>3.2. 结果分析</title><p>对UCI中的2个数据集分别在缺失率为5%、10%、20%、50%和70%的情况下，用CVI、NADel、Mean、PMM、Norm、Cart和Sample做填补处理。分类准确率的测试结果如表1和表2所示。由表1、表2可知，相比其他填补算法，CVI算法在大多数情况下均能得到更高的分类准确率。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Classification accuracy for Iris data se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >缺失率</th><th align="center" valign="middle" >CVI</th><th align="center" valign="middle" >NADel</th><th align="center" valign="middle" >Mean</th><th align="center" valign="middle" >PMM</th><th align="center" valign="middle" >Norm</th><th align="center" valign="middle" >Cart</th><th align="center" valign="middle" >Sample</th></tr></thead><tr><td align="center" valign="middle" >5%</td><td align="center" valign="middle" >0.95644</td><td align="center" valign="middle" >0.81200</td><td align="center" valign="middle" >0.94800</td><td align="center" valign="middle" >0.95778</td><td align="center" valign="middle" >0.78556</td><td align="center" valign="middle" >0.95578</td><td align="center" valign="middle" >0.95667</td></tr><tr><td align="center" valign="middle" >10%</td><td align="center" valign="middle" >0.95778</td><td align="center" valign="middle" >0.81244</td><td align="center" valign="middle" >0.94778</td><td align="center" valign="middle" >0.95533</td><td align="center" valign="middle" >0.72733</td><td align="center" valign="middle" >0.95556</td><td align="center" valign="middle" >0.95489</td></tr><tr><td align="center" valign="middle" >20%</td><td align="center" valign="middle" >0.95489</td><td align="center" valign="middle" >0.86022</td><td align="center" valign="middle" >0.95022</td><td align="center" valign="middle" >0.95156</td><td align="center" valign="middle" >0.56889</td><td align="center" valign="middle" >0.95422</td><td align="center" valign="middle" >0.95067</td></tr><tr><td align="center" valign="middle" >50%</td><td align="center" valign="middle" >0.94733</td><td align="center" valign="middle" >0.86044</td><td align="center" valign="middle" >0.95289</td><td align="center" valign="middle" >0.94867</td><td align="center" valign="middle" >0.46089</td><td align="center" valign="middle" >0.94844</td><td align="center" valign="middle" >0.94667</td></tr><tr><td align="center" valign="middle" >70%</td><td align="center" valign="middle" >0.93711</td><td align="center" valign="middle" >0.85356</td><td align="center" valign="middle" >0.94667</td><td align="center" valign="middle" >0.93733</td><td align="center" valign="middle" >0.41422</td><td align="center" valign="middle" >0.94356</td><td align="center" valign="middle" >0.93267</td></tr><tr><td align="center" valign="middle" >5%</td><td align="center" valign="middle" >0.95644</td><td align="center" valign="middle" >0.81200</td><td align="center" valign="middle" >0.94800</td><td align="center" valign="middle" >0.95778</td><td align="center" valign="middle" >0.78556</td><td align="center" valign="middle" >0.95578</td><td align="center" valign="middle" >0.95667</td></tr><tr><td align="center" valign="middle" >10%</td><td align="center" valign="middle" >0.95778</td><td align="center" valign="middle" >0.81244</td><td align="center" valign="middle" >0.94778</td><td align="center" valign="middle" >0.95533</td><td align="center" valign="middle" >0.72733</td><td align="center" valign="middle" >0.95556</td><td align="center" valign="middle" >0.95489</td></tr></tbody></table></table-wrap><p>表1. Iris数据集填补算法分类性能表</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Classification accuracy for Breast Cancer data se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >缺失率</th><th align="center" valign="middle" >CVI</th><th align="center" valign="middle" >NADel</th><th align="center" valign="middle" >Mean</th><th align="center" valign="middle" >PMM</th><th align="center" valign="middle" >Norm</th><th align="center" valign="middle" >Cart</th><th align="center" valign="middle" >Sample</th></tr></thead><tr><td align="center" valign="middle" >5%</td><td align="center" valign="middle" >0.94443</td><td align="center" valign="middle" >0.92057</td><td align="center" valign="middle" >0.94352</td><td align="center" valign="middle" >0.92567</td><td align="center" valign="middle" >0.76876</td><td align="center" valign="middle" >0.91319</td><td align="center" valign="middle" >0.94067</td></tr><tr><td align="center" valign="middle" >10%</td><td align="center" valign="middle" >0.94529</td><td align="center" valign="middle" >0.93729</td><td align="center" valign="middle" >0.94429</td><td align="center" valign="middle" >0.90848</td><td align="center" valign="middle" >0.71419</td><td align="center" valign="middle" >0.91381</td><td align="center" valign="middle" >0.93948</td></tr><tr><td align="center" valign="middle" >20%</td><td align="center" valign="middle" >0.94686</td><td align="center" valign="middle" >0.93729</td><td align="center" valign="middle" >0.94638</td><td align="center" valign="middle" >0.92533</td><td align="center" valign="middle" >0.69410</td><td align="center" valign="middle" >0.92014</td><td align="center" valign="middle" >0.94100</td></tr><tr><td align="center" valign="middle" >50%</td><td align="center" valign="middle" >0.95129</td><td align="center" valign="middle" >0.90781</td><td align="center" valign="middle" >0.94343</td><td align="center" valign="middle" >0.92614</td><td align="center" valign="middle" >0.65114</td><td align="center" valign="middle" >0.87686</td><td align="center" valign="middle" >0.92571</td></tr><tr><td align="center" valign="middle" >70%</td><td align="center" valign="middle" >0.95252</td><td align="center" valign="middle" >0.91243</td><td align="center" valign="middle" >0.94867</td><td align="center" valign="middle" >0.89952</td><td align="center" valign="middle" >0.65752</td><td align="center" valign="middle" >0.87848</td><td align="center" valign="middle" >0.90086</td></tr><tr><td align="center" valign="middle" >5%</td><td align="center" valign="middle" >0.94443</td><td align="center" valign="middle" >0.92057</td><td align="center" valign="middle" >0.94352</td><td align="center" valign="middle" >0.92567</td><td align="center" valign="middle" >0.76876</td><td align="center" valign="middle" >0.91319</td><td align="center" valign="middle" >0.94067</td></tr><tr><td align="center" valign="middle" >10%</td><td align="center" valign="middle" >0.94529</td><td align="center" valign="middle" >0.93729</td><td align="center" valign="middle" >0.94429</td><td align="center" valign="middle" >0.90848</td><td align="center" valign="middle" >0.71419</td><td align="center" valign="middle" >0.91381</td><td align="center" valign="middle" >0.93948</td></tr></tbody></table></table-wrap><p>表2. Breast Cancer数据集填补算法分类性能表</p><p>图2描绘了数据集Breast Cancer在5种不同数据缺失率的情况下，用7种填补算法得到的分类性能比较图。由折线图可知，CVI算法在7种填补算法中具有最好的分类性能。</p><p>图2. Breast Cancer的分类性能图</p></sec></sec><sec id="s8"><title>4. 结论</title><p>本文针对数据集中存在缺失数据的情况，结合C-vine Copula函数，利用条件分布函数填补缺失数据，再将变量的联合概率分布分解成二元Copula函数与边缘概率密度函数乘积的形式，分别对二元Copula函数和边缘概率密度函数进行优化估计，将特征变量之间的复杂相关性构建在条件概率密度函数中。与其他算法相比，提高了贝叶斯分类器处理具有复杂相关性特征变量数据的分类性能，在实际应用中得到了较好的分类结果。</p></sec><sec id="s9"><title>基金项目</title><p>辽宁省教育厅自然科学基金项目(LJC201914)；辽宁省自然科学基金(2019MS285)。</p></sec><sec id="s10"><title>文章引用</title><p>王 蕾,杨 光,付志慧. 基于C-Vine Copula理论的监督学习分类器的优化Optimization for C-Vine Copula-Based Supervised Learning Classification[J]. 统计学与应用, 2021, 10(01): 70-76. https://doi.org/10.12677/SA.2021.101007</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.40471-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012: 48-53.</mixed-citation></ref><ref id="hanspub.40471-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Sklar, A. (1959) Fonctions de repartition an dimensions et leurs marges. Publications de l’Institut de Statistique de l’Universite de Paris, 33, 229-231.</mixed-citation></ref><ref id="hanspub.40471-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Nelsen, R.B. (1999) An Introduction to Copulas. Springer-Verlag, New York.  
https://doi.org/10.1007/978-1-4757-3076-0</mixed-citation></ref><ref id="hanspub.40471-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Joe, H. (1997) Multivariate Models and Dependence Concepts. Chapman and Hall, London.  
https://doi.org/10.1201/9780367803896</mixed-citation></ref><ref id="hanspub.40471-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Bedford, T. and Cooke, R. (2001) Probability Density Decomposition for Conditionally Dependent Random Variables Modeled by Vines. Annals of Mathematics and Artificial Intelligence, 32, 245-268.  
https://doi.org/10.1023/A:1016725902970</mixed-citation></ref><ref id="hanspub.40471-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Aas, K., Czado, C., Frigessi, A., et al. (2009) Pair-Copula Constructions of Multiple Dependence. Insurance Mathematics and Economics, 44, 182-198. https://doi.org/10.1016/j.insmatheco.2007.02.001</mixed-citation></ref><ref id="hanspub.40471-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Czado, C., Schepsmeier, U. and Min, A. (2011) Maximum Likelihood Estimation of Mixed C-Vines with Application to Exchange Rates. Statistical Modelling, 12, 229-255. https://doi.org/10.1177/1471082X1101200302</mixed-citation></ref><ref id="hanspub.40471-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Brechmann, E.C., Schepsmeier, U., Grün, B., et al. (2013) Modeling Dependence with C- and D-Vine Copulas: The R Package CDVine. J of Statistical Software, 52, 1-27. https://doi.org/10.18637/jss.v052.i03</mixed-citation></ref><ref id="hanspub.40471-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Bedford, T. and Cooke, R. (2002) Vines a New Graphical Model for Dependent Random Variables. The Annals of Statistics, 30, 1031-1068. https://doi.org/10.1214/aos/1031689016</mixed-citation></ref><ref id="hanspub.40471-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Kurowicka, D. and Cooke, R. (2006) Uncertainty Analysis with High Dimensional Dependence Modeling. John Wiley &amp; Sons, Manhattan. https://doi.org/10.1002/0470863072</mixed-citation></ref><ref id="hanspub.40471-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">韦艳华, 张世英. Copula理论及其在金融分析上的应用[M]. 北京: 清华大学出版社, 2008: 1-40.</mixed-citation></ref><ref id="hanspub.40471-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Y.H. (2014) A Copula-Based Supervised Learning Classification for Continuous and Discrete Data. Journal of Data Science, No. 13, 769-790.</mixed-citation></ref></ref-list></back></article>