<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2019.98175</article-id><article-id pub-id-type="publisher-id">CSA-31754</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20190800000_67310196.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于深度学习的语义场景图像检索
  Deep Learning Based Semantic Scene Image Retrieval
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>徐</surname><given-names>海蛟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>展鸿</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>何</surname><given-names>佳蕾</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>方</surname><given-names>钰敏</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东第二师范学院，计算机科学系，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>08</month><year>2019</year></pub-date><volume>09</volume><issue>08</issue><fpage>1561</fpage><lpage>1568</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   随着互联网图像等多媒体内容的爆炸式增长，在线Web图像的语义场景检索问题引起了学者们的研究兴趣。传统的研究工作聚焦在基于单概念的图像检索上，未能很好检索含有复杂语义场景的图像。为解决语义场景Web图像检索问题，我们提出了一种基于多模态深度学习的语义场景图像检索方法(SSIR)。首先，使用一个多模态CNN训练网络作为概念分类器；其次，通过计算语义概念之间的依赖关系来精炼概念的语义分数，以进一步增强分类器的场景识别能力；最后，为提升对稀疏场景概念的检索性能，应用梯度下降算法来补偿在真实应用中不平衡图像集上语义概念的频率差。在MIR Flickr 2011标准图像数据集上对比了其他传统方法，结果表明我们的语义场景检索方法性能更优。 With the explosive growth of multimedia objects such as Web images over the Internet, online semantic scene image retrieval has been receiving increasing research interest. Conventional studies focus on single-concept-based image retrieval and cannot effectively retrieve semantic scene images including multiple concepts that describe characteristic semantic scene. To tackle this issue, i.e., semantic scene Web image retrieval, we propose a novel approach called multi-modal deep learning based Semantic Scene Image Retrieval (SSIR) in this paper. In particular, we first train a multi-modal Convolutional Neural Network (CNN) as a concept classifier for images and texts. Second, semantic interdependencies of the subconcepts included in the images are utilized to refine the predicted semantic scores in order to enhance holistic scene recognition. Finally, to improve the performance of retrieving rare scene concepts, a gradient descent algorithm is used for compensating the varying frequencies of concepts derived from imbalanced image datasets. The results of our experiments on MIR Flickr 2011 have shown that our proposed approach performs favorably compared with several traditional methods. 
  
 
</p></abstract><kwd-group><kwd>语义场景图像检索，卷积神经网络，深度学习, Semantic Scene Image Retrieval</kwd><kwd> Convolutional Neural Network</kwd><kwd> Deep Learning</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于深度学习的语义场景图像检索<sup> </sup></title><p>徐海蛟，张展鸿，何佳蕾，方钰敏</p><p>广东第二师范学院，计算机科学系，广东 广州</p><p><img src="//html.hanspub.org/file/12-1541501x1_hanspub.png" /></p><p>收稿日期：2019年7月28日；录用日期：2019年8月12日；发布日期：2019年8月19日</p><disp-formula id="hanspub.31754-formula14"><graphic xlink:href="//html.hanspub.org/file/12-1541501x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>随着互联网图像等多媒体内容的爆炸式增长，在线Web图像的语义场景检索问题引起了学者们的研究兴趣。传统的研究工作聚焦在基于单概念的图像检索上，未能很好检索含有复杂语义场景的图像。为解决语义场景Web图像检索问题，我们提出了一种基于多模态深度学习的语义场景图像检索方法(SSIR)。首先，使用一个多模态CNN训练网络作为概念分类器；其次，通过计算语义概念之间的依赖关系来精炼概念的语义分数，以进一步增强分类器的场景识别能力；最后，为提升对稀疏场景概念的检索性能，应用梯度下降算法来补偿在真实应用中不平衡图像集上语义概念的频率差。在MIR Flickr 2011标准图像数据集上对比了其他传统方法，结果表明我们的语义场景检索方法性能更优。</p><p>关键词 :语义场景图像检索，卷积神经网络，深度学习</p><disp-formula id="hanspub.31754-formula15"><graphic xlink:href="//html.hanspub.org/file/12-1541501x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/12-1541501x7_hanspub.png" /> <img src="//html.hanspub.org/file/12-1541501x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>在Web 2.0/3.0时代，社交媒体获得了前所未有的发展，“互动”是图像等社交多媒体最重要的传播特征和要求，用户常常自由地给予这些Web图像共享的文本社交标签(社会化标签) [<xref ref-type="bibr" rid="hanspub.31754-ref1">1</xref>] 或者用户评论。这样的多媒体内容已经大量地出现在人们的日常生活中，所以亟须有效的检索方法帮助人们寻找到目标多媒体内容。Web图像往往伴随着若干个文本标签，它们是由图像用户即兴给定的关键字或者文本描述符，可能部分或者间接地描述了图像的内容，是带有高噪音的弱语义信息，存在着语义表达不完整、语义错误、语义歧义、语义重复等问题，所以其不同于由专业人士给出的强语义标注或者类标签。一个直截了当的图像检索方法是直接在图像社交标签上执行图像检索，然而图像社交标签往往在语义上是高噪音的，并且可能并不会直接地与图像内容有关联，从而导致较低的图像检索性能。如图1所示，在测试图像I中存在一些带下划线的社交标签例如“remember_dreams_come_true”和“bravo”并没有直接描述该图像I的视觉内容，而训练图像的社交标签例如“Houston”和“Barco”仅部分描述了图像内容。虽然这些噪音文本社交标签不适宜直接用于图像查询概念，但是可以把它们看作是额外的、有价值的低层文本特征即文本模态数据以用于图像的监督学习。</p><p>对于在线图像检索，传统的单模态方法要么仅使用了图像模态数据 [<xref ref-type="bibr" rid="hanspub.31754-ref2">2</xref>] ，要么仅使用了文本模态数据 [<xref ref-type="bibr" rid="hanspub.31754-ref3">3</xref>] 。由于文本模态数据例如社会化标签可以用来增强多概念图像检索的性能，因此一些多模态跨模态检索方法 [<xref ref-type="bibr" rid="hanspub.31754-ref4">4</xref>] 研究了图像与文本等多个模态数据之间的关联以提升图像检索的性能。然而，这些传统的方法聚焦于基于单概念的图像检索而较少考虑多概念语义场景图像检索场景，这限制了其应用范围，因为用户使用检索系统的时候往往提交多概念的语义场景检索请求。为了处理语义场景图像检索，传统方法在单概念检索系统上执行多概念语义场景检索任务，这在一些场景中可能是低效的，这是因为多概念场景有其独特的视觉外观特性，这难以仅仅依靠单概念分类器去识别。于是，进一步研究语义场景图像检索有用且有必要。</p><p>近年来，卷积神经网络(CNNs) [<xref ref-type="bibr" rid="hanspub.31754-ref5">5</xref>] 作为一种重要的深度学习技术已广泛应用于基于单概念的图像检索。CNNs占据了很多检索测试基线的排行榜，这表明通过CNNs学习到的深度特征能较好表征图像的内部结构。因此，考虑使用深度学习技术去提升语义场景图像检索的性能。我们提出的方法SSIR整合了两类分类器：1) 多概念场景分类器(有助于整体场景识别)；2) 单概念目标分类器(有助于单概念识别)。从实</p><p>图1. 语义场景图像检索示例图</p><p>验结果看，这种设计能显著提高分类器对多概念语义场景的判断识别能力。具体来说，首先，设计一个多模态CNN以识别多概念语义场景。每个CNN分为卷积块层与全连接分类器层。学习时，图像(视觉模态)与关联文本(文本模态)被分别送入各自模态的卷积块层与全连接分类器层。全连接分类器层包含两类分类器，即适合单概念识别的单概念目标分类器以及有助于整体场景识别的多概念场景分类器。第二，语义概念之间的依赖关系被用来计算概念的语义分数，以进一步增强分类器的判断识别能力。假若一个概念c与其关联概念们以较高的频率共现在图像集中，一旦发现关联概念们频繁出现，c的预测分数值将被提高。最后，通过融合运算，SSIR方法整合了视觉模态和文本模态的预测分数值。为改善对稀疏概念的检索性能，应用梯度下降算法来补偿在真实应用中不平衡图像集上语义概念的频率差。</p></sec><sec id="s4"><title>2. 基于多模态深度学习的语义场景图像检索</title><sec id="s4_1"><title>2.1. 问题描述</title><p>设多模态图像训练集为 L = { ( I 1 , T 1 ) , ⋯ , ( I N , T N ) } ，多模态图像测试集为U。每个图像文本对 ( I i , T i ) ∈ L 或者 ( I , T ) ∈ U 表示一幅图像及其关联的若干个弱语义文本标签，分别由低层视觉特征和低层文本特征组成。给定一个包含有K个不同语义单概念的强语义单词表 V = { c 1 , ⋯ , c i , ⋯ , c K } ，其中每一个语义概念 c i ∈ V 是一个语义单概念(例如“rainbow”或者“wedding”)。在训练集L中的每幅图像被标注若干个语义单概念c<sub>i</sub>，而测试集U中的图像没有任何强语义标注。每个语义场景多概念 C i = { c 1 , ⋯ , c k } 是V的幂集中的一个元素即 C i ∈ 2 V 或者 C i ⊆ V ，其中k是C<sub>i</sub>的长度即 k = | C i | 。给定一个多概念语义场景查询 Q = { c 1 , ⋯ , c t } ∈ 2 V 和带强语义标注的训练集L以及未标注图像测试集U，目标是寻找包含所有t个目标单概念的最相关图像 I ∈ U 。</p></sec><sec id="s4_2"><title>2.2. 系统框架</title><p>SSIR检索框架如图2所示，包含3个部分：卷积块层、分类器层、在线检索。卷积块层学习出两类特征：深度视觉特征与深度文本特征。通过视觉卷积块层，将图像像素转换为视觉特征向量；通过文本卷积块层，将关联的文本社交标签转换为词嵌入 [<xref ref-type="bibr" rid="hanspub.31754-ref6">6</xref>] ，然后学习出文本特征向量。分类器层学习出从特征到概念的映射函数，包含两种分类器：单概念目标分类器和多概念场景分类器。这里，需要生成多概念单词表V<sup>*</sup>。由于V<sup>*</sup>中的概念分为语义单概念和场景多概念两类，所以共有四类映射函数：单概念视觉映射函数、多概念视觉映射函数、单概念文本映射函数、多概念文本映射函数。在线检索根据给定的多概念语义场景查询Q从测试集中检索到最相关的图像结果。由于要使用多概念之间的语义依赖关系，因此需要生成查询上下文R<sub>rc</sub>(Q)。</p><p>图2. SSIR系统框架</p><p>每个多概念 C i ∈ V * 可视为单个抽象场景概念C<sub>i</sub>，也可视为k个分离的单概念c<sub>i</sub>。对于多概念单词表V<sup>*</sup>的生成，为避免产生无意义的语义概念排列组合，SSIR模型基于训练集L上的共现规则选择出有场景语境的多概念C<sub>i</sub>用以生成一个强语义多概念单词表V<sup>*</sup>：</p><p>| C i | ≤ t (1)</p><p>| N r ( c 1 , ⋯ , c k ) | ≥ s (2)</p><p>其中 表示所有k个语义概念c<sub>i</sub>在训练集L中的共现总数，即多概念频率。如果集合V<sup>*</sup>太大，可以控制共现阈值s以减少训练开销。</p></sec><sec id="s4_3"><title>2.3. 多模态CNN结构</title><p>任何CNN网络都可融入我们的检索模型，不失一般性，我们选择了近年来一个有影响力的高效CNN模型ResNet [<xref ref-type="bibr" rid="hanspub.31754-ref7">7</xref>] 来作为我们的视觉模态CNN分类器。由于传统CNN分类器网络聚焦于单概念分类，而我们的语义场景检索任务是一个多概念分类任务，因此为ResNet模型定义了一个新的多概念softmax损失函数使之适应多概念语义场景检索任务。首先，第i张图片I与第j个概念 C j ∈ V * 的归一化关联概率可定义为：</p><p>p ( C j | I ) = exp ( q j ( I ) ) ∑ k exp ( q k ( I ) ) , (3)</p><p>其中， q j ( I ) 是图像I在第j个概念C<sub>j</sub>的离散概率分布，它由ResNet分类器产生。为最小化ResNet预测概率与真实概率的KL距离，我们使用如下场景多概念softmax损失函数：</p><p>f softmax = − 1 N ∑ i ∑ j p &#175; i , j log ( p ( C j | I ) ) , (4)</p><p>其中， p &#175; i , j 是一个图片I的指示器函数：当概念C<sub>j</sub>在图片I中存在则 p &#175; i , j = 1 否则 p &#175; i , j = 0 ；N是图像总数。传统的单概念CNN基本结构为卷积块层后接分类器层。卷积块层学习出深度特征，而分类器层执行从I到C<sub>j</sub>的映射，识别出语义概念C<sub>j</sub>。我们设计的SSIR场景检索模型包含两类分类器：单概念分类器和多概念场景分类器。当 | C j | = 1 时，C<sub>j</sub>为一个传统的单概念；当 | C j | &gt; 1 时，C<sub>j</sub>为一个场景多概念，其作为一个整体概念参加学习。</p><p>对于文本CNN网络，很多适合自然语言处理的CNN网络都可融入我们的场景检索模型，不失一般性，我们选择了一个有影响力的高效CNN模型SentenceCNN [<xref ref-type="bibr" rid="hanspub.31754-ref8">8</xref>] 作为我们的文本模态CNN分类器。为训练SentenceCNN，将一副图像所有的文本标签视为一个输入语句加以学习。类似视觉模态CNN分类器，修改分类器层，增加一种多概念场景分类器，于是，文本模态CNN分类器也包含两类分类器：单概念分类器和多概念场景分类器。</p></sec><sec id="s4_4"><title>2.4. 语义场景查询上下文R<sub>rc</sub>(Q)的生成</title><p>首先生成语义邻居集 R ( Q ) ⊂ V * ，通过选择出概率p(Q|C<sub>i</sub>) &gt; 0的邻居概念C<sub>i</sub>，这个对称的语义概率p(Q|C<sub>i</sub>)表示两个概念Q和C<sub>i</sub>之间的相关性，它可以基于多模态图像训练集L如下定义：</p><p>p ( Q | C i ) = 2 &#215; N r ( Q , C i ) N r ( Q ) + N r ( C i ) (5)</p><p>其中Nr(Q)与Nr(C<sub>i</sub>)表示多概念Q与C<sub>i</sub>的共现频率，Nr(Q, C<sub>i</sub>)表示同时包含有两个概念Q与C<sub>i</sub>的图像数，每一个多概念C<sub>i</sub>被看做是自己的语义邻居并且服从约束条件p(C<sub>i</sub>|C<sub>i</sub>) = 1。</p><p>其次，选择出所有的查询零件 C i ∈ Q 到集合R<sub>rc</sub>(Q)。最后从剩余的语义概念中选择出最相关的r个概念C<sub>r</sub>到集合R<sub>rc</sub>(Q)，于是包含K<sub>rc</sub>个元素的查询上下文R<sub>rc</sub>(Q)就生成了。后续实验中，在验证集上将K<sub>rc</sub>试验取值范围为2~20执行交叉验证，发现当设置K<sub>rc</sub> = 8时性能最佳，因此K<sub>rc</sub> = 8作为后续实验的默认值。为了保持语义相关性的概率属性，语义关联概率p(Q|C<sub>i</sub>)被如下归一化：</p><p>p ( Q | C i ) = { p ( Q | C i ) ∑ r = 1 K r c p ( Q | C r ) if C i , C r ∈ R r c ( Q ) 0 elsewhere . (6)</p></sec><sec id="s4_5"><title>2.5. 多模态场景语义映射</title><p>对于查询概念Q，上述K<sub>rc</sub>个有语义相关性的元素 C i ∈ R r c ( Q ) 联合参与未标注多模态图像文本对(I, T)的相关性计算，输出相关性估值 r ( Q , ( I , T ) ) ：</p><p>r ( Q , ( I , T ) ) = r ( Q , I ) + r ( Q , T ) (7)</p><p>r ( Q , I ) = α 1 &#215; r s ( Q , I ) + α 2 &#215; r m ( Q , I ) (8)</p><p>r ( Q , T ) = α 3 &#215; r s ( Q , T ) + α 4 &#215; r m ( Q , T ) (9)</p><p>其中，r(Q, I)和r(Q, T)分别表示图像I和文本T各自模态关于查询概念Q的相关性估值； r s ( Q , . ) 和 r m ( Q , . ) 分别表示CNN单概念分类器和CNN多概念分类器输出的单模态相关性估值； { α 1 , α 2 , α 3 , α 4 } 是SSIR场景检索方法的待优化参数，且服从约束条件： α 1 + α 2 + α 3 + α 4 = 1 。公式(7)体现了多模态预测数据的后期融合，公式(8)与(9)体现了单概念分类器与多概念场景分类器预测数据的后期融合。 r s ( Q , . ) 和 r m ( Q , . ) 相关性估值的计算如下，体现了SSIR场景检索方法对于语义相关性的利用。</p><p>r s ( Q , I ) = ∏ i = 1 t p ( c i | I ) , (10)</p><p>r m ( Q , I ) = ∑ i = 1 K r c p ( Q | C r ) &#215; p ( C r | I ) ,     C r ∈ R r c ( Q ) , (11)</p><p>r s ( Q , T ) = ∏ i = 1 t p ( c i | T ) , (12)</p><p>r m ( Q , T ) = ∑ i = 1 K r c p ( Q | C r ) &#215; p ( C r | T ) ,     C r ∈ R r c ( Q ) , (13)</p><p>其中，后验概率p(C<sub>i</sub>|I)和p(C<sub>i</sub>|T)经由视觉CNN多概念分类器和文本CNN多概念分类器计算；语义相关性p(Q|C<sub>r</sub>)可以看做是后验概率p(C<sub>i</sub>|I)和p(C<sub>i</sub>|T)的权重。</p></sec><sec id="s4_6"><title>2.6. SSIR参数估算</title><p>为找出较好的参数 α = { α 1 , α 2 , α 3 , α 4 } ，使用在训练数据上的极大对数似然函数方法。令 y Q i ∈ { 0 , 1 } 表示语义场景多概念Q是否出现在多模态图像数据 ( I i , T i ) ∈ L 中，相关性预测值p(y<sub>Qi</sub>)如下给出：</p><p>p ( y Q i = 1 ) = r ( Q , ( I , T ) ) , (14)</p><p>p ( y Q i = 0 ) = 1 − r ( Q , ( I , T ) ) , (15)</p><p>p ( y Q i ) = p ( y Q i = 1 ) y Q i p ( y Q i = 0 ) 1 − y Q i (16)</p><p>因此，检索概念Q的对数似然函数被如下改写：</p><p>L Q = ∑ i = 1 N n Q i log p ( y Q i ) (17)</p><p>其中n<sub>Qi</sub>是正反例数目N<sup>+</sup>和N<sup>−</sup>的不平衡性惩罚因子。假若y<sub>Qi</sub> = 1，设置n<sub>Qi</sub> = 1/N<sup>+</sup>；否则，n<sub>Qi</sub> = 1/N<sup>−</sup>。将公式(17)中的p(y<sub>Qi</sub>)项用公式(14)到(16)替换，可以获得下列对数似然函数：</p><p>L Q = ∑ i = 1 N n Q i log { ( α β ) y Q i ( 1 − α β ) 1 − y Q i } (18)</p><p>其中，</p><p>β = { ∏ i = 1 t p ( c i | I ) , ∑ i = 1 K r c p ( Q | C r ) &#215; p ( C r | I ) , ∏ i = 1 t p ( c i | T ) , ∑ i = 1 K r c p ( Q | C r ) &#215; p ( C r | T ) } .</p><p>通过梯度下降法 [<xref ref-type="bibr" rid="hanspub.31754-ref9">9</xref>] 最大化对数似然函数L<sub>Q</sub>求出 α 优化值。</p></sec></sec><sec id="s5"><title>3. 实验和评价</title><sec id="s5_1"><title>3.1. 数据集</title><p>评价实验采用了公开数据集MIR Flickr 2011 [<xref ref-type="bibr" rid="hanspub.31754-ref10">10</xref>] 。它包含有18,000张图像，每张图像含有3~26个标注，单词表V含有99个语义概念。每个图像I与其关联的文本标签T组成了多模态图像，即图像文本对(I, T)。采用随机抽样，8000张图像作为训练集，余下10,000张图像作为测试集，约70%概念频率低于平均概念频率。很明显，这是非平衡图像数据集，增大了场景识别难度。当一幅图像I包含Q中所有的单概念c<sub>i</sub>时，则I是相关的，否则I是不相关的。采用平均准确率均值(MAP)分数作为性能评价指标，所有指标值越高表示场景检索性能越好。</p></sec><sec id="s5_2"><title>3.2. 实验结果与分析</title><p>由于MIR Flickr 2011数据集中的概念最高频数是11，因此设定公式(1)的参数t = 11。s控制计算V<sup>*</sup>的计算开销，经验设置s = 200。于是，V<sup>*</sup>含有15,970个场景多概念。为测试语义场景检索性能，我们构建了检索测试集Q<sup>*</sup>：所有的单概念 c j ∈ V 加入Q<sup>*</sup>；随机生成500个双概念场景查询；随机生成500个三概念场景查询；随机生成500个四概念场景查询。这样，Q<sup>*</sup>共计包含1599个语义场景查询。</p><p>表1列出了与最新图像标注方法的对比实验结果。对于被比较的传统单概念检索方法，采用公式(10)和(12)计算语义多概念估值。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Performance comparisons of semantic scene image retrieva</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >检索方法</th><th align="center" valign="middle" >全部概念</th><th align="center" valign="middle" >双概念</th><th align="center" valign="middle" >三概念</th><th align="center" valign="middle" >四概念</th></tr></thead><tr><td align="center" valign="middle" >GResNets [<xref ref-type="bibr" rid="hanspub.31754-ref11">11</xref>]</td><td align="center" valign="middle" >0.23</td><td align="center" valign="middle" >0.25</td><td align="center" valign="middle" >0.18</td><td align="center" valign="middle" >0.17</td></tr><tr><td align="center" valign="middle" >DANE [<xref ref-type="bibr" rid="hanspub.31754-ref12">12</xref>]</td><td align="center" valign="middle" >0.22</td><td align="center" valign="middle" >0.24</td><td align="center" valign="middle" >0.16</td><td align="center" valign="middle" >0.18</td></tr><tr><td align="center" valign="middle" >AlexNet串联并联 [<xref ref-type="bibr" rid="hanspub.31754-ref13">13</xref>]</td><td align="center" valign="middle" >0.18</td><td align="center" valign="middle" >0.21</td><td align="center" valign="middle" >0.15</td><td align="center" valign="middle" >0.15</td></tr><tr><td align="center" valign="middle" >VGGNet-Dropout [<xref ref-type="bibr" rid="hanspub.31754-ref14">14</xref>]</td><td align="center" valign="middle" >0.18</td><td align="center" valign="middle" >0.20</td><td align="center" valign="middle" >0.15</td><td align="center" valign="middle" >0.14</td></tr><tr><td align="center" valign="middle" >VGGNet + KNN [<xref ref-type="bibr" rid="hanspub.31754-ref15">15</xref>]</td><td align="center" valign="middle" >0.18</td><td align="center" valign="middle" >0.21</td><td align="center" valign="middle" >0.15</td><td align="center" valign="middle" >0.14</td></tr><tr><td align="center" valign="middle" >VGGNet + SVM [<xref ref-type="bibr" rid="hanspub.31754-ref15">15</xref>]</td><td align="center" valign="middle" >0.18</td><td align="center" valign="middle" >0.21</td><td align="center" valign="middle" >0.14</td><td align="center" valign="middle" >0.13</td></tr><tr><td align="center" valign="middle" >MKL [<xref ref-type="bibr" rid="hanspub.31754-ref16">16</xref>]</td><td align="center" valign="middle" >0.16</td><td align="center" valign="middle" >0.19</td><td align="center" valign="middle" >0.13</td><td align="center" valign="middle" >0.12</td></tr><tr><td align="center" valign="middle" >Autoencoder [<xref ref-type="bibr" rid="hanspub.31754-ref17">17</xref>]</td><td align="center" valign="middle" >0.19</td><td align="center" valign="middle" >0.23</td><td align="center" valign="middle" >0.16</td><td align="center" valign="middle" >0.15</td></tr><tr><td align="center" valign="middle" >DBM [<xref ref-type="bibr" rid="hanspub.31754-ref17">17</xref>]</td><td align="center" valign="middle" >0.20</td><td align="center" valign="middle" >0.23</td><td align="center" valign="middle" >0.16</td><td align="center" valign="middle" >0.16</td></tr><tr><td align="center" valign="middle" >SSIR (Ours)</td><td align="center" valign="middle" >0.26</td><td align="center" valign="middle" >0.28</td><td align="center" valign="middle" >0.24</td><td align="center" valign="middle" >0.23</td></tr></tbody></table></table-wrap><p>表1. 语义场景图像检索性能比较</p><p>从该表中可见，我们的SSIR方法超越了其他对比方法，获得了更好的场景检索性能。与表中最好的对比检索方法GResNets比较，提出的SSIR方法的全部场景概念MAP提高了13%。一方面，多概念场景分类器有助于识别语义场景，另一方面，概念间语义依赖关联性更能准确判断多概念形成的语义场景，缓解误判率。在图像检索任务中，这两种类型的相关性都提供了有用的信息，具有一定的互补性，从这个角度上说我们的SSIR方法可提高图像检索的性能。此外，应用梯度下降算法来补偿在真实应用中不平衡图像集上语义概念的频率差，缓解了稀疏场景概念的误判率，所以，在不平衡数据集MIR Flickr 2011上，我们的SSIR方法具有更好的场景检索效果。</p></sec></sec><sec id="s6"><title>基金项目</title><p>2019年广东第二师范学院校级教学质量与教学改革工程项目(编号：2019jxgg18)；广东第二师范学院软件工程重点学科建设项目(编号：9030-1700207)；广东省自然科学基金项目(编号：2018A0303130169)；广东省科技计划项目(编号：粤财农[<xref ref-type="bibr" rid="hanspub.31754-ref2017">2017</xref>]94号，2016A010106007)；广东省应用型科技研发专项资金资助项目(编号：2016B090927010)；2019年广东第二师范学院大学生创新创业训练计划项目(编号：201914278146)。</p></sec><sec id="s7"><title>文章引用</title><p>徐海蛟,张展鸿,何佳蕾,方钰敏. 基于深度学习的语义场景图像检索Deep Learning Based Semantic Scene Image Retrieval[J]. 计算机科学与应用, 2019, 09(08): 1561-1568. https://doi.org/10.12677/CSA.2019.98175</p></sec><sec id="s8"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.31754-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">熊回香, 叶佳鑫. 基于同义词词林的社会化标签等级结构构建研究[J]. 情报杂志, 2018, 37(1): 126-131.</mixed-citation></ref><ref id="hanspub.31754-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">龚彦宇, 董剑利, 侯明亮. 综合颜色特征的图像检索方法研究[J]. 计算机科学与应用, 2016, 6(10): 583-589.</mixed-citation></ref><ref id="hanspub.31754-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Fang, Q., Xu, C. and Sang, J. (2016) Folksonomy-Based Visual Ontology Construction and Its Applica-tions. IEEE Transactions Multimedia, 18, 702-713. &lt;br&gt;https://doi.org/10.1109/TMM.2016.2527602</mixed-citation></ref><ref id="hanspub.31754-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Wang, W., Yang, X. and Ooi, B.C. (2016) Effective Deep Learning Based Multi-Modal Retrieval. The VLDB Journal, 25, 79-101. &lt;br&gt;https://doi.org/10.1007/s00778-015-0391-4</mixed-citation></ref><ref id="hanspub.31754-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">杨泽明, 刘军, 薛程, 于子红. 卷积神经网络在图像分类上的应用综述[J]. 人工智能与机器人研究, 2018, 7(1): 17-24.</mixed-citation></ref><ref id="hanspub.31754-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">叶天顺. 一种改进的社交词嵌入算法[J]. 计算机应用与软件, 2018(9): 132-137.</mixed-citation></ref><ref id="hanspub.31754-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S. and Sun, J. (2016) Deep Residual Learning for Image Recognition. IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.31754-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Kim, Y. (2014) Convolutional Neural Networks for Sentence Classi-fication. ACL International Conference on Empirical Methods in Natural Language Processing, Doha, 25-29 October 2014, 1746-1751.  
&lt;br&gt;https://doi.org/10.3115/v1/D14-1181</mixed-citation></ref><ref id="hanspub.31754-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">陶秉墨, 鲁淑霞. 基于自适应随机梯度下降方法的非平衡数据分类[J]. 计算机科学, 2018, 45(z1): 487-492.</mixed-citation></ref><ref id="hanspub.31754-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Nowak, S., Nagel, K. and Liebetrau, J. (2011) The CLEF 2011 Photo Annotation and Concept-Based Retrieval Tasks. CLEF Conference and Labs of the Evaluation Forum, Amsterdam, 19-22 September 2011, 1-25.</mixed-citation></ref><ref id="hanspub.31754-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">李鹏, 蒋品群, 曾上游, 夏海英, 廖志贤, 范瑞. 基于分组残差结构的轻量级卷积神经网络设计[J]. 微电子学与计算机, 2019, 36(7): 43-47.</mixed-citation></ref><ref id="hanspub.31754-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">王雅慧, 刘博, 袁晓彤. 基于近似牛顿法的分布式卷积神经网络训练[J]. 计算机科学, 2019, 46(7): 180-185.</mixed-citation></ref><ref id="hanspub.31754-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">张涛, 杨剑, 宋文爱, 郭雁蓉. 改进卷积神经网络模型设计方法[J]. 计算机工程与设计, 2019, 40(7): 1885-1890.</mixed-citation></ref><ref id="hanspub.31754-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">程俊华, 曾国辉, 鲁敦科, 黄勃. 基于Dropout的改进卷积神经网络模型平均方法[J]. 计算机应用, 2019, 39(6): 1601-1606.</mixed-citation></ref><ref id="hanspub.31754-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan, K. and Zis-serman, A. (2014) Very Deep Convolutional Networks for Large-Scale Image Recognition.</mixed-citation></ref><ref id="hanspub.31754-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Guillaumin, M., Ver-beek, J.J. and Schmid, C. (2010) Multimodal Semi-Supervised Learning for Image Classification. The 23rd IEEE Con-ference on Computer Vision and Pattern Recognition, San Francisco, 13-18 June 2010, 4307-4311.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2010.5540120</mixed-citation></ref><ref id="hanspub.31754-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Srivastava, N. and Salakhutdinov, R. (2014) Multimodal Learn-ing with Deep Boltzmann Machines. Journal of Machine Learning Research, 15, 2949-2980. &lt;br&gt;https://doi.org/10.1162/NECO_a_00311</mixed-citation></ref></ref-list></back></article>