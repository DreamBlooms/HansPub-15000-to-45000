<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2019.911237</article-id><article-id pub-id-type="publisher-id">CSA-33060</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20191100000_43165438.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于Boosting算法的医疗费用预测——以鼻咽癌为例
  Medical Expenses Prediction Based on Boosting Algorithms—Using Data of Nasopharyngeal Carcinoma (NPC)
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>曹</surname><given-names>蕾</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>何</surname><given-names>轶辉</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>柳</surname><given-names>岳霖</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>姜</surname><given-names>玉山</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>东北大学秦皇岛分校数学与统计学院，河北 秦皇岛;东北大学秦皇岛分校数据分析与智能计算研究所，河北 秦皇岛</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>31</day><month>10</month><year>2019</year></pub-date><volume>09</volume><issue>11</issue><fpage>2115</fpage><lpage>2128</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   本文的数据来源于广东省某肿瘤医院，共计2064个鼻咽癌病案，我们对其进行数据挖掘，并预测病人的医疗费用。本文通过以下四步对数据进行研究。首先，我们选取了病人的年龄、性别、TNM诊断分期以及住院天数等特征为预测变量。然后，基于回归决策树算法(CART)建立费用预测模型。其后，分别使用两种Boosting算法，AdaBoost和Gradient Boosting对已有模型进行改进。接着，通过直观比照和回归评价指标，分析三种算法建立的预测模型的效果并进行比较，得到效果最好的DBRT (Gradient Boosting Decision Tree)预测模型，其预测准确率约为85%。最后，通过特征重要度和部分依赖关系图，解释基于Boosting算法的模型的现实意义，为医疗保险资源的分配和单个病例预期费用提供了参考。 The data of this paper come from 2064 cases of NPC in a Cancer Hospital of Guangdong Province. We mine the data and predict the medical cost per patient. This paper studies the data through the following four steps. First, we select the characteristics of patients’ age, gender, TNM diagnosis stage and length of stay as the prediction variables. Then, we build the cost prediction model based on the regression decision tree algorithm (CART). Then, two boosting algorithms, AdaBoost and gradient boosting, are used to improve the existing model. Then, through the visual comparison and regression evaluation index, the effect of the prediction model established by the three algorithms is analyzed and compared, and the best DBRT (gradient boosting decision tree) prediction model is obtained, with the prediction accuracy of about 85%. Finally, the significance of the model based on boosting algorithm is explained through the feature importance and partial dependency graph, which provides a reference for the allocation of medical insurance resources and the expected cost of a single case. 
  
 
</p></abstract><kwd-group><kwd>CART，鼻咽癌，AdaBoost，Gradient Boosting，DBRT，回归评价指标，特征重要度，部分依赖关系, CART</kwd><kwd> NPC</kwd><kwd> AdaBoost</kwd><kwd> Gradient Boosting</kwd><kwd> DBRT</kwd><kwd> Regression Valuation Index</kwd><kwd> Feature Importance</kwd><kwd> Partial Dependency</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于Boosting算法的医疗费用预测</title><p>——以鼻咽癌为例</p><p>曹蕾<sup>1,2</sup>，何轶辉<sup>1,2</sup>，柳岳霖<sup>1,2</sup>，姜玉山<sup>1,2*</sup></p><p><sup>1</sup>东北大学秦皇岛分校数学与统计学院，河北 秦皇岛</p><p><sup>2</sup>东北大学秦皇岛分校数据分析与智能计算研究所，河北 秦皇岛</p><disp-formula id="hanspub.33060-formula18"><graphic xlink:href="//html.hanspub.org/file/17-1541593x5_hanspub.png"  xlink:type="simple"/></disp-formula><p>收稿日期：2019年11月1日；录用日期：2019年11月14日；发布日期：2019年11月21日</p><disp-formula id="hanspub.33060-formula19"><graphic xlink:href="//html.hanspub.org/file/17-1541593x6_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>本文的数据来源于广东省某肿瘤医院，共计2064个鼻咽癌病案，我们对其进行数据挖掘，并预测病人的医疗费用。本文通过以下四步对数据进行研究。首先，我们选取了病人的年龄、性别、TNM诊断分期以及住院天数等特征为预测变量。然后，基于回归决策树算法(CART)建立费用预测模型。其后，分别使用两种Boosting算法，AdaBoost和Gradient Boosting对已有模型进行改进。接着，通过直观比照和回归评价指标，分析三种算法建立的预测模型的效果并进行比较，得到效果最好的DBRT (Gradient Boosting Decision Tree)预测模型，其预测准确率约为85%。最后，通过特征重要度和部分依赖关系图，解释基于Boosting算法的模型的现实意义，为医疗保险资源的分配和单个病例预期费用提供了参考。</p><p>关键词 :CART，鼻咽癌，AdaBoost，Gradient Boosting，DBRT，回归评价指标，特征重要度， 部分依赖关系</p><disp-formula id="hanspub.33060-formula20"><graphic xlink:href="//html.hanspub.org/file/17-1541593x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/17-1541593x8_hanspub.png" /> <img src="//html.hanspub.org/file/17-1541593x9_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>尽管医疗保险资金再分配是医疗保险机制中重要的一环，但在日常的运行过程中常常遇到资源分配不合理的问题。而对患者来说，难以预先得知较为准确的治疗费用，也将对其治疗过程产生不利的影响。所以，研究医疗费用预测对建立单病种的支付额度和患者的治疗十分重要。但是，由于很多因素的影响，单病种支付额度不可能是始终不变的，尤其是癌症这类医疗费用极差较大的病种来说，其治疗过程中产生的费用更是难以准确预测。究竟该如何准确给出治疗费用是摆在研究人员面前的棘手问题。</p><p>国内外对于该问题已有一定的研究，近些年，Robert B. Fetter等人提出的DRGs (Prospective Payment System Based On Diagnosis Related Groups) [<xref ref-type="bibr" rid="hanspub.33060-ref1">1</xref>]，即诊断相关分类，有着迅速发展，它是当今世界公认的比较先进的支付方式之一。它以病例组合为基本依据，考虑了患者的个体特征以及并发症和合并症情况等因素，将诊疗过程相似、费用支出相近的病例分到同一个组，进而接受统一标准的诊疗预付费。这一方法通过统一的诊断分组定额支付，激励医院加强质量管理、优化资源利用。而国内对于医保赔付和医疗费用预测问题也有一些成果，林倩、杜剑亮、Ai-Jing Luo等人 [<xref ref-type="bibr" rid="hanspub.33060-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.33060-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.33060-ref4">4</xref>] 利用决策树实现DRGs，并对医保赔付做出指导。张凯、王若佳 [<xref ref-type="bibr" rid="hanspub.33060-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.33060-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.33060-ref7">7</xref>] 引入数据挖掘技术解决该问题，提供了新的思路。</p><p>本文对大量鼻咽癌患者病历进行数据挖掘，根据影响疾病治疗费用的主要因素，基于回归决策树构建了针对鼻咽癌(NPC)患者个体的费用预测模型，并运用Boosting算法进行模型改进，最后解释模型的现实意义，为医疗保险资源的分配和单个病例预期费用提供参考。</p></sec><sec id="s4"><title>2. 描述性统计</title><sec id="s4_1"><title>2.1. 数据预处理</title><p>本文处理的鼻咽癌患者数据采样于广东省某肿瘤医院，但由于原始数据存在缺失、不规范等原因，需要对数据进行预处理。针对此次课题，本小组利用莱文斯坦编辑距离算法、正则表达式、余弦相似性等方法对原始数据进行了如下处理：剔除罕见的接受手术的样本、限制ICD编码范围、去除费用大于三倍标准差的样本(如表1所示)、TNM分期标准化、费用明细整合，最终获得2064例标准化数据 [<xref ref-type="bibr" rid="hanspub.33060-ref8">8</xref>]。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Removing data out of three times the standard deviatio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >标准差</th><th align="center" valign="middle" >平均值</th></tr></thead><tr><td align="center" valign="middle" >删除前</td><td align="center" valign="middle" >168,842.4</td><td align="center" valign="middle" >244,894.1</td></tr><tr><td align="center" valign="middle" >删除后</td><td align="center" valign="middle" >121,342</td><td align="center" valign="middle" >226,751</td></tr></tbody></table></table-wrap><p>表1. 去掉大于三倍标准差的数据</p></sec><sec id="s4_2"><title>2.2. 描述性统计</title><sec id="s4_2_1"><title>2.2.1. 基本情况</title><p>对鼻咽癌病例数据进行描述性统计，具体统计情况如表2 [<xref ref-type="bibr" rid="hanspub.33060-ref8">8</xref>]。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Basic statistic</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >项目</th><th align="center" valign="middle" >分类</th><th align="center" valign="middle" >人次(人)</th><th align="center" valign="middle" >平均费用(元)</th><th align="center" valign="middle" >占比(%)</th></tr></thead><tr><td align="center" valign="middle" >年龄</td><td align="center" valign="middle" >30</td><td align="center" valign="middle" >212</td><td align="center" valign="middle" >153,781</td><td align="center" valign="middle" >10.27</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >31~40</td><td align="center" valign="middle" >480</td><td align="center" valign="middle" >160,551</td><td align="center" valign="middle" >23.25</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >41~50</td><td align="center" valign="middle" >740</td><td align="center" valign="middle" >170,860</td><td align="center" valign="middle" >35.85</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >51~60</td><td align="center" valign="middle" >492</td><td align="center" valign="middle" >156,222</td><td align="center" valign="middle" >23.83</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >61</td><td align="center" valign="middle" >140</td><td align="center" valign="middle" >158,057</td><td align="center" valign="middle" >6.78</td></tr><tr><td align="center" valign="middle" >性别</td><td align="center" valign="middle" >男</td><td align="center" valign="middle" >1605</td><td align="center" valign="middle" >162,233</td><td align="center" valign="middle" >77.76</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >女</td><td align="center" valign="middle" >460</td><td align="center" valign="middle" >162,761</td><td align="center" valign="middle" >22.28</td></tr><tr><td align="center" valign="middle" >TNM分期</td><td align="center" valign="middle" >一期</td><td align="center" valign="middle" >660</td><td align="center" valign="middle" >169,298</td><td align="center" valign="middle" >31.97</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >二期</td><td align="center" valign="middle" >120</td><td align="center" valign="middle" >175,484</td><td align="center" valign="middle" >5.81</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >三期</td><td align="center" valign="middle" >528</td><td align="center" valign="middle" >169,177</td><td align="center" valign="middle" >25.58</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >四期</td><td align="center" valign="middle" >752</td><td align="center" valign="middle" >149,461</td><td align="center" valign="middle" >36.43</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >未知</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >144,164</td><td align="center" valign="middle" >0.19</td></tr><tr><td align="center" valign="middle" >治疗时长</td><td align="center" valign="middle" >0~1年</td><td align="center" valign="middle" >1149</td><td align="center" valign="middle" >106,585</td><td align="center" valign="middle" >55.66</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >1~2年</td><td align="center" valign="middle" >410</td><td align="center" valign="middle" >195,115</td><td align="center" valign="middle" >19.86</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >2~3年</td><td align="center" valign="middle" >227</td><td align="center" valign="middle" >243,938</td><td align="center" valign="middle" >10.99</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >3~4年</td><td align="center" valign="middle" >138</td><td align="center" valign="middle" >277,088</td><td align="center" valign="middle" >6.69</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" >4年以上</td><td align="center" valign="middle" >140</td><td align="center" valign="middle" >278,693</td><td align="center" valign="middle" >6.78</td></tr></tbody></table></table-wrap><p>表2. 基本情况统计</p></sec><sec id="s4_2_2"><title>2.2.2. 社会学数据分布情况</title><p>在2064例鼻咽癌患者病例中，男性共1605例，占病例总数的77.76%，女性460例，占病例总数的22.24%。由年龄和支付的密度散点图，即图1，可见年龄多分布于区间[41, 50]岁，支付总费用在10万元到20万元的鼻咽癌患者是密度最大的 [<xref ref-type="bibr" rid="hanspub.33060-ref8">8</xref>]。</p><p>图1. 住院费用随年龄变化的密度散点图</p></sec><sec id="s4_2_3"><title>2.2.3. TNM分期数据分布情况</title><p>观察病例数据在不同分期中的分布，发现男性患者较多的现象普遍存在，并且鼻咽癌分期主要集中在三期、四期中，占比62%。</p><p>观察每个TNM分期与平均费用的关系，可知一期费用普遍较低，而二期费用最高，我们推测导致该现象的原因为二期鼻咽癌症状相对一期更为严重，但相较于三期、四期更容易被治愈，存活率相对较高，导致二期患者疗程更长，花费更多 [<xref ref-type="bibr" rid="hanspub.33060-ref8">8</xref>]。</p></sec><sec id="s4_2_4"><title>2.2.4. 对数据集的概况性描述</title><p>1) 男性更容易患鼻咽癌，占总患者数78%；</p><p>2) 患者多为40至60岁的中老年人；</p><p>3) 患者的治疗费用集中在20万左右，其中二期的鼻咽癌患者花费最多；</p><p>4) 就诊患者主要为鼻咽癌中晚期(三期和四期)患者。</p></sec></sec></sec><sec id="s5"><title>3. 基于CART算法的费用预测</title><sec id="s5_1"><title>3.1. CART回归树生成算法原理</title><p>CART回归树的生成算法基于最小二乘法。在训练数据集所在的输入空间中，递归地将每个区域划分成两个子区域并决定每个子区域上的输出值，构建二叉决策树。例如给定数据集<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x11_hanspub.png" xlink:type="simple"/></inline-formula>，输出回归树<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x12_hanspub.png" xlink:type="simple"/></inline-formula>：</p><p>1) 第一步、选择数据集<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x13_hanspub.png" xlink:type="simple"/></inline-formula>中最优切分变量<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x14_hanspub.png" xlink:type="simple"/></inline-formula>与切分点<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x15_hanspub.png" xlink:type="simple"/></inline-formula>：求解：</p><disp-formula id="hanspub.33060-formula21"><graphic xlink:href="//html.hanspub.org/file/17-1541593x16_hanspub.png"  xlink:type="simple"/></disp-formula><p>遍历变量<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x17_hanspub.png" xlink:type="simple"/></inline-formula>，对固定的切分变量<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x18_hanspub.png" xlink:type="simple"/></inline-formula>扫描切分点<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x19_hanspub.png" xlink:type="simple"/></inline-formula>，选择使上式达到最小值的二元有序数对<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x20_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>2) 第二步、用选定的有序数对<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x21_hanspub.png" xlink:type="simple"/></inline-formula>划分区域并决定相应的输出值：</p><disp-formula id="hanspub.33060-formula22"><graphic xlink:href="//html.hanspub.org/file/17-1541593x22_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x23_hanspub.png" xlink:type="simple"/></inline-formula>是一个用二元有序数对(切分变量及其取值)表示的定义域。由此进一步可以解得枝结点和叶节点的因变量的预测值(回归拟合值)为其结点内样本均值：</p><disp-formula id="hanspub.33060-formula23"><graphic xlink:href="//html.hanspub.org/file/17-1541593x24_hanspub.png"  xlink:type="simple"/></disp-formula><p>3) 第三步、对两个子区域(枝结点)重复前两步，直到满足停止条件(最大树深度、结点内最小样本量、复杂度参数阈值等)。</p><p>4) 第四步、输出回归决策树<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x25_hanspub.png" xlink:type="simple"/></inline-formula>：</p><disp-formula id="hanspub.33060-formula24"><graphic xlink:href="//html.hanspub.org/file/17-1541593x26_hanspub.png"  xlink:type="simple"/></disp-formula><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x27_hanspub.png" xlink:type="simple"/></inline-formula>将输入空间划分为<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x28_hanspub.png" xlink:type="simple"/></inline-formula>个区域<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x29_hanspub.png" xlink:type="simple"/></inline-formula>，即<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x30_hanspub.png" xlink:type="simple"/></inline-formula>个叶节点(<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x31_hanspub.png" xlink:type="simple"/></inline-formula>类) [<xref ref-type="bibr" rid="hanspub.33060-ref8">8</xref>]。</p></sec><sec id="s5_2"><title>3.2. 基于CART算法的费用预测</title><p>我们使用Python的sklearn.tree库实现CART回归预测，建立并训练最大深度为4的CART回归树，之后进行费用预测并与真实数据进行对比，得到如下预测费用与年龄分布散点图，由图2可知，CART算法预测的费用集中于1万至3万，对于费用较低或费用较高的病例预测效果较差。</p><p>图2. 基于CART算法的预测费用与真实费用对比</p></sec></sec><sec id="s6"><title>4. 基于Boosting算法的费用预测</title><p>Boosting (提升方法)是Ensemble Learning算法的一类。这类算法的思想是将弱学习器提升为强学习器。即先用初始训练集训练出一个基学习器，再根据基学习器表现对训练样本分布进行调整，那些基学习器判断错误的样本，在之后会被赋予更大的权值，然后基于调整后的样本来训练下一个基学习器，一直反复进行，直到达到理想效果。在2000年左右，Friedman的几篇论文提出Boosting算法的成功实现 [<xref ref-type="bibr" rid="hanspub.33060-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.33060-ref10">10</xref>]。</p><p>Boosting常用的两种方法为AdaBoost (Adaptive Boost)和梯度提升(Gradient Boosting)。本章及下一章选取已出院鼻咽癌病例样本进行如下的费用预测，即所得到的费用可以认为是该病例治疗的最终花费。其中，取性别、年龄、住院次数、住院天数和首诊分期为自变量，建立并训练模型预测最终费用，并比较CART算法、AdaBoost算法、梯度提升算法的预测效果。经过反复的实验，最大树深度为4且弱学习器为300个的梯度提升算法表现最好。</p><sec id="s6_1"><title>4.1. AdaBoost算法原理</title><p>由于CART预测效果较差，我们选用集成算法中的提升方法进行改进。首先研究基于AdaBoost算法的费用预测。</p><sec id="s6_1_1"><title>4.1.1. AdaBoost算法思路</title><p>AdaBoost的思路为，先从初始训练集中训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，对所有基学习器采用加权结合，增大分类或回归误差小的基学习器的权值，减少误差率大的基学习器的权值。</p></sec><sec id="s6_1_2"><title>4.1.2. AdaBoost算法步骤</title><p>输入：训练集<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x33_hanspub.png" xlink:type="simple"/></inline-formula>，其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x34_hanspub.png" xlink:type="simple"/></inline-formula>；基学习算法<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x35_hanspub.png" xlink:type="simple"/></inline-formula>；基学习器个数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x36_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>过程：</p><p>1) 初始化训练样本的权值分布</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x37_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>2) 对迭代轮次<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x38_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>a) 使用具有当前分布<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x39_hanspub.png" xlink:type="simple"/></inline-formula>的训练数据集训练基学习器<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x40_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>b) 计算训练集上的样本最大误差：</p><disp-formula id="hanspub.33060-formula25"><graphic xlink:href="//html.hanspub.org/file/17-1541593x41_hanspub.png"  xlink:type="simple"/></disp-formula><p>计算每个样本的相对误差：</p><p>如果是线性误差，则<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x42_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>如果是平方误差，则<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x43_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>如果是指数误差，则<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x44_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>c) 基学习器在训练数据集上的回归误差率：</p><disp-formula id="hanspub.33060-formula26"><graphic xlink:href="//html.hanspub.org/file/17-1541593x45_hanspub.png"  xlink:type="simple"/></disp-formula><p>d) 基学习器的权重系数：</p><disp-formula id="hanspub.33060-formula27"><graphic xlink:href="//html.hanspub.org/file/17-1541593x46_hanspub.png"  xlink:type="simple"/></disp-formula><p>e) 更新训练集的样本分布：</p><disp-formula id="hanspub.33060-formula28"><graphic xlink:href="//html.hanspub.org/file/17-1541593x47_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.33060-formula29"><graphic xlink:href="//html.hanspub.org/file/17-1541593x48_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x49_hanspub.png" xlink:type="simple"/></inline-formula>是规范化因子：</p><disp-formula id="hanspub.33060-formula30"><graphic xlink:href="//html.hanspub.org/file/17-1541593x50_hanspub.png"  xlink:type="simple"/></disp-formula><p>f) 构建基学习器的线性组合，得到最终的强学习器：</p><disp-formula id="hanspub.33060-formula31"><graphic xlink:href="//html.hanspub.org/file/17-1541593x51_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.33060-formula32"><graphic xlink:href="//html.hanspub.org/file/17-1541593x52_hanspub.png"  xlink:type="simple"/></disp-formula><p>输出：强学习器<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x53_hanspub.png" xlink:type="simple"/></inline-formula> [<xref ref-type="bibr" rid="hanspub.33060-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.33060-ref12">12</xref>]。</p></sec></sec><sec id="s6_2"><title>4.2. Gradient Boosting算法原理</title><p>由于对AdaBoost算法对异常样本敏感，异常样本在迭代中可能获得较大权重，影响强学习器的预测准确性。接下来将使用Gradient Boosting算法进一步改进费用预测模型。</p><sec id="s6_2_1"><title>4.2.1. Gradient Boosting算法</title><p>由Freidman提出的梯度提升(gradient boosting, GB)算法 [<xref ref-type="bibr" rid="hanspub.33060-ref10">10</xref>]，若将梯度提升算法里的弱算法选择为决策树，则的到GBDT算法(Gradient Boosting Decision Tree)。该算法主要利用最速下降法的近似方法，利用损失函数的负梯度在当前模型的值</p><disp-formula id="hanspub.33060-formula33"><graphic xlink:href="//html.hanspub.org/file/17-1541593x54_hanspub.png"  xlink:type="simple"/></disp-formula><p>作为回归问题提升树算法中的残差的近似值，拟合一个回归树。</p></sec><sec id="s6_2_2"><title>4.2.2. Gradient Boosting算法步骤</title><p>接下来是该算法的训练步骤 [<xref ref-type="bibr" rid="hanspub.33060-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.33060-ref12">12</xref>] ：</p><p>给定训练数据集</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x55_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>其中<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x56_hanspub.png" xlink:type="simple"/></inline-formula>是变量自变量的个数。损失函数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x57_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>1) 初始化</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x58_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>2) 对<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x59_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>a) 对<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x60_hanspub.png" xlink:type="simple"/></inline-formula>，计算</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x61_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>b) 对<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x62_hanspub.png" xlink:type="simple"/></inline-formula>拟合一个回归树，得到第<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x63_hanspub.png" xlink:type="simple"/></inline-formula>棵树的叶节点区域<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x64_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>c) 对<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x65_hanspub.png" xlink:type="simple"/></inline-formula>，计算</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x66_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>d) 更新<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x67_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>3) 得到回归树</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x68_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec></sec><sec id="s6_3"><title>4.3. 基于Boosting算法的费用预测</title><p>我们使用Python的sklearn.ensemble库分别实现AdaBoost算法、GBDT算法，最终建立如下两种费用预测模型模型：</p><p>基于AdaBoost算法：以最大深度为4的CART回归树作为基学习器，基学习器300个，学习速度为1，损失函数为linear函。</p><p>基于GBDT算法：以最大深度为4的回归树作为基学习器，基学习器100个，学习速度为0.1，损失函数为l s函数。</p><p>下面进行费用预测并与真实数据进行对比，得到预测费用与年龄分布的散点图，如图3、图4。</p><p>图3. 基于AdaBoost算法的预测费用与真实费用对比</p><p>图4. 基于Gradient Boosting算法的预测费用与真实费用对比</p></sec></sec><sec id="s7"><title>5. 预测结果分析及评价</title><sec id="s7_1"><title>5.1. 三种算法对总费用的预测结果与真实结果的比较</title><p>现对比研究基于CART算法、AdaBoost算法、GBDT算法对总费用预测的结果。</p><p>直观地，由图5可知，三种算法预测效果有明显不同。尤其值得指出Boosting算法的预测结果较CART算法的预测结果更加分散，其中GBDT算法的预测结果最为分散，且预测结果的分布更接近于真实值，Boosting算法对于预测效果有着明显的改进。</p><p>图5. 三种算法费用预测效果对比</p><p>图6是基于AdaBoost算法和GBDT算法的费用预测结果与真实值比较的折线图，黑色的圆形散点为对应患者的总费用的真实值(经过排序)，蓝色曲线为基于AdaBoost算法的预测结果，红色曲线为基于GBDT算法的预测结果。显然红色曲线更趋近于真实值的分布，即基于GBDT算法的预测效果的效果最好。</p><p>图6. 基于AdaBoost算法、GBDT算法的预测效果对比</p></sec><sec id="s7_2"><title>5.2. 对集成学习算法的5个回归评价指标</title><p>我们将通过5个回归评价指标，量化对比三种模型的预测效果。下面先引入这5个指标，分别是均方对数误差平均值(Mean squared logarithmic error, MSLE)、平均绝对误差(Mean Absolute Error, MAE)、中值绝对误差(Median Absolute Error, MedAE)、确定系数(The Coefficient of Determination, R<sup>2</sup>)、解释方差分(Explained Variance Score, EVS)。</p><p>1) 均方对数误差平均值(Mean squared logarithmic error, MSLE)。由于本文中预测值较大，均方误差受单位影响较大，于是采用MSLE来代替均方误差。</p><disp-formula id="hanspub.33060-formula34"><graphic xlink:href="//html.hanspub.org/file/17-1541593x75_hanspub.png"  xlink:type="simple"/></disp-formula><p>2) 平均绝对误差(Mean Absolute Error, MAE)是指预测值与真实值之间的平均差值。</p><disp-formula id="hanspub.33060-formula35"><graphic xlink:href="//html.hanspub.org/file/17-1541593x76_hanspub.png"  xlink:type="simple"/></disp-formula><p>3) 中值绝对误差(Median Absolute Error, MedAE)对异常值有很好的稳定性，它通过获取预测值和真实值之间差值的绝对值的中值来计算损失。</p><disp-formula id="hanspub.33060-formula36"><graphic xlink:href="//html.hanspub.org/file/17-1541593x77_hanspub.png"  xlink:type="simple"/></disp-formula><p>4) 决定系数(The Coefficient of Determination)即R<sup>2</sup>，用来反映模型拟合效果的好坏。最大取值为1，但它也可能是负数。一般来说，R<sup>2</sup>越大，表示模型拟合效果越好，由于随着样本数量的增加，R<sup>2</sup>必然增加，无法真正定量说明模型拟合效果，只能大概定量。</p><disp-formula id="hanspub.33060-formula37"><graphic xlink:href="//html.hanspub.org/file/17-1541593x78_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x79_hanspub.png" xlink:type="simple"/></inline-formula></p><p>5) 解释方差分(Explained Variance Score, EVS)是用来解释回归模型方差大小的分数，取值范围为0到1，EVS越大，表示回归效果越好。</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x80_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec><sec id="s7_3"><title>5.3. 基于5个回归评价指标的三种算法预测效果及比较</title><p>我们使用Python语言的sklearn库metrics模块计算上述五个回归评价指标，三种算法预测效果的量化数值如表3、表4所示。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Evaluation indicators of prediction model</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >MSLE</th><th align="center" valign="middle" >EVS</th><th align="center" valign="middle" >MAE</th><th align="center" valign="middle" >MedAE</th><th align="center" valign="middle" >R<sup>2</sup></th></tr></thead><tr><td align="center" valign="middle" >CART</td><td align="center" valign="middle" >0.20</td><td align="center" valign="middle" >0.41</td><td align="center" valign="middle" >67,412.85</td><td align="center" valign="middle" >61,480.30</td><td align="center" valign="middle" >0.41</td></tr><tr><td align="center" valign="middle" >AdaBoost</td><td align="center" valign="middle" >0.21</td><td align="center" valign="middle" >0.55</td><td align="center" valign="middle" >66,946.16</td><td align="center" valign="middle" >71,903.27</td><td align="center" valign="middle" >0.53</td></tr><tr><td align="center" valign="middle" >Gradient Boosting</td><td align="center" valign="middle" >0.07</td><td align="center" valign="middle" >0.85</td><td align="center" valign="middle" >34,588.13</td><td align="center" valign="middle" >29,508.20</td><td align="center" valign="middle" >0.85</td></tr></tbody></table></table-wrap><p>表3. 三种预测模型的回归评价指标</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Evaluation indicators of adjusted prediction model</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法改进</th><th align="center" valign="middle" >MSLE</th><th align="center" valign="middle" >EVS</th><th align="center" valign="middle" >MAE</th><th align="center" valign="middle" >MedAE</th><th align="center" valign="middle" >R<sup>2</sup></th></tr></thead><tr><td align="center" valign="middle" >CART→AdaBoost</td><td align="center" valign="middle" >+0.01</td><td align="center" valign="middle" >+0.14</td><td align="center" valign="middle" >−466.69</td><td align="center" valign="middle" >+10,422.97</td><td align="center" valign="middle" >+0.12</td></tr><tr><td align="center" valign="middle" >AdaBoost→Gradient Boosting</td><td align="center" valign="middle" >−0.14</td><td align="center" valign="middle" >+0.30</td><td align="center" valign="middle" >−32,358.03</td><td align="center" valign="middle" >−42,395.07</td><td align="center" valign="middle" >+0.32</td></tr></tbody></table></table-wrap><p>表4. 三种预测模型拟合效果的改进</p><p>由上表中数据可知，虽然AdaBoost算法在MSLE、EVS、MAE、R<sup>2</sup>评价指标上的表现优于CART算法，但AdaBoost算法和CART算法的预测效果都较差，其拟合效果均低于60%，即这两种算法的预测结果与真实值并不接近。而GBDT算法在五个指标上的表现远远优于AdaBoost算法和CART算法，其拟合效果达到了85%，表明GBDT算法的预测结果与真实值较为相近，在三种算法中效果最好。该结论与先前对比的直观结论相符合。</p><p>为了检验费用预测模型是否过拟合，我们从总数据集中抽取20%的样本作为测试集，代入进行预测效果评价并与之前的评价结果进行比较。综合表5和表6可知，GBDT的拟合程度达到了85%，在测试集中拟合效果83%。基于GBDT算法建立的费用预测模型没有过拟合。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Evaluation indicators of prediction models on test se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >MSLE</th><th align="center" valign="middle" >EVS</th><th align="center" valign="middle" >MAE</th><th align="center" valign="middle" >MedAE</th><th align="center" valign="middle" >R<sup>2</sup></th></tr></thead><tr><td align="center" valign="middle" >CART</td><td align="center" valign="middle" >0.25</td><td align="center" valign="middle" >0.29</td><td align="center" valign="middle" >79,480.59</td><td align="center" valign="middle" >70,992.09</td><td align="center" valign="middle" >0.29</td></tr><tr><td align="center" valign="middle" >AdaBoost</td><td align="center" valign="middle" >0.23</td><td align="center" valign="middle" >0.57</td><td align="center" valign="middle" >71,578.61</td><td align="center" valign="middle" >74,328.32</td><td align="center" valign="middle" >0.56</td></tr><tr><td align="center" valign="middle" >Gradient Boosting</td><td align="center" valign="middle" >0.09</td><td align="center" valign="middle" >0.83</td><td align="center" valign="middle" >40,957.83</td><td align="center" valign="middle" >33,752.57</td><td align="center" valign="middle" >0.83</td></tr></tbody></table></table-wrap><p>表5. 将测试集分别带入三种预测模型的拟合效果</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Evaluation indicators of adjusted prediction models on test se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法改进</th><th align="center" valign="middle" >MSLE</th><th align="center" valign="middle" >EVS</th><th align="center" valign="middle" >MAE</th><th align="center" valign="middle" >MedAE</th><th align="center" valign="middle" >R<sup>2</sup></th></tr></thead><tr><td align="center" valign="middle" >CART→AdaBoost</td><td align="center" valign="middle" >−0.02</td><td align="center" valign="middle" >+0.28</td><td align="center" valign="middle" >−7901.98</td><td align="center" valign="middle" >+3336.23</td><td align="center" valign="middle" >+0.27</td></tr><tr><td align="center" valign="middle" >AdaBoost→Gradient Boosting</td><td align="center" valign="middle" >−0.14</td><td align="center" valign="middle" >+0.26</td><td align="center" valign="middle" >−30,620.78</td><td align="center" valign="middle" >−40,575.75</td><td align="center" valign="middle" >+0.27</td></tr></tbody></table></table-wrap><p>表6. 三种预测模型的对于测试集拟合效果的改进</p></sec><sec id="s7_4"><title>5.4. 费用预测模型的可解释性</title><p>通过简单地可视化树结构，可以轻松解释单个决策树。然而，集成算法模型包括数百个回归树，因此通过对各个树的可视化无法轻易解释它们。然而可以通过计算特征重要程度以及绘制部分依赖图，来直观的概括和解释Boosting模型。</p><sec id="s7_4_1"><title>5.4.1. 特征重要程度</title><p>Friedman在论文中提出在使用决策树集成算法(Tree Ensemble)时评价特征在模型中重要程度的方法 [<xref ref-type="bibr" rid="hanspub.33060-ref8">8</xref>]，便于更好的理解特征和模型。接下来介绍一下特征重要程度的计算原理，并给出已建立的三种回归模型的特征重要性。</p><p>单个决策树通过选择适当的分裂点本质上执行特征选择。此信息可用于衡量每个功能的重要性；基本思想是：在树的分裂点中使用特征的次数越多，特征就越重要。通过简单地平均每棵树的特征重要性，可以将这种重要性概念扩展到决策树集成算法中。</p><p>特征<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x81_hanspub.png" xlink:type="simple"/></inline-formula>在整个模型中的重要程度为：</p><disp-formula id="hanspub.33060-formula38"><graphic xlink:href="//html.hanspub.org/file/17-1541593x82_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x83_hanspub.png" xlink:type="simple"/></inline-formula>是模型中树的数量。特征<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x84_hanspub.png" xlink:type="simple"/></inline-formula>在单独一个树上的特征重要度为：</p><disp-formula id="hanspub.33060-formula39"><graphic xlink:href="//html.hanspub.org/file/17-1541593x85_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x86_hanspub.png" xlink:type="simple"/></inline-formula>是树中非叶子节点数量，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x87_hanspub.png" xlink:type="simple"/></inline-formula>表示在内部节点<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x88_hanspub.png" xlink:type="simple"/></inline-formula>进行分裂时选择的特征，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/17-1541593x89_hanspub.png" xlink:type="simple"/></inline-formula>是内部节点分裂后平方损失的减少量。</p><p>接下来给出五个特征分别在三种算法下的特征重要程度，由表7可知三种算法均将年龄、住院次数、住院天数作为训练模型时较为重要的特征，然而性别和TNM分期对预测模型的贡献较少。</p><table-wrap id="table7" ><label><xref ref-type="table" rid="table7">Table 7</xref></label><caption><title> Magnitude of each attribute by using CART, AdaBoost and GBD</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >性别</th><th align="center" valign="middle" >年龄</th><th align="center" valign="middle" >TNM分期</th><th align="center" valign="middle" >住院次数</th><th align="center" valign="middle" >治疗天数</th></tr></thead><tr><td align="center" valign="middle" >CART</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0.205</td><td align="center" valign="middle" >0.022</td><td align="center" valign="middle" >0.393</td><td align="center" valign="middle" >0.380</td></tr><tr><td align="center" valign="middle" >AdaBoost</td><td align="center" valign="middle" >0.017</td><td align="center" valign="middle" >0.293</td><td align="center" valign="middle" >0.049</td><td align="center" valign="middle" >0.177</td><td align="center" valign="middle" >0.463</td></tr><tr><td align="center" valign="middle" >Gradient Boosting</td><td align="center" valign="middle" >0.034</td><td align="center" valign="middle" >0.268</td><td align="center" valign="middle" >0.037</td><td align="center" valign="middle" >0.220</td><td align="center" valign="middle" >0.442</td></tr></tbody></table></table-wrap><p>表7. 五个特征分别在三种算法下的特征重要程度</p></sec><sec id="s7_4_2"><title>5.4.2. 部分依赖关系</title><p>部分依赖图显示了训练模型得到的目标函数与一组特征之间的依赖关系。单项部分依赖图只研究一个特征，边缘化了所有其他特征的值，直观的展示一个特征对模型的影响。</p><p>由图7，即Gradient Boosting预测模型中五个特征的单项部分依赖图可以得到以下关系：</p><p>图7. 基于两个目标特征的单项部分依赖图</p><p>1) 男性患者(sex = 2)倾向于产生更多费用；</p><p>2) 小于45岁或大于60岁的患者，费用呈现年龄越大花费越多的趋势，而大于45岁小于60岁的患者，费用呈现年龄越大花费越少的趋势；</p><p>3) 住院次数与模型有较为规律的依赖关系，住院次数小于15次，次数越多费用越高，而住院次数大于15次，则费用趋于稳定；</p><p>4) 治疗天数对于模型的依赖关系较为复杂。</p><p>多项部分依赖图研究多个特征，边缘化了其他特征的值，直观的展示了多个特征对模型的综合影响。由图8，即Gradient Boosting预测模型中四个特征的多项部分依赖图可以得到以下关系：</p><p>图8. 基于两个目标特征的多项部分依赖图</p><p>1) TNM分期在二期和三期的男性患者群体将产生大量费用；</p><p>2) 住院次数大于13次的患者，无论男女，都易产生大额医疗费用，其中二、三期的患者中这种依赖关系更显著；</p><p>3) 小于45岁的患者，费用呈现年龄越大花费越多的趋势，其中二、三期的患者中该关系更显著。</p><p>通过分析部分依赖图，能够直观的理解和解释基于GBDT算法预测模型中复杂的目标函数，这是分析基于Boosting算法以及其他集成算法的模型的有效方法之一。</p></sec></sec></sec><sec id="s8"><title>致谢</title><p>在该项目研究阶段，东北大学秦皇岛分校以及东软集团医疗事业部给予团队许多支持，特在此致谢。同时该项目受到东北大学秦皇岛分校创新训练项目，教育部科技发展中心科研创新基金的支持，使得项目有充足的资金得以顺利进行。最后感谢姜玉山博士为项目指明了研究方向，同时对论文的撰写提出了许多建议，使得项目及论文撰写都圆满成功。</p></sec><sec id="s9"><title>基金项目</title><p>东北大学秦皇岛分校创新训练项目，教育部科技发展中心科研创新基金(2018A03031)。</p></sec><sec id="s10"><title>文章引用</title><p>曹 蕾,何轶辉,柳岳霖,姜玉山. 基于Boosting算法的医疗费用预测——以鼻咽癌为例Medical Expenses Prediction Based on Boosting Algorithms—Using Data of Nasopharyngeal Carcinoma (NPC)[J]. 计算机科学与应用, 2019, 09(11): 2115-2128. https://doi.org/10.12677/CSA.2019.911237</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.33060-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Fetter, R.B., Shin, Y., Freeman, J.L., Averill, R.F. and Thompson, J.D. (1984) Case Mix Definition by Diagno-sis-Related Groups. Medical Care, 18, 1-53.</mixed-citation></ref><ref id="hanspub.33060-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">林倩, 王冬, 郭煜, 詹志颖, 吴志明. 基于CHAID算法的阑尾炎患者DRGs分组研究[J]. 卫生经济研究, 2017(8): 29-32.</mixed-citation></ref><ref id="hanspub.33060-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">杜剑亮, 刘骏峰, 陈倩. 不同决策树算法建立DRGs模型的差异[J]. 中国病案, 2014, 15(7): 38-41.</mixed-citation></ref><ref id="hanspub.33060-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Luo, A.-J., Chang, W.-F., Xin, Z.-R., Ling, H., Li, J.-J., Dai, P.-P., Deng, X.-T., Zhang, L. and Li, S.-G. (2018) Diagnosis Related Group Grouping Study of Senile Cataract Pa-tients Based on E-CHAID Algorithm. International Journal of Ophthalmology, 11, 308-313.</mixed-citation></ref><ref id="hanspub.33060-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">张凯. 数据挖掘技术在医疗费用数据中的应用研究[D]: [硕士学位论文]. 北京: 北京邮电大学, 2015.</mixed-citation></ref><ref id="hanspub.33060-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">王若佳, 魏思仪, 赵怡然, 王继民. 数据挖掘在健康医疗领域中的应用研究综述[J]. 图书情报知识, 2018(5): 114-123+9.</mixed-citation></ref><ref id="hanspub.33060-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">徐昆. 业健康保险与医疗大数据对接交互系统研究[J]. 金融理论与实践, 2018(7): 103-108.</mixed-citation></ref><ref id="hanspub.33060-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">曹蕾, 柳岳霖, 何轶辉, 姜玉山. 基于决策树的DRGs制度研究——以鼻咽癌为例[J]. 应用数学进展, 2019, 8(6): 1121-1132.</mixed-citation></ref><ref id="hanspub.33060-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Friedman, J.H., Hastie, T. and Tibshirani, R. (2000) Additive Logistic Regression: A Statistical View of Boosting. Annals of Statis-tics, 28, 337-407. &lt;br&gt;https://doi.org/10.1214/aos/1016218223</mixed-citation></ref><ref id="hanspub.33060-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Friedman, J.H. (2001) Greedy Function Approxi-mation: A Gradient Boosting Machine. Annals of Statistics, 29, 1189-1232. &lt;br&gt;https://doi.org/10.1214/aos/1013203451</mixed-citation></ref><ref id="hanspub.33060-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012.</mixed-citation></ref><ref id="hanspub.33060-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">吕晓玲, 宋捷. 大数据挖掘与统计机器学习[M]. 北京: 中国人民大学出版社, 2016.</mixed-citation></ref></ref-list></back></article>