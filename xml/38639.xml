<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2020.911221</article-id><article-id pub-id-type="publisher-id">AAM-38639</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20201100000_25108376.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  求解极大极小问题的共轭梯度法
  Conjugate Gradient Method for Solving Minimax Problems
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郝</surname><given-names>月</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>青岛大学数学与统计学院，山东 青岛</addr-line></aff><pub-date pub-type="epub"><day>10</day><month>11</month><year>2020</year></pub-date><volume>09</volume><issue>11</issue><fpage>1916</fpage><lpage>1924</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    本文研究了极大极小问题的求解方法，利用指数罚函数对该问题进行光滑化处理，将其转化成光滑的无约束优化问题，并利用共轭梯度法来求解含有罚参数的无约束优化问题。最后，我们给出了数值算例来验证该算法求解极大极小问题的有效性。
    This paper studies the method for solving the minimax problem, the exponential penalty function is used for smoothing the problem which can be transformed into a smooth unconstrained optimization problem. We also use the conjugate gradient method to solve the unconstrained optimization with penalty parameters problem. Finally, numerical results are given to illustrate the effectiveness of the algorithm for solving minimax problems. 
  
 
</p></abstract><kwd-group><kwd>极大极小，指数罚函数，光滑化，共轭梯度法, Minimax</kwd><kwd> Exponential Penalty Function</kwd><kwd> Smoothing Method</kwd><kwd> Conjugate Gradient Method</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>本文研究了极大极小问题的求解方法，利用指数罚函数对该问题进行光滑化处理，将其转化成光滑的无约束优化问题，并利用共轭梯度法来求解含有罚参数的无约束优化问题。最后，我们给出了数值算例来验证该算法求解极大极小问题的有效性。</p></sec><sec id="s2"><title>关键词</title><p>极大极小，指数罚函数，光滑化，共轭梯度法</p></sec><sec id="s3"><title>Conjugate Gradient Method for Solving Minimax Problems<sup> </sup></title><p>Yue Hao</p><p>School of Mathematics and Statistics, Qingdao University, Qingdao Shandong</p><p><img src="//html.hanspub.org/file/6-2621362x4_hanspub.png" /></p><p>Received: Oct. 25<sup>th</sup>, 2020; accepted: Nov. 11<sup>th</sup>, 2020; published: Nov. 18<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/6-2621362x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>This paper studies the method for solving the minimax problem, the exponential penalty function is used for smoothing the problem which can be transformed into a smooth unconstrained optimization problem. We also use the conjugate gradient method to solve the unconstrained optimization with penalty parameters problem. Finally, numerical results are given to illustrate the effectiveness of the algorithm for solving minimax problems.</p><p>Keywords:Minimax, Exponential Penalty Function, Smoothing Method, Conjugate Gradient Method</p><disp-formula id="hanspub.38639-formula31"><graphic xlink:href="//html.hanspub.org/file/6-2621362x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/6-2621362x7_hanspub.png" /> <img src="//html.hanspub.org/file/6-2621362x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>极大极小问题是一类重要的非光滑优化问题，且在工程设计、经济管理 [<xref ref-type="bibr" rid="hanspub.38639-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.38639-ref2">2</xref>] 等领域有着广泛的应用，同时也与非线性规划问题有着密切的联系。</p><p>极大极小问题的一般形式为</p><p>min x ∈ R n F ( x ) F ( x ) = max i = 1 , ⋯ , m f i ( x ) , (1)</p><p>其中， f i ( x ) : R n → R 是连续可微的函数。</p><p>近年来，众多学者对极大极小问题的算法进行了研究。通过构造罚函数，提出一个有效的算法求解极大极小问题，如文献 [<xref ref-type="bibr" rid="hanspub.38639-ref3">3</xref>]；通过构造可调节熵函数的区间扩展和一些区域删除测试规则， [<xref ref-type="bibr" rid="hanspub.38639-ref4">4</xref>] 提出了一种新的区间算法，该算法可以获得极大极小问题的最优解；文献 [<xref ref-type="bibr" rid="hanspub.38639-ref5">5</xref>] 提出了一种截断的集合平滑牛顿法来解决极小极大问题。最近，一种用于求解极大极小问题的平滑FR共轭梯度法由文献 [<xref ref-type="bibr" rid="hanspub.38639-ref6">6</xref>] 提出，其实验结果表现良好。</p><p>综上文献，我们知道罚函数在极大极小问题的求解过程中起到了重要的作用，该罚函数由 [<xref ref-type="bibr" rid="hanspub.38639-ref7">7</xref>] 提出，其形式为</p><p>g ( x , t ) = t ln ∑ i = 1 m exp ( g i ( x ) t ) , (2)</p><p>其中，t是罚参数， g i ( x ) 是光滑可微的函数。利用罚函数(2)，极大极小问题(1)可以被转化为一个光滑的无约束优化问题，其形式为</p><p>min F ˜ ( x , t ) , (3)</p><p>F ˜ ( x , t ) 是光滑可微函数，且被定义为</p><p>F ˜ ( x , t ) = t ln ∑ i = 1 m exp ( f i ( x ) t ) ,</p><p>其中 t &gt; 0 是罚参数。此时，对非光滑的极大极小问题(1)的求解可以等价于对光滑的无约束优化问题(3)的求解。</p><p>另外，我们还对带有张量结构的极大极小问题进行研究。一个m阶n维的张量 A = ( a i 1 , i 2 , ⋯ , i m ) ，其中 1 ≤ i 1 , i 2 , ⋯ , i m ≤ n ，对一个n维向量x， A x m − 1 是一个向量，它的第i的元素是</p><p>( A x m − 1 ) i = ∑ i 2 , … , i m = 1 n a i i 2 ⋯ i m x i 2 ⋯ x i m ,     i , j = 1 , ⋯ , n</p><p>下面我们给出具有张量结构的极大极小问题的形式。首先定义如下函数：</p><p>w i ( x ) = A i x m − 1 − | x | − b i ,     i = 1 , 2 , ⋯ , h (4)</p><p>其中， A 是一个m阶n维的张量， b i 是n维向量。对(4)中的绝对值项进行光滑化处理后，有</p><p>w i ( x ) = A i x m − 1 − x 2 + t − b i ,     i = 1 , 2 , ⋯ , h</p><p>则对于 max { w 1 i ( x , t ) , w 2 i ( x , t ) , ⋯ , w m i ( x , t ) } ，由(2)可得出如下函数</p><p>w ˜ i ( x , t ) = t ln ∑ j = 1 n exp ( ( A i x m − 1 ) j − x j 2 + t − b j i t )</p><p>此时，对于下式</p><p>max { w ˜ 1 i ( x , t ) , w ˜ 2 i ( x , t ) , ⋯ , w ˜ m i ( x , t ) }</p><p>利用指数罚函数(2)，我们可以得到如下光滑的函数</p><p>O ˜ i ( x , t ) = t ln ∑ i = 1 h exp ( w ˜ i ( x , t ) t ) (5)</p><p>那么，对于张量结构的极大极小问题就可以转化为对(5)式的求解。</p><p>本文的目的是使用共轭梯度法来求解光滑后的极大极小问题，具体安排如下：第2部分，给出求解光滑后的极大极小问题的算法；第3部分给出该算法求解两类极大极小问题的数值实验；第4部分总结。</p></sec><sec id="s6"><title>2. 共轭梯度算法</title><p>共轭梯度类算法 [<xref ref-type="bibr" rid="hanspub.38639-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.38639-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.38639-ref10">10</xref>] 由于其收敛速度快、存储量小以及稳定性高等优点被广泛应用于无约束优化问题的求解中，其最早由Hestenes和Stiefle提出。著名的共轭梯度法有Fletcher-reeves (FR)方法，Hestenes-Stiefel (HS)方法和Dai-Yuan (DY)方法等。本节给出针对光滑后的极大极小问题的共轭梯度法及其算法。</p><p>设 x k 为当前迭代点，对光滑后的极大极小问题(3)，即一个无约束优化问题</p><p>min F ˜ ( x , t ) ,</p><p>记函数值 F ˜ k = F ˜ ( x k , t k ) ，梯度 g ˜ k = g ˜ ( x k , t k ) ，则搜索方向为</p><p>d ˜ k = { − g ˜ k , if   k = 0 , − g ˜ k + β k d ˜ k − 1 , if   k &gt; 0 ,</p><p>其中， β ˜ k 为一个参数，常用的 β ˜ k 有：</p><p>β ˜ k H S = g ˜ k T y ˜ k − 1 d ˜ k − 1 T y ˜ k − 1 ,       β ˜ k D Y = ‖ g ˜ k ‖ 2 d ˜ k − 1 T y ˜ k − 1 ,       β ˜ k L S = g ˜ k T y ˜ k − 1 − g ˜ k − 1 T d ˜ k − 1 , β ˜ k F R = ‖ g ˜ k ‖ 2 ‖ g ˜ k − 1 ‖ 2 ,       β ˜ k P R = g ˜ k T y ˜ k − 1 ‖ g ˜ k − 1 ‖ 2 ,       β ˜ k P R + = max { g ˜ k T y ˜ k − 1 ‖ g ˜ k − 1 ‖ 2 , 0 } .</p><p>其中， y ˜ k − 1 = g ˜ k − g ˜ k − 1 。此时，共轭梯度法的迭代形式为</p><p>x k + 1 = x k + α k d ˜ k ,</p><p>其中 α k 为线搜索产生的步长。常用的线搜索为Wolfe线搜索：</p><p>F ˜ ( x k + α k d ˜ k , t k ) − F ˜ ( x k , t k ) ≤ δ α k g ˜ k T d ˜ k ,</p><p>d ˜ k T g ˜ ( x k + α k d ˜ k , t k ) ≥ σ d ˜ k T g ˜ k ,</p><p>以及Amijo线搜索</p><p>α k = max { ρ − j , j = 0 , 1 , 2 , ⋯ } ,</p><p>F ˜ ( x k + α k d ˜ k , t k ) − F ˜ ( x k , t k ) ≤ δ α k g ˜ k T d ˜ k .</p><p>为解决更复杂的无约束优化问题以及得到更精确的解，谱共轭梯度法 [<xref ref-type="bibr" rid="hanspub.38639-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.38639-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.38639-ref13">13</xref>]、三项共轭梯度法 [<xref ref-type="bibr" rid="hanspub.38639-ref14">14</xref>] [<xref ref-type="bibr" rid="hanspub.38639-ref15">15</xref>] [<xref ref-type="bibr" rid="hanspub.38639-ref16">16</xref>] 等方法被提出，在解决大规模无约束优化问题、互补问题以及张量等问题都有良好的数值效果。结合文献 [<xref ref-type="bibr" rid="hanspub.38639-ref11">11</xref>]，本文给出针对光滑化后的极大极小问题的搜索方向与步长：</p><p>d ˜ k = { − g ˜ k , if   k = 0 , − θ ˜ k g ˜ k + β ˜ k F R d ˜ k − 1 , if   k &gt; 0 , (6)</p><p>在(6)中，</p><p>θ ˜ k = d ˜ k − 1 T y ˜ k − 1 ‖ g ˜ k − 1 ‖ 2 .</p><p>步长 α k &gt; 0 满足</p><p>F ˜ ( x k + α k d ˜ k , t k ) − F ˜ ( x k , t k ) ≤ ρ α k 2 ‖ d ˜ k ‖ 2 , (7)</p><p>d ˜ k T g ˜ ( x k + α k d ˜ k , t k ) ≥ − 2 σ α k ‖ d ˜ k ‖ 2 , (8)</p><p>其中， 0 &lt; ρ &lt; σ &lt; 1 。</p><p>下面，我们给出求解极大极小问题的光滑化共轭梯度法：</p><p>算法2.1</p><p>步1：给定常数 ς ∈ ( 0 , 1 ) , 0 &lt; ρ &lt; σ &lt; 1 , ε &gt; 0 ，初始点 ( x 0 , t 0 ) ∈ ( R n , R ) ，令 k = 0 ；</p><p>步2：若 ‖ g ˜ k ‖ ≤ ε ，则停止计算，否则转步3；</p><p>步3：根据(7)、(8)计算步长 α k ；</p><p>步4：令 x k + 1 = x k + α k d ˜ k ， t k + 1 = ς t k ，由(6)计算搜索方向 d ˜ k + 1 ；</p><p>步5：计算 F ˜ k + 1 ， g ˜ k + 1 ，令 k = k + 1 ，转步2。</p><p>为证明算法的收敛性，我们首先给出如下假设：</p><p>假设2.1：</p><p>(A1) 水平集 Ω 有界，水平集为 Ω = { x ∈ R n | F ˜ ( x , t ) ≤ F ˜ ( x 0 , t 0 ) } ；</p><p>(A2) 在 Ω 的一个邻域U中，函数是连续可微且其导数是Lipschitz连续的，即</p><p>‖ g ˜ ( x , t ) − g ˜ ( y , t ) ‖ ≤ L ‖ x − y ‖ ,     ∀ x , y ∈ U ,</p><p>其中L是Lipschitz常数。</p><p>由假设2.1可得，存在一个常数 Γ &gt; 0 ，使得</p><p>‖ g ˜ ( x , t ) ‖ ≤ Γ ,     x ∈ Ω .</p><p>引理2.1： d ˜ k 由(6)式给出，则</p><p>g ˜ k T d k ≤ − c ‖ g ˜ k ‖ 2 ,     c &gt; 0 (9)</p><p>证 当 k = 0 时，(9)式显然成立，假设 k ≥ 1 时满足 g ˜ k − 1 T d k − 1 ≤ − c ‖ g ˜ k − 1 ‖ 2 ，则</p><p>g ˜ k T d ˜ k = − θ ˜ k ‖ g ˜ k ‖ 2 + ‖ g ˜ k ‖ 2 ‖ g ˜ k − 1 ‖ 2 d ˜ k − 1 T g ˜ k = g ˜ 2 ‖ g ˜ k − 1 ‖ 2 ( − d ˜ k − 1 T g ˜ k + d ˜ k − 1 T g ˜ k − 1 + d ˜ k − 1 T g ˜ k ) = ‖ g ˜ k ‖ 2 ‖ g ˜ k − 1 ‖ 2 d ˜ k − 1 T g ˜ k − 1 ≤ c ‖ g ˜ k ‖ 2 ‖ g ˜ k − 1 ‖ 2 ( − ‖ g ˜ k − 1 ‖ 2 ) = − c ‖ g ˜ k ‖ 2 .</p><p>引理2.2：若假设2.1成立，则</p><p>∑ k ≥ 0 ‖ g ˜ k ‖ 4 ‖ d ˜ k ‖ 2 &lt; + ∞ (10)</p><p>证 由假设A(2)和(8)式，我们有</p><p>( 2 σ + L ) α k ‖ d ˜ k ‖ 2 ≥ − g ˜ k d ˜ k ,</p><p>移项后两边同时平方可得</p><p>α k 2 ‖ d ˜ k ‖ 2 ≥ ( 1 2 σ + L ) 2 ( g ˜ k d ˜ k ) 2 ‖ d ˜ k ‖ 2 ,</p><p>结合(7)式，</p><p>∑ k ≥ 0 ( g ˜ k d ˜ k ) 2 ‖ d ˜ k ‖ 2 ≤ ∑ k ≥ 0 { F ˜ ( x k ) − F ˜ ( x k + 1 ) } &lt; + ∞</p><p>故由引理2.1可得(10)式成立。</p><p>由引理2.1和引理2.2，我们可以得出如下的收敛性定理。</p><p>定理2.1：若假设2.1成立，则算法2.1满足</p><p>lim inf k → ∞ ‖ g ˜ k ‖ = 0. (11)</p><p>证 反证法，假设定理不成立，存在常数 ε &gt; 0 ，对任意 k ≥ 0 ，有</p><p>‖ g ˜ k ‖ ≥ ε ,</p><p>根据(6)式，有</p><p>d ˜ k = − θ ˜ k g ˜ k + β ˜ k F R d ˜ k − 1 ,</p><p>两边同时平方后除以 ( g ˜ k T d ˜ k ) 2 ，有</p><p>‖ d ˜ k ‖ 2 ‖ g ˜ k ‖ 4 = ‖ d ˜ k ‖ 2 ( g ˜ k T d ˜ k ) 2 = ( ‖ g ˜ k ‖ 2 ‖ g ˜ k − 1 ‖ 2 ) 2 ‖ d ˜ k − 1 ‖ 2 ‖ g ˜ k ‖ 4 + 2 θ ˜ k ‖ g ˜ k ‖ 2 − θ ˜ k 2 ‖ g ˜ k ‖ 2 ≤ ‖ d ˜ k − 1 ‖ 2 ‖ g ˜ k − 1 ‖ 4 + 1 ‖ g ˜ k ‖ 2</p><p>即</p><p>‖ d ˜ k ‖ 2 ‖ g ˜ k ‖ 4 ≤ ∑ i = 0 k − 1 1 ‖ g ˜ i ‖ 2 ≤ k ε 2</p><p>因此，我们有</p><p>∑ k ≥ 0 ‖ g ˜ k ‖ 4 ‖ d ˜ k ‖ 2 ≥ ε 2 ∑ k ≥ 0 1 k = + ∞</p><p>与假设矛盾，因此(11)式成立。</p></sec><sec id="s7"><title>3. 数值实验</title><p>本部分，我们给出用算法2.1求解极大极小问题的数值算例，3.1中计算常规的极大极小问题，算例取自文献 [<xref ref-type="bibr" rid="hanspub.38639-ref3">3</xref>]，通过给出算例结果与最优值的误差证明算法的有效性。3.2中给出算法2.1计算带有张量结构的极大极小问题的数值结果，数值结果表明算法2.1在求解带有张量结构的极大极小问题时有良好的数值表现。</p><sec id="s7_1"><title>3.1. 极大极小问题</title><p>本部分给出光滑化算法计算极大极小问题的数值算例，其中的参数设置具体为 ε = 10 e − 4 ， ρ = 0.3 ， σ = 0.7 ， ς = 0.5 ， t 0 = 2 。表1给出实验结果，其中n，m分别为变量、函数的数目，图1~3为算法2.1在求解算例时，函数值随着迭代步数的数值表现。由表1和图1~3我们可以观察到算法2.1在求解极大极小问题时具有良好的数值表现。</p><p>例1 Cresent [<xref ref-type="bibr" rid="hanspub.38639-ref3">3</xref>]</p><p>h ( x ) = max { x 1 2 + ( x 2 − 1 ) 2 + x 2 − 1 , − x 1 2 − ( x 2 − 1 ) 2 + x 2 + 1 } ,</p><p>n = 2 , h ( x * ) = 0 , x 0 = ( 0 , 0 ) T .</p><p>例2 Mifflin 1 [<xref ref-type="bibr" rid="hanspub.38639-ref3">3</xref>]</p><p>h ( x ) = − x 1 + max { x 1 2 + x 2 2 − 1 , 0 } ,</p><p>n = 2 , h ( x * ) = − 1 , x 0 = ( 2 , 2 ) T .</p><p>例3 Mifflin 2 [<xref ref-type="bibr" rid="hanspub.38639-ref3">3</xref>]</p><p>h ( x ) = − x 1 + 2 ( x 1 2 + x 2 2 − 1 ) + 1.75 max { &#177; ( x 1 2 + x 2 2 − 1 ) } ,</p><p>n = 2 , h ( x * ) = − 1 , x 0 = ( 0 , 0 ) T .</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Numerical results for example 1 - </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >问题</th><th align="center" valign="middle" >n</th><th align="center" valign="middle" >m</th><th align="center" valign="middle" >误差</th><th align="center" valign="middle" >迭代步数</th></tr></thead><tr><td align="center" valign="middle" >Cresent</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >6.8645e−05</td><td align="center" valign="middle" >99</td></tr><tr><td align="center" valign="middle" >Mifflin 1</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >8.4613e−05</td><td align="center" valign="middle" >36</td></tr><tr><td align="center" valign="middle" >Mifflin 2</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >5.4972e−05</td><td align="center" valign="middle" >276</td></tr></tbody></table></table-wrap><p>表1. 例1~例3的实验结果</p><p>图1. Cresent的实验结果</p><p>图2. Mifflin 1的实验结果</p><p>图3. Mifflin 2的实验结果</p></sec><sec id="s7_2"><title>3.2. 带张量结构的极大极小问题</title><p>本部分给出算法2.1求解带有张量结构的极大极小问题的数值算例，为了便于计算，数值算例中的张量使用的是对称张量，其中的参数设置为 ε = 10 e − 2 ， σ = 0.7 ， ρ = 0.3 ， ς = 0.5 ， t 0 = 2 ，且选取 ‖ x k + 1 − x k ‖ ≤ ε 作为算法的终止条件。表2给出实验结果，其中n，m分别是张量的阶数和维数，图4~5为算法2.1在求解算例时，函数值随着迭代步数的数值表现。由表2和图4~5我们可以观察到算法2.1可以有效求解带有张量结构的极大极小问题。</p><p>考虑两个3阶2维的张量 A 1 = ( a i 1 i 2 i 3 1 ) 和 A 2 = ( a i 1 i 2 i 3 2 ) ，以及两个向量为 b 1 = ( 6 , 2 ) T 和 b 2 = ( 2 , 8 ) T ，给出如下两个算例：</p><p>例4： A 1 为 a 1 1 1 1 = a 222 1 = 1 0 ，其余元素为3， A 2 为 a 1 1 1 2 = a 222 2 = 6 ，其余元素为3。初始向量为 x 0 = ( 2 , 6 ) T 。</p><p>例5： A 1 为 a 1 1 1 1 = a 222 1 = 1 ，其余元素为3， A 2 为 a 1 1 1 1 = a 222 1 = 1 0 ，其余元素为6。初始向量为 x 0 = ( 5 , 2 ) T 。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Numerical results for example 4 - </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >问题</th><th align="center" valign="middle" >n</th><th align="center" valign="middle" >m</th><th align="center" valign="middle" >误差</th><th align="center" valign="middle" >迭代步数</th></tr></thead><tr><td align="center" valign="middle" >例4</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >−2.3002</td><td align="center" valign="middle" >6</td></tr><tr><td align="center" valign="middle" >例5</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >−2.3032</td><td align="center" valign="middle" >22</td></tr></tbody></table></table-wrap><p>表2. 例4~例5的实验结果</p><p>图4. 例4的实验结果</p><p>图5. 例5的实验结果</p></sec></sec><sec id="s8"><title>4. 结论</title><p>本文通过指数罚函数，对极大极小问题进行光滑化处理，将非光滑的极大极小问题转化为光滑的无约束优化问题，并给出针对带有罚参数的无约束优化问题的共轭梯度算法。为验证算法的有效性，我们同时给出了该算法求解常规的极大极小问题和带有张量结构的极大极小问题的数值实验，根据数值实验的结果，该算法在迭代次数、下降速度、误差等方面都具有良好的表现。在以后的研究中，对于更复杂的大规模的极大极小问题，可以运用不同的线搜索或不同的步长进行优化，以得到更快的下降速度和更良好的数值表现。</p></sec><sec id="s9"><title>文章引用</title><p>郝 月. 求解极大极小问题的共轭梯度法Conjugate Gradient Method for Solving Minimax Problems[J]. 应用数学进展, 2020, 09(11): 1916-1924. https://doi.org/10.12677/AAM.2020.911221</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.38639-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Polak, E., Higgins, J.E. and Maynes, D.Q. (1992) A Barrier Function Method for Minimax Problems. Mathematical Programming, 54, 155-176. &lt;br&gt;https://doi.org/10.1007/BF01586049</mixed-citation></ref><ref id="hanspub.38639-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Polak, E. (1987) On the Mathematical Foundations of Non Differentiable Optimization in Engineering Design. SIAM Review, 29, 21-91. &lt;br&gt;https://doi.org/10.1137/1029002</mixed-citation></ref><ref id="hanspub.38639-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Pillo, G., Grippo, L. and Lucidi, S. (1993) A Smooth Method for the Finite Minimax Problem. Mathematical Programming, 60, 187-214. &lt;br&gt;https://doi.org/10.1007/BF01580609</mixed-citation></ref><ref id="hanspub.38639-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Li, S.B., Cao, D.X., Wang, H.J. and Deng, K.Z. (2004) Interval Adjustable Entropy Algorithm for a Class of Unconstrained Discrete Minimax Problems. Applied Mathematics, 19, 37-43. &lt;br&gt;https://doi.org/10.1007/s11766-004-0019-8</mixed-citation></ref><ref id="hanspub.38639-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Xiao, Y. and Bo, Y. (2010) A Truncated Aggregate Smoothing Newton Method for Minimax Problems. Applied Mathematics and Computation, 216, 1868-1879. &lt;br&gt;https://doi.org/10.1016/j.amc.2009.11.034</mixed-citation></ref><ref id="hanspub.38639-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Pang, D.Y., Du, S.Q. and Ju, J.J. (2016) The Smoothing Fletcher-Reeves Conjugate Gradient Method for Solving Finite Minimax Problems. Science Asia, 42, 40-45. &lt;br&gt;https://doi.org/10.2306/scienceasia1513-1874.2016.42.040</mixed-citation></ref><ref id="hanspub.38639-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Li, X.S. (1991) An Aggregate Function Method for Nonlinear Programming. Science in China, Series A, 12, 1467-1473.</mixed-citation></ref><ref id="hanspub.38639-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Fletcher, R. and Reeves, C.M. (1964) Function Minimization by Conjugate Gradients. Computer Journal, 7, 149-154.  
&lt;br&gt;https://doi.org/10.1093/comjnl/7.2.149</mixed-citation></ref><ref id="hanspub.38639-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">张丽, 周伟军. Armijo线性搜索下Hager-Zhang共轭梯度法的全局收敛性[J]. 数学物理学, 2018, 28A(5): 840-845.</mixed-citation></ref><ref id="hanspub.38639-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">戴彧虹, 袁亚湘. 非线性共轭梯度法[M]. 上海: 上海科学技术出版社, 2000.</mixed-citation></ref><ref id="hanspub.38639-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Du, S.Q. and Chen, Y.Y. (2008) Global Convergence of a Modified Spectral FR Conjugate Gradient Method. Applied Mathematics and Computation, 202, 766-770. &lt;br&gt;https://doi.org/10.1016/j.amc.2008.03.020</mixed-citation></ref><ref id="hanspub.38639-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Birgin, E.G. and Martinez, J.M. (2001) A Spectral Conjugate Gradient Method for Unconstrained Optimization. Applied Mathematics and Computation, 43, 117-128. &lt;br&gt;https://doi.org/10.1007/s00245-001-0003-0</mixed-citation></ref><ref id="hanspub.38639-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">李向利, 师娟娟, 董晓亮. 一类修正的非单调谱共轭梯度法及其在非负矩阵分解中的应用[J]. 数学物理学报, 2018, 38(5): 954-962.</mixed-citation></ref><ref id="hanspub.38639-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Andrei, N. (2013) A Simple Three-Term Conjugate Gradient Algorithm for Unconstrained Optimization. Journal of Computational and Applied Mathematics, 241, 19-29. &lt;br&gt;https://doi.org/10.1016/j.cam.2012.10.002</mixed-citation></ref><ref id="hanspub.38639-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Dong, X.L., Liu, H.W. and He, Y.B. (2015) New Version of the Three-Term Conjugate Gradient Method Based on Spectral Scaling Conjugacy Condition That Generates Descent Search Direction. Applied Mathematics and Computation, 269, 606-617. &lt;br&gt;https://doi.org/10.1016/j.amc.2015.07.067</mixed-citation></ref><ref id="hanspub.38639-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Koorapetse, M.S. and Kaelo, P. (2018) Globally Convergent Three-Term Conjugate Gradient Projection Methods for Solving Nonlinear Monotone Equations. Arabian Journal of Mathematics, 7, 289-301.  
&lt;br&gt;https://doi.org/10.1007/s40065-018-0206-8</mixed-citation></ref></ref-list></back></article>