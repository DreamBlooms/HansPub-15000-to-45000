<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.116167</article-id><article-id pub-id-type="publisher-id">CSA-42870</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210600000_55828293.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  双重判别的SAR图像超分辨率重建
  SAR Image Super-Resolution Reconstruction Based on Dual Discrimination
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>肖</surname><given-names>光义</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>董</surname><given-names>张玉</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>杨</surname><given-names>学志</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>合肥工业大学软件学院，安徽 合肥</addr-line></aff><aff id="aff2"><addr-line>合肥工业大学计算机与信息学院，安徽 合肥</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>02</day><month>06</month><year>2021</year></pub-date><volume>11</volume><issue>06</issue><fpage>1617</fpage><lpage>1626</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   基于生成对抗网络的SAR图像超分辨率重建算法可以产生非常好的视觉效果。但是，现有的生成对抗网络在图像超分辨率重建的过程中仅判别生成的高分辨率图像，而忽略了低分辨率图像的作用。这不能保证生成的高分辨率图像可以被准确地降采样到最初的低分辨率图像。为了有效利用高分辨率图像和低分辨率图像，提出了双重判别的SAR图像超分辨率重建算法。双重判别在对高分辨率图像进行判别的基础上增加对低分辨率图像的判别，这使生成的高分辨率图像可以被准确地降采样到原始的低分辨率图像。而且，生成器网络在残差密集网络的基础上，融合了不同层次的变换特征，提高了参数利用率。提出的双重判别算法已成功应用于SAR图像，并且在视觉效果和客观评估指标方面都优于最新的深度学习算法。 Synthetic Aperture Radar (SAR) image super-resolution reconstruction algorithms based on Generative Adversarial Networks (GANs) can produce very good visual effects. However, the existing super-resolution reconstruction algorithms based on GANs discriminate only the generated high-resolution images but ignore the role of low-resolution images. It fails to guarantee that the generated high-resolution image can be accurately downsampled to the original low-resolution image. In order to effectively utilize high-resolution images and low-resolution images, a dual discriminative GAN is proposed for SAR image super-resolution reconstruction. Dual discrimination increases the discrimination of the low-resolution images based on the discrimination of the high-resolution images, which enables the generated high-resolution images to be accurately downsampled to the original low-resolution images. Moreover, the generator network integrates different levels of transformation feature based on the residual dense network, which improves the parameter utilization. The proposed dual discrimination algorithm has been successfully applied to SAR images, and is superior to the latest deep learning algorithms in terms of visual effects and objective evaluation indicators. 
  
 
</p></abstract><kwd-group><kwd>合成孔径雷达，超分辨率，生成对抗网络，双重判别, Synthetic Aperture Radar</kwd><kwd> Super-Resolution</kwd><kwd> Generative Adversarial Network</kwd><kwd> Dual Discrimination</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>基于生成对抗网络的SAR图像超分辨率重建算法可以产生非常好的视觉效果。但是，现有的生成对抗网络在图像超分辨率重建的过程中仅判别生成的高分辨率图像，而忽略了低分辨率图像的作用。这不能保证生成的高分辨率图像可以被准确地降采样到最初的低分辨率图像。为了有效利用高分辨率图像和低分辨率图像，提出了双重判别的SAR图像超分辨率重建算法。双重判别在对高分辨率图像进行判别的基础上增加对低分辨率图像的判别，这使生成的高分辨率图像可以被准确地降采样到原始的低分辨率图像。而且，生成器网络在残差密集网络的基础上，融合了不同层次的变换特征，提高了参数利用率。提出的双重判别算法已成功应用于SAR图像，并且在视觉效果和客观评估指标方面都优于最新的深度学习算法。</p></sec><sec id="s2"><title>关键词</title><p>合成孔径雷达，超分辨率，生成对抗网络，双重判别</p></sec><sec id="s3"><title>SAR Image Super-Resolution Reconstruction Based on Dual Discrimination<sup> </sup></title><p>Guangyi Xiao<sup>1</sup>, Zhangyu Dong<sup>1</sup>, Xuezhi Yang<sup>2</sup></p><p><sup>1</sup>School of Computer and Information, Hefei University of Technology, Hefei Anhui</p><p><sup>2</sup>School of Software, Hefei University of Technology, Hefei Anhui</p><p><img src="//html.hanspub.org/file/2-1542103x4_hanspub.png" /></p><p>Received: May 1<sup>st</sup>, 2021; accepted: May 26<sup>th</sup>, 2021; published: Jun. 2<sup>nd</sup>, 2021</p><p><img src="//html.hanspub.org/file/2-1542103x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Synthetic Aperture Radar (SAR) image super-resolution reconstruction algorithms based on Generative Adversarial Networks (GANs) can produce very good visual effects. However, the existing super-resolution reconstruction algorithms based on GANs discriminate only the generated high-resolution images but ignore the role of low-resolution images. It fails to guarantee that the generated high-resolution image can be accurately downsampled to the original low-resolution image. In order to effectively utilize high-resolution images and low-resolution images, a dual discriminative GAN is proposed for SAR image super-resolution reconstruction. Dual discrimination increases the discrimination of the low-resolution images based on the discrimination of the high-resolution images, which enables the generated high-resolution images to be accurately downsampled to the original low-resolution images. Moreover, the generator network integrates different levels of transformation feature based on the residual dense network, which improves the parameter utilization. The proposed dual discrimination algorithm has been successfully applied to SAR images, and is superior to the latest deep learning algorithms in terms of visual effects and objective evaluation indicators.</p><p>Keywords:Synthetic Aperture Radar, Super-Resolution, Generative Adversarial Network, Dual Discrimination</p><disp-formula id="hanspub.42870-formula14"><graphic xlink:href="//html.hanspub.org/file/2-1542103x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/2-1542103x7_hanspub.png" /> <img src="//html.hanspub.org/file/2-1542103x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>合成孔径雷达(Synthetic Aperture Radar, SAR)图像具有极强的抗干扰特性，在航空航天工程、地质勘探、测绘和军事侦察中发挥着关键作用。为了确保SAR图像在图像分类和目标识别方面具有良好的性能，可以提高现有SAR图像的分辨率。但是，许多高分辨率图像是从低分辨率图像中重建出来的，很难区分生成的哪一张高分辨率图像效果更好，这对图像超分辨率来说是一个挑战。</p><p>现有的超分辨率生成对抗网络，例如SRGAN [<xref ref-type="bibr" rid="hanspub.42870-ref1">1</xref>]、ESRGAN [<xref ref-type="bibr" rid="hanspub.42870-ref2">2</xref>] 和A-SRGAN [<xref ref-type="bibr" rid="hanspub.42870-ref3">3</xref>]，仅判别高频特征空间，而忽略低频特征空间的作用。在图像超分辨率重建过程中，生成的高分辨率图像不能保证准确地降采样到最初的低分辨率图像。如果重建的高分辨率图像更接近真实的高分辨率图像，则对生成的高分辨率图像进行降采样后得到的低分辨率图像也更接近真实的低分辨率图像。为了充分利用高分辨率图像和低分辨率图像的信息以获得具有更好视觉效果的高频纹理细节，提出了双重判别的SAR图像超分辨率重建算法。首先，双重判别通过利用高分辨率图像和低分辨率图像之间的相关性，同时判别两种不同分辨率的图像，这使得生成的高分辨率图像更加逼真。其次，考虑到在最近的超分辨率工作中残差密集块RDB [<xref ref-type="bibr" rid="hanspub.42870-ref4">4</xref>] 和多层特征融合的优越性能，将这两种技术引入到了双重判别的生成器中，提出了能够融合多层变换特征的生成器网络，有效地提高了参数的利用率。</p></sec><sec id="s6"><title>2. 相关工作</title><p>大多数研究工作已致力于图像超分辨率重建的研究。最初，使用插值方法生成高分辨率图像，例如最近邻插值、双线性插值和双三次插值。尽管插值方法简单、方便、易于实现，但是它们容易产生模糊的纹理。后来提出了模型统计的方法，学习从低分辨率图像到高分辨率图像的映射，典型方法包括基于示例的方法 [<xref ref-type="bibr" rid="hanspub.42870-ref5">5</xref>]、自相似性方法 [<xref ref-type="bibr" rid="hanspub.42870-ref6">6</xref>]、字典对方法 [<xref ref-type="bibr" rid="hanspub.42870-ref7">7</xref>] 和卷积稀疏编码方法 [<xref ref-type="bibr" rid="hanspub.42870-ref8">8</xref>]。随着卷积神经网络(Convolutional Neural Network, CNN)的出现，许多基于CNN的方法也已应用于图像超分辨率重建中。Dong等人首先提出了SRCNN [<xref ref-type="bibr" rid="hanspub.42870-ref9">9</xref>]，该方法将三层CNN应用于图像超分辨率重建中，并获得了良好的性能。为了加速网络训练，提出了FSRCNN [<xref ref-type="bibr" rid="hanspub.42870-ref10">10</xref>] 和ESPCN [<xref ref-type="bibr" rid="hanspub.42870-ref11">11</xref>]。随后，通过去除批归一化层并加深网络，提出了增强的深度残差网络EDSR [<xref ref-type="bibr" rid="hanspub.42870-ref4">4</xref>]，目的是提高超分辨率的重建性能。密集连接网络 [<xref ref-type="bibr" rid="hanspub.42870-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.42870-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.42870-ref14">14</xref>] 通过增加每个残差块的复杂度来提高超分辨率性能，并减少由于网络深度过深而导致的训练困难。由于注意力机制 [<xref ref-type="bibr" rid="hanspub.42870-ref15">15</xref>] 在图像分类中的出色表现，基于注意力的图像超分辨率重建算法 [<xref ref-type="bibr" rid="hanspub.42870-ref16">16</xref>] [<xref ref-type="bibr" rid="hanspub.42870-ref17">17</xref>] 也取得了良好的效果。</p><p>生成对抗网络 [<xref ref-type="bibr" rid="hanspub.42870-ref18">18</xref>] 的提出是深度学习的另一项重大突破，其在图像生成 [<xref ref-type="bibr" rid="hanspub.42870-ref19">19</xref>] 和风格迁移 [<xref ref-type="bibr" rid="hanspub.42870-ref20">20</xref>] [<xref ref-type="bibr" rid="hanspub.42870-ref21">21</xref>] 等方面的出色表现备受关注。SRGAN [<xref ref-type="bibr" rid="hanspub.42870-ref1">1</xref>] 是首次在单图像超分辨率重建中使用生成对抗网络的模型，并产生高频纹理细节，从而实现了非常好的视觉效果。SRGAN的生成器是残差神经网络(SRResNet)，其视觉效果优于SRResNet，但是SRGAN在客观评估指标(例如峰值信噪比(PSNR)和结构相似性(SSIM))方面表现不佳。可能的原因是，判别器根据图像中是否包含高频纹理信息来确定图像的真实性，却不关注高频纹理信息是否是真实的。因此，SRGAN生成的高频伪像不能用于非常严格的情况下，例如医学诊断和军事目标侦察。后来，提出了增强型SRGAN (ESRGAN [<xref ref-type="bibr" rid="hanspub.42870-ref2">2</xref>] )，通过增加每个残差块的复杂度来生成更逼真的高频细节，并使用RaGAN [<xref ref-type="bibr" rid="hanspub.42870-ref22">22</xref>] 来确定在超分辨图像和真实图像之间哪个更真实。同时，一些使用特征判别的算法也可以准确地重建高分辨率图像的纹理，例如EnhanceNet [<xref ref-type="bibr" rid="hanspub.42870-ref23">23</xref>] 和SRfeat。</p></sec><sec id="s7"><title>3. 方法</title><p>从低分辨率图像 I L R 中恢复相应的高分辨率图像 I H R 是单图像超分辨率重建的主要目标。高分辨率图像的重建基于双重判别。生成器G的主要目的是通过生成类似于真实高分辨率图像的超分辨图像 I S R 来欺骗这两个鉴别器，使高分判别器 D 1 无法将伪造的高分辨率图像与真实的高分辨率图像区分开，并使低分判别器 D 2 无法区分伪造的低分辨率图像和真实的低分辨率图像。高分判别器和低分判别器的训练目标是防止被生成器欺骗。生成器和两个判别器的目标函数如下。</p><p>min G L ( G ) = 1 2 { E I H R ~ p ( I H R ) [ D 1 ( I H R ) ] − E I L R ~ p ( I L R ) [ D 1 ( G ( I L R ) ) ] } 2   + 1 2 { E I L R ~ p ( I L R ) [ D 2 ( I L R ) ] − E I L R ' ~ p ( I L R ' ) [ D 2 ( I L R ' ) ] } 2 (1)</p><p>min D 1 L ( D 1 ) = − 1 2 { E I H R ~ p ( I H R ) [ D 1 ( I H R ) ] − E I L R ~ p ( I L R ) [ D 1 ( G ( I L R ) ) ] } 2 (2)</p><p>min D 2 L ( D 2 ) = − 1 2 { E I L R ~ p ( I L R ) [ D 2 ( I L R ) ] − E I L R ' ~ p ( I L R ' ) [ D 2 ( I L R ' ) ] } 2 (3)</p><p>其中， I L R ' 表示生成的高分辨率图像降采样后得到的低分辨率图像。 p ( I H R ) 和 p ( I L R ) 分别表示真实的高分辨率图像和真实的低分辨率图像的分布， p ( I L R ' ) 表示降采样得到的低分辨率图像的分布。生成器和两个判别器以交替的方式训练。训练生成器时，两个判别器保持不变，并且生成器生成尽可能真实的高分辨率图像。当重建的高分辨率图像更接近实际的高分辨率图像时，真实的低分辨率图像和重建高分辨率图像的降采样图像也更接近。当训练两个判别器时，生成器保持不变。高分判别器判别重建的高分辨图像与真实的高分辨率图像，低分判别器判别超分辨图像的降采样图像与真实的低分辨率图像。</p><p>下面将详细讨论生成器和判别器的网络结构以及双重判别的判别原理。</p><sec id="s7_1"><title>3.1. 生成器网络</title><p>在SRGAN和ESRGAN中，用于恢复高分辨率图像的信息只是从最后一个残差块中提取的深层特征，而忽略了前面残差块提取的浅层特征。最后一个残差块不能代表前面所有残差块的贡献。经过一系列卷积运算后，网络前端提取的特征将发生变换、改变甚至丢失。将这些特征传输到网络末端后，它们与网络前端的浅层特征有很大的不同。浅层特征很难保留，但这些特征对于高分辨率图像的重建并非没有意义。因此，为了利用不同层的特征，将从每个残差块提取的特征直接传送到网络的末端。然而，每个残差块位于网络的不同位置，这些位置代表特征演化的不同阶段。因此，不同残差块提取的特征对图像超分辨率重建性能的影响不同。因此，必须变换这些特征，然后再将其直接送到网络末端进行融合，以重建准确的高分辨率图像。RDB用作基本残差块。提出的生成器网络模型在图1中进行了说明。生成器网络简记为TRDN。k表示卷积核的大小，n表示卷积核的数目，s表示卷积的步长。</p><p>图1. 生成器网络结构</p><p>生成器网络包括浅层特征提取、多层变换特征融合、全局残差学习和上采样操作。</p><sec id="s7_1_1"><title>3.1.1. 浅层特征提取</title><p>首先，提取输入的低分辨率图像的浅层特征：</p><p>F 0 = H 0 ( I L R ) (4)</p><p>其中 H 0 表示浅层特征提取操作，并且 F 0 表示浅层特征。</p></sec><sec id="s7_1_2"><title>3.1.2. 多层变换特征融合</title><p>提取的浅层特征用于多层特征提取：</p><p>F i = H i ( F i − 1 ) , i = 1 , 2 , ⋯ , 16 (5)</p><p>其中 H i 表示多层特征提取操作，并且 F i − 1 和 F i 分别表示第i个残差块的输入特征和输出特征。然后，获得的浅层特征 F 0 和特征 F i ( i = 1 , 2 , ⋯ , 16 ) 被变换：</p><p>T i = t r a n s ( F i ) , i = 0 , 1 , 2 , ⋯ , 16 (6)</p><p>其中， t r a n s ( . ) 表示变换操作，并且 T i 表示变换后的多层特征。变换操作的目的是使每个残差块提取的不同层的特征发挥不同的作用，而不是被同等对待。为了简化计算，使用大小为3 &#215; 3的卷积核完成此变换操作。然后将变换后的特征进行融合以获得多层特征：</p><p>M F = f 3 &#215; 3 ( C ( T 0 , T 1 , T 2 , ⋯ , T 16 ) ) (7)</p><p>其中 C ( ⋅ ) 表示在特征在通道维度上的叠加， f 3 &#215; 3 ( ⋅ ) 表示3 &#215; 3卷积运算， M F 并且表示多层特征。</p></sec><sec id="s7_1_3"><title>3.1.3. 全局残差学习</title><p>为了促进网络中信息的有效传输，执行全局残差学习：</p><p>G = F 0 + M F (8)</p><p>其中G表示全局特征。</p></sec><sec id="s7_1_4"><title>3.1.4. 上采样操作</title><p>最后，将获得的全局特征上采样两次：</p><p>I S R = f 3 &#215; 3 ( H u p &#215; 2 ( H u p &#215; 2 ( G ) ) ) (9)</p><p>其中 H u p &#215; 2 表示上采样操作，上采样因子为2。</p></sec></sec><sec id="s7_2"><title>3.2. 双重判别</title><p>对抗损失的使用使生成的高分辨率图像有更多的高频纹理细节。一些现有的判别算法(例如SRGAN，ESRGAN和A-SRGAN)仅包括高分辨率图像的判别，却忽略了低分辨率图像的作用。根据一张低分辨率图像，可以获得许多高分辨率图像。但是，根据一张高分辨率图像，只能获得一张低分辨率图像。实验中使用的低分辨率图像都是通过双三次插值方法进行降采样得到的。如果重建的高分辨率图像足够准确，则对超分辨图像进行降采样获得的低分辨率图像也应该与最初的低分辨率图像相同。因此，通过对低分辨率图像的判别保证了生成的高分辨率图像可以被正确地降采样到原始的低分辨率图像，以此来指导超分辨图像的生成。判别器网络包含高分判别器和低分判别器，分别用来判别高分辨率图像和低分辨率图像。高分判别器和低分判别器的网络结构如图2所示。</p><p>图2. 判别器网络结构</p><p>生成器损失函数包含内容损失和对抗损失的加权总和：</p><p>L G = L C o n t e n t + α L A d v e r (10)</p><p>其中 α = 0.00001 。</p><p>内容损失定义为真实的高分辨率图像和生成的高分辨率图像之间的均方误差(MSE)：</p><p>L C o n t e n t = 1 r 2 W H ∑ x = 1 r W ∑ y = 1 r H ( I x , y H R − G θ G ( I L R ) x , y ) 2 (11)</p><p>其中W和H分别表示低分辨率图像的宽和高，r表示上采样因子。</p><p>对抗损失包括高分辨率图像和低分辨率图像的判别，定义为：</p><p>L A d v e r = [ D 1 ( I H R ) − D 1 ( G ( I L R ) ) ] 2 + [ D 2 ( I L R ) − D 2 ( I L R ' ) ] 2 (12)</p><p>根据输入的低分辨率图像，生成器生成尽可能真实的超分辨图像。如果超分辨图像足够准确，则超分辨图像的降采样图像也应与真实的低分辨率图像一致。</p><p>高分判别器和低分判别器分别区分高分辨率图像和低分辨率图像的特征，并且将损失函数定义为</p><p>L D 1 = − [ D ( I H R ) − D ( G ( I L R ) ) ] 2 (13)</p><p>L D 2 = − [ D ( I L R ) − D ( I L R ' ) ] 2 (14)</p><p>高分判别器区分真实和重建的高分辨率图像，低分判别器区分真实的低分辨率图像和超分辨图像的降采样图像。</p></sec></sec><sec id="s8"><title>4. 实验</title><sec id="s8_1"><title>4.1. 实验设置和数据集</title><p>在本实验中使用的SAR图像来自哨兵1号(Sentinel-1)，分别包括500张图像的训练集和100张图像的测试集。图像均为单波段。高分辨率图像的大小为256 &#215; 256，低分辨率图像的大小64 &#215; 64。低分辨率图像都是真实的高分辨率图像通过双三次插值降采样得到的。</p><p>本实验中使用的神经网络框架是TensorFlow 1.14。两个判别器和生成器以交替方式训练。所有网络模型都是从头开始进行训练，而且不依赖任何的基础训练模型。批大小为4，训练迭代次数为10<sup>6</sup>，使用Adam优化器，学习率为0.001，对抗损失权重α为0.00001。</p></sec><sec id="s8_2"><title>4.2. 评估方法</title><p>算法的有效性评估分为客观和主观两个方面。算法的重建性能由超分辨率工作中通常使用的客观评估指标(PSNR和SSIM)来衡量。PSNR从像素级角度衡量真实的高分辨率图像和重建的高分辨率图像之间的差异，其计算公式为</p><p>PSNR = 20 &#215; log 255 ∑ x = 1 r W ∑ y = 1 r W ( I x , y H R − G ( I L R ) x , y ) 2 (15)</p><p>PSNR包含真实高分辨率图像和超分辨图像之间的均方误差。较高的PSNR表明生成的高分辨率图像的像素值更接近于真实的高分辨率图像。</p><p>SSIM从图像结构的角度评估重建的高分辨率图像和实际高分辨率图像之间的相似度，其计算公式如下：</p><p>SSIM = ( 2 μ H μ S + c 1 ) &#215; ( 2 σ H S + c 2 ) ( μ H 2 + μ S 2 + c 1 ) &#215; ( σ H 2 + σ S 2 + c 2 ) (16)</p><p>其中 μ H 和 μ S 分别表示真实高分辨率图像和重建高分辨率图像的像素平均值； σ H 和 σ S 分别表示真实高分辨率图像和重建高分辨率图像的像素标准差，并且 σ H S 表示真实的高分辨率图像和生成的高分辨率图像的像素协方差。 c 1 和 c 2 是常数。 c 1 = ( k 1 L ) 2 ， c 2 = ( k 2 L ) 2 ，其中 L = 255 ， k 1 = 0.01 ， k 2 = 0.03 。当两个图像的SSIM接近1时，两个图像更相似。</p><p>除了客观评估，还比较了主观效果。生成对抗网络的优势在于其视觉效果。因此，主观效果的比较更好地反映了重建算法的性能。在Sentinel-1数据集上进行的实验在第4.3和4.4节中介绍。</p></sec><sec id="s8_3"><title>4.3. 消融实验</title><p>消融实验的目的是分别验证TRDN和双重判别的作用。首先验证TRDN的作用。RDN用作基准模型。为了确保在公平的环境中进行实验，TRDN中每个残差密集块包含三个卷积操作，而RDN中的残差密集块包含四个卷积操作。此设置可确保TRDN和RDN具有相同的参数。从客观和主观两个角度评估了这两个网络的性能。计算结果列于表1。整个图像和局部区域的放大效果如图3所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Objective evaluation of RDN and TRD</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >PSNR</th><th align="center" valign="middle" >SSIM</th></tr></thead><tr><td align="center" valign="middle" >RND</td><td align="center" valign="middle" >24.4180</td><td align="center" valign="middle" >0.9403</td></tr><tr><td align="center" valign="middle" >TRDN</td><td align="center" valign="middle" >24.7836</td><td align="center" valign="middle" >0.9432</td></tr></tbody></table></table-wrap><p>表1. RDN和TRDN的客观评价</p><p>图3. RDN和TRDN的比较</p><p>从以上实验结果来看，虽然TRDN使用的参数数量与RDN使用的参数数量相同，但是TRDN网络的重建性能优于RDN。RDN将从每个RDB中提取的特征直接送到网络的末端，以进行特征融合，而没有区别对待。变换操作通过对每个特征应用带权重的卷积运算，区别对待从每个残差密集块中提取的特征。TRDN考虑了从不同残差块中提取的特征存在差异，通过变换操作将最有用的信息送到网络末端以进行叠加融合。</p><p>接下来，验证了双重判别的作用。TRDN作为基本的生成器网络，并验证了单重判别和双重判别的作用。表2列出了计算得出的客观评估指标。主观效果如图4所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of objective evaluation between single discrimination and dual discriminatio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >PSNR</th><th align="center" valign="middle" >SSIM</th></tr></thead><tr><td align="center" valign="middle" >TRND</td><td align="center" valign="middle" >24.7836</td><td align="center" valign="middle" >0.9432</td></tr><tr><td align="center" valign="middle" >SGAN</td><td align="center" valign="middle" >24.4832</td><td align="center" valign="middle" >0.9397</td></tr><tr><td align="center" valign="middle" >DGAN</td><td align="center" valign="middle" >24.6688</td><td align="center" valign="middle" >0.9434</td></tr></tbody></table></table-wrap><p>表2. 单重判别与双重判别的客观评价比较</p><p>图4. 单重判别与双重判别的比较</p><p>SGAN是基于TRDN的单重判别的结果。DGAN在TRDN的基础上添加了两个判别器，以执行双重判别。从实验结果来看，单重判别的效果显然不如双重判别。单重判别SGAN仅对高分辨率图像进行判别，而双重判别DGAN不仅考虑了高分辨率图像的判别，而且通过低分辨率图像的判别指导高分辨率图像的合成。在重建高分辨率图像纹理细节方面，双重判别比单重判别具有更好的性能。</p></sec><sec id="s8_4"><title>4.4. 对比实验</title><p>为了验证双重判别算法在重建高频纹理细节方面的优越性能，将其与传统方法(双三次插值)和近年来提出的深度学习算法(SRCNN, SRResNet, SRGAN和RDN)进行了比较。对比实验是从客观和主观两个方面进行的，分别为表3中的客观评价指标和图5中的主观效果。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Comparison of DGAN with other algorithm</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >PSNR</th><th align="center" valign="middle" >SSIM</th></tr></thead><tr><td align="center" valign="middle" >bicubic</td><td align="center" valign="middle" >18.6748</td><td align="center" valign="middle" >0.7887</td></tr><tr><td align="center" valign="middle" >SRCNN</td><td align="center" valign="middle" >17.7079</td><td align="center" valign="middle" >0.6851</td></tr><tr><td align="center" valign="middle" >SRResNet</td><td align="center" valign="middle" >22.1664</td><td align="center" valign="middle" >0.9037</td></tr><tr><td align="center" valign="middle" >SRGAN</td><td align="center" valign="middle" >22.0847</td><td align="center" valign="middle" >0.8983</td></tr><tr><td align="center" valign="middle" >RDN</td><td align="center" valign="middle" >24.4180</td><td align="center" valign="middle" >0.9403</td></tr><tr><td align="center" valign="middle" >DGAN</td><td align="center" valign="middle" >24.6688</td><td align="center" valign="middle" >0.9434</td></tr></tbody></table></table-wrap><p>表3. DGAN与其他算法的比较</p><p>图5. DGAN与其他算法的比较</p><p>从以上比较结果可以看出，DGAN重建的图像在客观评价指标(PSNR和SSIM)上具有明显的优势。从整个图像来看，由SRResNet和SRGAN生成的图像具有良好的效果，但是在放大局部纹理细节之后，所得到的纹理细节不如双重判别重建的图像。由于使用了对抗损失，SRGAN的纹理比SRResNet要清晰，但是从客观评估指标(PSNR, SSIM)来看，SRGAN不如SRResNet。这一结果也与Ledig [<xref ref-type="bibr" rid="hanspub.42870-ref1">1</xref>] 等人的研究结论一致。此外，在重建高频纹理细节方面，RDN不如DGAN好。融合了多层变换特征的生成器网络和双重判别使DGAN重建的高分辨率图像有更好的视觉效果。</p></sec></sec><sec id="s9"><title>5. 结论</title><p>为了充分利用低分辨率图像的作用，提出了双重判别的SAR图像超分辨率重建算法。在高分辨率图像判别的基础上，双重判别利用了低分辨率图像的唯一性并增加了低分辨率图像的判别。双重判别利用高分辨率和低分辨率图像之间的对应关系来指导网络的训练和高频纹理细节的合成。另外，TRDN结合了来自不同层的变换特征，提高了参数利用率。在SAR图像上进行的实验表明，提出的DGAN算法的性能比最新的超分辨算法更好。</p></sec><sec id="s10"><title>文章引用</title><p>肖光义,董张玉,杨学志. 双重判别的SAR图像超分辨率重建SAR Image Super-Resolution Reconstruction Based on Dual Discrimination[J]. 计算机科学与应用, 2021, 11(06): 1617-1626. https://doi.org/10.12677/CSA.2021.116167</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.42870-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Ledig, C., Theis, L., Huszár, F., Caballero, J., Aitken, A., Tejani, A., Totz, J., Wang, Z. and Shi, W. (2017) Pho-to-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. 2017 IEEE Conference on Com-puter Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 105-114.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.19</mixed-citation></ref><ref id="hanspub.42870-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y.-H., Dong, C., Loy, C.C., Qiao, Y. and Tang, X. (2018) ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks. European Conference on Computer Vision Workshops, Munich, 8-14 September 2018, 63-79. &lt;br&gt;https://doi.org/10.1007/978-3-030-11021-5_5</mixed-citation></ref><ref id="hanspub.42870-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Pathak, H., Li, X., Minaee, S. and Cowan, B. (2018) Efficient Super Resolution for Large-Scale Images Using Attentional GAN. 2018 IEEE International Conference on Big Data (Big Data), Seattle, 10-13 December 2018, 1777-1786.  
&lt;br&gt;https://doi.org/10.1109/BigData.2018.8622477</mixed-citation></ref><ref id="hanspub.42870-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Lim, B., Son, S., Kim, H., Nah, S. and Lee, K.M. (2017) En-hanced Deep Residual Networks for Single Image Super-Resolution. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, Honolulu, 21-26 July 2017, 1132-1140. &lt;br&gt;https://doi.org/10.1109/CVPRW.2017.151</mixed-citation></ref><ref id="hanspub.42870-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Freeman, W., Jones, T. and Pasztor, E. (2002) Example-Based Super-Resolution. IEEE Computer Graphics and Applications, 22, 56-65. &lt;br&gt;https://doi.org/10.1109/38.988747</mixed-citation></ref><ref id="hanspub.42870-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Huang, J.-B., Singh, A. and Ahuja, N. (2015) Single Image Su-per-Resolution from Transformed Self-Exemplars. 2015 IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 5197-5206.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2015.7299156</mixed-citation></ref><ref id="hanspub.42870-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Yang, J., Wright, J., Huang, T. and Ma, Y. (2010) Image Su-per-Resolution via Sparse Representation. IEEE Transactions on Image Processing, 19, 2861-2873. &lt;br&gt;https://doi.org/10.1109/TIP.2010.2050625</mixed-citation></ref><ref id="hanspub.42870-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Gu, S., Zuo, W., Xie, Q., Meng, D., Feng, X. and Zhang, L. (2015) Convolutional Sparse Coding for Image Super-Resolution. 2015 IEEE International Conference on Computer Vision, Santiago, 7-13 December 2015, 1823-1831.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2015.212</mixed-citation></ref><ref id="hanspub.42870-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Dong, C., Loy, C.C., He, K. and Tang, X. (2016) Image Su-per-Resolution Using Deep Convolutional Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 295-307.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2015.2439281</mixed-citation></ref><ref id="hanspub.42870-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Dong, C., Loy, C.C. and Tang, X. (2016) Accelerating the Super-Resolution Convolutional Neural Network. European Conference on Computer Vision, Amsterdam, 8-16 October 2016, 391-407.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46475-6_25</mixed-citation></ref><ref id="hanspub.42870-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A., Bishop, R., Rueckert, D. and Wang, Z. (2016) Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network. 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 1874-1883.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.207</mixed-citation></ref><ref id="hanspub.42870-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Tong, T., Li, G., Liu, X. and Gao, Q. (2017) Image Su-per-Resolution Using Dense Skip Connections. 2017 IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 4809-4817.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.514</mixed-citation></ref><ref id="hanspub.42870-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Y., Tian, Y., Kong, Y., Zhong, B. and Fu, Y. (2018) Residual Dense Network for Image Super-Resolution. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 2472-2481. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00262</mixed-citation></ref><ref id="hanspub.42870-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X. and Tang, X. (2017) Residual Attention Network for Image Classifica-tion. 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 6450-6458. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.683</mixed-citation></ref><ref id="hanspub.42870-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Y., Li, K.P., Li, K., Wang, L., Zhong, B. and Fu, Y. (2018) Image Super-Resolution Using Very Deep Residual Channel Attention Networks. European Conference on Computer Vision 2018, Munich, 8-14 September, 294-310.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01234-2_18</mixed-citation></ref><ref id="hanspub.42870-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Muqeet, A., Tauhid Bin Iqbal, Md. and Bae, S.-H. (2019) HRAN: Hybrid Residual Attention Network for Single Image Super-Resolution. IEEE Access, 7, 137020-137029. &lt;br&gt;https://doi.org/10.1109/ACCESS.2019.2942346</mixed-citation></ref><ref id="hanspub.42870-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C. and Bengio, Y. (2014) Generative Adversarial Nets. Neural Information Processing Systems 2014, Bangkok, 23-27 November 2014, 2672-2680.</mixed-citation></ref><ref id="hanspub.42870-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Karras, T., Laine, S. and Aila, T. (2019) A Style-Based Generator Architecture for Generative Adversarial Networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, 15-20 June 2019, 4396-4405.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2019.00453</mixed-citation></ref><ref id="hanspub.42870-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Phillip, I., Zhu, J.-Y., Zhou, T. and Efros, A.A. (2017) Im-age-to-Image Translation with Conditional Adversarial Networks. 2017 IEEE Conference on Computer Vision and Pat-tern Recognition, Honolulu, July 2017, 5967-5976.</mixed-citation></ref><ref id="hanspub.42870-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, J.-Y., Park, T., Isola, P. and Efros, A.A. (2017) Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. 2017 IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 2242-2251. &lt;br&gt;https://doi.org/10.1109/ICCV.2017.244</mixed-citation></ref><ref id="hanspub.42870-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Alexia, J.-M. (2019)The Relativistic Discriminator: A Key Element Missing from Standard GAN. ArXiv abs/1807.00734.</mixed-citation></ref><ref id="hanspub.42870-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Sajjadi, M.S.M., Schölkopf, B. and Hirsch, M. (2017) EnhanceNet: Single Image Su-per-Resolution through Automated Texture Synthesis. 2017 IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 4501-4510.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.481</mixed-citation></ref><ref id="hanspub.42870-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Park, S., Son, H., Cho, S., Hong, K.-S. and Lee, S. (2018) SRFeat: Single Image Super-Resolution with Feature Discrimination. European Conference on Computer Vision 2018, Munich, 8-14 September, 455-471.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01270-0_27</mixed-citation></ref></ref-list></back></article>