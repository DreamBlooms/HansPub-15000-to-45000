<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">SA</journal-id><journal-title-group><journal-title>Statistics and Application</journal-title></journal-title-group><issn pub-type="epub">2325-2251</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/SA.2020.91006</article-id><article-id pub-id-type="publisher-id">SA-34033</article-id><article-categories><subj-group subj-group-type="heading"><subject>SA20200100000_29812316.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  主成分分析与线性判别分析降维比较
  Dimension Reduction Comparison between PCA and LDA
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>保</surname><given-names>丽红</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>云南财经大学，云南 昆明</addr-line></aff><pub-date pub-type="epub"><day>02</day><month>01</month><year>2020</year></pub-date><volume>09</volume><issue>01</issue><fpage>47</fpage><lpage>52</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    主成分分析(PCA)和线性判别分析(LDA)是机器学习领域中常用的降维方法。本文针对矩阵型数据结构，将一维的降维方法PCA和LDA推广为二维PCA和二维LDA，2DPCA和2DLDA对矩阵型数据进行降维处理时，克服了维数灾难的问题。实验研究表明，对比降维效果和分类错误率，2DLDA相比2DPCA是一种更为出色的降维分类方法。
    Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are commonly used in machine learning. In this paper, we extend PCA and LDA to 2DPCA and 2DLDA, 2DPCA and 2DLDA are directly to reduce the dimension on the original matrix data structure, overcoming the problem of the curse of dimensionality. Empirical research suggests that 2DLDA is a better dimensionality reduction classification method than 2DPCA compared with the effect of dimension reduction and the error rate of classification. 
  
 
</p></abstract><kwd-group><kwd>主成分分析，线性判别分析，矩阵型数据, PCA</kwd><kwd> LDA</kwd><kwd> Matrix Data</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>主成分分析与线性判别分析降维比较<sup> </sup></title><p>保丽红</p><p>云南财经大学，云南 昆明</p><p>收稿日期：2019年12月27日；录用日期：2020年1月12日；发布日期：2020年1月19日</p><disp-formula id="hanspub.34033-formula32"><graphic xlink:href="//html.hanspub.org/file/6-2580569x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>主成分分析(PCA)和线性判别分析(LDA)是机器学习领域中常用的降维方法。本文针对矩阵型数据结构，将一维的降维方法PCA和LDA推广为二维PCA和二维LDA，2DPCA和2DLDA对矩阵型数据进行降维处理时，克服了维数灾难的问题。实验研究表明，对比降维效果和分类错误率，2DLDA相比2DPCA是一种更为出色的降维分类方法。</p><p>关键词 :主成分分析，线性判别分析，矩阵型数据</p><disp-formula id="hanspub.34033-formula33"><graphic xlink:href="//html.hanspub.org/file/6-2580569x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/6-2580569x7_hanspub.png" /> <img src="//html.hanspub.org/file/6-2580569x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>在数据降维处理中 [<xref ref-type="bibr" rid="hanspub.34033-ref1">1</xref>]，比较常用的方法为线性变换技术。即通过线性投影将原样本数据投影到低维子空间中，其中典型的有PCA与LDA。PCA的意义是在重构时其平方误差最小；LDA是一种有监督的线性降维算法 [<xref ref-type="bibr" rid="hanspub.34033-ref2">2</xref>]，其基本思想是选择使得Fisher准则函数达到极值的向量作为最佳投影方向，从而使得样本在该方向上投影后，达到最大的类间散布距离和最小的类内散布距离。PCA和LDA的不同之处在于，无监督的PCA能保持数据信息，而LDA是使降维后的数据点尽可能地容易被区分。</p><p>针对矩阵型的数据结构，我们将一维的降维方法PCA和LDA在矩阵模式上推广为二维PCA和二维LDA。采用二维降维方法2DPCA和2DLDA，其最大的优势于是不需要将高数据转化向量，克服了维数灾难 [<xref ref-type="bibr" rid="hanspub.34033-ref3">3</xref>]。</p><p>本文将在真实的两个数据集上，验证在不同的多元时间序列数据集下，2DPCA和2DLDA两种数据降维方法效果的优劣。</p></sec><sec id="s4"><title>2. 研究方法概述</title><p>本节分别介绍二维主成分分析(2DPCA)算法，二维线性判别分析(2DLDA)算法。</p><sec id="s4_1"><title>2.1. 二维主成分分析(2DPCA)算法</title><p>令 X = { x i } i = 1 N 是一组样本， x i ∈ ℝ r &#215; c ，则样本平均值为：</p><p>X &#175; = 1 N ∑ i = 1 N x i (1)</p><p>定义协方差矩阵为 G ：</p><p>G = 1 N ∑ i = 1 N ( x i − X &#175; ) ( x i − X &#175; ) T (2)</p><p>其中 G 为 r &#215; r 的非负定矩阵。对 G 进行特征值分解，最大的 d 个特征值所对应的标准正交的特征向量构成投影向量组 U = ( u 1 , ⋯ , u d ) 。</p></sec><sec id="s4_2"><title>2.2. 二维线性判别分析(2DLDA)算法</title><p>令 X = { x i } i = 1 N 是一组样本， x i ∈ ℝ r &#215; c ，其中 X 分为 π i ( i = 1 , 2 , ⋯ , k ) 类， n i 为第 i 类样本的个数，</p><p>则样本均值为：</p><p>X &#175; = 1 N ∑ i = 1 N x i (3)</p><p>第 i 类样本的类内平均：</p><p>X &#175; π i = 1 n i ∑ x j ∈ π i n i x j (4)</p><p>定义如下类间散步矩阵：</p><p>S b = 1 N ∑ i = 1 k n i ( X &#175; π i − X &#175; ) ( X &#175; π i − X &#175; ) ′ (5)</p><p>类内散布矩阵：</p><p>S w = 1 N ∑ i = 1 k ∑ x ∈ π i ( x − X &#175; π i ) ( x − X &#175; π i ) ′ (6)</p><p>2DLDA寻找的最佳投影矩阵 U ：</p><p>J ( U ) = U ′ S b U U ′ S w U 。 (7)</p></sec></sec><sec id="s5"><title>3. 实证分析</title><p>本节将在Wafer、Ausla这两个真实的数据集，验证在不同的数据集下，2DPCA和2DLDA两种数据降维方法效果的优劣。</p><sec id="s5_1"><title>3.1. 数据集介绍</title><sec id="s5_1_1"><title>3.1.1. Wafer数据集</title><p>Wafer数据集是由一个真空传感器应用一个硅晶片在刻画中记录下来的。该数据集记录的晶片类型分为两个类型：“正常”与“不正常”。其中“正常”类型的样本数为1067个，“不正常”类型的样本数为127个。</p></sec><sec id="s5_1_2"><title>3.1.2. AUSLAN数据集</title><p>AUSLAN数据集有2565个数据样本，包含95个语音信号，每个信号由27个样本组成，其中每个样本的长度在47到95之间。每一个样本有22个变量，记录了这25个由当地人发音的语音信息。</p><p>2个数据集描述见表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Data set description summar</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Wafer</th><th align="center" valign="middle" >AUSLAN</th></tr></thead><tr><td align="center" valign="middle" >变量数</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >22</td></tr><tr><td align="center" valign="middle" >最大样本长度</td><td align="center" valign="middle" >198</td><td align="center" valign="middle" >95</td></tr><tr><td align="center" valign="middle" >最小样本长度</td><td align="center" valign="middle" >104</td><td align="center" valign="middle" >47</td></tr><tr><td align="center" valign="middle" >分类个数</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >25</td></tr><tr><td align="center" valign="middle" >样本量</td><td align="center" valign="middle" >327</td><td align="center" valign="middle" >675</td></tr></tbody></table></table-wrap><p>表1. 数据集描述汇总</p></sec></sec><sec id="s5_2"><title>3.2. 降维效果</title><p>为了降低实验结果的变化性，我们在每次数据集上重复5次实验，下面分别给出了各个数据集2DPCA和2DLDA的实验结果。</p><sec id="s5_2_1"><title>3.2.1. Wafer数据集上的实验效果</title><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Error rate results and dimension reduction results of 2DPCA and 2DLDA on Wafer datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >实验 次数</th><th align="center" valign="middle"  rowspan="2"  >初始 维数</th><th align="center" valign="middle"  colspan="2"  >2DPCA</th><th align="center" valign="middle"  colspan="2"  >2DLDA</th></tr></thead><tr><td align="center" valign="middle" >错误率</td><td align="center" valign="middle" >维数</td><td align="center" valign="middle" >错误率</td><td align="center" valign="middle" >维数</td></tr><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >0.1523</td><td align="center" valign="middle" >[8, 4]</td><td align="center" valign="middle" >0.0457</td><td align="center" valign="middle" >[12, 5]</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >0.1421</td><td align="center" valign="middle" >[9, 5]</td><td align="center" valign="middle" >0.0660</td><td align="center" valign="middle" >[27, 5]</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >[104, 6]</td><td align="center" valign="middle" >0.1624</td><td align="center" valign="middle" >[8, 4]</td><td align="center" valign="middle" >0.0660</td><td align="center" valign="middle" >[28, 5]</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >0.1777</td><td align="center" valign="middle" >[9, 5]</td><td align="center" valign="middle" >0.0508</td><td align="center" valign="middle" >[1, 6]</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >0.1066</td><td align="center" valign="middle" >[14, 6]</td><td align="center" valign="middle" >0.0457</td><td align="center" valign="middle" >[4, 5]</td></tr></tbody></table></table-wrap><p>表2. 2DPCA和2DLDA在Wafer数据集上的错误率结果和降维结果</p><p>表2给出了2DPCA和2DLDA在Wafer数据集上的5次实验的错误率结果和降维结果。由表2可知，相比较，2DPCA的降维效果比2DLDA的降维效果好，但2DPCA的分类错误率比2DLDA的高。图1直观呈现出两种方法实验的分类错误率。</p><p>图1. 2DPCA和2DLDA在wafer数据集上的错误率</p></sec><sec id="s5_2_2"><title>3.2.2. AUSLAN数据集上的实验效果</title><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Error rate results and dimensionality reduction results of 2DPCA and 2DLDA on the AUSLAN datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >实验 次数</th><th align="center" valign="middle"  rowspan="2"  >初始 维数</th><th align="center" valign="middle"  colspan="2"  >2DPCA</th><th align="center" valign="middle"  colspan="2"  >2DLDA</th></tr></thead><tr><td align="center" valign="middle" >错误率</td><td align="center" valign="middle" >维数</td><td align="center" valign="middle" >错误率</td><td align="center" valign="middle" >维数</td></tr><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >0.0500</td><td align="center" valign="middle" >[1, 21]</td><td align="center" valign="middle" >0.0300</td><td align="center" valign="middle" >[1, 21]</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >0.0733</td><td align="center" valign="middle" >[1, 20]</td><td align="center" valign="middle" >0.0233</td><td align="center" valign="middle" >[1, 22]</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >[47, 22]</td><td align="center" valign="middle" >0.0500</td><td align="center" valign="middle" >[1, 20]</td><td align="center" valign="middle" >0.0300</td><td align="center" valign="middle" >[2, 20]</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >0.0500</td><td align="center" valign="middle" >[1, 20]</td><td align="center" valign="middle" >0.0233</td><td align="center" valign="middle" >[1, 20]</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >0.0633</td><td align="center" valign="middle" >[1, 21]</td><td align="center" valign="middle" >0.0233</td><td align="center" valign="middle" >[1, 21]</td></tr></tbody></table></table-wrap><p>表3. 2DPCA和2DLDA在AUSLAN数据集上的错误率结果和降维结果</p><p>表3给出了2DPCA和2DLDA在AUSLAN数据集上的5次实验的错误率结果和降维结果。由表3可知，相比较，2DPCA的降维效果与2DLDA的降维效果差别不大，但2DPCA的分类错误率明显比2DLDA的高。图2直观呈现出两种方法实验的分类错误率。</p><p>图2. 2DPCA和2DLDA在AUSLAN数据集上的错误率</p></sec></sec></sec><sec id="s6"><title>4. 结论</title><p>通过提取2DPCA和2DLDA分别在wafer、Auslan这两个真实的数据集上的最小错误率，我们确定了用不同方法进行降维的最佳维数，通过每种方法降维的最佳维数提取了与之相对应的错误率，最后我们求出相应的平均错误率，如表4所示。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Average classification error rate results and dimensionality reduction results of 2DPCA and 2DLDA on 3 dataset</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >数据集</th><th align="center" valign="middle"  colspan="2"  >2DPCA</th><th align="center" valign="middle"  colspan="2"  >2DLDA</th></tr></thead><tr><td align="center" valign="middle" >错误率</td><td align="center" valign="middle" >维数</td><td align="center" valign="middle" >错误率</td><td align="center" valign="middle" >维数</td></tr><tr><td align="center" valign="middle" >wafer</td><td align="center" valign="middle" >0.1534</td><td align="center" valign="middle" >[9, 6]</td><td align="center" valign="middle" >0.0670</td><td align="center" valign="middle" >[2, 5]</td></tr><tr><td align="center" valign="middle" >Auslan</td><td align="center" valign="middle" >0.0653</td><td align="center" valign="middle" >[1, 20]</td><td align="center" valign="middle" >0.0320</td><td align="center" valign="middle" >[2, 20]</td></tr></tbody></table></table-wrap><p>表4. 2DPCA和2DLDA在3个数据集上的平均分类错误率结果和降维结果</p><p>由表4可知：2DLDA的平均分类错误率均小于2DPCA。这说明判别分析的分类效果要优于主成分分析的分类效果。因此，综合降维效果和分类错误率这两个因素，对比实验证实了2DLDA相比2DPCA是一种更为出色的分类方法。</p></sec><sec id="s7"><title>文章引用</title><p>保丽红. 主成分分析与线性判别分析降维比较Dimension Reduction Comparison between PCA and LDA[J]. 统计学与应用, 2020, 09(01): 47-52. https://doi.org/10.12677/SA.2020.91006</p></sec><sec id="s8"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.34033-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, K. and Kwok, J.T. (2010) Clustered Nyström Method for Large Scale Manifold Learning and Dimension Reduction. IEEE Transactions on Neural Networks, 21, 1576-1587. &lt;br&gt;https://doi.org/10.1109/TNN.2010.2064786</mixed-citation></ref><ref id="hanspub.34033-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Zuo, W.M., Zhang, D., Yang, J. and Wang, K.Q. (2006) BDPCA Plus LDA: A Novel Fast Feature Extraction Technique for Face Recognition. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 36, 946-953.  
&lt;br&gt;https://doi.org/10.1109/TSMCB.2005.863377</mixed-citation></ref><ref id="hanspub.34033-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Xie, X.C., Yan, S.C., Kwok, J.T. and Huang, T.S. (2008) Matrix-Variate Factor Analysis and Its Applications. IEEE Transactions on Neural Networks, 19, 1821-1826. &lt;br&gt;https://doi.org/10.1109/TNN.2008.2004963</mixed-citation></ref></ref-list></back></article>