<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2021.105178</article-id><article-id pub-id-type="publisher-id">AAM-42682</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20210500000_62863820.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  广义线性模型的平方根Lasso选择性推断
  Selective Inference of Generalized Linear Models by the Square Root Lasso
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>梁</surname><given-names>博</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>石</surname><given-names>翔宇</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>齐</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff2"><addr-line>青岛大学，山东 青岛</addr-line></aff><pub-date pub-type="epub"><day>12</day><month>05</month><year>2021</year></pub-date><volume>10</volume><issue>05</issue><fpage>1668</fpage><lpage>1680</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  已经有很多人对线性模型的相关问题做出了大量的选择性推断工作。但是，其适用范围并不全面。实际上，我们会遇到很多非正态非连续的数据，并且随机误差项是异方差等方面的问题，故本文将一些选择性推断的工作推广到广义线性模型上。我们建议使用数据雕琢方法，对数据样本先拆分再进行推断。另外，很多的模型变量选择的工作，都是基于已知噪声水平的。但是，在很多实际情况下，误差方差是未知的，而且在高维数据中，对它的估计存在一定难度。本文中，我们使用平方根lasso，进行变量选择，证明出对于广义线性模型，调优参数的选择不受数据中噪声波动的影响。因此，平方根lasso比lasso调优参数选择更方便，应用范围更广泛。模拟结果表明，使用数据雕琢得到的参数的置信区间更小。
   In view of the fact that many people have done a lot of work on the selective inference of linear models, however, its scope of application is not comprehensive. In fact, we will encounter many problems such as non normal and non continuous data, and the random error term is heteroscedasticity. Therefore, this paper extends some selective inference work to generalized linear model. We suggest that we use the data carving to split the sample first and then infer. In addition, many of the work of model variable selection are based on the known noise level. However, in practical cases, the error variance is unknown, and in high-dimensional data, it is difficult to estimate. In this paper, we use the square root lasso to select variables, and prove that for the generalized linear model, the selection of tuning parameters is not affected by the noise fluctuations in the data. So the square root lasso is more convenient and widely used than the lasso, and the application is more extensive. Simulation shows that we get the shorter interval length by data carving.
 
</p></abstract><kwd-group><kwd>广义线性模型，平方根Lasso，一致最优势无偏检验，置信区间, Generalized Linear Model</kwd><kwd> The Square Root Lasso</kwd><kwd> Uniformly Most Powerful Unbiased Test</kwd><kwd> 
Confidence Interval</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>已经有很多人对线性模型的相关问题做出了大量的选择性推断工作。但是，其适用范围并不全面。实际上，我们会遇到很多非正态非连续的数据，并且随机误差项是异方差等方面的问题，故本文将一些选择性推断的工作推广到广义线性模型上。我们建议使用数据雕琢方法，对数据样本先拆分再进行推断。另外，很多的模型变量选择的工作，都是基于已知噪声水平的。但是，在很多实际情况下，误差方差是未知的，而且在高维数据中，对它的估计存在一定难度。本文中，我们使用平方根lasso，进行变量选择，证明出对于广义线性模型，调优参数的选择不受数据中噪声波动的影响。因此，平方根lasso比lasso调优参数选择更方便，应用范围更广泛。模拟结果表明，使用数据雕琢得到的参数的置信区间更小。</p></sec><sec id="s2"><title>关键词</title><p>广义线性模型，平方根Lasso，一致最优势无偏检验，置信区间</p></sec><sec id="s3"><title>Selective Inference of Generalized Linear Models by the Square Root Lasso</title><p>Bo Liang, Xiangyu Shi, Qi Zhang<sup>*</sup></p><p>Qingdao University, Qingdao Shandong</p><p><img src="//html.hanspub.org/file/29-2621626x4_hanspub.png" /></p><p>Received: Apr. 25<sup>th</sup>, 2021; accepted: May 8<sup>th</sup>, 2021; published: May 27<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/29-2621626x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In view of the fact that many people have done a lot of work on the selective inference of linear models, however, its scope of application is not comprehensive. In fact, we will encounter many problems such as non normal and non continuous data, and the random error term is heteroscedasticity. Therefore, this paper extends some selective inference work to generalized linear model. We suggest that we use the data carving to split the sample first and then infer. In addition, many of the work of model variable selection are based on the known noise level. However, in practical cases, the error variance is unknown, and in high-dimensional data, it is difficult to estimate. In this paper, we use the square root lasso to select variables, and prove that for the generalized linear model, the selection of tuning parameters is not affected by the noise fluctuations in the data. So the square root lasso is more convenient and widely used than the lasso, and the application is more extensive. Simulation shows that we get the shorter interval length by data carving.</p><p>Keywords:Generalized Linear Model, The Square Root Lasso, Uniformly Most Powerful Unbiased Test, Confidence Interval</p><disp-formula id="hanspub.42682-formula60"><graphic xlink:href="//html.hanspub.org/file/29-2621626x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/29-2621626x8_hanspub.png" /> <img src="//html.hanspub.org/file/29-2621626x9_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>完整的统计调查包括模型选择和推断两个过程。选择过程：我们使用数据来选择模型、进而确定出模型的假设检验、以及关于模型的任何其他问题。推断过程：我们根据所选择出的模型和数据，回答选择过程中提出的问题。简单地说，选择过程决定要问什么问题，推断过程决定回答那些问题。我们以前所做的工作大都是先用所有的数据进行模型选择，然后再进行模型推断。这样两个过程会重复利用数据，造成结果并不那么地精确。以至于，后来统计学家们思考有没有更简单精确的方法进行模型的选择性推断，数据(样本)分割方法 [<xref ref-type="bibr" rid="hanspub.42682-ref1">1</xref>] 将数据分成两个独立随机部分，其中一部分数据用于模型选择，另一部分用于推断。Wasserman &amp; Roeder [<xref ref-type="bibr" rid="hanspub.42682-ref2">2</xref>] 使用样本分割来解决高维模型中的一致变量选择问题。Meinshausen et al. [<xref ref-type="bibr" rid="hanspub.42682-ref3">3</xref>] 通过数据分割处理高维回归模型中的显著性问题，由此产生的p值可以控制错误发现率(FDR)和族错误率(FWER)。样本分割方法解决了控制选择性误差的问题，但这种方法减少了用于模型选择和推断过程中的数据量，成本较高。本文采用数据雕琢的方法，利用其中的一部分数据进行模型选择，另一部分数据和模型选择的剩余数据进行推断，它相对于数据分割方法提高了对于数据的利用率。</p><p>针对于线性模型，它仅在响应变量为正态随机变量时适用。Nelder [<xref ref-type="bibr" rid="hanspub.42682-ref4">4</xref>] 和Wedderburn [<xref ref-type="bibr" rid="hanspub.42682-ref5">5</xref>] 将线性模型推广到广义线性模型。广义线性模型可以适用于以下情况：响应变量为正态和非正态变量，响应变量的分布可以是任意指数族分布。比如，二项分布和伽马分布。可以看出，广义线性模型能很好地解决回归模型中随机误差项的异方差问题。所以，把线性模型推广到广义线性模型上研究问题有重要意义。</p><p>近些年来，如生物信息、金融管理等领域产生的高维数据为模型选择带来了更大的挑战。这些领域的实验数据的维数有的甚至超过样本量大小。面对这样的高维数据，我们需要建立一个合适的数学模型，并且还要尽量使用少而有效的数据来进行建模分析，这就产生了一些新的模型选择方法。传统的模型选择方法包括最优子集选择法 [<xref ref-type="bibr" rid="hanspub.42682-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.42682-ref7">7</xref>]，AIC [<xref ref-type="bibr" rid="hanspub.42682-ref8">8</xref>] 和BIC [<xref ref-type="bibr" rid="hanspub.42682-ref9">9</xref>] 等。当然，还有现在常用的模型选择方法，Robert Tibshirani于1996年提出使用lasso [<xref ref-type="bibr" rid="hanspub.42682-ref10">10</xref>]，采用惩罚项压缩一些不重要的变量使其系数变为零。此外，与岭回归相比较，lasso对于重要变量的系数压缩较轻，这样提高了参数估计的精度。自适应lasso [<xref ref-type="bibr" rid="hanspub.42682-ref11">11</xref>] 和Relaxed lasso [<xref ref-type="bibr" rid="hanspub.42682-ref12">12</xref>] 可以提高参数估计的准确性和一致性。Belloni et al. [<xref ref-type="bibr" rid="hanspub.42682-ref13">13</xref>] 提出平方根lasso调优参数γ的选择与误差方差无关。这是相较于lasso方法，用平方根lasso选择模型的一个优点。利用平方根lasso选择变量对正态分布数据进行了大量的研究，但对非正态分布数据的研究还比较少。在本文中，我们尝试使用平方根lasso为广义线性模型选择变量，研究其拥有的特点。</p><p>在模型选择后，研究选定模型的推断工作，很多学者已经做了大量的工作。Lockhart et al. [<xref ref-type="bibr" rid="hanspub.42682-ref14">14</xref>] 基于lasso拟合值，推导出了原假设下的渐近检验统计量。Lee &amp; Taylor [<xref ref-type="bibr" rid="hanspub.42682-ref15">15</xref>]，Lee et al. [<xref ref-type="bibr" rid="hanspub.42682-ref16">16</xref>] 提出使用固定调优参数值的lasso进行精确检验。Belloni [<xref ref-type="bibr" rid="hanspub.42682-ref17">17</xref>]，Belloni et al. [<xref ref-type="bibr" rid="hanspub.42682-ref18">18</xref>]，Zhang &amp; Zhang [<xref ref-type="bibr" rid="hanspub.42682-ref19">19</xref>]，重点研究高维稀疏回归模型的推断。Tian et al. [<xref ref-type="bibr" rid="hanspub.42682-ref20">20</xref>] 提出在高维线性回归模型中，用平方根lasso进行方差未知的选择性推断。Lehmann &amp; Romano [<xref ref-type="bibr" rid="hanspub.42682-ref21">21</xref>] 主要处理了关于指数族的检验问题。Lehmann &amp; Scheff&#233; [<xref ref-type="bibr" rid="hanspub.42682-ref22">22</xref>] 提出了一种对于指数族的一致最优势无偏(UMPU)检验的构造方法。Benjamini &amp; Hochberg [<xref ref-type="bibr" rid="hanspub.42682-ref23">23</xref>] 提出通过控制FDR来解决多重显著性检验问题。Fithian et al. [<xref ref-type="bibr" rid="hanspub.42682-ref24">24</xref>] 提出了通过控制选择性第I型错误来进行模型选择后的推断。Shi et al. [<xref ref-type="bibr" rid="hanspub.42682-ref25">25</xref>] 将后选推断推广到广义线性模型，并提出了使用惩罚最小二乘法进行后选推断的新方法。综上所述，大部分工作是在数据服从正态分布时，对线性模型的选择性推断，而对高维异方差数据的研究较少。因此，我们基于平方根lasso方法对广义线性模型的选择性推断。</p><p>本篇文章的研究内容主要包括以下几个部分。在第二章中，我们提出广义线性模型的选择性推断包括两个过程：模型选择阶段和模型推断。我们应该注意控制在选定模型下的选择性第I类错误率。第三章，指出了我们使用平方根lasso和lasso选择变量，并且研究了选择事件的特征形式，将选择事件简写成有关于数据的表达式。在第四章中，我们给出了指数族分布的UMPU检验。第五章，给出了指数族分布的参数的充分统计量，以及参数所选模型下的分布形式。第六章，是对于线性回归模型和逻辑回归模型的模拟。</p></sec><sec id="s6"><title>2. 广义线性模型</title><p>对于给定的数据 ( x i , y i ) , i = 1 , 2 , ⋯ , n ，是由数据 x i = ( x i 1 , x i 2 , ⋯ , x i p ) 和数据 y i , i = 1 , 2 , ⋯ , n 组成，令 X = { x i j } 是设计矩阵，响应向量 y ∈ R n 。使用下面的广义线型模型来描述设计矩阵和响应向量的关系，</p><p>g ( y ) = X β + e 2-(1)</p><p>其中， β 是一个p维参数向量，e是一个n维的随机向量，假设它满足以下条件： E ( e ) = 0 ， V a r ( e ) = σ 2 I n 。g(&#183;)是连接函数，这里把响应向量和设计矩阵二者进行了连接。</p><p>在广义线性模型中，被解释向量y服从的指数族分布可以被表示成如下的形式，</p><p>y ~ exp { y X β − B ( X β ) a ( φ ) } + c ( y , φ ) 2-(2)</p><p>其中， β 是感兴趣参数， φ 是讨厌参数。</p><p>在没有使用选择后推断方法的时候，统计人员在进行分析数据之前，往往根据经验指定模型一个模型M以及要检验的假设。在选定模型M和指定的原假设下，对于原假设的水平 α 检验，必须控制第I型错误率，</p><p>P M , H 0 ( reject H 0 ) ≤ α 2-(3)</p><p>从上面的公式可以看到，在计算第I类错误率时，是假定了数据来自模型M，和原假设为真的条件。此时，若我们选定的模型M是不合理的，我们不能保证上面的拒绝率是小于水平 α 的。</p><p>因为在大多数的统计工作中，完全排除模型选择这一过程也是不现实的。所以统计专家会利用数据去检查他们的模型，如果诊断出问题，他们就会调整自己的模型。我们认为，如果模型的确定，是通过先用我们手里的数据进行模型选择的，随之，确定原假设。我们就能有效地控制选择性第I型错误率，</p><p>P M , H 0 ( reject H 0 | M selected ) ≤ α 2-(4)</p><p>如果用于模型选择和推断的数据是独立的，则上述条件概率(2-(4))可以写成无条件概率(2-(3))，从而得到如下公式，</p><p>P M , H 0 ( reject H 0 | M selected ) = P M , H 0 ( reject H 0 ) ≤ α 2-(5)</p><p>由此，我们通过数据分割方式将数据y分成两部分，即数据 y = ( y 1 , y 2 ) ，让y<sub>1</sub>独立于y<sub>2</sub>。那么，将样本数据y<sub>1</sub>用于模型选择，样本推断过程仅仅依赖于数据y<sub>2</sub>。基于数据y<sub>2</sub>的水平 α 的检验满足公式(2-(4))，因此，基于数据y<sub>2</sub>的检验可以控制选择性第I类错误率(2-(3))。</p><p>数据拆分方法方便了我们的模型选择和推断的工作，也易于让人们理解，这就是当时这种方法被提出后大受欢迎的理由。但是，该方法也同时减少了模型选择和推断过程中的数据量，要付出很大代价。本文采用数据雕刻的方法，利用一部分数据y<sub>1</sub>进行模型选择，另一部分数据y<sub>2</sub>和选择模型中的数据一起来用于推断。此方法与数据分割方法相比，提高了数据的利用率，进而能够提高参数估计的精度。</p></sec><sec id="s7"><title>3. 通过平方根Lasso选择变量</title><p>对于广义线性模型，我们考虑使用平方根lasso选择变量。它是对lasso方法的修正，通过解下面的公式，可以得到 β 的最优估计，</p><p>β ^ = arg min ‖ g ( y ) − X β ‖ 2 + γ ‖ β ‖ 1 3-(1)</p><p>其中，参数 γ 是调优参数。第一项是广义线性模型的最小二乘目标函数，第二项是平方根lasso的惩罚项，使得许多的变量系数变成零，由此，平方根lasso产生了稀疏解，保留了lasso方法选变量的优点，并且通过定理1可观察到在调优参数的选择上要更加方便。</p><p>定理1. 平方根lasso调优参数 γ 的选择独立于方差 σ 。</p><p>以下集合定义为平方根lasso选择的模型，</p><p>M ^ = { j : β ^ j ≠ 0 }</p><p>平方根lasso选择的变量符号定义如下，</p><p>s ^ = s i g n { β ^ j : β ^ j ≠ 0 }</p><p>注意到事件 { M ^ = M } 是通过平方根lasso进行变量选择产生的，接下来我们将继续注意这个选择事件。并将此选择事件进行具体的公式表达。</p><p>对于(3-(1))式的KKT条件的解，其结果等价于下式，</p><p>X T ( g ( y ) − X β ^ ) ‖ g ( y ) − X β ^ ‖ 2 = γ s ^ sign ( β ^ M ^ ) = s ^ M ^ ‖ s ^ − M ^ ‖ ∞ ≤ 1 3-(2)</p><p>子集 M ^ = { i ∈ { 1 , 2 , ⋯ , p } : | s ^ j | = 1 } 是使用平方根lasso方法得到的选择模型。 β ^ j ≠ 0 , | s ^ j | = 1 意味着选定模型的系数是非0的。明显地，从公式(3-(2))可以看出，有些系数是0。但对于几乎所有的参数 γ ，上述集合意味着预测变量具有非零系数。选择事件 { M ^ = M } 是通过对对每一个选择事件 { M ^ = M , s ^ = s } 的中的符号函数s求并集得到的，并将选择事件 { M ^ = M , s ^ = s } 表达成数据y的表达式。</p><p>引理1.X是设计矩阵，M和s表示所选变量及其符号的候选集合。给出以下定义，</p><p>v * ( M , s ) = ( X M T X M ) − 1 ( X M T g ( y ) − γ D ( y ) s ) u * ( M , s ) = X − M T ( X M T ) + s + 1 γ D ( y ) X − M T ( I − P M ) g ( y ) 3-(3)</p><p>其中， D ( y ) = ‖ g ( y ) − X β ^ ‖ 2 ， P M = X M ( X M T X M ) − 1 X M T 是 X M 的投影矩阵。选择事件可以被表示成如下形式，</p><p>{ M ^ = M , s ^ = s } = { sign ( v * ( M , s ) ) = s , ‖ u * ‖ ∞ &lt; 1 } 3-(4)</p><p>注意到此时， { ‖ u * ‖ ∞ &lt; 1 } 是无效约束，表示当某些变量未被选入模型 M ^ 。然而，当变量被选择进入</p><p>模型时，使用条件 { sign ( v * ) = s } 表示有效约束。</p><p>引理2. v * 和 u * 是被定义成上面(3-(3))式。它们可以进一步被重写为如下的形式，</p><p>{ sign ( v * ( M , s ) ) = s , ‖ u * ‖ ∞ &lt; 1 } = { ( C 0 * ( M , s ) ( I − P M ) C 1 * ( M , s ) ) g ( y ) &lt; ( b 0 * ( M , s ) b 1 * ( M , s ) ) } 3-(5)</p><p>其中， C 0 * ( I − P M ) ， b 0 * 对应约束条件 { ‖ u * ‖ ∞ &lt; 1 } ， C 1 * ， b 1 * 对应于约束条件 { sign ( v * ( M , s ) ) = s } ，它们被定义成下面的形式，</p><p>C 0 * ( M , s ) = 1 γ D ( y ) ( X − M − X − M T )</p><p>b 0 * ( M , s ) = ( 1 − X − M T ( X − M T ) + s 1 + X − M T ( X − M T ) + s )</p><p>C 1 * ( M , s ) = − diag ( s ) ( X M T X M ) − 1 X M T</p><p>b 1 * ( M , s ) = − γ D ( y ) diag ( s ) ( X M T X M ) − 1 s</p><p>定理2. 基于引理1和引理2，令 C = ( C 0 * ( I − P M ) C 1 * ) ， b = ( b 0 * ( M , s ) b 1 * ( M , s ) ) ，我们选择事件的形式，</p><p>{ M ^ = M , s ^ = s } = { C * ( M , s ) g ( y ) &lt; b * ( M , s ) }</p><p>所以，选择事件 { M ^ = M } 被表示成以下关于数据y的不等式的并集，</p><p>{ M ^ = M } = ∪ s ∈ { − 1 , 1 } { C * ( M , s ) g ( y ) &lt; b * ( M , s ) }</p></sec><sec id="s8"><title>4. 广义线性模型的选择性推断</title><sec id="s8_1"><title>4.1. 选定模型下的参数推断</title><p>在上一章中，已经得到了选定的模型 M ^ ，和它所对应的选择性事件 { M ^ = M } 。在本章中，我们考虑以选定的模型 M ^ 作为条件，响应向量y的分布的推断。此分布表示为，</p><p>L { y | { M ^ = M } } 4-(1)</p><p>数据y服从带有干扰参数的p维参数的指数族分布，</p><p>f θ , φ ( y ) = exp { θ ′ T ( y ) + φ ′ U ( y ) − c ( θ , φ ) } h 0 ( y ) 4-(2)</p><p>其中， θ 是感兴趣参数， φ 是讨厌参数。T(y)和U(y)是它们相对应的充分统计量，它们分别是k维和p-k维的向量。</p><p>对于任意给定的事件E，在 y ∈ E 的条件下，y的这个条件分布也是指数族分布，并且与数据y自身的分布拥有相同的参数和充分统计量，即，可以被表达成下面的形式，</p><p>f θ , φ ( y | y ∈ E ) = exp { θ ′ T ( y ) + φ ′ U ( y ) − c E ( θ , φ ) } h 0 ( y ) 1 E ( y ) 4-(3)</p><p>我们上面提到的模型选择事件 { M ^ = M } ，它作为任意事件E的子集，故样本数据y在选定模型 M ^ 下也是服从指数族，且它们的参数和充分统计量都是相同的。例如，在第三章中，我们提到的模型选择事件 { M ^ = M } ，此选择事件被表示成数据y的相关表达式，是事件 { y ∈ E } 的一中特例，故y的条件密度函数具有类似的形式，也可以写成指数族分布，</p><p>f θ , φ ( y | M ^ = M ) = exp { θ * ′ T * ( y ) + φ * ′ U * ( y ) − c * ( θ * ) } h 0 * ( y ) 1 M ^ ( y ) 4-(4)</p><p>其中 θ * ， φ * 为选定模型的参数， T * ( y ) 和 U * ( y ) 为相应的充分统计量。接下来，我们通过消除干扰参数，来对感兴趣的参数进行推断。</p><p>如果一维参数 θ 是感兴趣参数，其它参数 φ 是干扰参数，T和U是它们分别对应的充分统计量，那么，当讨厌参数的充分统计量 { U = u } 作为已知条件时，以下条件分布仅依赖于感兴趣参数 θ ，</p><p>( T | U = u ) ~ ψ θ ( t | u ) = exp { θ ′ t − c ψ ( θ | u ) } ψ 0 ( t | u ) 4-(5)</p><p>上述条件分布通过在U上的条件分布消除了讨厌参数φ，当k=1时，我们得到了充分统计量T的单参数指数族。</p><p>根据公式(4-(4))，在选择事件 { M ^ = M } 下，y的分布仍然是指数族分布。根据公式(4-(5))感兴趣参数 θ 可由下面的条件分布进行推断，</p><p>L θ ( T ( y ) | U ( y ) , M ^ = M )</p></sec><sec id="s8_2"><title>4.2. UMPU选择性检验</title><p>给出了原假设 H 0 : θ ∈ Θ 0 和备择假设 H 1 : θ ∈ Θ 1 。如果满足以下不等式，则水平 α 检验 ϕ ( y ) 可以成为是选择性无偏的，</p><p>p o w ϕ ( θ | E ) = E θ [ ϕ ( y ) | E ] ≥ α , θ ∈ Θ 1 4-(6)</p><p>在所有满足上述公式的水平 α 的检验中，UMPU水平 α 选择性检验的势函数是一致最优的。</p><p>引理3. (UMPU检验)y服从指数族分布(4-(2))，k=1，在水平 α 检验下，考虑以下原假设和备择假设，</p><p>H 0 : θ = θ 0 ↔ H 1 : θ ≠ θ 0 ，</p><p>有UMPU检验 ϕ ( y ) = f ( T ( y ) , U ( y ) ) ，其中，</p><p>f ( t , u ) = { 1 t &lt; d 1 ( u ) 或 t &gt; d 2 ( u ) π i t = d i ( u ) 0 d 1 ( u ) &lt; t &lt; d 2 ( u )</p><p>其中， d i 和 π i 是下面方程组的解，</p><p>E θ 0 [ f ( T , U ) | U = u ] = a E θ 0 [ T f ( T , U ) | U = u ] = a E θ 0 [ f ( T , U ) | U = u ]</p><p>由公式(4-(4))可知，当y服从指数族分布时，条件分布 L ( y | { M ^ = M } ) 也是指数族分布，所以我们得到如下定理。</p><p>定理3. (UMPU选择性检验) y遵循指数族分布(4-(2))，k=1，考虑以下在选择事件 { M ^ = M } 下的原假设和备择假设，</p><p>H 0 : θ = θ 0 ↔ H 1 : θ ≠ θ 0</p><p>有UMPU选择性检验， ϕ ( y ) = f ( T ( y ) , U ( y ) ) ，满足，</p><p>f ( t , u ) = { 1 t &lt; d 1 ( u ) 或 t &gt; d 2 ( u ) π i t = d i ( u ) 0 d 1 ( u ) &lt; t &lt; d 2 ( u )</p><p>这里的 d i 和 π i 满足下面的方程组，</p><p>E θ 0 [ f ( T , U ) | U = u , M ^ = M ] = a E θ 0 [ T f ( T , U ) | U = u , M ^ = M ] = a E θ 0 [ f ( T , U ) | U = u , M ^ = M ]</p><p>在下一章中，让我们关注广义线性模型选择性推断的几个特定示例。</p></sec></sec><sec id="s9"><title>5. 例子</title><p>例1. 针对于前面讨论的指数族框架下的一些结论，本章中，我们打算举几个具体例子。当数据y来自于多元正态分布时，</p><p>y ~ N n ( μ , σ 2 I n ) ， μ = X β</p><p>在选定的模型 M ^ 的指数族分布形式如下，</p><p>y ~ exp { 1 σ 2 β ′ X M y − 1 2 σ 2 ‖ y ‖ 2 2 − C ( β , σ 2 ) }</p><p>若误差方差 σ 2 已知， X ′ j y 是用来推断参数 β 的充分统计量，</p><p>L β j ( X ′ j y | X ′ M \ j y , M ^ = M ) .</p><p>若误差方差 σ 2 未知， ( X ′ j y , ‖ y ‖ 2 2 ) 是参数 ( β j σ 2 , σ 2 ) 的充分统计量，依据下面的分布进行参数推断，</p><p>L β j σ 2 ( X ′ j y | X ′ M \ j y , ‖ y ‖ 2 2 , M ^ = M ) ，</p><p>同样地，对于参数 σ 2 的推断，根据下面的分布，</p><p>L σ 2 ( ‖ y ‖ 2 2 | X ′ M y , M ^ = M )</p><p>例2. 假设数据y由伯努利分布生成，即， y i ~ b ( 1 , p i ) 。</p><p>y i ~ p i y i ( 1 − p i ) y i ,   ln p i 1 − p i = θ i ,   i = 1 , 2 , ⋯ , n</p><p>我们把上面这个分布写成指数族的表达形式，</p><p>y ~ exp { ∑ i = 1 n y i ln p i 1 − p i + c ( p i ) }</p><p>其中，这个分布有n个参数和相应的充分统计量。如果我们想要对感兴趣参数 θ 1 进行推断，则我们先选定需要的模型 { M ^ = M } = { i = 1 } ，和讨厌参数 { θ i } , i = 2 , 3 , ⋯ , n 及所对应的充分统计量，故对它的推断依赖于以下的条件分布，</p><p>L θ 1 ( y 1 | y 2 , ⋯ , y n , { i = 1 } ) ，</p><p>例3. 若数据 y = ( y 1 , ⋯ , y n ) 服从伽马分布 G a m m a ( α , λ ) ，即， y i ~ Γ ( α , λ ) ，</p><p>f ( y i ) = λ α Γ ( α ) y i α − 1 exp − λ y i ,   α , λ &gt; 0 ,   i = 1 , ⋯ , n</p><p>把上述分布进一步写成概率密度的表示形式，</p><p>y ~ exp { ( α − 1 ) T ( y ) + λ N ( y ) − c ( α , λ ) }</p><p>其中， T ( y ) = ∑ i = 1 n ln y i ， N ( y ) = ∑ i = 1 n y i 分别是参数 α ， λ 的充分统计量。</p><p>在任一选择事件E下，我们想要对参数 α 进行推断，则依赖于下面的分布表达式，</p><p>L α ( T ( y ) | N ( y ) , E )</p></sec><sec id="s10"><title>6. 模拟</title><sec id="s10_1"><title>6.1. 多元高斯分布</title><p>我们用平方根lasso对线性回归模型来选择变量。例如，设置n=100，p=200，X的行是来自变量之间具有成对相关系数为 ρ = 0. 4 的等相关多元正态分布，y服从以下的多元正态分布，</p><p>y = X β + e ,   E e = 0 ,   V a r ( e ) = I n</p><p>将β设置为200维向量，它的前7个分量为7，调整参数设置成 E ( ‖ X T e ‖ ∞ ‖ e ‖ 2 ) 。设置置信水平 α = 0.0 5 。</p><p>我们考虑将数据分成独立的两部分，以获得Pscreen和参数β的区间长度。其中，Pscreen表示所有的7个变量被成功选择的概率。Carve(n)表示使用样本y的前n个数据进行模型选择，用剩余的数据和选择模型的数据对模型进行推断，以及No carve意味着使用所有的数据进行模型选择，然后再使用所有的数据进行推断。表1展示了我们的模拟运行结果。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Pscreen &amp; confidence interval length</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Pscreen</th><th align="center" valign="middle" >置信区间长度</th></tr></thead><tr><td align="center" valign="middle" >Carve (50)</td><td align="center" valign="middle" >0.894737</td><td align="center" valign="middle" >0.415657</td></tr><tr><td align="center" valign="middle" >Carve (70)</td><td align="center" valign="middle" >0.932692</td><td align="center" valign="middle" >0.460035</td></tr><tr><td align="center" valign="middle" >Carve (80)</td><td align="center" valign="middle" >0.9375</td><td align="center" valign="middle" >0.717047</td></tr><tr><td align="center" valign="middle" >Carve (90)</td><td align="center" valign="middle" >0.953947</td><td align="center" valign="middle" >0.734330</td></tr><tr><td align="center" valign="middle" >No Carve</td><td align="center" valign="middle" >0.961539</td><td align="center" valign="middle" >0.568038</td></tr></tbody></table></table-wrap><p>表1. Pscreen和置信区间长度</p><p>此外，Fithian et al. [<xref ref-type="bibr" rid="hanspub.42682-ref25">25</xref>] 利用lasso方法对线性模型进行了后选推断。当n = 100和p = 200时，X服从多元正态分布，其变量之间两两相关 ρ = 0. 3 ，y服从多元正态分布，列被规范化为长度为1。Pscreen和carve (n)的含义与上面表1中的相同。Power是样本观测值落到拒绝域的概率。表2的模拟运行结果表示如下。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Pscreen &amp; powe</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Pscreen</th><th align="center" valign="middle" >Power</th><th align="center" valign="middle" >Level</th></tr></thead><tr><td align="center" valign="middle" >Carve (50)</td><td align="center" valign="middle" >0.09</td><td align="center" valign="middle" >0.99</td><td align="center" valign="middle" >0.06</td></tr><tr><td align="center" valign="middle" >Carve (75)</td><td align="center" valign="middle" >0.68</td><td align="center" valign="middle" >0.97</td><td align="center" valign="middle" >0.05</td></tr><tr><td align="center" valign="middle" >No Carve</td><td align="center" valign="middle" >0.99</td><td align="center" valign="middle" >0.80</td><td align="center" valign="middle" >0.05</td></tr></tbody></table></table-wrap><p>表2. Pscreen和power</p><p>通过对上述结果的分析，用数据雕琢方法推断参数得到的表1的结果与表2的结果是一致的。Pscreen表示通过雕琢选择所有7个变量的概率，概率会随着我们有更多数据用于变量选择而增加，并且它趋于1。上面表1中的Pscreen符合这个理论规律，所得结果与表2的变化趋势一致。在表2中，power随着用于选择模型的数据n的增加而逐渐减小，通过不进行数据雕琢的方式获得的power，比数据雕琢获得的power小。结果表明，通过数据雕琢的得到的power较大。在表1中，通过数据雕琢获得的间隔的长度也随着用于模型选择的数据n的增加而增加。没有用数据雕琢方法得到的置信区间长度介于使用两次不同的数据雕琢后模型选择推断得到的两个区间长度值之间，说明数据雕琢方法可以使得参数区间估计更小。表1中的区间长度和表2中的power变化趋势表明，数据雕琢所得结果与表2的结果一致。</p></sec><sec id="s10_2"><title>6.2. 伯努利分布</title><p>我们用平方根lasso模拟高斯分布，然后将此方法推广到非高斯分布。例如，我们使用数据雕琢方法来选择和推断logistic回归模型，在模型选择阶段，我们使用平方根lasso方法。当n = 100，p = 200，预</p><p>测变量y服从伯努利分布，且连接函数为 g ( y ) = ln ( y 1 − y ) 时，我们有以下广义线性模，</p><p>g ( y ) = X β + e ,   E e = 0 ,   V a r ( e ) = I n</p><p>将β设置为200维向量，其前7个分量为7。在数据雕琢方法中，我们使用一部分数据进行选择，另一部分数据和选择模型数据一起用于推断。我们用平方根lasso选择模型，并在推断过程中设置置信水平 α = 0.0 5 。我们考虑使用数据雕琢方法，将数据分别拆分成不同的部分，来获得Pscreen，参数β的置信区间和覆盖率。其中，Pscreen和置信区间的含义与第一个模拟中的含义相同，覆盖率表示参数β的覆盖率。表3显示了模拟结果。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Pscreen &amp; confidence interval lengths &amp; coverag</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Pscreen</th><th align="center" valign="middle" >confidence interval lengths</th><th align="center" valign="middle" >coverage</th></tr></thead><tr><td align="center" valign="middle" >Carve (50)</td><td align="center" valign="middle" >0.92045</td><td align="center" valign="middle" >0.42066</td><td align="center" valign="middle" >0.974</td></tr><tr><td align="center" valign="middle" >Carve (75)</td><td align="center" valign="middle" >0.93913</td><td align="center" valign="middle" >0.72218</td><td align="center" valign="middle" >0.986</td></tr><tr><td align="center" valign="middle" >Carve (90)</td><td align="center" valign="middle" >0.95679</td><td align="center" valign="middle" >0.85426</td><td align="center" valign="middle" >0.984</td></tr><tr><td align="center" valign="middle" >No carve</td><td align="center" valign="middle" >0.96067</td><td align="center" valign="middle" >0.67334</td><td align="center" valign="middle" >0.995</td></tr></tbody></table></table-wrap><p>表3. Pscreen和置信区间长度和覆盖率</p><p>通过分析表3中得到的数据，我们得到以下结论：第一，对于广义线性模型，随着用于模型选择的数据的增加，我们得到的Pscreen会逐渐增大。结果表明，随着模型选择所用数据的增加，模型选择的效果会更好。第二，随着用于模型选择的数据的增加，参数的置信区间将会增大，但是，发现使用数据雕琢会比不使用数据雕琢方法获得的区间长度短。比如，直接用所有数据进行选择性推断获得的置信区间长度值，是在carve (50)和carve (75)之间的。这说明我们可以利用数据雕琢来减小参数的置信区间，从而使我们的选择性推断效果更好。第三，通过数据雕琢获得的参数覆盖率约为0.97~0.98，而不通过数据雕刻获得的覆盖率略大。因此，结合这三个方面，可以利用数据雕琢进行模型选择性推断，这样使得结果更好。</p></sec></sec><sec id="s11"><title>文章引用</title><p>梁 博,石翔宇,张 齐. 广义线性模型的平方根Lasso选择性推断Selective Inference of Generalized Linear Models by the Square Root Lasso[J]. 应用数学进展, 2021, 10(05): 1668-1680. https://doi.org/10.12677/AAM.2021.105178</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.42682-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Cox, D.R. (1975) A Note on Data-Splitting for the Evaluation of Significance Levels. Narnia, 62, 441-444. 
&lt;br&gt;https://doi.org/10.1093/biomet/62.2.441</mixed-citation></ref><ref id="hanspub.42682-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Wasserman, L. and Roeder, K. (2009) High-Dimensional Variable Selection. The Annals of Statistics, 37, 2178-2201. 
&lt;br&gt;https://doi.org/10.1214/08-AOS646</mixed-citation></ref><ref id="hanspub.42682-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Meinshausen, N., Meier, L. and Bühlmann, P. (2009) p-Values for High-Dimensional Regression. Journal of the American Association Statistical, 104, 1671-1681. &lt;br&gt;https://doi.org/10.1198/jasa.2009.tm08647</mixed-citation></ref><ref id="hanspub.42682-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Nelder, J.A. and Wedderburn, R. (1972) Generalized Linear Models. Journal of the Royal Statistical Society, 135, 370-384. &lt;br&gt;https://doi.org/10.2307/2344614</mixed-citation></ref><ref id="hanspub.42682-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Wedderburn, R. (1974) Quasi-Likelihood Functions, Generalized Linear Models, and the Gauss—Newton Method. Biometrika, 61, 439-447. &lt;br&gt;https://doi.org/10.1093/biomet/61.3.439</mixed-citation></ref><ref id="hanspub.42682-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Hocking, R.R. (1976) A Biometrics Invited Paper. The Analysis and Selection of Variables in Linear Regression. Biometrics, 32, 1-49. &lt;br&gt;https://doi.org/10.2307/2529336</mixed-citation></ref><ref id="hanspub.42682-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Guyon, I. and Elisseeff, A. (2003) An Introduction to Variable and Feature Selection. Journal of Machine Learning Research, 3, 1157-1182.</mixed-citation></ref><ref id="hanspub.42682-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Akaike, H. (1998) Information Theory and an Extension of the Maximum Likelihood Principle. In: Parzen, E., Tanabe, K. and Kitagawa, G., Eds., Selected Papers of Hirotugu Akaike, Springer, New York, 199-213.  
&lt;br&gt;https://doi.org/10.1007/978-1-4612-1694-0_15</mixed-citation></ref><ref id="hanspub.42682-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Schwarz, G.E. (1978) Estimating the Dimension of a Model. The Annals of Statistics, 6, 461-464. 
&lt;br&gt;https://doi.org/10.1214/aos/1176344136</mixed-citation></ref><ref id="hanspub.42682-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Tibshirani, R. (1996) Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58, 267-288. &lt;br&gt;https://doi.org/10.1111/j.2517-6161.1996.tb02080.x</mixed-citation></ref><ref id="hanspub.42682-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Zou, H. (2006) The Adaptive Lasso and Its Oracle Properties. Journal of the American Statistical Association, 101, 1418-1429. &lt;br&gt;https://doi.org/10.1198/016214506000000735</mixed-citation></ref><ref id="hanspub.42682-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Meinshausen, N. (2007) Relaxed Lasso. Computational Statistics &amp; Data Analysis, 52, 374-393.  
&lt;br&gt;https://doi.org/10.1016/j.csda.2006.12.019</mixed-citation></ref><ref id="hanspub.42682-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Belloni, A., Chernozhukov, V. and Wang, L. (2011) Square-Root Lasso: Pivotal Recovery of Sparse Signals via Conic Programming. Biometrika, 98, 791-806.</mixed-citation></ref><ref id="hanspub.42682-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Lockhart, R., Taylor, J, Tibshirani, R.J. and Tibshirani, R. (2013) Rejoinder: “A Significance Test for the Lasso”. The Annals of Statistics, 42, 518-531. &lt;br&gt;https://doi.org/10.1214/14-AOS1175REJ</mixed-citation></ref><ref id="hanspub.42682-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Lee, J.D and Taylor, J.E. (2014) Exact Post Model Selection Inference for Marginal Screening. In: Advances in Neural Information Processing Systems, 136-144.  
&lt;br&gt;https://proceedings.neurips.cc/paper/2014/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf</mixed-citation></ref><ref id="hanspub.42682-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Lee, J.D., Sun, D.L., Sun, Y. and Taylor, J.E. (2016) Exact Post-Selection Inference, with Application to the Lasso. Annals of Statistics, 44, 907-927.&lt;br&gt;https://doi.org/10.1214/15-AOS1371</mixed-citation></ref><ref id="hanspub.42682-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Belloni, A., Chernozhukov, V. and Hansen, C. (2011) Inference for High-Dimensional Sparse Econometric Models. Centre for Microdata Methods and Practice, Institute for Fiscal Studies. &lt;br&gt;https://arxiv.org/pdf/1201.0220v1.pdf</mixed-citation></ref><ref id="hanspub.42682-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Belloni, A., Chernozhukov, V., Fernández-Val, I. and Hansen, C. (2013) Program Evaluation and Causal Inference with High-Dimensional Data. Econometrica, 85, 233-298. &lt;br&gt;https://doi.org/10.3982/ECTA12723</mixed-citation></ref><ref id="hanspub.42682-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, C.H. and Zhang, S.S. (2014) Confidence Intervals for Low Dimensional Parameters in High Dimensional Linear Models. Journal of the Royal Statistical Society, Series B: Statistical Methodology, 76, 217-242.  
&lt;br&gt;https://doi.org/10.1111/rssb.12026</mixed-citation></ref><ref id="hanspub.42682-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Tian, X.Y., Loftus, J.R. and Taylor, J.E. (2018) Selective Inference with Unknown Variance via the Square-Root Lasso. Biometrika, 105, 755-768. &lt;br&gt;https://doi.org/10.1093/biomet/asy045</mixed-citation></ref><ref id="hanspub.42682-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Neath, A.A. (2006) Testing Statistical Hypotheses. Journal of the American Statistical Association, 101, 847-848.  
&lt;br&gt;https://doi.org/10.1198/jasa.2006.s100</mixed-citation></ref><ref id="hanspub.42682-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Scheffé, L.H. (1955) Completeness, Similar Regions, and Unbiased Estimation: Part II. Sankhyā: The Indian Journal of Statistics, 15, 219-236.</mixed-citation></ref><ref id="hanspub.42682-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Benjamini, Y. and Hochberg, Y. (1995) Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society. Series B: Methodological, 57, 289-300.  
&lt;br&gt;https://doi.org/10.1111/j.2517-6161.1995.tb02031.x</mixed-citation></ref><ref id="hanspub.42682-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Fithian, W., Sun, D. and Taylor, J. (2014) Optimal Inference after Model Selection. arXiv: 1410.2597.</mixed-citation></ref><ref id="hanspub.42682-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Shi, X.Y., Liang, B. and Zhang, Q. (2020) Post-Selection Inference of Generalized Linear Models Based on the Lasso and the Elastic Net. Communication in Statistics-Theory and Methods, No. 725, 1-18.  
&lt;br&gt;https://doi.org/10.1080/03610926.2020.1821892</mixed-citation></ref></ref-list></back></article>