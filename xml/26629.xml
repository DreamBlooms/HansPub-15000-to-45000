<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2018.89141</article-id><article-id pub-id-type="publisher-id">CSA-26629</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20180900000_91466137.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于隐马尔科夫模型和卷积神经网络的图像标注方法
  Automatic Image Annotation Based on Hidden Markov Model and Convolutional Neural Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>徐</surname><given-names>海蛟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>黄</surname><given-names>琼浩</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>汪</surname><given-names>凡</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>文</surname><given-names>瑶</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>赵</surname><given-names>美华</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>华南师范大学教育信息技术学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>28</day><month>08</month><year>2018</year></pub-date><volume>08</volume><issue>09</issue><fpage>1309</fpage><lpage>1316</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   开发大规模图像库的搜索和浏览算法，使得图像自动标注的重要性日益增强。基于隐马尔科夫模型(HMM)与卷积神经网络(CNN)，我们提出了一种新的图像标注方法HMM + CNN。首先，训练一个多标签学习的CNN网络作为概念分类器；其次，通过一阶HMM模型把图像内容与语义相关性相结合以精炼该CNN的预测分数；最后，为改善对稀疏概念的标注性能，应用梯度下降算法来补偿在真实应用中不平衡图像集上标注概念的频率差。在IAPR TC-12标准图像标注数据集上对比了其他传统方法，结果表明我们的标注方法在查准率和查全率上性能更优。 Automatic image annotation is becoming increasingly important in order to develop algorithms that are able to search and browse large-scale image databases. In this paper, we propose a novel annotation approach termed HMM + CNN, which is based on Hidden Markov Model (HMM) and Convolutional Neural Network (CNN). First, a multilabel CNN is trained as a concept classifier. Then, through a first-order HMM, image content and semantics correlation is combined to refine the predicted semantic scores. Finally, to improve the performance of labeling rare concepts, the gradient descent algorithm is applied for compensating the varying frequencies of concepts derived from imbalanced image datasets. Experiments have been carried out on IAPR TC-12 image annotation database. The results show that our proposed approach performs favorably compared with several conventional methods.
    
  
 
</p></abstract><kwd-group><kwd>图像自动标注，隐马尔可夫模型，卷积神经网络，多标签学习, Automatic Image Annotation</kwd><kwd> Hidden Markov Model</kwd><kwd> Convolutional Neural Network</kwd><kwd> Multi-Label Learning</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于隐马尔科夫模型和卷积神经网络的 图像标注方法<sup> </sup></title><p>徐海蛟，黄琼浩，汪凡，文瑶，赵美华</p><p>华南师范大学教育信息技术学院，广东 广州</p><p><img src="//html.hanspub.org/file/1-1541129x1_hanspub.png" /></p><p>收稿日期：2018年8月6日；录用日期：2018年8月21日；发布日期：2018年8月28日</p><disp-formula id="hanspub.26629-formula3"><graphic xlink:href="//html.hanspub.org/file/1-1541129x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>开发大规模图像库的搜索和浏览算法，使得图像自动标注的重要性日益增强。基于隐马尔科夫模型(HMM)与卷积神经网络(CNN)，我们提出了一种新的图像标注方法HMM + CNN。首先，训练一个多标签学习的CNN网络作为概念分类器；其次，通过一阶HMM模型把图像内容与语义相关性相结合以精炼该CNN的预测分数；最后，为改善对稀疏概念的标注性能，应用梯度下降算法来补偿在真实应用中不平衡图像集上标注概念的频率差。在IAPR TC-12标准图像标注数据集上对比了其他传统方法，结果表明我们的标注方法在查准率和查全率上性能更优。</p><p>关键词 :图像自动标注，隐马尔可夫模型，卷积神经网络，多标签学习</p><disp-formula id="hanspub.26629-formula4"><graphic xlink:href="//html.hanspub.org/file/1-1541129x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2018 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/1-1541129x7_hanspub.png" /> <img src="//html.hanspub.org/file/1-1541129x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>随着互联网技术与多媒体共享社区的不断发展，大量的多媒体内容已进入我们的日常生活，如何高效准确地对海量的未标注图像等媒体内容进行搜索、浏览、管理变得尤为重要，这也使得图像自动标注的重要性日益增强。近年来众多学者对图像自动标注方法做了大量的研究，取得了若干阶段性成果，例如浅度学习方法：支持向量机SVM [<xref ref-type="bibr" rid="hanspub.26629-ref1">1</xref>] 、核典型相关分析KCCA-2PKNN [<xref ref-type="bibr" rid="hanspub.26629-ref2">2</xref>] 、稀疏核学习SKL-CRM [<xref ref-type="bibr" rid="hanspub.26629-ref3">3</xref>] 、快速标注FastTag [<xref ref-type="bibr" rid="hanspub.26629-ref4">4</xref>] 、离散多重伯努利模型SVM-DMBRM [<xref ref-type="bibr" rid="hanspub.26629-ref5">5</xref>] 、图像距离尺度学习NSIDML [<xref ref-type="bibr" rid="hanspub.26629-ref6">6</xref>] 、生成判别联合模型GDM [<xref ref-type="bibr" rid="hanspub.26629-ref7">7</xref>] ；以及最近流行的深度学习方法：渐进式深度自动图像标注ADA [<xref ref-type="bibr" rid="hanspub.26629-ref8">8</xref>] 、图像标签对齐模型SEM [<xref ref-type="bibr" rid="hanspub.26629-ref9">9</xref>] 和图拉普拉斯正则化深度神经网络HQ-III [<xref ref-type="bibr" rid="hanspub.26629-ref10">10</xref>] 等。这些传统的图像标注方法考虑了视觉特征与语义概念之间的关联，而在标注概念之间语义关联方面还存在诸多未得到很好解决的问题。很多方法仅在平衡的小概念字典上完成，而在带有大概念字典的数据集上，语义概念分布或者语义概念出现频率呈现较大差异(即概念的不平衡性)，这大大影响了标注方法的效果。因此，研究在不平衡图像库上的自动图像标注很有必要也很有意义。</p><p>在图像标注领域，深度学习方法(如卷积神经网络CNN)比传统浅度学习方法在性能上大大提升，然而，其并未很好考虑语义概念之间的关联，这影响了其性能的进一步改善。本文针对该问题，提出了一种基于隐马尔科夫模型(HMM)与卷积神经网络(CNN)的自动图像标注方法HMM + CNN，该方法使用HMM模型来校正语义标签：把图像标注过程视为检索有相互关联的隐藏语义概念序列过程，它提高了高度关联的相关概念语义分数而弱化了毫无关联的概念语义分数，提高了标注精度。在HMM模型里，所有的隐状态可以构成一条一阶马尔可夫链，而每个隐状态代表一个隐藏语义概念，两个隐状态之间的边权重表示它们的语义相关性，隐状态到可观测状态之间的边表示由CNN分类器产生的视觉语义分数。在学习过程中，考虑到真实图像集上语义概念分布的不均衡性，引入了语义概念的权重学习，其在计算发射概率和转移概率的过程中减弱了频繁概念的权重，而提升稀疏概念的权重，于是大大提高了稀疏概念标注的性能。最后，把我们的标注方法HMM + CNN应用于标准标注图像集IAPR TC-12 [<xref ref-type="bibr" rid="hanspub.26629-ref11">11</xref>] ，结果表明我们提出的标注方法HMM + CNN标注精度比较高，是自动图像标注的一种有效方法。</p></sec><sec id="s4"><title>2. 隐马尔科夫模型</title><p>隐马尔可夫模型(Hidden Markov Model，简称HMM) [<xref ref-type="bibr" rid="hanspub.26629-ref12">12</xref>] 可表达离散时间序列状态数据，它的隐状态X<sub>i</sub>(隐变量)不能直接观察到，但能通过观测向量O<sub>i</sub>序列间接观察到。每个观测向量都是通过某些概率密度分布表现为各种状态，每一个观测向量是由一个具有相应概率密度分布的状态序列产生。所以，隐马尔可夫模型是一个双重随机过程——具有一定隐状态数的马尔可夫链和随机函数集，即两个状态集合与三个矩阵。两个状态集合是指隐状态集{X<sub>1</sub>, X<sub>2</sub>, …}和观察状态集{O<sub>1</sub>, O<sub>2</sub>, …}。HMM的假设是隐状态X<sub>1</sub>之间是一个马尔可夫链，对应一个初始状态矩阵π、隐藏状态转移矩阵A = (a<sub>ij</sub>)和发射矩阵B = (b<sub>ij</sub>)。如图1所示，虚线箭头与实线箭头分别表示转移概率a<sub>ij</sub>与发射概率b<sub>ij</sub>，当前隐状态X<sub>1</sub> (隐变量)不是独立被确定出来，而是依赖于前x个观测向量的隐藏状态X<sub>i</sub><sub>-1</sub>。通过使用双重随机过程，HMM模型可以寻找到最合适的隐藏变量序列。在本模型HMM + CNN中，为了简化求解，仅考虑x = 1时候的一阶情况。</p><p>假若分别使用隐语义标注w和未标注测试图像I替换HMM模型中的隐状态与可观测向量，则转移矩阵A与发射矩阵B分别体现了语义概念信息与视觉内容信息。我们提出的标注方法HMM + CNN把图像标注过程视为一个关联隐语义检索过程，与经典的标准HMM模型相比较，HMM + CNN不需要传统的复杂的维特比算法，也不需要估算观测向量I的概率分布。</p></sec><sec id="s5"><title>3. 基于HMM与CNN的自动图像标注方法</title><sec id="s5_1"><title>3.1. 问题描述</title><p>我们的标注方法HMM + CNN将一个图像标注过程看作是一个图像隐语义的检索过程。设训练集为∆ = {I<sub>1</sub>, ∙∙∙, I<sub>|∆|</sub>}，其中I<sub>i</sub>表示一副含有若干语义标注的图像。语义标注w<sub>i</sub>来自于包含|D|个不同语义概念的概念字典D，如“dance”、“horizon”和“flower”。测试集Ω中的图像没有包含任何语义概念标注。给定测试图像I &#206; Ω，图像自动语义标注(隐语义检索)的目标是在概念字典D中，检索出最相关的语义概念集{w<sub>1</sub>, w<sub>2</sub>, ∙∙∙, w<sub>K</sub>}，以描述I的视觉内容。</p><p>在检索的过程中，HMM + CNN标注方法能够同时结合视觉内容的相关性与语义概念的相关性，在一个用户查询概念字典D的时候，其目标是希望检索到与未标注图像内容相一致的语义概念。一般情况</p><p>图1. 隐马尔可夫模型示例图</p><p>下，用户提交的检索图像I在一次检索过程中是不变的，于是，未标注图像I &#206; Ω可以看做是一个可观测向量，随着检索返回语义概念数的增加，该图像I可重复地构成一个可观测序列{I(1), I(2), ∙∙∙, I(K)}。</p><p>对于转移概率矩阵A = (a<sub>ij</sub>)中的每一个元素a<sub>ij</sub>表示两个隐状态间的转移数据，在我们的标注方法HMM + CNN中将a<sub>ij</sub>视为两个语义概念w<sub>i</sub>和w<sub>j</sub>的关联性。而CNN分类器产生的视觉相关分数可看做是相应隐状态的发射概率b<sub>ij</sub> &#206; B，该发射概率b<sub>ij</sub>表示了CNN分类器把图像I映射到语义概念w<sub>j</sub>的过程。根据上述的转移概率矩阵A和发射矩阵B，第t − 1步被检索的隐藏语义概念w<sub>i</sub>可以关联概率值a<sub>ij</sub>转移到第t步被检索隐藏语义概念w<sub>j</sub>，被检索隐藏语义概念w<sub>j</sub>与图像I以发射概率值b<sub>ij</sub>关联。由于I是固定不变的，所以b<sub>ij</sub>简记为b<sub>j</sub>。第t步隐藏语义概念检索w<sub>j</sub>依赖两个因素，即t − 1步检索到的隐语义概念w<sub>i</sub>到该隐语义概念w<sub>j</sub>的内关联转移概率a<sub>ij</sub>以及视觉关联产生的发射概率b<sub>j</sub>。从上述分析可知，检索序列中相邻隐藏语义概念w<sub>i</sub>与w<sub>j</sub>具有相关的视觉内容与语义概念，整个相互关联的语义概念序列可以描述I的“故事”线索。</p><p>图2中给出了HMM + CNN标注方法的结构，矩形框表示一个未标注的检索图像I，圆形框表示被检索的隐语义概念w<sub>j</sub> (隐状态)。虚线箭头表示隐状态之间的转移概率a<sub>ij</sub>即语义关联性，而实线箭头表示每个隐藏状态的发射概率即视觉相关性b<sub>j</sub>。每一个隐状态w<sub>j</sub>可以提供语义概念关联性数据a<sub>ij</sub>与视觉内容相关性数据b<sub>j</sub>，然后基于HMM + CNN算法实现了对隐语义概念的排序输出。为了叙述方便，表1定义了HMM + CNN标注方法所使用的主要符号标记。</p></sec><sec id="s5_2"><title>3.2. 发射概率估算</title><p>CNN分类器产生的视觉相关性分数可视为相应隐状态的发射概率b<sub>j</sub>，该发射概率表示了CNN分类器把图像I映射到语义概念w<sub>j</sub>的过程。任何CNN网络都可融入我们的标注模型，不失一般性，我们选择了近年来一个有影响力的高效CNN模型ResNet [<xref ref-type="bibr" rid="hanspub.26629-ref13">13</xref>] 来作为我们的CNN分类器。</p><p>传统CNN网络聚焦于单概念分类，而我们的标注任务是一个多概念分类任务，因此为ResNet模型<sub> </sub></p><p>图2. HMM + CNN结构图</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> List of main notation</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >符号</th><th align="center" valign="middle" >含义</th></tr></thead><tr><td align="center" valign="middle" >D</td><td align="center" valign="middle" >概念字典</td></tr><tr><td align="center" valign="middle" >N</td><td align="center" valign="middle" >被检索隐语义概念集的大小，N = |D|</td></tr><tr><td align="center" valign="middle" >K</td><td align="center" valign="middle" >返回隐藏语义标注的数量，K ≤ |D|</td></tr><tr><td align="center" valign="middle" >I</td><td align="center" valign="middle" >被检索的未标注图像，I &#206; Ω</td></tr><tr><td align="center" valign="middle" >S(w)</td><td align="center" valign="middle" >出现语义概念w的图像集合</td></tr><tr><td align="center" valign="middle" >a<sub>ij</sub></td><td align="center" valign="middle" >语义概念w<sub>i</sub>和w<sub>j</sub>之间的转移概率，即二者的语义关联概率</td></tr><tr><td align="center" valign="middle" >A</td><td align="center" valign="middle" >概念词典D的状态转移矩阵，A = {a<sub>ij</sub> |i,j = 1, ∙∙∙, N}</td></tr><tr><td align="center" valign="middle" >b<sub>j</sub></td><td align="center" valign="middle" >语义概念w<sub>j</sub>的发射概率，即w<sub>j</sub>与I的视觉内容相关概率</td></tr><tr><td align="center" valign="middle" >B</td><td align="center" valign="middle" >概念词典D的发射概率矩阵，B = {b<sub>j</sub> |j = 1, ∙∙∙, N}</td></tr><tr><td align="center" valign="middle" >R(w<sub>i</sub>)</td><td align="center" valign="middle" >语义概念w<sub>i</sub>的语义邻居集</td></tr><tr><td align="center" valign="middle" >K<sub>R</sub></td><td align="center" valign="middle" >语义邻居集R(w<sub>i</sub>)的大小，即K<sub>R</sub> = |R(w<sub>i</sub>)|</td></tr></tbody></table></table-wrap><p>表1. 符号标记表</p><p>定义了一个新的多概念softmax损失函数使之适应多概念标注任务。首先，第i张图片I与第j个概念w<sub>j</sub>的归一化关联概率可定义为：</p><p>p ( w j | I ) = exp ( q j ( I ) ) ∑ k exp ( q k ( I ) ) , (1)</p><p>其中， q j ( I ) 是图像I在第j个概念w<sub>j</sub>的离散概率分布，它由ResNet分类器产生。为最小化ResNet预测概率与真实概率的KL距离，我们使用如下多概念softmax损失函数：</p><p>f softmax = − 1 N ∑ i ∑ j p &#175; i , j log ( p ( w j | I ) ) , (2)</p><p>其中， p &#175; i , j 是一个图片I的指示器函数：当概念w<sub>j</sub>在图片I中存在则 p &#175; i , j = 1 否则 p &#175; i , j = 0 。</p></sec><sec id="s5_3"><title>3.3. 转移概率估算</title><p>两个语义概念之间的转移概率a<sub>ij</sub>可视为二者的语义关联概率。共现概念是指以一定频率共同出现于文档中的语义概念。对于共现于图像I中的两个关联语义概念，由于它们共同描述了一副图像的主题或者“故事”线索，所以，可以依据共现率来估算二者的语义关联。</p><p>给定隐语义概念w<sub>i</sub>和w<sub>j</sub>，在训练图像集∆上考虑使用如下共现度量来计算二者的语义关联概率，HMM + CNN标注模型以此作为两概念的转移概率a<sub>ij</sub>：</p><p>a i j = | S ( w i ) ∩ S ( w j ) | | S ( w i ) | (3)</p><p>该公式描述了概念w<sub>i</sub>和w<sub>j</sub>共现的频率，然后被语义概念w<sub>i</sub>的频率归一化。它可以被理解为给定含有标注w<sub>i</sub>的图像I，其包含语义概念w<sub>i</sub>的概率有多大，其值范围是[0.0, 1.0]。</p><p>如果隐语义概念w<sub>i</sub>是w<sub>j</sub>的语义近邻，即w<sub>i</sub> &#206; R(w<sub>j</sub>)，那么转移概率a<sub>ij</sub>的值为二者的语义关联性，否则，该值被设置为0。由于自转移的概率值很大(a<sub>jj</sub> = 1)，这导致被检索的隐语义概念可能会一直自循环在某个隐状态，导致输出无效的隐藏语义检索结果，所以自转移的概率值a<sub>jj</sub>被设置为0。综上所述，我们的HMM + CNN标注方法可以抽象为∧ = (N, K, I, S(w), A, B, R(w<sub>i</sub>), K<sub>R</sub>)。</p></sec><sec id="s5_4"><title>3.4. HMM + CNN图像标注算法</title><p>给定一个测试集Ω，若任意的一副图像I &#206; Ω被看做是检索对象，那么概念词典D中的全部语义概念w<sub>j</sub> &#206; D可构成被检索数据集。给定一个由检索对象I所构成的一个固定观测序列，HMM + CNN标注方法的目标是返回能恰当描述图像I的最佳隐语义概念序列O = {O<sub>1</sub>, ∙∙∙, O<sub>j</sub>, ∙∙∙, O<sub>K</sub>}，Oj &#206; D的选择取决于a<sub>ij</sub>和b<sub>j</sub>两个元素，算法1总结了HMM + CNN隐语义标注的检索方法。其中，p<sub>1</sub>和p<sub>2</sub>是待优化参数，且满足约束条件：p<sub>1</sub> + p<sub>2</sub> = 1，它们表示发射概率(视觉相关性)与转移概率(语义关联性)二者的权重。考虑到真实图像集的概念词典D是不平衡的，即不同语义概念w上的图像集合S(w)的大小是有差异的，不同语义概念的权重p<sub>1</sub>和p<sub>2</sub>需要通过在训练图像集上以交叉验证方法获取，而不是直接经验设定为某个固定值。</p><p>算法1 HMM + CNN自动图像标注</p><p>输入：训练集∆，概念字典D，未标注图片I &#206; Ω；</p><p>输出：标注结果集O = {O<sub>1</sub>, O<sub>2</sub>, ∙∙∙, O<sub>K</sub>}。</p><p>1 构建状态转移矩阵A = {a<sub>ij</sub>|i,j = 1, ∙∙∙, N};</p><p>2 构建发射概率矩阵B = {b<sub>j</sub>|j = 1, ∙∙∙, N};</p><p>3 初始化标注结果集 O 1 = max 1 ≤ j ≤ N ( b j ) ,   O = O 1 ；</p><p>4 for k = 2 to K do</p><p>5 设上一步检索概念O<sub>k</sub><sub>−1</sub> = w<sub>i</sub>，则 O j = max 1 ≤ j ≤ N ( p 1 b j + p 2 a i j ) ；</p><p>6 O = O ∪ O j ；</p><p>7 end for</p><p>8 返回结果集O。</p></sec></sec><sec id="s6"><title>4. 实验和评价</title><sec id="s6_1"><title>4.1. 数据集</title><p>评价实验采用了公开标注数据集IAPR TC-12 [<xref ref-type="bibr" rid="hanspub.26629-ref11">11</xref>] 。它包含有19,627张图像，每张图像含有1~23个标注，单词表D含有291个语义概念。采用随机抽样，17,665张图像作为训练集，余下1962张图像作为测试集，约75%概念频率低于平均概念频率。我们采用与文献 [<xref ref-type="bibr" rid="hanspub.26629-ref8">8</xref>] 相同的评价指标：平均准确率P、平均召回率R、调和均值F1与正召回概念数N<sup>+</sup>。所有指标值越高表示标注性能越好。</p></sec><sec id="s6_2"><title>4.2. 实验结果与分析</title><p>为观测发射概率和转移概率的语义权重{p<sub>1</sub>, p<sub>2</sub>}的影响，考虑使用交叉验证方法：训练图像被随机分成等份5组，当每组图像集交替构成验证集时，其余4组图像集则组成一个训练集。</p><p>首先，考虑第一种情况：忽略图像集上语义概念的不平衡性，直接给权重参数{p<sub>1</sub>, p<sub>2</sub>}赋经验值，观测其对于标注性能的影响。该方法记为HMM + CNN (without weight learning)。在第二种情况，考虑语义概念分布并非是平衡的，对不同出现频次的语义概念给予相同的经验权重会导致标注性能的下降，因此，对于不同的语义概念w<sub>i</sub> &#206; D给予不同权重{p<sub>1</sub>, p<sub>2</sub>}，全部概念权重组成不同的发射概率向量P1与转移概率向量P2，{P1, P2}可在验证集上用如下方法求出来。</p><p>1) 初始化权重向量，即P1 = 0, P2 = 1 − P1；</p><p>2) 对于任意的i(1 ≤ I ≤ N)：</p><p>2a) 对于不同的权重p<sub>1</sub> &#206; {0, 0.1, ∙∙∙, 1}执行算法1获得标注结果集O并记录最大F1性能分数，写入相对应的权重值pm<sub>i</sub>：P1<sub>i</sub> = pm<sub>i</sub>, P2<sub>i</sub> = 1 − P1<sub>i</sub>；</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Performance comparisons of multi-label image annotatio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >标注方法</th><th align="center" valign="middle" >平均准确率P</th><th align="center" valign="middle" >平均召回率R</th><th align="center" valign="middle" >调和均值F1</th><th align="center" valign="middle" >正召回概念数N+</th></tr></thead><tr><td align="center" valign="middle" >SVM [<xref ref-type="bibr" rid="hanspub.26629-ref1">1</xref>]</td><td align="center" valign="middle" >0.27</td><td align="center" valign="middle" >0.31</td><td align="center" valign="middle" >0.29</td><td align="center" valign="middle" >157</td></tr><tr><td align="center" valign="middle" >KCCA-2PKNN [<xref ref-type="bibr" rid="hanspub.26629-ref2">2</xref>]</td><td align="center" valign="middle" >0.59</td><td align="center" valign="middle" >0.30</td><td align="center" valign="middle" >0.39</td><td align="center" valign="middle" >259</td></tr><tr><td align="center" valign="middle" >SKL-CRM [<xref ref-type="bibr" rid="hanspub.26629-ref3">3</xref>]</td><td align="center" valign="middle" >0.47</td><td align="center" valign="middle" >0.32</td><td align="center" valign="middle" >0.38</td><td align="center" valign="middle" >274</td></tr><tr><td align="center" valign="middle" >FastTag [<xref ref-type="bibr" rid="hanspub.26629-ref4">4</xref>]</td><td align="center" valign="middle" >0.47</td><td align="center" valign="middle" >0.26</td><td align="center" valign="middle" >0.34</td><td align="center" valign="middle" >280</td></tr><tr><td align="center" valign="middle" >SVM-DMBRM [<xref ref-type="bibr" rid="hanspub.26629-ref5">5</xref>]</td><td align="center" valign="middle" >0.56</td><td align="center" valign="middle" >0.29</td><td align="center" valign="middle" >0.38</td><td align="center" valign="middle" >283</td></tr><tr><td align="center" valign="middle" >NSIDML [<xref ref-type="bibr" rid="hanspub.26629-ref6">6</xref>]</td><td align="center" valign="middle" >0.57</td><td align="center" valign="middle" >0.37</td><td align="center" valign="middle" >0.45</td><td align="center" valign="middle" >282</td></tr><tr><td align="center" valign="middle" >GDM [<xref ref-type="bibr" rid="hanspub.26629-ref7">7</xref>]</td><td align="center" valign="middle" >0.32</td><td align="center" valign="middle" >0.29</td><td align="center" valign="middle" >0.30</td><td align="center" valign="middle" >252</td></tr><tr><td align="center" valign="middle" >ADA [<xref ref-type="bibr" rid="hanspub.26629-ref8">8</xref>]</td><td align="center" valign="middle" >0.42</td><td align="center" valign="middle" >0.30</td><td align="center" valign="middle" >0.35</td><td align="center" valign="middle" >280</td></tr><tr><td align="center" valign="middle" >SEM [<xref ref-type="bibr" rid="hanspub.26629-ref9">9</xref>]</td><td align="center" valign="middle" >0.41</td><td align="center" valign="middle" >0.39</td><td align="center" valign="middle" >0.40</td><td align="center" valign="middle" >-</td></tr><tr><td align="center" valign="middle" >HQ-III [<xref ref-type="bibr" rid="hanspub.26629-ref10">10</xref>]</td><td align="center" valign="middle" >0.43</td><td align="center" valign="middle" >0.41</td><td align="center" valign="middle" >0.42</td><td align="center" valign="middle" >281</td></tr><tr><td align="center" valign="middle" >HMM + CNN (Ours)</td><td align="center" valign="middle" >0.64</td><td align="center" valign="middle" >0.45</td><td align="center" valign="middle" >0.53</td><td align="center" valign="middle" >285</td></tr></tbody></table></table-wrap><p>表2. 多标签图像标注性能比较</p><p>2b) i值加一，即i = i + 1；</p><p>3) 输出权重向量P1, P2。</p><p>显然，上述权重提升方法的时间复杂度是O(11 &#215; N)。从实验结果见，第二种情况下的基于权重学习方法的HMM + CNN图像标注方法效果优于第一种情况下的HMM + CNN (without weight learning)标注方法。</p><p>表2列出了与最新图像标注方法的对比实验结果。</p><p>从表2中可见，我们的HMM + CNN方法超越了其他对比方法，获得了更好的标注性能。与表中最好的对比标注方法NSIDML比较，HMM + CNN方法的平均准确率、平均召回率、调和均值F1分别提高了12%、22%、18%。一方面，视觉内容的相关性(发射概率)可以挖掘视觉内容与语义概念的相关性，另一方面，语义关联性(转移概率)反映出隐标注概念之间的语义关联，其更准确地描述了图像的“故事”线索。在图像标注任务中，这两种类型的相关性都提供了有用的信息，具有一定的互补性，从这个角度上说我们的HMM + CNN方法可提高图像标注的性能。此外，在隐语义标注检索中使用了语义权重学习方法来获得合理的语义权重，所以，在不平衡数据集上，我们的HMM + CNN方法具有更好的标注效果。</p></sec></sec><sec id="s7"><title>文章引用</title><p>徐海蛟,黄琼浩,汪 凡,文 瑶,赵美华. 基于隐马尔科夫模型和卷积神经网络的图像标注方法 Automatic Image Annotation Based on Hidden Markov Model and Convolutional Neural Network[J]. 计算机科学与应用, 2018, 08(09): 1309-1316. https://doi.org/10.12677/CSA.2018.89141</p></sec><sec id="s8"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.26629-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Chang, C.C. and Lin, C.J. (2011) LIBSVM: A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2, 1-27. &lt;br&gt;https://doi.org/10.1145/1961189.1961199</mixed-citation></ref><ref id="hanspub.26629-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Ballan, L., Uricchio, T., Seidenari, L. and Bimbo, A.D. (2014) A Cross-Media Model for Automatic Image Annotation. International Conference on Multimedia Retrieval, 73. &lt;br&gt;https://doi.org/10.1145/2578726.2578728</mixed-citation></ref><ref id="hanspub.26629-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Moran, S. and Lavrenko, V. (2014) Sparse Kernel Learning for Image Annotation. International Conference on Multimedia Retrieval, 113-120. &lt;br&gt;https://doi.org/10.1145/2578726.2578734</mixed-citation></ref><ref id="hanspub.26629-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Chen, M., Zheng, A. and Weinberger, K. (2013) Fast Image Tagging. International Conference on Machine Learning, 1274-1282.</mixed-citation></ref><ref id="hanspub.26629-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Murthy, V.N., Can, E.F. and Manmatha, R. (2014) A Hybrid Model for Automatic Image Annotation. International Conference on Multimedia Retrieval, 369-376. &lt;br&gt;https://doi.org/10.1145/2578726.2578774</mixed-citation></ref><ref id="hanspub.26629-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Jin, C. and Jin, S.W. (2016) Image Distance Metric Learning Based on Neighborhood Sets for Automatic Image Annotation. Journal of Visual Communication and Image Representation, 34, 167-175.  
&lt;br&gt;https://doi.org/10.1016/j.jvcir.2015.10.017</mixed-citation></ref><ref id="hanspub.26629-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Ji, P., Gao, X. and Hu, X. (2017) Automatic Image Annotation By Combining Generative and Discriminant Models. Neurocomputing, 236, 48-55. &lt;br&gt;https://doi.org/10.1016/j.neucom.2016.09.108</mixed-citation></ref><ref id="hanspub.26629-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">周铭柯, 柯逍, 杜明智. 基于数据均衡的增进式深度自动图像标注[J]. 软件学报, 2017, 28(7): 1862-1880.</mixed-citation></ref><ref id="hanspub.26629-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Ma, Y., Liu, Y., Xie, Q. and Li, L. (2018) CNN-Feature Based Automatic Image Annotation Method. Multimedia Tools &amp; Applications, 1-14. &lt;br&gt;https://doi.org/10.1007/s11042-018-6038-x</mixed-citation></ref><ref id="hanspub.26629-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Mojoo, J., Kurosawa, K. and Kurita, T. (2017) Deep CNN with Graph Laplacian Regularization for Multi-Label Image Annotation. International Conference on Image Analysis and Recognition, 19-26.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-59876-5_3</mixed-citation></ref><ref id="hanspub.26629-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Grubinger, M., Clough, P. and Müller, H. (2006) The IAPR Benchmark : A New Evaluation Resource for Visual Information Systems. International Conference on Language Resources and Evaluation, 13-23.</mixed-citation></ref><ref id="hanspub.26629-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Saini, R., Roy, P. and Dogra, D. (2018) A Segmental HMM Based Trajectory Classification Using Genetic Algorithm. Expert System Application, 93, 169-181. &lt;br&gt;https://doi.org/10.1016/j.eswa.2017.10.021</mixed-citation></ref><ref id="hanspub.26629-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S. and Sun, J. (2016) Deep Residual Learning for Image Recognition. IEEE Conference on Computer Vision and Pattern Recognition, 770-778.</mixed-citation></ref></ref-list></back></article>