<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.113068</article-id><article-id pub-id-type="publisher-id">CSA-41221</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210300000_57674591.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  人脸转正GAN模型的高效压缩
  Efficient Compression of Face Frontalization GAN Model
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>魏</surname><given-names>雷</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>邱</surname><given-names>卫根</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>立臣</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>03</month><year>2021</year></pub-date><volume>11</volume><issue>03</issue><fpage>661</fpage><lpage>671</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   生成对抗网络(Generative Adversarial Network)在正面人脸图像生成方面大放异彩，生成的正面人脸极其逼真受到研究人缘的青睐。但其强大的图像生成能力源自于其训练和使用过程中巨大的计算量，GAN结构越复杂，其计算量需求，这极大地限制了其交互式部署。为增强其部署的便利性，减少GAN的计算量需求，本文提出了一种通用的压缩算法，该算法对人脸转正GAN进行了压缩，减少了GAN中生成器的推理时间和模型大小。本文的实验证明了本文算法在相较于原网络大幅减少了计算量的情况下，压缩后的GAN网络仍然保持了较好的图片质量。 GAN performed extremely well in the frontal face image generation, the generated frontal face is very realistic favored by most researches. However, its powerful image generation ability comes from the huge calculation power and storage space required, and the more complex the GAN structure, the greater the demand for computation, which greatly limits its interactive deployment applications. To enhance the convenience of its deployment and reduce the computational requirements of the GAN, the paper proposes a general compression algorithm. The algorithm compresses the GAN model size of face frontalization and reduces the inference time. The experiments in this paper show that the compressed GAN network still obtains better image quality under the condition that the computation and storage load are greatly reduced compared with the original networks. 
  
 
</p></abstract><kwd-group><kwd>GAN，图像生成，人脸正面化，模型压缩, GAN</kwd><kwd> Image Generation</kwd><kwd> Face Frontalization</kwd><kwd> Model Compression</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>生成对抗网络(Generative Adversarial Network)在正面人脸图像生成方面大放异彩，生成的正面人脸极其逼真受到研究人缘的青睐。但其强大的图像生成能力源自于其训练和使用过程中巨大的计算量，GAN结构越复杂，其计算量需求，这极大地限制了其交互式部署。为增强其部署的便利性，减少GAN的计算量需求，本文提出了一种通用的压缩算法，该算法对人脸转正GAN进行了压缩，减少了GAN中生成器的推理时间和模型大小。本文的实验证明了本文算法在相较于原网络大幅减少了计算量的情况下，压缩后的GAN网络仍然保持了较好的图片质量。</p></sec><sec id="s2"><title>关键词</title><p>GAN，图像生成，人脸正面化，模型压缩</p></sec><sec id="s3"><title>Efficient Compression of Face Frontalization GAN Model</title><p>Lei Wei, Weigen Qiu, Lichen Zhang</p><p>Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/22-1542080x4_hanspub.png" /></p><p>Received: Feb. 23<sup>rd</sup>, 2021; accepted: Mar. 18<sup>th</sup>, 2021; published: Mar. 26<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/22-1542080x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>GAN performed extremely well in the frontal face image generation, the generated frontal face is very realistic favored by most researches. However, its powerful image generation ability comes from the huge calculation power and storage space required, and the more complex the GAN structure, the greater the demand for computation, which greatly limits its interactive deployment applications. To enhance the convenience of its deployment and reduce the computational requirements of the GAN, the paper proposes a general compression algorithm. The algorithm compresses the GAN model size of face frontalization and reduces the inference time. The experiments in this paper show that the compressed GAN network still obtains better image quality under the condition that the computation and storage load are greatly reduced compared with the original networks.</p><p>Keywords:GAN, Image Generation, Face Frontalization, Model Compression</p><disp-formula id="hanspub.41221-formula15"><graphic xlink:href="//html.hanspub.org/file/22-1542080x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/22-1542080x7_hanspub.png" /> <img src="//html.hanspub.org/file/22-1542080x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>人脸正面化是一项旨在将各种视角下的人脸对齐到规范位置(即正面)的计算机视觉处理任务。2D/3D纹理映射 [<xref ref-type="bibr" rid="hanspub.41221-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref3">3</xref>]，统计建模 [<xref ref-type="bibr" rid="hanspub.41221-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref5">5</xref>] 和基于深度学习的方法 [<xref ref-type="bibr" rid="hanspub.41221-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref7">7</xref>] 都在这一领域取得了一定进展。</p><p>人脸正面化生成为人脸数据扩充提供了一种有效的方式，在数据量小的情况下可以通过这一方法扩增数据量。还有一些研究人员用其提高非受限环境下人脸识别的正确率，也取得了很好的效果。TP-GAN (Twopathway GAN) [<xref ref-type="bibr" rid="hanspub.41221-ref7">7</xref>] 是第一个用双通路结构来进行人脸生成的，这使得它能同时关注整体结构和局部细节。CR-GAN [<xref ref-type="bibr" rid="hanspub.41221-ref8">8</xref>] 的双通路架构被用来学习完整的嵌入空间。FF-GAN [<xref ref-type="bibr" rid="hanspub.41221-ref9">9</xref>] 是基于3D人脸模型的GAN。PIM [<xref ref-type="bibr" rid="hanspub.41221-ref10">10</xref>] 对TP-GAN做了一些改进。具体地说，这种改进是使用了一种域自适应策略，可以提高具有极端姿势变化的面部的识别性能。</p><p>使用GAN进行正面人脸生成比以往基于CNN (Convolutional Neural Networks)的方法需要的计算强度多很多倍，甚至是高1~2个数量级。例如TP-GAN有很好的生成效果，但是模型复杂，训练耗时，与MobileNet-v3的0.44G MAC (multiply-accumulate)相比，TP-GAN生成一个图像消耗281G MAC，不仅需要训练全局网络，还要训练多个补丁网络，另外其数据处理也有很高的延迟，这极大地限制了它的使用场景。如下图1，清晰、逼真的生成结果的代价就是大量的计算需求，在模型大小相差不多的情况下，人脸生成GAN也比用于识别的CNN所需计算量多很多。</p><p>图1. 图像生成GAN需要的计算量比图片分类CNN多得多</p><p>近日，来自MIT、Adobe、上海交通大学的研究者提出了一种用于压缩条件GAN的通用方法 [<xref ref-type="bibr" rid="hanspub.41221-ref11">11</xref>]。这一新技术在保持视觉保真度的同时，将pix2pix，CycleGAN和GauGAN等广泛使用的条件GAN模型的计算量减少到1/9~1/21。本文提出了一种算法，将此思想应用到人脸转正的GAN模型上来。实验证明，改进后的模型显著地减少了进行人脸转正工作的计算量，压缩后GAN所需计算量减少了7~19倍，生成图片的质量仍然得到了很好地保持。</p></sec><sec id="s6"><title>2. 相关工作</title><sec id="s6_1"><title>2.1. GAN</title><p>GAN使用对抗的学习方案，通过生成器和判别器之间的零一博弈，使双方都在训练上有所改进。生成对抗网络的目标函数可以表示为：</p><p>min G max D E x ~ p data [ log D ( x ) ] + E z ~ p z [ log ( 1 − D ( G ( z ) ) ) ] (1)</p><p>其中，x取样自真实的样本分布，z取样自噪声的随机分布。G和D分别表示生成网络和对抗网络。通过这种机制，GAN可以很好地发现从源数据域到目的数据域的映射。基于它的这一特性，研究人员挖掘出了其在图像生成方面的巨大潜力。随着GAN模型的不断发展，人脸正面化取得了显著成就。Twopathway GAN (TP-GAN) [<xref ref-type="bibr" rid="hanspub.41221-ref12">12</xref>] 率先提出了一种用于正面人脸合成的两通道方法，该方法能够捕获局部细节并同时理解全局结构。此后不久， [<xref ref-type="bibr" rid="hanspub.41221-ref13">13</xref>] 开发了一个基于GAN的框架，该框架重新组合了不同的身份和属性，以在开放域中合成人脸时保留身份。之后的PIM [<xref ref-type="bibr" rid="hanspub.41221-ref10">10</xref>] 以相互促进的方式共同学习姿势不变特征提取和人脸合成。</p></sec><sec id="s6_2"><title>2.2. 模型加速</title><p>为了减少模型权重中的冗余，加速深度学习模型的训练进程，研究人员提出了一种对模型中各个层之间的连接进行剪枝的方法，并以某些特定的硬件来加速其运行。一些后续的研究工作指出可以修剪所有的卷积滤波器来提高计算的规律性。用于模型压缩的AutoML (AMC) [<xref ref-type="bibr" rid="hanspub.41221-ref14">14</xref>] 利用强化学习来自动确定每层的修剪率。后来，Liu等人 [<xref ref-type="bibr" rid="hanspub.41221-ref15">15</xref>] 用进化搜索算法代替了强化学习。</p></sec><sec id="s6_3"><title>2.3. 知识蒸馏</title><p>Hinton等 [<xref ref-type="bibr" rid="hanspub.41221-ref16">16</xref>] 首次提出了知识蒸馏的概念，用于将较大的教师网络中的知识转移到较小的学生网络中，学生网络被训练来模仿教师网络的行为。有几种方法利用知识蒸馏来压缩识别模型 [<xref ref-type="bibr" rid="hanspub.41221-ref17">17</xref>]。最近，Aguinaldo等人 [<xref ref-type="bibr" rid="hanspub.41221-ref18">18</xref>] 采用这种方法来加速无条件GAN。与他们不同的是，韩松等人 [<xref ref-type="bibr" rid="hanspub.41221-ref11">11</xref>] 专注于条件GAN。他们对条件GAN进行了几种蒸馏方法的实验，但是仅观察到了边际改进，不足以进行交互式应用。所以要达到压缩人脸生成GAN的目的，还需要结合NAS技术。</p></sec><sec id="s6_4"><title>2.4. NAS</title><p>神经体系结构搜索(NAS)已成功设计出了在大型图像分类任务中胜过手工设计的神经网络体系结构 [<xref ref-type="bibr" rid="hanspub.41221-ref19">19</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref20">20</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref21">21</xref>]。为了有效地降低搜索成本，研究人员最近提出了一步法(One-Shot)神经网络模型搜索框架 [<xref ref-type="bibr" rid="hanspub.41221-ref22">22</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref23">23</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref24">24</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref25">25</xref>]，其中不同的候选子网可以共享同一组权重。尽管所有这些方法都专注于图像分类模型，但我们仍可以使用这项技术来寻找高效的人脸生成GAN架构。</p></sec></sec><sec id="s7"><title>3. 方法</title><p>对人脸生成GAN进行压缩有两个难点。首先，GANs的训练动态本质上是高度不稳定的。其次，识别模型和生成模型之间的巨大架构差异使得现有CNN压缩算法难以直接被应用。为解决上述问题，韩松等 [<xref ref-type="bibr" rid="hanspub.41221-ref11">11</xref>] 提出了针对有效生成模型量身定制的训练协议，并通过神经体系结构搜索(NAS)进一步提高了压缩率。整体架构如图2所示，使用ResNet生成器作为示例。①使用预训练教师生成器G，我们蒸馏了一个更小的“one-for-all”学生生成器，它包含所有可能的通道数量，我们在每个训练步骤选择不同通道数量的学生生成器。②然后我们可以从“one-for-all”生成器中提取很多的子生成器，并评估它们的表现。提取出的子生成器不需要再次进行训练，这就是使用“one-for-all”生成器的好处。③最终，我们选择在给定压缩比率和指标下表现最好的子生成器，进行微调，就获得了最终的压缩模型。</p><p>本文将这一思想运用于人脸生成GAN上，实验证明了其对人脸生成GAN的有效性。</p><p>图2. GAN压缩框架</p><sec id="s7_1"><title>3.1. 训练目标</title><p>人脸正面化GAN的目标是学习从侧面人脸X到正面人脸Y的映射函数函数G，可使用成对的数据进行训练( { x i , y i } i = 1 N ， x i ∈ X 且 y i ∈ Y )。N表示训练图片的数目。模型学习目标形式化如下，其中 E x , y ≜ E x , y ~ p data ( x , y ) ， ‖ ‖ 1 表示L1正则模。</p><p>L recon = E x , y ‖ G ( x ) − y ‖ 1 (2)</p><sec id="s7_1_1"><title>3.1.1. 继承鉴别器</title><p>尽管旨在压缩发生器，但鉴别器D也会存储训练好的GAN中的知识，用来发现当前生成器的弱点 [<xref ref-type="bibr" rid="hanspub.41221-ref26">26</xref>]。因此，本文采用相同的鉴别器架构，使用来自教师的预先训练的权重，并让压缩好的生成器和鉴别器一起进行微调。在本文实验中，可以观察到预训练的鉴别器可以指导学生生成器的训练。使用随机初始化的鉴别器通常会导致严重的训练不稳定和图像质量下降。GAN目标的形式化为：</p><p>L cGAN = E x , y [ log D ( x , y ) ] + E x [ log ( 1 − D ( x , G ( x ) ) ) ] (3)</p><p>本文模型中，使用教师鉴别器D'的权重初始化学生鉴别器D。G和D使用标准的minimax优化进行训练。</p></sec><sec id="s7_1_2"><title>3.1.2. 中间特征蒸馏</title><p>CNN模型压缩的一种广泛使用的方法是知识蒸馏 [<xref ref-type="bibr" rid="hanspub.41221-ref17">17</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref27">27</xref>]。通过匹配输出层的逻辑分布，可以将暗知识从教师模型转移到学生模型，从而提高学生模型的表现。但是，人脸生成GAN会输出确定性图像，而不是概率分布。因此，很难从教师模型的输出中提取暗知识。</p><p>为了解决上述问题，本文模型改为从教师生成器的中间表示形式提取暗知识，如先前的工作 [<xref ref-type="bibr" rid="hanspub.41221-ref17">17</xref>] 所述。模型中间层包含更多通道，提供更丰富的信息，并允许学生模型除输出外获取更多信息。可以将蒸馏目标形式化为</p><p>L distill = ∑ t = 1 T ‖ G t ( x ) − f t ( G ′ t ( x ) ) ‖ 2 (4)</p><p>其中 G t ( x ) 和 G ′ t ( x ) 是在学生和教师模型中所选择的第t层的中间特征激活函数，T表示层数。1 &#215; 1卷积层f<sub>t</sub>将学生模型的特征映射到教师模型的特征中相同数量的通道，以共同优化G<sub>t</sub>和f<sub>t</sub>，以最大程度地减少蒸馏损失l<sub>distill</sub>。</p></sec><sec id="s7_1_3"><title>3.1.3. 中间特征蒸馏</title><p>本文最终目标函数如下：</p><p>L = L cGAN + λ recon L recon + λ distill L distill (5)</p><p>其中，超参数λ<sub>recon</sub>和λ<sub>distill</sub>控制每个项的重要程度。</p></sec></sec><sec id="s7_2"><title>3.2. 高效的生成器设计空间</title><p>选择一个设计良好的学生模型架构对于最终进行知识蒸馏至关重要。一般地，单纯地缩小教师模型的通道数并不能简单地生成一个紧凑的学生模型。经常地，在减少了4倍的计算量之后，性能开始显着下降。可能的原因之一是现有的生成器架构经常使用来自图像识别模型的 [<xref ref-type="bibr" rid="hanspub.41221-ref28">28</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref29">29</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref30">30</xref>]，这也许不是图像合成任务的最佳选择。下面，本文展示了如何从现有的GAN生成器中获得更好的架构设计空间，并在该空间内执行神经架构搜索(NAS)。</p><sec id="s7_2_1"><title>3.2.1. 卷积分解和层敏感性</title><p>通常地，有效的CNN设计广泛采用了分解版本的卷积(depthwise + pointwise) [<xref ref-type="bibr" rid="hanspub.41221-ref31">31</xref>]，在表现与计算量之间的取得了很好的平衡。本文发现使用分解卷积也适用于GAN中的生成器设计。但是，将分解卷积直接应用于所有卷积层(如在分类器中)将大大降低图像质量。在某些层使用分解卷积会造成性能下降，而其他层则不会。此外，GAN的层敏感性与识别模型的不同。例如，在ResNet生成器 [<xref ref-type="bibr" rid="hanspub.41221-ref32">32</xref>] 中，resBlock层消耗了大部分模型参数和计算成本，几乎不受分解的影响。相反，上采样层参数要少得多，却对模型压缩相当敏感。</p></sec><sec id="s7_2_2"><title>3.2.2. 使用NAS自动减少通道</title><p>现有的发生器通常在所有层之间使用人工设置的通道数，其中存在冗余并且远非最佳。为了进一步提高压缩率，本文使用通道修剪 [<xref ref-type="bibr" rid="hanspub.41221-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref14">14</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref33">33</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref34">34</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref35">35</xref>] 在生成器中自动选择通道宽度，以消除冗余，再次减少计算量。本文模型支持对通道数的细粒度选择，对于每个卷积层，可以从8的倍数中选择通道数，这可以平衡MACs和硬件并行性。</p><p>给定可能的通道配置 { c 1 , c 2 , ⋯ , c K } ，其中K是要修剪的层数，本文试图使用神经体系结构搜索找到最佳的通道配置 { c 1 ∗ , c 2 ∗ , ⋯ , c K ∗ } = arg min c 1 , c 2 , ⋯ , c K l ，s.t. MACs &lt; F<sub>t</sub>，其中F<sub>t</sub>是计算约束。通常，直接的方法是遍历所有可能的通道配置，训练它们直至收敛，然后从中评估并选择性能最佳的生成器。但是，随着K的增加，可能配置的数量呈指数增长，并且每种配置可能需要在每个阶段设置学习率和权重的不同超参数，非常耗时。</p></sec></sec><sec id="s7_3"><title>3.3. 解耦训练和搜索</title><p>为了解决这个问题，本文模型遵循一步法神经体系结构搜索方法的最新研究 [<xref ref-type="bibr" rid="hanspub.41221-ref23">23</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref25">25</xref>] [<xref ref-type="bibr" rid="hanspub.41221-ref36">36</xref>]，将模型训练与体系结构搜索分离。首先训练一个支持不同通道数的“once-for-all”网络 [<xref ref-type="bibr" rid="hanspub.41221-ref25">25</xref>]。具有不同数目通道的每个子网都经过相同的训练过程，可以独立运行。子网与“once-for-all”网络共享权重。图2说明了整个框架。假设原始教师生成器具有 { c k 0 } k = 1 K 个通道。对于给定的信道编号配置 { c k } k = 1 K ， c k ≤ c k 0 ，通过从“once-for-all”的相应权重张量中提取开始的 { c k } k = 1 K 通道来获得子网的权重。遵循Guo等人的理论 [<xref ref-type="bibr" rid="hanspub.41221-ref36">36</xref>]，在每个训练步骤中，使用一定的通道数配置对子网进行随机抽样，计算输出和梯度，并使用学习目标来更新提取的权重(等式5)。由于前几个通道的权重被更频繁地更新，因此它们在所有权重中扮演着更为关键的角色。</p><p>在训练了“once-for-all”网络之后，通过直接在验证集上评估每个候选子网的性能来找到最佳子网。由于“once-for-all”网络经过权重共享的方式直接训练，因此无需进行微调。通过这种方式，可以将训练和搜索生成器体系结构分离开来：只需要训练一次，然后就可以在无需进一步训练的情况下评估所有可能的通道配置，并选择最佳的作为搜索结果。然后将所选择的网络进行微调来进一步提升它的表现。</p></sec></sec><sec id="s8"><title>4. 实验</title><sec id="s8_1"><title>4.1. 实验设置</title><sec id="s8_1_1"><title>4.1.1. 模型</title><p>本文在以下两种模型上进行实验以论证方法的有效性。</p><p>TP-GAN [<xref ref-type="bibr" rid="hanspub.41221-ref12">12</xref>] 是一个能够考虑整体和局部信息的双路径对抗生成网络，可以用于从单一侧面照片合成高清晰的正面人脸图像。通过该方法合成的人脸图像能够很好地保留身份特征，并且可以处理大量不同姿势和光照的图片。</p><p>CR-GAN [<xref ref-type="bibr" rid="hanspub.41221-ref8">8</xref>] 也是一个双路径生成对抗网络，除了一个重建路径外，还引入了一条路径以保持学习到的嵌入空间的完整性。这两条路径以参数共享的方式进行协作和竞争，从而极大地提高了能力。</p></sec><sec id="s8_1_2"><title>4.1.2. 数据集</title><p>MultiPIE数据集 [<xref ref-type="bibr" rid="hanspub.41221-ref37">37</xref>] 是在受控环境中用于人脸合成和识别的最大公共数据库。MultiPIE数据库包含337个身份个体，每个个体下含有20种光照、15种姿态以及两种标签，分成四个时间段拍摄而成。并且，数据库中还有少量的高像素度的人脸图片供研究使用。整个MultiPIE数据库共有超过750,000张人脸图像，是一个大规模的能用于多姿态人脸识别的数据库。本文实验使用前200名受试者的图像进行训练，其中包括在 &#177; 90˚内具有13个姿势和20个照明水平的样本。其余137个身份的样本构成了测试集，而表达和照明中性的样本构成了画廊。请注意，训练和测试集之间没有重叠的科目。</p><p>CAS_PEAL_R1数据集 [<xref ref-type="bibr" rid="hanspub.41221-ref38">38</xref>] 是一个公开发布的大规模中国人人脸数据库，其姿态，表情，配饰和灯光变化都受到控制。它包括1040人的30,863张灰度图片，其中男性595名，女性445名。</p></sec><sec id="s8_1_3"><title>4.1.3. 实验细节</title><p>为了训练本文模型，需要成对的人脸图像，其中是侧面人脸图像，是正面人脸图像。首先，需要将所有图片修剪到128,128 [<xref ref-type="bibr" rid="hanspub.41221-ref16">16</xref>]。所使用的身份保留网络在MS-Celeb-1M上进行预训练，并在Multi-PIE的训练集上进行微调。对于CAS-PEAL-R1，所有图片都被设置为灰度图片。用于训练CAS-PEAL-R1的身份保留网络已在MS-Celeb-1M的灰度图片上进行了预训练。</p></sec></sec><sec id="s8_2"><title>4.2. 结果</title><sec id="s8_2_1"><title>4.2.1. 身份保留能力</title><p>为了定量地论证压缩后模型的身份保留能力，本文评估了在合成出的人脸图片上进行人脸识别的准确率。本文使用一个训练好的29层Light-CNN做为人脸识别模型来提取特征，然后用余弦距离度量来计算这些特征的相似度。大姿态下的人脸图片提供的信息太少，使得模型很难在生成的正面人脸图片中保留身份信息。表1比较了在MultiPIE数据集上不同角度下各个模型的表现，表2比较了在CAS_PEAL_R1数据集上不同角度下各个模型的表现，结果都以rank-1识别率表示。如表所示，各个角度下的表现，原模型与压缩模型都相当接近。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Rank-1 recognition rate from various angles on the MultiPIE dataset (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >90&#176;</th><th align="center" valign="middle" >75&#176;</th><th align="center" valign="middle" >60&#176;</th><th align="center" valign="middle" >45&#176;</th><th align="center" valign="middle" >30&#176;</th><th align="center" valign="middle" >15&#176;</th><th align="center" valign="middle" >Avg</th></tr></thead><tr><td align="center" valign="middle" >TP-GAN</td><td align="center" valign="middle" >64.64</td><td align="center" valign="middle" >77.43</td><td align="center" valign="middle" >87.72</td><td align="center" valign="middle" >95.38</td><td align="center" valign="middle" >98.06</td><td align="center" valign="middle" >98.68</td><td align="center" valign="middle" >86.99</td></tr><tr><td align="center" valign="middle" >CR-GAN</td><td align="center" valign="middle" >71.60</td><td align="center" valign="middle" >92.50</td><td align="center" valign="middle" >97.00</td><td align="center" valign="middle" >98.60</td><td align="center" valign="middle" >99.30</td><td align="center" valign="middle" >99.40</td><td align="center" valign="middle" >93.07</td></tr><tr><td align="center" valign="middle" >TP-GAN(c)</td><td align="center" valign="middle" >60.73</td><td align="center" valign="middle" >72.32</td><td align="center" valign="middle" >83.65</td><td align="center" valign="middle" >92.19</td><td align="center" valign="middle" >96.01</td><td align="center" valign="middle" >97.24</td><td align="center" valign="middle" >83.69</td></tr><tr><td align="center" valign="middle" >CR-GAN(c)</td><td align="center" valign="middle" >67.84</td><td align="center" valign="middle" >88.21</td><td align="center" valign="middle" >93.37</td><td align="center" valign="middle" >95.28</td><td align="center" valign="middle" >97.15</td><td align="center" valign="middle" >98.75</td><td align="center" valign="middle" >90.10</td></tr></tbody></table></table-wrap><p>表1. MultiPIE数据集上各个角度下的rank-1识别率(%)</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Rank-1 recognition rate on the CAS_PEAL_R1 dataset (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle"  colspan="5"  >Pitch(-15˚)</th><th align="center" valign="middle"  colspan="4"  >Pitch (0˚)</th><th align="center" valign="middle"  colspan="5"  >Pitch (+15˚)</th></tr></thead><tr><td align="center" valign="middle" >Yaw</td><td align="center" valign="middle" >0˚</td><td align="center" valign="middle" >15˚</td><td align="center" valign="middle" >30˚</td><td align="center" valign="middle" >45˚</td><td align="center" valign="middle" >Avg_1</td><td align="center" valign="middle" >15˚</td><td align="center" valign="middle" >30˚</td><td align="center" valign="middle" >45˚</td><td align="center" valign="middle" >Avg_2</td><td align="center" valign="middle" >0˚</td><td align="center" valign="middle" >15˚</td><td align="center" valign="middle" >30˚</td><td align="center" valign="middle" >45˚</td><td align="center" valign="middle" >Avg_3</td></tr><tr><td align="center" valign="middle" >TP-GAN</td><td align="center" valign="middle" >98.86</td><td align="center" valign="middle" >98.94</td><td align="center" valign="middle" >98.89</td><td align="center" valign="middle" >97.62</td><td align="center" valign="middle" >98.58</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >99.94</td><td align="center" valign="middle" >98.71</td><td align="center" valign="middle" >99.55</td><td align="center" valign="middle" >97.68</td><td align="center" valign="middle" >97.73</td><td align="center" valign="middle" >97.45</td><td align="center" valign="middle" >95.83</td><td align="center" valign="middle" >97.17</td></tr><tr><td align="center" valign="middle" >CR-GAN</td><td align="center" valign="middle" >83.98</td><td align="center" valign="middle" >83.91</td><td align="center" valign="middle" >83.71</td><td align="center" valign="middle" >80.38</td><td align="center" valign="middle" >82.86</td><td align="center" valign="middle" >97.61</td><td align="center" valign="middle" >95.80</td><td align="center" valign="middle" >89.73</td><td align="center" valign="middle" >94.38</td><td align="center" valign="middle" >89.74</td><td align="center" valign="middle" >89.44</td><td align="center" valign="middle" >87.95</td><td align="center" valign="middle" >83.90</td><td align="center" valign="middle" >87.76</td></tr><tr><td align="center" valign="middle" >TP-GAN(c)</td><td align="center" valign="middle" >97.73</td><td align="center" valign="middle" >96.94</td><td align="center" valign="middle" >97.47</td><td align="center" valign="middle" >96.52</td><td align="center" valign="middle" >97.16</td><td align="center" valign="middle" >99.57</td><td align="center" valign="middle" >99.43</td><td align="center" valign="middle" >97.18</td><td align="center" valign="middle" >98.93</td><td align="center" valign="middle" >95.93</td><td align="center" valign="middle" >95.42</td><td align="center" valign="middle" >94.17</td><td align="center" valign="middle" >93.28</td><td align="center" valign="middle" >94.70</td></tr><tr><td align="center" valign="middle" >CR-GAN(c)</td><td align="center" valign="middle" >80.73</td><td align="center" valign="middle" >80.26</td><td align="center" valign="middle" >80.43</td><td align="center" valign="middle" >79.92</td><td align="center" valign="middle" >80.34</td><td align="center" valign="middle" >95.25</td><td align="center" valign="middle" >93.74</td><td align="center" valign="middle" >87.10</td><td align="center" valign="middle" >92.03</td><td align="center" valign="middle" >87.32</td><td align="center" valign="middle" >87.07</td><td align="center" valign="middle" >85.28</td><td align="center" valign="middle" >80.15</td><td align="center" valign="middle" >84.96</td></tr></tbody></table></table-wrap><p>表2. CAS_PEAL_R1数据集上的rank-1识别率(%)</p></sec><sec id="s8_2_2"><title>4.2.2. 模型压缩效果</title><p>在表3中，记录了在MultiPIE数据集上对TP-GAN，PIM和CR-GAN进行压缩的定量结果。通过使用“once-for-all”网络中性能最佳的子网，这一GAN压缩方法可实现较大的压缩率。它可以将人脸生成GAN的计算量减少7~25倍，并将模型的参数量缩小5~31倍，从上一节可以看到，在这么大的压缩比下，模型的表现下降极小。这说明了该方法的有效性。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Quantitative evaluation of GAN compressio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >参数</th><th align="center" valign="middle" >MACs</th></tr></thead><tr><td align="center" valign="middle"  rowspan="2"  >TP-GAN</td><td align="center" valign="middle" >原始</td><td align="center" valign="middle" >14.3 M</td><td align="center" valign="middle" >78.3 G</td></tr><tr><td align="center" valign="middle" >压缩</td><td align="center" valign="middle" >0.52 M (27.3&#215;)</td><td align="center" valign="middle" >4.03 G (19.4&#215;)</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >PIM</td><td align="center" valign="middle" >原始</td><td align="center" valign="middle" >12.7 M</td><td align="center" valign="middle" >57.9 G</td></tr><tr><td align="center" valign="middle" >压缩</td><td align="center" valign="middle" >0.84 M (15.1&#215;)</td><td align="center" valign="middle" >5.17 G (11.2&#215;)</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >CR-GAN</td><td align="center" valign="middle" >原始</td><td align="center" valign="middle" >10.5 M</td><td align="center" valign="middle" >32.4 G</td></tr><tr><td align="center" valign="middle" >压缩</td><td align="center" valign="middle" >1.81 M (5.8&#215;)</td><td align="center" valign="middle" >4.43 G (7.3&#215;)</td></tr></tbody></table></table-wrap><p>表3. 定量评估GAN压缩</p></sec><sec id="s8_2_3"><title>4.2.3. 生成图片效果</title><p>在这一部分，我们在视觉上比较了不同模型与其压缩模型的正面人脸生成效果。图3展示了在MultiPIE数据集上的定性比较结果。图4显示了在CAS_PEAL_R1各个模型的生成效果。我们提供输入，其真实正脸图片，不同角度下原始模型的输出以及压缩模型的输出。从图片可以看出压缩后的模型仍然可以生成相当接近原模型的逼真人脸图像。</p><p>图3. MultiPIE数据集下各模型表现</p><p>图4. CAS_PEAL_R1数据集上各模型的表现</p></sec><sec id="s8_2_4"><title>4.2.4. 加快硬件推断</title><p>对于现实世界的交互式应用程序，硬件上的计算加速比减少计算量更为关键。为了验证本文方法的实际有效性，本实验在几种具有不同计算能力的设备上测量压缩模型的推理速度。为了模拟交互式应用程序，我们使用1的批量大小。首先执行100次热身运行，并测量接下来100次运行的平均时间。结果如表4所示。注意到，与CPU相比，GPU上的加速效果不那么明显，这主要是由于GPU高度的并行性。 不过，专注于在可能无法使用强大GPU的设备上使模型更易于访问，以便更多人可以使用交互式人脸生成GAN应用程序。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Measured memory reduction and latency speedu</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >TP-GAN</th><th align="center" valign="middle" >CR-GAN</th></tr></thead><tr><td align="center" valign="middle" >MAC减少</td><td align="center" valign="middle" >17.8&#215;</td><td align="center" valign="middle" >11.7&#215;</td></tr><tr><td align="center" valign="middle" >内存减少</td><td align="center" valign="middle" >2.3&#215;</td><td align="center" valign="middle" >1.4&#215;</td></tr><tr><td align="center" valign="middle" >3060TI加速</td><td align="center" valign="middle" >0.007 s (2.7&#215;)</td><td align="center" valign="middle" >0.010 s (1.5&#215;)</td></tr><tr><td align="center" valign="middle" >I5 10400 CPU加速</td><td align="center" valign="middle" >0.13 s (3.2&#215;)</td><td align="center" valign="middle" >0.67 s (2.5&#215;)</td></tr></tbody></table></table-wrap><p>表4. 测量内存减少和模型加速</p></sec></sec></sec><sec id="s9"><title>5. 结论</title><p>本文借鉴韩松团队提出的通用压缩框架思想，提出了一种人脸生成GAN压缩算法，使用知识蒸馏和神经体系结构搜索，来减轻训练的不稳定性并提高模型效率。本文实验表明，本文方法在保留视觉质量的同时，可以有效压缩多个人脸生成GAN模型。</p></sec><sec id="s10"><title>文章引用</title><p>魏 雷,邱卫根,张立臣. 人脸转正GAN模型的高效压缩Efficient Compression of Face Frontalization GAN Model[J]. 计算机科学与应用, 2021, 11(03): 661-671. https://doi.org/10.12677/CSA.2021.113068</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41221-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Ferrari, C., Lisanti, G., Berretti, S. and Del Bimbo, A. (2016) Effective 3D Based Frontalization for Unconstrained Face Recognition. 2016 23rd International Conference on Pattern Recognition (ICPR), Cancun, 1047-1052.  
&lt;br&gt;https://doi.org/10.1109/ICPR.2016.7899774</mixed-citation></ref><ref id="hanspub.41221-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Hassner, T., Harel, S., Paz, E. and Enbar, R. (2015) Effective Face Frontalization in Unconstrained Images. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, 7-12 June 2015, 4295-4304.</mixed-citation></ref><ref id="hanspub.41221-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Jeni, L.A. and Cohn, J.F. (2016) Person-Independent 3d Gaze Estimation Using Face Frontalization. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Las Vegas, 26 June-1 July 2016, 87-95.</mixed-citation></ref><ref id="hanspub.41221-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Booth, J., Roussos, A., Ponniah, A., et al. (2018) Large Scale 3D Morphable Models. International Journal of Computer Vision, 126, 233-254. &lt;br&gt;https://doi.org/10.1007/s11263-017-1009-7</mixed-citation></ref><ref id="hanspub.41221-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Booth, J., Roussos, A., Zafeiriou, S., Ponniah, A. and Dunaway, D. (2016) A 3d Morphable Model Learnt from 10,000 Faces. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, 26 June-1 July 2016, 5543-5552.</mixed-citation></ref><ref id="hanspub.41221-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Cao, J., Hu, Y., Zhang, H., et al. (2018) Learning a High Fidelity Pose Invariant Model for High-Resolution Face Frontalization.</mixed-citation></ref><ref id="hanspub.41221-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Huang, R., Zhang, S., Li, T., et al. (2017) Beyond Face Rotation: Global and Local Perception Gan for Photorealistic and Identity Preserving Frontal View Synthesis. Proceedings of the IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 2439-2448. vision. 2439-2448. 
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.267</mixed-citation></ref><ref id="hanspub.41221-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Tian, Y., Peng, X., Zhao, L., et al. (2018) CR-GAN: Learning Com-plete Representations for Multi-View Generation. Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence Main Track, Stockholm, 13-19 July 2018, 942-948. &lt;br&gt;https://doi.org/10.24963/ijcai.2018/131</mixed-citation></ref><ref id="hanspub.41221-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Yin, X., Yu, X., Sohn, K., Liu, X.M. and Chandraker, M. (2017) Towards Large-Pose Face Frontalization in the Wild. Proceedings of the IEEE International Conference on Computer Vision (ICCV), Venice, 22-29 October 2017, 3990-3999. &lt;br&gt;https://doi.org/10.1109/ICCV.2017.430</mixed-citation></ref><ref id="hanspub.41221-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Zhao, J., Cheng, Y., Xu, Y., Xiong, L., Li, J., Zhao, F., Jayashree, K., Pranata, S., Shen, S., Xing, J., et al. (2018) Towards Pose Invariant Face Recognition in the Wild. Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), Salt Lake City, 18-23 June 2018, 2207-2216. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00235</mixed-citation></ref><ref id="hanspub.41221-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Li, M., Lin, J., Ding, Y., et al. (2020) GAN Compression: Efficient Architectures for Interactive Conditional Gans. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, 14-19 June 2020, 5284-5294. &lt;br&gt;https://doi.org/10.1109/CVPR42600.2020.00533</mixed-citation></ref><ref id="hanspub.41221-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">He, Y.H., Zhang, X.Y. and Sun, J. (2017) Channel Pruning for Accelerating Very Deep Neural Networks. Proceedings of the IEEE International Conference on Computer Vision (ICCV), Venice, 22-29 October 2017, 1389-1397.</mixed-citation></ref><ref id="hanspub.41221-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Shen, Y., Luo, P., Yan, J., Wang, X. and Tang, X. (2018) Faceid-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, 18-23 June 2018, 821-830. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00092</mixed-citation></ref><ref id="hanspub.41221-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">He, Y.H., Lin, J., Liu, Z.J., Wang, H.R., Li, L.-J. and Han, S. (2018) AMC: AutoML for Model Compression and Acceleration on Mobile Devices. Proceedings of the European Conference on Computer Vision (ECCV), Munich, 8-14 September 2018, 784-800. &lt;br&gt;https://doi.org/10.1007/978-3-030-01234-2_48</mixed-citation></ref><ref id="hanspub.41221-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Liu, Z.C., Mu, H.Y., Zhang, X.Y., Guo, Z.C., Yang, X., Kwang-Ting Cheng, T. and Sun, J. (2019) Metapruning: Meta Learning for Automatic Neural Network Channel Pruning. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, 27 October-2 November 2019, 3296-3305. &lt;br&gt;https://doi.org/10.1109/ICCV.2019.00339</mixed-citation></ref><ref id="hanspub.41221-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Hinton, G., Vinyals, O. and Dean, J. (2015) Distilling the Knowledge in a Neural Network.</mixed-citation></ref><ref id="hanspub.41221-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Chen, G., Choi, W., Yu, X., et al. (2017) Learning Efficient Object Detection Models with Knowledge Distillation. Proceedings of the 31st International Conference on Neural Information Processing Systems, Long Beach, 4-9 December 2017, 742-751.</mixed-citation></ref><ref id="hanspub.41221-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Aguinaldo, A., Chiang, P.-Y., Gain, A., Patil, A., Pearson, K. and Feizi, S. (2019) Compressing Gans Using Knowledge Distillation.</mixed-citation></ref><ref id="hanspub.41221-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Zoph, B. and Le, Q.V. (2016) Neural Architecture Search with Reinforcement Learning.</mixed-citation></ref><ref id="hanspub.41221-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Liu, C., Zoph, B., Neumann, M., et al. (2018) Progres-sive Neural Architecture Search. Proceedings of the European Conference on Computer Vision (ECCV), Munich, 8-14 September 2018, 19-34.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01246-5_2</mixed-citation></ref><ref id="hanspub.41221-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Liu, H., Simonyan, K., Vinyals, O., et al. (2017) Hierarchical Representations for Efficient Architecture Search.</mixed-citation></ref><ref id="hanspub.41221-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Liu, H., Simonyan, K. and Yang, Y. (2018) Darts: Differentiable Architecture Search.</mixed-citation></ref><ref id="hanspub.41221-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Cai, H., Zhu, L. and Han, S. (2018) Proxylessnas: Direct Neural Architecture Search on Target Task and Hardware.</mixed-citation></ref><ref id="hanspub.41221-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Wu, B.C., et al. (2019) Fbnet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seoul, 27 October-2 November 2019, 10734-10742. &lt;br&gt;https://doi.org/10.1109/CVPR.2019.01099</mixed-citation></ref><ref id="hanspub.41221-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Cai, H., Gan, C., Wang, T., et al. (2019) Once-for-All: Train One Network and Specialize It for Efficient Deploy-ment.</mixed-citation></ref><ref id="hanspub.41221-ref26"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Azadi, S., Olsson, C., Darrell, T., et al. (2018) Discriminator Rejection Sampling.</mixed-citation></ref><ref id="hanspub.41221-ref27"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Y., Wang, N. and Zhang, Z. (2018) DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer. Proceedings of the AAAI Conference on Artificial Intelligence, 32, 2852-2859.  
&lt;br&gt;https://ojs.aaai.org/index.php/AAAI/article/view/11783</mixed-citation></ref><ref id="hanspub.41221-ref28"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">Long, J., Shelhamer, E. and Darrell, T. (2015) Fully Convolutional Networks for Semantic Segmentation. CVPR, Boston, 7-12 June 2015, 3431-3440. &lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298965</mixed-citation></ref><ref id="hanspub.41221-ref29"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">He, K.M., Zhang, X.Y., Ren, S.Q. and Sun, J. (2016) Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-nition (CVPR), Las Vegas, 26 June-1 July 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.41221-ref30"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Ronneberger, O., Fischer, P. and Brox, T. (2015) U-net: Convolution-al Networks for Biomedical Image Segmentation. MICCAI, Munich, 5-9 October 2015, 234-241. &lt;br&gt;https://doi.org/10.1007/978-3-319-24574-4_28</mixed-citation></ref><ref id="hanspub.41221-ref31"><label>31</label><mixed-citation publication-type="other" xlink:type="simple">Howard, A.G., Zhu, M.L., Chen, B., Kalenichenko, D., et al. (2017) Efficient Convolutional Neural Networks for Mobile Vision Applications.</mixed-citation></ref><ref id="hanspub.41221-ref32"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Johnson, J., et al. (2016) Per-ceptual Losses for Real-Time Style Transfer and Super-Resolution. In: European Conference on Computer Vision, Springer, Cham, 694-711. &lt;br&gt;https://doi.org/10.1007/978-3-319-46475-6_43</mixed-citation></ref><ref id="hanspub.41221-ref33"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">Liu, Z., Li, J.G., Shen, Z.Q., Huang, G., Yan, S.M. and Zhang, C.S. (2017) Learning Efficient Convolutional Networks through Network Slimming. Pro-ceedings of the IEEE International Conference on Computer Vision (ICCV), Venice, 22-29 October 2017, 2736-2744.</mixed-citation></ref><ref id="hanspub.41221-ref34"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">Zhuang, Z.W., Tan, M.K., Zhuang, B.H., Liu, J., Guo, Y., Wu, Q.Y., et al. (2018) Discrimina-tion-Aware Channel Pruning for Deep Neural Networks. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, 3-8 December 2018, 875-886.</mixed-citation></ref><ref id="hanspub.41221-ref35"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Luo, J.H., Wu, J. and Lin, W. (2017) ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression. Proceedings of the IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 5058-5066.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.541</mixed-citation></ref><ref id="hanspub.41221-ref36"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">Guo, Z.C., Zhang, X.Y., Mu, H.Y., Heng, W., Liu, Z.C., Wei, Y.C. and Sun, J. (2019) Single Path Oneshot Neural Architecture Search with Uniform Sampling.</mixed-citation></ref><ref id="hanspub.41221-ref37"><label>37</label><mixed-citation publication-type="other" xlink:type="simple">Gross, R., Matthews, I., Cohn, J., et al. (2010) Multi-Pie. Image and Vision Computing, 28, 807-813.  
&lt;br&gt;https://doi.org/10.1016/j.imavis.2009.08.002</mixed-citation></ref><ref id="hanspub.41221-ref38"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Gao, W., Cao, B., Shan, S., et al. (2007) The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations. IEEE Transactions on Systems, Man, and Cybernetics Part A: Systems and Humans, 38, 149-161.  
&lt;br&gt;https://doi.org/10.1109/TSMCA.2007.909557</mixed-citation></ref></ref-list></back></article>