<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.104077</article-id><article-id pub-id-type="publisher-id">CSA-35242</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200400000_93800739.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于LSH技术的试题相似度检测方法
  The Application of LSH Technology in Similar Question Detection
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>瑞</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>松</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>梅</surname><given-names>莹</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>杨</surname><given-names>云源</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>楚雄师范学院地理科学与旅游管理学院，云南 楚雄</addr-line></aff><aff id="aff2"><addr-line>楚雄师范学院经济与管理学院，云南 楚雄</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>01</day><month>04</month><year>2020</year></pub-date><volume>10</volume><issue>04</issue><fpage>741</fpage><lpage>748</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   试题内容重复率是评价试题库及试卷质量的重要指标之一，为了快速找出题库中的相似试题，本文主要研究了基于K-shingles的Jaccard相似度、MinHash和LSH技术应用于相似试题的检测方法。此方法首先将题干内容进行中文分词，进行适当处理后转换成K-shingle集，通过MinHash计算出签名，最后使用LSH技术快速地找出候选相似试题对并计算出相应的Jaccard相似度，若该相似度大于给定的阈值，则发现相似试题。该方法通过在题库系统中的使用，充分验证了该方法的可行性，达到了很好的效果。 The repetition rate of test questions is one of the important indexes to evaluate the quality of test questions and test papers. In order to quickly find out similar questions in the test bank, this paper mainly studies the detection methods of similar questions based on K-shingles, Jaccard similarity, MinHash and LSH technology. First of all, the main content of the question is segmented into Chinese words, then converted into K-shingle set after proper processing, and the signature is calculated by MinHash. Finally, LSH technology is used to quickly find out the candidate pairs of similar questions and calculate the corresponding Jaccard similarity. If the similarity is greater than the given threshold, similar questions are found. Experiments prove to be practicable and effective. 
  
 
</p></abstract><kwd-group><kwd>试题查重，LSH算法，Jaccard相似度，K-shingle, Examination Checking</kwd><kwd> LSH Algorithm</kwd><kwd> Jaccard Similarity</kwd><kwd> K-Shingle</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于LSH技术的试题相似度检测方法<sup> </sup></title><p>陈瑞<sup>1</sup>，王松<sup>1</sup>，梅莹<sup>1</sup>，杨云源<sup>2</sup></p><p><sup>1</sup>楚雄师范学院经济与管理学院，云南 楚雄</p><p><sup>2</sup>楚雄师范学院地理科学与旅游管理学院，云南 楚雄</p><p>收稿日期：2020年4月2日；录用日期：2020年4月17日；发布日期：2020年4月24日</p><disp-formula id="hanspub.35242-formula31"><graphic xlink:href="//html.hanspub.org/file/17-1541740x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>试题内容重复率是评价试题库及试卷质量的重要指标之一，为了快速找出题库中的相似试题，本文主要研究了基于K-shingles的Jaccard相似度、MinHash和LSH技术应用于相似试题的检测方法。此方法首先将题干内容进行中文分词，进行适当处理后转换成K-shingle集，通过MinHash计算出签名，最后使用LSH技术快速地找出候选相似试题对并计算出相应的Jaccard相似度，若该相似度大于给定的阈值，则发现相似试题。该方法通过在题库系统中的使用，充分验证了该方法的可行性，达到了很好的效果。</p><p>关键词 :试题查重，LSH算法，Jaccard相似度，K-shingle</p><disp-formula id="hanspub.35242-formula32"><graphic xlink:href="//html.hanspub.org/file/17-1541740x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/17-1541740x7_hanspub.png" /> <img src="//html.hanspub.org/file/17-1541740x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>试题内容重复是影响试题库及试卷质量的重要因素之一，所以对相似试题的检测和去重就尤为重要。目前相似试题的发现方法主要使用自然语言处理(NLP)中的文本相似性检测方法。这些方法大致可分为以下几类：</p><p>&#183; 单词共现方法(Word co-occurrence)。</p><p>&#183; 基于词汇数据库(lexical database)的相似度方法。</p><p>&#183; 基于词向量(word vectors)的递归神经网络和深度神经网络方法。</p><p>单词共现方法通常用于识别文本数据中的重要部分 [<xref ref-type="bibr" rid="hanspub.35242-ref1">1</xref>]。该方法具有明显的缺点，例如：</p><p>&#183; 它忽略了句子的词序。</p><p>&#183; 它不考虑单词在句子上下文中的含义。</p><p>但是它具有以下优点：</p><p>&#183; 可匹配任何文本。</p><p>&#183; 能成功地从中提取关键字。</p><p>词汇数据库方法，使用具有在树状结构中编译的词、含义和与其他词关系的预定义词层次来计算相似度 [<xref ref-type="bibr" rid="hanspub.35242-ref2">2</xref>]。在比较两个词时，它考虑了词之间的路径距离以及包含者在层次中的深度。包含者指的是与被比较的两个词有关的相对根节点。它还使用单词语料库来计算单词的“信息内容”，这会影响最终的相似度。该方法具有以下局限性：</p><p>&#183; 在计算相似度时不考虑单词的适当含义，而是选择最佳匹配对，即使单词在两个不同的句子中的含义完全不同。</p><p>&#183; 来自一个语料库的词的信息内容与另一个语料库中的词的信息内容不同。</p><p>近年来，基于神经网络的模型在语义相似度有了很大的改进 [<xref ref-type="bibr" rid="hanspub.35242-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.35242-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.35242-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.35242-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.35242-ref7">7</xref>]。其中乐雨泉等 [<xref ref-type="bibr" rid="hanspub.35242-ref8">8</xref>] 提出了一个称为ACV-tree句子建模方法。这个方法可以看成一种通用的句子建模框架，此框架试图将句法知识(syntactic information)、语义信息(semantic information)、注意力权重机制(attention weight mechanism)合并到一个统一的结构中来吸收它们的优点。解决了自然语言处理中句子建模的问题。梁圣 [<xref ref-type="bibr" rid="hanspub.35242-ref9">9</xref>] 提出了循环神经网络用于计算试题相似度的方法，对比了不同词向量维度以及LSTM隐含层神经单元个数对相似度计算结果的影响。田星等 [<xref ref-type="bibr" rid="hanspub.35242-ref10">10</xref>] 提出了一种基于词向量的Jaccard句子相似度算法，该方法首先通过训练将每个词语映射为语义层面的高维向量，然后计算各个词向量之间的相似度，高于阈值α的作为共现部分，从而计算句子的相似度。陈勤等 [<xref ref-type="bibr" rid="hanspub.35242-ref11">11</xref>] 提出了一种基于上下文对齐的RNN (CA-RNN)模型，该模型将对齐后的词的上下文信息整合到一个句子对中，用于内部hidden state的生成。虽然神经网络方法在语义的相似性上取得了很大的改善，但神经网络方法却需要成千上万个标记样本作为训练数据，而训练往往需要花费大量的时间。</p><p>总体而言，上述所有方法在试题相似性检测上往往不太适合。这些方法过于追求于语义上的精准相似，代价高昂。其忽略了在多个样本中，两两相似比较的时间复杂度。本文的研究方法，即不需要额外的语料数据库，也不需要大量的训练样本，更无需借助除了Office之外的其它功能即可快速地实现相似试题的检测。</p><p>该方法基于K-shingle，K-shingle是连续K个词的唯一序列，通过K-shingles将试题内容转换成一组位向量0和1来表示，其中，0表示该试题中不存在此shingle，1表示该试题存在此shingle。近似相同的两个试题应该有许多相同的shingle。两个试题内容是否相似可以通过使用Jaccard相似度来计算得到。Jaccard相似度在数据集较小时，相似对的比较和相似度计算都表现良好。然而，在数据集较大时，时间复杂度将几何上升，此时可以通过MinHash转换的签名矩阵和位置敏感散列(LSH)技术来解决这个问题，从而在更短的时间内快速逼近真正的Jaccard相似度。</p></sec><sec id="s4"><title>2. 研究内容</title><sec id="s4_1"><title>2.1. Jaccard相似度</title><p>两个集合M和N的Jaccard相似度是 | M ∩ N | / | M ∪ N | ，即两个集合交集的数目与两个集合并集数目的比例(Rajaraman and Ullman, 2011) [<xref ref-type="bibr" rid="hanspub.35242-ref12">12</xref>]。</p><p>J S i m ( M , N ) = | M ∩ N | | M ∪ N |</p><p>Jaccard相似度具有如下性质：当且仅当两个集合相等时 M = N ， J S i m ( M , N ) = 1 ；当且仅当 M ≠ N ， J S i m ( M , N ) = 0 ； J S i m 是对称的。基于Jaccard相似度的文档相似度在很多方面均有应用，如：文档抄袭、镜像页面、同源新闻稿等。如下面两个集合的相似度为 J S i m ( M , N ) = 1 。</p><p>若有两个集合 A = { a , b , c , d , e } , B = { c , d , e , f , g , h } ， 则 J S i m ( A , B ) = 3 / 8 。</p></sec><sec id="s4_2"><title>2.2. 中文分词</title><p>在中文中，词语之间没有分隔，且边界模糊。因此进行中文自然语言处理通常是先将汉语文本中的字符串切分成合理的词语序列，然后再在此基础上进行其它分析处理。中文分词是中文信息处理的一个基础环节，已被广泛应用于中文文本处理、信息提取、文本挖掘等应用中。例如：T1 = {“关系数据库理论包括函数依赖和_____”}，经过分词之后为Ts1 = {“关系”，“数据库”，“理论”，“包括”，“函数”，“依赖”，“和”}。</p></sec><sec id="s4_3"><title>2.3. 文档的K-shingles</title><p>为了识别字面上相似的文档，将文档表示成集合的最有效的方法是构建文档中的短字符串集合(即shingle)。如果文档采用这样的集合表示，那么有相同句子甚至短语的文档之间将会拥有很多公共的shingle。其中K表示子串的长度。为了通过K-shingle表示一个文档，需要提前对文档进行处理。如删除首尾空格、删除停用词等。然后再把文本拆分成分词。例如经过分词之后的T1 = {“关系”，“数据库”，“理论”，“包括”，“函数”，“依赖”}，如果K = 2，则通过K-shingle，Ts1 = {“关系数据库”，“数据库理论”，“理论包括”，“包括函数”，“函数依赖”}，如果还具有下列两个文本T2 = {“数据库的理论包括函数依赖和______”}、T3 = {“关系数据库理论包括______和规范化”}，则3个文本的shingle元素矩阵如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Shingle element matrix of tex</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >shingles</th><th align="center" valign="middle" >T1</th><th align="center" valign="middle" >T2</th><th align="center" valign="middle" >T3</th></tr></thead><tr><td align="center" valign="middle" >关系数据库</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >数据库理论</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >理论包括</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >包括函数</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >函数依赖</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >包括规范化</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr></tbody></table></table-wrap><p>表1. 文本的shingle元素矩阵</p><p>表1中的文本对相似度如下： J S i m ( T 1 , T 2 ) = 4 / 5 ， J S i m ( T 1 , T 3 ) = 3 / 6 ， J S i m ( T 2 , T 3 ) = 2 / 6 。</p></sec><sec id="s4_4"><title>2.4. Shingle的哈希</title><p>由于所有shingle集合的存储需要占用大量的空间。可以哈希shingle进行散列，使用散列之后的桶号来代替shingle串。我们使用以下形式的散列函数 [<xref ref-type="bibr" rid="hanspub.35242-ref13">13</xref>]：</p><p>h ( x ) = ( a x + b ) m o d p</p><p>其中： x 为原始矩阵中的shingle串，a和b是从(0, Shingle全集大小)之间的一个随机数，p是比Shingle全集稍大的数。</p><p>假定表1中的shingle串被哈希到0~5的桶中，见表2。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Characteristic matrix </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >shingles</th><th align="center" valign="middle" >T1</th><th align="center" valign="middle" >T2</th><th align="center" valign="middle" >T3</th></tr></thead><tr><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr></tbody></table></table-wrap><p>表2. 特征矩阵M</p><p>表2中的矩阵也称为特征矩阵M，该矩阵的行对应shingle，列对应文本集合。由于特征往往非常稀疏，在处理大量文本对的相似度计算时非常不方便。</p></sec><sec id="s4_5"><title>2.5. 最小哈希签名(MinHash)</title><p>通过最小哈希可以把特征矩阵转变为签名矩阵，该签名矩阵的行对应最小哈希函数，列对应文本集合。通常情况下签名矩阵所需的存储空间比特征矩阵M的存储空间要小很多。最小哈希签名的计算过程如算法1所示：</p><p>算法1：最小哈希签名</p><p>输入：特征矩阵M，哈希函数 h 1 , h 2 , ⋯ , h n</p><p>输出：签名矩阵S</p><p>对于所有的每个哈希函数(i)，每一列(c)，签名矩阵中S的 S ( i , c ) 表示第i个哈希函数及第c列对应的最小哈希值。</p><p>Step1循环对特征矩阵中的每一行进行处理，计算当前行r的哈希值， h 1 ( r ) , h 2 ( r ) , ⋯ , h n (r)</p><p>Step2对每列c进行如下操作</p><p>(a) 若特征矩阵中 M ( i , c ) = 0 ，则什么都不做；</p><p>(b) 若 M ( i , c ) = 1 ，那么对于每个 i = 1 , 2 , ⋯ , n ，将 S ( i , c ) 设为原来 S ( i , c ) 和 h i ( r ) 之中的最小值。</p><p>若 h 1 = ( x + 1 ) mod 7 , h 2 = ( 3 x + 1 ) mod 7 ，则经过两个哈希函数变换之后的特征矩阵M见表3，签名矩阵计算结果见表4，最后通过签名矩阵计算得到的Jaccard相似度见表5。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Characteristic matrix M after two hash function transformation</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >shingles</th><th align="center" valign="middle" >T1</th><th align="center" valign="middle" >T2</th><th align="center" valign="middle" >T3</th><th align="center" valign="middle" >h 1 = ( x + 1 ) mod 7</th><th align="center" valign="middle" >h 2 = ( 3 x + 3 ) mod 7</th></tr></thead><tr><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >3</td></tr><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >6</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >2</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >5</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >4</td></tr></tbody></table></table-wrap><p>表3. 经过两个哈希函数变换之后的特征矩阵M</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Signature Matrix </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Hash函数</th><th align="center" valign="middle" >T1</th><th align="center" valign="middle" >T2</th><th align="center" valign="middle" >T3</th></tr></thead><tr><td align="center" valign="middle" >h 1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >h 2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2</td></tr></tbody></table></table-wrap><p>表4. 签名矩阵S</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Jaccard similarity of text pair</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >J S i m ( T 1 , T 2 )</th><th align="center" valign="middle" >J S i m ( T 1 , T 3 )</th><th align="center" valign="middle" >J S i m ( T 2 , T 3 )</th></tr></thead><tr><td align="center" valign="middle" >真实Jaccard相似度</td><td align="center" valign="middle" >4/5</td><td align="center" valign="middle" >3/6</td><td align="center" valign="middle" >2/6</td></tr><tr><td align="center" valign="middle" >通过签名估计的Jaccard相似度</td><td align="center" valign="middle" >1/2</td><td align="center" valign="middle" >1/2</td><td align="center" valign="middle" >0</td></tr></tbody></table></table-wrap><p>表5. 文本对的Jaccard相似度</p></sec><sec id="s4_6"><title>2.6. LSH</title><p>若有 N 个文本，进行两两相似度将需要比较 N 2 / 2 次。这将极大增加计算量。LSH仅仅关注散列到同一个桶中的候选对，只检查这些候选对之间的相似度，它的基本思想就是将签名矩阵划分成b个行条，每个行条由r行组成。其中n = br，n为哈希函数的个数，即签名矩阵的行数。行条化策略能够使得相似列比不相似列更有可能成为候选对，从而大大减少文本对之间的比较次数。</p></sec><sec id="s4_7"><title>2.7. 基于LSH技术的试题相似度检测算法</title><p>本文具体处理的过程见算法2。</p><p>算法2：基于LSH技术的试题相似度检测算法</p><p>输入：试题文本集合 T 1 , T 2 , ⋯ , T N ，Jaccard相似度阈值T<sup>*</sup></p><p>输出：相似试题文本对及相应的Jaccard估计相似度</p><p>Step1对所有试题文本进行中文分词，同时去除停用词，删除首尾空格</p><p>Step2把上一步中的结果转换成K-shingle集，得到特征矩阵M</p><p>Step3随机生成n_hashes个哈希函数 h 1 , h 2 , ⋯ , h n _ h a s h e s ，根据算法1计算出签名矩阵S</p><p>Step4通过LSH得到所有的候选相似对，具体是将签名矩阵划分成n_bands个行条，每个行条由 r = N / n _ b a n d s 行组成，每个行条采用一个哈希函数对r个整数构成的列向量进行散列。若两向量相同，则会散列到同一个桶中，从而得到候选相似对。</p><p>Step5计算所有候选相似对的Jaccard相似度，若大于相似度阈值T<sup>*</sup>，则两试题文本相似。</p></sec></sec><sec id="s5"><title>3. 实验结果及分析</title><p>本文采用的数据为《数据库课程》试题库，其中，判断题100道，填空题100道，选择题100道，解答题50道，综合题20道。考虑到组卷系统的大众化使用，更方便的在任何地方组卷，不借助除了Office之外的其它任何组件，所有功能全部在VBA中实现。程序采用了Word自带的中文分词功能，对试题内容去除停用词、删除空格。然后转换为K-shingle集，从而把所有的试题内容转换成了特征矩阵M，通过最小哈希(Minhash)计算出试题签名，最后通过LSH快速的找出候选相似对，同时计算出相应的Jaccard相似度。若该相似度大于给定的阈值，则输出。同时在组卷的过程中避免了相似试题，从而由此提高组卷的质量。</p><p>在实验中，设置题3与题5相似，题7与题9相似，题269与题271相似。哈希函数个数n_hashes为20个，LSH中的行条数n_bands为4个。运行结果如表6所示，其中的真实相似度是在不同Shingle长度下，对两Shingle集计算的Jaccard相似度。估计相似度是对两试题的最小哈希签名计算的Jaccard相似度。</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Results of different shingle length</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >Shingle-Size</th><th align="center" valign="middle"  colspan="2"  >题3与题5</th><th align="center" valign="middle"  colspan="2"  >题7与题9</th><th align="center" valign="middle"  colspan="2"  >题269与题271</th><th align="center" valign="middle"  rowspan="2"  >总耗时</th></tr></thead><tr><td align="center" valign="middle" >真实相似度</td><td align="center" valign="middle" >估计相似度</td><td align="center" valign="middle" >真实相似度</td><td align="center" valign="middle" >估计相似度</td><td align="center" valign="middle" >真实相似度</td><td align="center" valign="middle" >估计相似度</td></tr><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0.92</td><td align="center" valign="middle" >0.8</td><td align="center" valign="middle" >0.71</td><td align="center" valign="middle" >0.48</td><td align="center" valign="middle" >0.89</td><td align="center" valign="middle" >0.74</td><td align="center" valign="middle" >30.3S</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >0.8</td><td align="center" valign="middle" >0.67</td><td align="center" valign="middle" >0.67</td><td align="center" valign="middle" >0.43</td><td align="center" valign="middle" >0.87</td><td align="center" valign="middle" >0.67</td><td align="center" valign="middle" >31.33S</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >0.67</td><td align="center" valign="middle" >0.48</td><td align="center" valign="middle" >0.6</td><td align="center" valign="middle" >0.48</td><td align="center" valign="middle" >0.84</td><td align="center" valign="middle" >0.6</td><td align="center" valign="middle" >31.33S</td></tr></tbody></table></table-wrap><p>表6. 不同shingle长度的程序运行情况</p><p>表6表明：由于试题文本一般不会太长，所以shingle长度不宜过大，设置为2比较合适。</p><p>当改变哈希函数n_hashes的数目时，运行情况表7所示。</p><p>表7表明：当哈希函数数目和行条数目较少时，真实度与近似相似有较大差距，且会漏检；随着哈希数目和行条数目的增多，差距越来越小。漏检率越来越低，精度越来越高。</p><p>组卷情况统计如图1所示。</p><table-wrap id="table7" ><label><xref ref-type="table" rid="table7">Table 7</xref></label><caption><title> Shingle length is 2, the result of different hash functions and band</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >n_hashes</th><th align="center" valign="middle" >n_bands</th><th align="center" valign="middle" >题3与题5 签名相似度</th><th align="center" valign="middle" >题7与题9 签名相似度</th><th align="center" valign="middle" >题269与题271 签名相似度</th><th align="center" valign="middle" >候选相似对检测情况</th><th align="center" valign="middle" >总耗时</th></tr></thead><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >0.67</td><td align="center" valign="middle" >未发现</td><td align="center" valign="middle" >0.67</td><td align="center" valign="middle" >发现3对(包括2对目标对)</td><td align="center" valign="middle" >31.3S</td></tr><tr><td align="center" valign="middle" >100</td><td align="center" valign="middle" >20</td><td align="center" valign="middle" >0.72</td><td align="center" valign="middle" >0.57</td><td align="center" valign="middle" >0.82</td><td align="center" valign="middle" >发现5对(包括3对目标对)</td><td align="center" valign="middle" >31.6S</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >40</td><td align="center" valign="middle" >0.72</td><td align="center" valign="middle" >0.58</td><td align="center" valign="middle" >0.878</td><td align="center" valign="middle" >发现8对(包括3对目标对)</td><td align="center" valign="middle" >32.8S</td></tr><tr><td align="center" valign="middle" >400</td><td align="center" valign="middle" >80</td><td align="center" valign="middle" >0.76</td><td align="center" valign="middle" >0.59</td><td align="center" valign="middle" >0.90</td><td align="center" valign="middle" >发现15对(包括3对目标对)</td><td align="center" valign="middle" >36.5S</td></tr></tbody></table></table-wrap><p>表7. Shingle长度为2，不同哈希函数和行条数目的运行情况</p><p>图1. 组卷情况</p></sec><sec id="s6"><title>4. 结语</title><p>本文提出的基于LSH技术的试题相似度检测方法，实验结果表明，通过该方法检测能有效减少相似试题的重复率，并极大地减少相似对的比较次数，降低时间复杂度。</p></sec><sec id="s7"><title>基金项目</title><p>楚雄师范学院校级教改项目，项目编号：1807。</p></sec><sec id="s8"><title>文章引用</title><p>陈 瑞,王 松,梅 莹,杨云源. 基于LSH技术的试题相似度检测方法The Application of LSH Technology in Similar Question Detection[J]. 计算机科学与应用, 2020, 10(04): 741-748. https://doi.org/10.12677/CSA.2020.104077</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.35242-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Muskan, K.M. (2017) Identifying Influential Segments from Word Co-Occurrence Networks Using AHP. Cognitive Systems Research, S138904171630198X.</mixed-citation></ref><ref id="hanspub.35242-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Pawar, A. and Mago, V. (2018) Calculating the Similarity between Words and Sentences Using a Lexical Database and Corpus Statistics.</mixed-citation></ref><ref id="hanspub.35242-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Abujar, S., Hasan, M. and Hossain, S.A. (2019) Sentence Similarity Estimation for Text Summarization Using Deep Learning. In: Kulkarni, A., Satapathy, S., Kang, T. and Kashan, A., Eds., Proceedings of the 2nd International Conference on Data Engineering and Communi-cation Technology, Advances in Intelligent Systems and Computing, Vol. 828, Springer, Singapore. &lt;br&gt;https://doi.org/10.1007/978-981-13-1610-4_16</mixed-citation></ref><ref id="hanspub.35242-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Q., Hu, Q.M., Huang, X.J. and He, L. (2018) CAN: Enhancing Sentence Similarity Modeling with Collaborative and Adversarial Network. 815-824. &lt;br&gt;https://doi.org/10.1145/3209978.3210019</mixed-citation></ref><ref id="hanspub.35242-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Chi, Z. and Zhang, B. (2018) A Sentence Similarity Estimation Method Based on Improved Siamese Network. Journal of Intelligent Learning Systems and Applications, 10, 121-134. &lt;br&gt;https://doi.org/10.4236/jilsa.2018.104008</mixed-citation></ref><ref id="hanspub.35242-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Yao, H., Liu, H. and Zhang, P. (2018) A Novel Sentence Similarity Model with Word Embedding Based on Convolutional Neural Network. Concurrency and Computation: Practice and Experience, 30, e4415.  
&lt;br&gt;https://doi.org/10.1002/cpe.4415</mixed-citation></ref><ref id="hanspub.35242-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Quan, Z., Wang, Z., Le, Y., Yao, B., Li, K. and Yin, J. (2019) An Efficient Framework for Sentence Similarity Modeling. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27, 853-865.  
&lt;br&gt;https://doi.org/10.1109/TASLP.2019.2899494</mixed-citation></ref><ref id="hanspub.35242-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Le, Y.Q., Wang, Z.-J., Quan, Z., He, J.W. and Yao, B. (2018) ACV-Tree: A New Method for Sentence Similarity Modeling. IJCAI, 4137-4143.</mixed-citation></ref><ref id="hanspub.35242-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">梁圣. 基于RNN的试题相似度计算模型研究与实现[J]. 数码设计, 2018, 7(1): 21-23.</mixed-citation></ref><ref id="hanspub.35242-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">田星, 郑瑾, 张祖平. 基于词向量的Jaccard相似度算法[J]. 计算机科学, 2018, 45(7): 192-195.</mixed-citation></ref><ref id="hanspub.35242-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Q., Hu, Q., Huang, J.X. and He, L. (2018) CA-RNN: Using Context-Aligned Recurrent Neural Networks for Modeling Sentence Similarity. Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, LA, 2-7 February 2018.</mixed-citation></ref><ref id="hanspub.35242-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Leskovec, J., Rajaraman, A. and Ullman, J.D. (2015) Mining of Massive Datasets. 2nd Edition. Posts &amp; Telecom Press, Beijing, 56-70. &lt;br&gt;https://doi.org/10.1017/CBO9781139924801</mixed-citation></ref><ref id="hanspub.35242-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Manaa, M.E. and Abdulameer, G. (2018) Web Documents Similarity Using K-Shingle Tokens and MinHash Technique. Journal of Engineering and Applied Sciences, 13, 1499-1505.</mixed-citation></ref></ref-list></back></article>