<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114114</article-id><article-id pub-id-type="publisher-id">CSA-41957</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_39445273.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于标签一致性哈希的跨模态检索算法
  Label Consistency Hashing for Cross-Modal Retrieval
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>志虎</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>广东工业大学计算机学院，广东 广州</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>1104</fpage><lpage>1112</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对跨模态检索任务中，不同数据之间存在异构性以及语义鸿沟等特点，本文提出了一种新的监督哈希方法。该方法利用矩阵分解学习训练数据集在低维潜在语义空间表示，同时本文将标签信息也视为一个单独的模态，也利用矩阵分解将其映射到低维潜在语义子空间中；然后，在子空间中最大化它们之间的相关性，从而得到相应的低维潜在语义代表；之后，本文利用正交旋转矩阵学习性能更好的哈希函数得到相应的哈希码。在三个常用的数据集Wiki，MIRFlick和NUS-WIDE进行了大量的实验，并与一些常用的跨模态哈希方法进行了比较，结果证明了该算法的优越性。 In view of the heterogeneity and semantic gap between different data in cross-modal retrieval, a new supervised hash method is proposed. This method uses matrix factorization technique learning training data set to represent the low-dimensional potential semantic space. At the same time, this paper considers the semantic features as a separate mode, and maps them to the low-dimensional latent semantic subspace by using matrix factorization, then maximizes the correlation among them in the subspace, and obtains the corresponding low-dimensional potential semantic representation. After that, the hash codes are obtained by using the hash function with better learning performance of orthogonal rotation matrix. A lot of experiments have been carried out in three commonly used data sets Wiki, MIRFlick and NUS-WIDE, and compared with some common cross-modal hashing methods, the results show the superior of this algorithm. 
  
 
</p></abstract><kwd-group><kwd>跨模态检索，哈希，矩阵分解, Cross-Modal Retrieval</kwd><kwd> Hashing</kwd><kwd> Matrix Factorization</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>针对跨模态检索任务中，不同数据之间存在异构性以及语义鸿沟等特点，本文提出了一种新的监督哈希方法。该方法利用矩阵分解学习训练数据集在低维潜在语义空间表示，同时本文将标签信息也视为一个单独的模态，也利用矩阵分解将其映射到低维潜在语义子空间中；然后，在子空间中最大化它们之间的相关性，从而得到相应的低维潜在语义代表；之后，本文利用正交旋转矩阵学习性能更好的哈希函数得到相应的哈希码。在三个常用的数据集Wiki，MIRFlick和NUS-WIDE进行了大量的实验，并与一些常用的跨模态哈希方法进行了比较，结果证明了该算法的优越性。</p></sec><sec id="s2"><title>关键词</title><p>跨模态检索，哈希，矩阵分解</p></sec><sec id="s3"><title>Label Consistency Hashing for Cross-Modal Retrieval</title><p>Zhihu Liu</p><p>School of Computers, Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/35-1542125x4_hanspub.png" /></p><p>Received: Mar. 28<sup>th</sup>, 2021; accepted: Apr. 21<sup>st</sup>, 2021; published: Apr. 28<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/35-1542125x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In view of the heterogeneity and semantic gap between different data in cross-modal retrieval, a new supervised hash method is proposed. This method uses matrix factorization technique learning training data set to represent the low-dimensional potential semantic space. At the same time, this paper considers the semantic features as a separate mode, and maps them to the low-dimensional latent semantic subspace by using matrix factorization, then maximizes the correlation among them in the subspace, and obtains the corresponding low-dimensional potential semantic representation. After that, the hash codes are obtained by using the hash function with better learning performance of orthogonal rotation matrix. A lot of experiments have been carried out in three commonly used data sets Wiki, MIRFlick and NUS-WIDE, and compared with some common cross-modal hashing methods, the results show the superior of this algorithm.</p><p>Keywords:Cross-Modal Retrieval, Hashing, Matrix Factorization</p><disp-formula id="hanspub.41957-formula46"><graphic xlink:href="//html.hanspub.org/file/35-1542125x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/35-1542125x7_hanspub.png" /> <img src="//html.hanspub.org/file/35-1542125x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>随着信息科技的迅速发展，人们不仅接收到各种信息也创造出各种信息，这些信息的表现方式多样，如图片、文本、视频和音频等等。其中，以任意一个单独形式表现出来的数据被称为单模态数据，多个单模态数据组合的方式表现出来的数据被称为多模态数据。如今，人们接触到的信息往往是多模态数据，例如，人们在微博上发布消息时，不仅可以上传图片或者视频，也可以加上相应的文章描述。在实际的生活和应用中，人们往往需要通过一种模态的数据去检索另外一种模态数据，例如利用图像去检索相对应的文本，这种检索方式被称为跨模态检索。然而不同模态对数据的表达不同会导致异构鸿沟，同时不同模态数据在语义描述上存在差异会产生语义鸿沟，这些是跨模态检索的难点。如何在海量高维数据中快速准确搜索到需要的内容成为一个急需解决的问题。</p><p>为了更好的解决海量数据信息检索问题，研究学者提出最近邻搜索问题，最近邻检索的核心思想是，给定一个待检测样本，返回数据库中与待检测样本距离最为接近的样本信息作为检索结果。实现最近邻检索的是线性查找，即计算待检测样本与数据集中所有样本之间的距离，然后返回满足检索样本信息作为检测结果。该方法在数据量不大的时候，具有很强的实用性，可以很高效的在数据集汇总找到符合要求的样本信息。然而面对海量的数据时，计算量迅速增加，对计算机的计算能力要求越来越高，因此该方法很难得到广泛的应用。为了有效降低计算量，研究学者提出了近似最近邻域检索方法，并且由于其高效性，在实际应用中获得广泛的应用。</p><p>在近似最近邻检索算法中，其中最有效的方法之一是哈希算法。哈希算法通过机器学习算法或随机的方法把数据映射到相应的潜在语义空间，再通过哈希函数将原始数据表示成二进制编码，利用二进制编码的位运算来进行检索，不仅可以降低存储开销，同时降低了计算复杂度，提高检索效率。因此，基于哈希的跨模态检索受到越来越多研究人员的关注，逐渐成为一个研究热点。</p></sec><sec id="s6"><title>2. 相关工作</title><p>跨模态哈希检索是在建立两个模态语义关联的过程中学习哈希码，并将哈希检索的优点运用到跨模态检索问题中，根据是否利用数据本身的标签信息，跨模态哈希方法可以分为两种类型：无监督跨模态哈希和有监督跨模态哈希。</p><p>无监督跨模态哈希算法是通过多模态数据的模态内和模态间的关系来学习哈希码。一般的学习过程是将原始数据投影到低维的汉明空间，然后在汉明空间中生成相应的哈希码。例如，协同矩阵分解哈希(Collective Matrix Factorization Hashing, CMFH) [<xref ref-type="bibr" rid="hanspub.41957-ref1">1</xref>]，该算法利用协同矩阵分解学习多模态数据共同的潜在语义表达，然后再生成统一的哈希码。潜在语义稀疏哈希(Latent Semantic Sparse Hashing, LSSH) [<xref ref-type="bibr" rid="hanspub.41957-ref2">2</xref>]，该算法利用稀疏编码学习图像的潜在语义表达，同时利用矩阵分解得到文本的潜在语义表达，然后再将学习到的潜在语义表达映射到一个联合的抽象空间中得到相应的哈希码。中间模态哈希(Inter-Media Hashing, IMH) [<xref ref-type="bibr" rid="hanspub.41957-ref3">3</xref>]，该算法同时考虑两中模态数据中模态内和模态间的相关性，提出中间模态的哈希变换。</p><p>有监督跨模态哈希算法利用数据的类标签信息进行学习，往往能得到比无监督方法更好的结果。最大语义相关哈希算法(Semantic Correlation Maximization, SCM) [<xref ref-type="bibr" rid="hanspub.41957-ref4">4</xref>]，该算法利用标签信息重构多模态数据的相关性矩阵，有监督矩阵分解哈希(Supervised Matrix Factorization Hashing for Cross-modal Retrieval, SMFH) [<xref ref-type="bibr" rid="hanspub.41957-ref5">5</xref>]，该算法利用协同矩阵分解的方法得到多模态数据的潜在语义表示，然后再利用标签信息构造图约束，加强生成哈希码的鉴别能力。语义保持哈希(Semantics-Preserving Hashing, SePH) [<xref ref-type="bibr" rid="hanspub.41957-ref6">6</xref>]，该算法利用标签信息构造一个亲和矩阵，并通过最小化该亲和矩阵和对应哈希编码之间的KL散度来学习哈希函数，从而使得学习到的哈希编码和原始数据之间的相似性保持一致。</p></sec><sec id="s7"><title>3. 基于标签一致性哈希的跨模态检索算法</title><p>这一章节，将详细介绍基于标签一致性哈希的跨模态检索算法(LCH)，第一小节介绍它的目标函数。第二小节介绍它的优化方法和主要步骤。</p><sec id="s7_1"><title>3.1. 目标函数</title><p>假设本文有一个数据集，该数据集由n个对象组成，每个对象有图片和文本两种表示模态，本文定义 X 1 = { x 1 1 , x 2 1 , ⋯ , x n 1 } 代表图像模态， X 2 = { x 1 2 , x 2 2 , ⋯ , x n 2 } 表示文本模态。对于第i个对象， x i 1 ∈ R d 1 代表 d 1 维度的图像特征向量， x i 2 ∈ R d 2 代表 d 2 维度的文本特征向量(一般 d 1 ≠ d 2 )。每个模态的数据都可以通过下面的公式进行矩阵分解：</p><p>X 1 = U 1 V 1 , X 2 = U 2 V 2 (1)</p><p>其中， U 1 ∈ R d 1 &#215; q , U 2 ∈ R d 2 &#215; q , V 2 ∈ R q &#215; n , V 2 ∈ R q &#215; n ， q 代表潜在语义代表的长度， U 1 , U 2 分别代表图像和文本的投影矩阵， V 1 , V 2 分别代表图像和文本的低维潜在语义表示。为了方便不同模态的潜在语义表示在这个低维子空间中进行耦合，本文将标签信息也视为一个单独的模态，定义为 X L ∈ R d l &#215; n ， d l 表示类别的数量。同样利用矩阵分解的得到相应的低维潜在语义代表，即 X L = U L V L ， U L ∈ R d l &#215; q , V L ∈ R q &#215; n ， U L , V L 分别为标签信息的投影矩阵和潜在语义代表。本文假设图像和文本的语义代表通过线性转换之后能和标签信息的潜在语义代表进行耦合，用公式表示如下：</p><p>W 1 V 1 = V L , W 2 V 2 = V L (2)</p><p>W 1 , W 2 分别是图像和文本的线性转换。在得到潜在语义代表之后，大部分的哈希算法是利用符号函数 sgn ( ⋅ ) 直接得到相应的哈希码，然而，这种生成哈希码的方法没有考虑量化损失带来的影响。本文通过最小化量化误差学习一个旋转矩阵，得到相应的哈希函数，从而产生性能更好的哈希码。</p><p>min B , R ‖ B − R V L ‖ F 2 s . t . B ∈ { − 1 , 1 } r &#215; n , R R T = I (3)</p><p>其中， R ∈ R q &#215; q 是一个正交旋转矩阵，B是所有训练数据的哈希码，q代表哈希码的长度。</p><p>因此整个目标函数为：</p><p>min U 1 , U 2 , W 1 , W 2 , , V 1 , V 2 , V L λ 1 ‖ X 1 − U 1 V 1 ‖ F 2 + λ 2 ‖ X 2 − U 2 V 2 ‖ F 2 + λ 3 ‖ X L − U L V L ‖ F 2 + α 1 ‖ V L − W 1 V 1 ‖ F 2 + α 2 ‖ V L − W 2 V 2 ‖ F 2 + ‖ B − R V L ‖ F 2 + γ r e g ( U 1 , U 2 , V t , V 2 , V L , R ) s . t . B ∈ { − 1 , 1 } r &#215; n , R R T = I (4)</p><p>其中， | | ⋅ | | F 表示矩阵的Frobenius范数，值为矩阵中每个元素的平方和再开平方的值。 λ 1 , λ 2 , λ 3 , α 1 , α 2 , γ 分别为对应的权重因子，reg是正则化函数。</p></sec><sec id="s7_2"><title>3.2. 优化方式</title><p>由于目标函数含有多个变量，因此它是非凸，值得注意，对于其中任意一个参数目标函数是凸的，因此本文使用交替迭代最小化方案来解决这个问题，即固定所有的变量，而只更新其中一个变量，然后采用相应的方式迭代求解所有变量，具体步骤如下：</p><p>步骤1：固定其它变量，求解 U 1 , U 2 , U L ，目标函数可以分别重写为：</p><p>{ λ 1 ‖ X 1 − U 1 V 1 ‖ F 2 + γ ‖ U 1 ‖ F 2 λ 2 ‖ X 2 − U 2 V 2 ‖ F 2 + γ ‖ U 2 ‖ F 2 λ 3 ‖ X L − U L V L ‖ F 2 + γ ‖ U L ‖ F 2 (5)</p><p>通过对上面的式子分别求偏导，求解得到：</p><p>{ U 1 = X 1 V 1 T ( V 1 V 1 T + ( γ / λ 1 ) I ) − 1 U 2 = X 2 V 2 T ( V 2 V 2 T + ( γ / λ 2 ) I ) − 1 U L = X L V L T ( V L V L T + ( γ / λ 3 ) I ) − 1 (6)</p><p>步骤2：固定其它变量，求解 W 1 , W 2 ，目标函数重写为：</p><p>{ α 1 ‖ V L − W 1 V 1 ‖ F 2 + γ ‖ W 1 ‖ F 2 α 2 ‖ V L − W 2 V 2 ‖ F 2 + γ ‖ W 2 ‖ F 2 (7)</p><p>通过对上面的式子分别求偏导，求解得到：</p><p>{ W 1 = V L V 1 T ( ( V 1 V 1 T + γ / α 1 ) I ) − 1 W 2 = V L V 2 T ( ( V 2 V 2 T + γ / α 2 ) I ) − 1 (8)</p><p>步骤3：固定其它变量，求解 V 1 , V 2 , V L ，目标函数重写为：</p><p>{ λ 1 ‖ X 1 − U 1 V 1 ‖ F 2 + α 1 ‖ V L − W 1 V 1 ‖ F 2 + γ ‖ V 1 ‖ F 2 λ 2 ‖ X 2 − U 2 V 2 ‖ F 2 + α 2 ‖ V L − W 2 V 2 ‖ F 2 + γ ‖ V 2 ‖ F 2 λ 3 ‖ X L − U L V L ‖ F 2 + α 1 ‖ V L − W 1 V 1 ‖ F 2 + α 2 ‖ V L − W 2 V 2 ‖ F 2 + ‖ B − R V L ‖ F 2 + γ ‖ V 2 ‖ F 2 (9)</p><p>通过对上面的式子分别求偏导，求解得到：</p><p>{ V 1 = ( λ 1 U 1 T U 1 + α 1 W 1 T W 1 + γ I ) − 1 ( λ 1 U 1 T X 1 + α 1 W 1 T V L ) V 2 = ( λ 2 U 2 T U 2 + α 2 W 2 T W 2 + γ I ) − 1 ( λ 2 U 2 T X 2 + α 2 W 2 T V L ) V L = ( λ 3 U L T U L + R T R + ( α 1 + α 2 + γ ) I ) − 1 ( λ L U L T X L + α 1 W 1 V 1 + α 2 W 2 V 2 + R T B ) (10)</p><p>步骤4：固定其它变量，求解 R ，目标函数重写为：</p><p>min R ‖ B − R V L ‖ F 2 , s . t . R R T = I (11)</p><p>这是一个经典的Orthogonal Procrustes Problem [<xref ref-type="bibr" rid="hanspub.41957-ref7">7</xref>]，可以通过奇异值分解的方法求解。在进行奇异值分解后，可以得到 B V L T = S Ω S ˜ T ，然后求解 R ，得到:</p><p>R = S S ˜ T (12)</p><p>步骤5：固定其它变量，求解 B ，目标函数重写为：</p><p>min B ‖ B − R V L ‖ F 2 , s . t . B ∈ { − 1 , 1 } q &#215; n (13)</p><p>对上式求解，很容易就可以得到：</p><p>B = sgn ( R V L ) (14)</p><p>算法1总结了LCH的优化框架如下：</p></sec></sec><sec id="s8"><title>4. 实验</title><p>本章详细介绍本方法在Wiki [<xref ref-type="bibr" rid="hanspub.41957-ref8">8</xref>]，MIRFick [<xref ref-type="bibr" rid="hanspub.41957-ref9">9</xref>]，NUS-WIDE [<xref ref-type="bibr" rid="hanspub.41957-ref10">10</xref>] 三个基准数据集上的实验结果及相应的分析。</p><sec id="s8_1"><title>4.1. 数据集介绍</title><p>Wiki：该数据集从维基百科中搜集而来，包含了2866对图像-文本数据对，共分成10个语义类别。其中，该数据集用128维的SIFT特征表示图片数据，用10维LDA特征表示文本数据。在实验中，我们随机将选择75%作为训练集，25%作为测试集。</p><p>MIRFlick：该数据集是从Flickr网站汇总下载的图片及相应的文本，共包含了25,000幅图像，有24个语义类别，其中每幅图像至少属于一个语义类别，且一幅图像对应多个文本，该数据集是一个多标签数据集。该数据集用125维的边缘特征表示图片数据，用500维的PCA特征表示文本数据。在实验中，将该数据集中没有文本标记的数据以及标签出现次数少于20次的数据剔除，然后将剩余数据中的95%作为训练集，5%作为测试集。</p><p>NUS-WIDE：该数据集是新加坡国立大学公开的数据集，共包含269,648张从Flickr网站上收集的图片及相应的文本，每张图片平均有6个标注，这些图像-文本对可以被分为81个类。该数据集用500维的SIFT表示图片数据，1000维的词向量特征表示文本数据。在实验中，为了保证每类有足够多的训练样本，选取数据中数量最多的10个类的数据进行实验，从这些数据中选择95%作为训练集，5%作为测试集。</p></sec><sec id="s8_2"><title>4.2. 评估度量及参数设置</title><p>本文选跨模态检索平均查准率均值(mAP)作为主要评价算法的整体性能，其公式为：</p><p>mAP = 1 N ∑ i = 1 N A P ( q i ) (14)</p><p>其中， q i 为一个查询样本，N为查询样本量，AP为平均查准率，它的计算公式为：</p><p>A P = 1 T ∑ r = 1 R P q ( r ) δ ( r ) (15)</p><p>其中，T是检索集中所有相关的实体个数， P q ( r ) 是按照相关度排名的前r个实体的查准率， δ ( r ) 是一个指示函数，当第r个被检索到的实体与检索内容相关是，其值为1，反之为0。本文主要验证两种跨模态检索任务，一种是利用图像去检索相关文本，用Img2-Text表示，另一种是利用文本去检索相关图像，用Text2-Img表示。</p><p>本文针对三个数据集设置不同的参数进行实验，对于Wiki数据集，参数 { λ 1 , λ 2 , λ 3 , α 1 , α 2 , γ } 的值分别为{1, 1, 1, 0.1, 0.1, 0.1}；对于MIRFlick数据集，参数 { λ 1 , λ 2 , λ 3 , α 1 , α 2 , γ } 的值分别为{0.1, 100, 1, 0.1, 1, 1}；对于NUS-WIDE参数 { λ 1 , λ 2 , λ 3 , α 1 , α 2 , γ } 的值分别为{0.1, 1, 100, 0.1, 1, 1}。为了全面评估本文算法，哈希的长度分别设置为16 bits, 32 bits, 64 bits以及128 bits。本文的实验在Matlab2019b, Intel(R) Core(TM) i7-6700 CPU @3.40GHz环境下进行。</p></sec><sec id="s8_3"><title>4.3. 实验结果和分析</title><p>为了验证LCH算法的有效性，我们选取了几个跨模态检索算法进行对比，它们分别是IMH，SCM，CMFH，SMFH，LSSH，以及SePH，这些算法在前面章节已经作了简单的介绍。实验过程中，对比算法的实验参数都是依据相关文件建议中的参数进行设置的。本文取10次相应的实验结果的评价值作为最终的实验结果。</p><sec id="s8_3_1"><title>4.3.1. 实验结果</title><p>表1展示了LCH算法和对比算法在Wiki数据集上的结果。表2展示了LCH算法和对比算法在MIRFlick数据集上的结果。表3展示了LCH算法和对比算法在NUS-WIDE数据集上的结果。</p><table-wrap-group id="1"><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Experimental results on the Wiki datase</title></caption><table-wrap id="1_1"><table><tbody><thead><tr><th align="center" valign="middle" >任务</th><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >16</th><th align="center" valign="middle" >32</th><th align="center" valign="middle" >64</th><th align="center" valign="middle" >128</th></tr></thead><tr><td align="center" valign="middle"  rowspan="3"  >Img2-Text</td><td align="center" valign="middle" >IMH</td><td align="center" valign="middle" >0.1952</td><td align="center" valign="middle" >0.2003</td><td align="center" valign="middle" >0.2084</td><td align="center" valign="middle" >0.2097</td></tr><tr><td align="center" valign="middle" >SCM</td><td align="center" valign="middle" >0.1611</td><td align="center" valign="middle" >0.1434</td><td align="center" valign="middle" >0.1366</td><td align="center" valign="middle" >0.1356</td></tr><tr><td align="center" valign="middle" >CMFH</td><td align="center" valign="middle" >0.2132</td><td align="center" valign="middle" >0.2259</td><td align="center" valign="middle" >0.2632</td><td align="center" valign="middle" >0.2419</td></tr></tbody></table></table-wrap><table-wrap id="1_2"><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="4"  ></th><th align="center" valign="middle" >SMFH</th><th align="center" valign="middle" >0.2699</th><th align="center" valign="middle" >0.2835</th><th align="center" valign="middle" >0.2920</th><th align="center" valign="middle" >0.2981</th></tr></thead><tr><td align="center" valign="middle" >LSSH</td><td align="center" valign="middle" >0.2141</td><td align="center" valign="middle" >0.22216</td><td align="center" valign="middle" >0.2218</td><td align="center" valign="middle" >0.2211</td></tr><tr><td align="center" valign="middle" >SePH</td><td align="center" valign="middle" >0.2787</td><td align="center" valign="middle" >0.2956</td><td align="center" valign="middle" >0.3064</td><td align="center" valign="middle" >0.3134</td></tr><tr><td align="center" valign="middle" >LCH</td><td align="center" valign="middle" >0.2681</td><td align="center" valign="middle" >0.3005</td><td align="center" valign="middle" >0.3118</td><td align="center" valign="middle" >0.3233</td></tr><tr><td align="center" valign="middle"  rowspan="7"  >Text2-Img</td><td align="center" valign="middle" >IMH</td><td align="center" valign="middle" >0.1508</td><td align="center" valign="middle" >0.1581</td><td align="center" valign="middle" >0.1636</td><td align="center" valign="middle" >0.1668</td></tr><tr><td align="center" valign="middle" >SCM</td><td align="center" valign="middle" >0.1542</td><td align="center" valign="middle" >0.1373</td><td align="center" valign="middle" >0.1294</td><td align="center" valign="middle" >0.1239</td></tr><tr><td align="center" valign="middle" >CMFH</td><td align="center" valign="middle" >0.4884</td><td align="center" valign="middle" >0.5132</td><td align="center" valign="middle" >0.5269</td><td align="center" valign="middle" >0.5379</td></tr><tr><td align="center" valign="middle" >SMFH</td><td align="center" valign="middle" >0.6051</td><td align="center" valign="middle" >0.6257</td><td align="center" valign="middle" >0.6357</td><td align="center" valign="middle" >0.6428</td></tr><tr><td align="center" valign="middle" >LSSH</td><td align="center" valign="middle" >0.5031</td><td align="center" valign="middle" >0.5224</td><td align="center" valign="middle" >0.5293</td><td align="center" valign="middle" >0.5346</td></tr><tr><td align="center" valign="middle" >SePH</td><td align="center" valign="middle" >0.6318</td><td align="center" valign="middle" >0.6577</td><td align="center" valign="middle" >0.6646</td><td align="center" valign="middle" >0.6609</td></tr><tr><td align="center" valign="middle" >LCH</td><td align="center" valign="middle" >0.6801</td><td align="center" valign="middle" >0.6984</td><td align="center" valign="middle" >0.7203</td><td align="center" valign="middle" >0.7345</td></tr></tbody></table></table-wrap></table-wrap-group><p>表1. 在Wiki数据集上的实验结果</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Experimental results on the MIRFlick datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >任务</th><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >16</th><th align="center" valign="middle" >32</th><th align="center" valign="middle" >64</th><th align="center" valign="middle" >128</th></tr></thead><tr><td align="center" valign="middle"  rowspan="7"  >Img2-Text</td><td align="center" valign="middle" >IMH</td><td align="center" valign="middle" >0.5671</td><td align="center" valign="middle" >0.5654</td><td align="center" valign="middle" >0.5655</td><td align="center" valign="middle" >0.5654</td></tr><tr><td align="center" valign="middle" >SCM</td><td align="center" valign="middle" >0.5876</td><td align="center" valign="middle" >0.5757</td><td align="center" valign="middle" >0.5704</td><td align="center" valign="middle" >0.5627</td></tr><tr><td align="center" valign="middle" >CMFH</td><td align="center" valign="middle" >0.5861</td><td align="center" valign="middle" >0.5835</td><td align="center" valign="middle" >0.5844</td><td align="center" valign="middle" >0.5849</td></tr><tr><td align="center" valign="middle" >SMFH</td><td align="center" valign="middle" >0.6237</td><td align="center" valign="middle" >0.6343</td><td align="center" valign="middle" >0.6448</td><td align="center" valign="middle" >0.6489</td></tr><tr><td align="center" valign="middle" >LSSH</td><td align="center" valign="middle" >0.5784</td><td align="center" valign="middle" >0.5804</td><td align="center" valign="middle" >0.5797</td><td align="center" valign="middle" >0.5816</td></tr><tr><td align="center" valign="middle" >SePH</td><td align="center" valign="middle" >0.6732</td><td align="center" valign="middle" >0.6771</td><td align="center" valign="middle" >0.6783</td><td align="center" valign="middle" >0.6817</td></tr><tr><td align="center" valign="middle" >LCH</td><td align="center" valign="middle" >0.6708</td><td align="center" valign="middle" >0.6892</td><td align="center" valign="middle" >0.6955</td><td align="center" valign="middle" >0.7103</td></tr><tr><td align="center" valign="middle"  rowspan="7"  >Text2-Img</td><td align="center" valign="middle" >IMH</td><td align="center" valign="middle" >0.5672</td><td align="center" valign="middle" >0.5653</td><td align="center" valign="middle" >0.5647</td><td align="center" valign="middle" >0.5647</td></tr><tr><td align="center" valign="middle" >SCM</td><td align="center" valign="middle" >0.5862</td><td align="center" valign="middle" >0.5745</td><td align="center" valign="middle" >0.5657</td><td align="center" valign="middle" >0.5590</td></tr><tr><td align="center" valign="middle" >CMFH</td><td align="center" valign="middle" >0.5937</td><td align="center" valign="middle" >0.5919</td><td align="center" valign="middle" >0.5931</td><td align="center" valign="middle" >0.5919</td></tr><tr><td align="center" valign="middle" >SMFH</td><td align="center" valign="middle" >0.6133</td><td align="center" valign="middle" >0.6209</td><td align="center" valign="middle" >0.6295</td><td align="center" valign="middle" >0.7354</td></tr><tr><td align="center" valign="middle" >LSSH</td><td align="center" valign="middle" >0.5998</td><td align="center" valign="middle" >0.5927</td><td align="center" valign="middle" >0.5932</td><td align="center" valign="middle" >0.5932</td></tr><tr><td align="center" valign="middle" >SePH</td><td align="center" valign="middle" >0.7197</td><td align="center" valign="middle" >0.7271</td><td align="center" valign="middle" >0.7309</td><td align="center" valign="middle" >0.7387</td></tr><tr><td align="center" valign="middle" >LCH</td><td align="center" valign="middle" >0.6996</td><td align="center" valign="middle" >0.7485</td><td align="center" valign="middle" >0.7364</td><td align="center" valign="middle" >0.7564</td></tr></tbody></table></table-wrap><p>表2. 在MIRFlickr数据集上的实验结果</p><table-wrap-group id="3"><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Experimental results on the NUS-WIDE datase</title></caption><table-wrap id="3_1"><table><tbody><thead><tr><th align="center" valign="middle" >任务</th><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >16</th><th align="center" valign="middle" >32</th><th align="center" valign="middle" >64</th><th align="center" valign="middle" >128</th></tr></thead><tr><td align="center" valign="middle"  rowspan="7"  >Img2-Text</td><td align="center" valign="middle" >IMH</td><td align="center" valign="middle" >0.4029</td><td align="center" valign="middle" >0.4090</td><td align="center" valign="middle" >0.4268</td><td align="center" valign="middle" >0.4248</td></tr><tr><td align="center" valign="middle" >SCM</td><td align="center" valign="middle" >0.3907</td><td align="center" valign="middle" >0.3758</td><td align="center" valign="middle" >0.3646</td><td align="center" valign="middle" >0.3542</td></tr><tr><td align="center" valign="middle" >CMFH</td><td align="center" valign="middle" >0.4267</td><td align="center" valign="middle" >0.4229</td><td align="center" valign="middle" >0.4207</td><td align="center" valign="middle" >0.4182</td></tr><tr><td align="center" valign="middle" >SMFH</td><td align="center" valign="middle" >0.4553</td><td align="center" valign="middle" >0.4623</td><td align="center" valign="middle" >0.4658</td><td align="center" valign="middle" >0.4680</td></tr><tr><td align="center" valign="middle" >LSSH</td><td align="center" valign="middle" >0.3900</td><td align="center" valign="middle" >0.3924</td><td align="center" valign="middle" >0.3962</td><td align="center" valign="middle" >0.3966</td></tr><tr><td align="center" valign="middle" >SePH</td><td align="center" valign="middle" >0.5421</td><td align="center" valign="middle" >0.5499</td><td align="center" valign="middle" >0.5537</td><td align="center" valign="middle" >0.5601</td></tr><tr><td align="center" valign="middle" >LCH</td><td align="center" valign="middle" >0.5841</td><td align="center" valign="middle" >0.5991</td><td align="center" valign="middle" >0.6053</td><td align="center" valign="middle" >0.6061</td></tr></tbody></table></table-wrap><table-wrap id="3_2"><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="7"  >Text2-Img</th><th align="center" valign="middle" >IMH</th><th align="center" valign="middle" >0.4002</th><th align="center" valign="middle" >0.4091</th><th align="center" valign="middle" >0.4315</th><th align="center" valign="middle" >0.4317</th></tr></thead><tr><td align="center" valign="middle" >SCM</td><td align="center" valign="middle" >0.3875</td><td align="center" valign="middle" >0.3693</td><td align="center" valign="middle" >0.3608</td><td align="center" valign="middle" >0.3538</td></tr><tr><td align="center" valign="middle" >CMFH</td><td align="center" valign="middle" >0.4627</td><td align="center" valign="middle" >0.4556</td><td align="center" valign="middle" >0.4518</td><td align="center" valign="middle" >0.4478</td></tr><tr><td align="center" valign="middle" >SMFH</td><td align="center" valign="middle" >0.5033</td><td align="center" valign="middle" >0.5056</td><td align="center" valign="middle" >0.5065</td><td align="center" valign="middle" >0.5079</td></tr><tr><td align="center" valign="middle" >LSSH</td><td align="center" valign="middle" >0.4286</td><td align="center" valign="middle" >0.4248</td><td align="center" valign="middle" >0.4228</td><td align="center" valign="middle" >0.4175</td></tr><tr><td align="center" valign="middle" >SePH</td><td align="center" valign="middle" >0.6302</td><td align="center" valign="middle" >0.6425</td><td align="center" valign="middle" >0.6425</td><td align="center" valign="middle" >0.6584</td></tr><tr><td align="center" valign="middle" >LCH</td><td align="center" valign="middle" >0.7382</td><td align="center" valign="middle" >0.7541</td><td align="center" valign="middle" >0.7671</td><td align="center" valign="middle" >0.7678</td></tr></tbody></table></table-wrap></table-wrap-group><p>表3. 在NUS-WIDE数据集上的实验结果</p></sec><sec id="s8_3_2"><title>4.3.2. 实验结果分析</title><p>表1，表2和表3分别给出了LCH与对比算法在Wiki，MIRFlick和NUS-WIDE这三个数据集上的两种跨模态任务的mAP数值，哈希码的长度分别为16 bit 32 bit 64 bit和128 bit。</p><p>对于Wiki数据集，从表1的数据可以看出，LCH在不同哈希码长度下的mAP值优于所对比的算法，验证了LCH在跨模态检索任务中的有效性。同时，观察表1，可以发现大部分有监督跨模态哈希方法比无监督跨模态哈希方法检索效果更好，这是因为有监督的方法通过嵌入真实标签信息到哈希码中，可以大大增加哈希码的判别力。文本检索图片任务的mAP值比图片检索文本的mAP值普遍要高，这是因为文本所包含的信息比图片信息要直观，能更好表达数据的核心语义。此外，通过表1还可以观察到，哈希码越长，哈希码所能保存的信息越多，因此检索效果越好。</p><p>对于MIRFlick数据集和NUS-WIDE数据集，从表2和表3中的mAP数值对比可以观察到，LCH优于其它方法，这与在Wiki数据集中的观察一致，再次验证了本文方法在跨模态检索任务中的有效性。此外，我们可以观察到，这两个数据集中，mAP值都比Wiki数据集中要高，这是因为这两个数据集都是多标签数据集，图像和文本的语义关系更加紧密，这也表明利用标签信息能更好的指导跨模态检索任务。</p></sec><sec id="s8_3_3"><title>4.3.3. 模型收敛性分析</title><p>图1为LCH在三个数据集中迭代收敛过程，从图1的实验结果中可以证明，LCH算法不仅是有效收敛的，而且收敛速度很快，适合在大规模数据中进行跨模态检索任务。</p><p>图1. 模型收敛性分析</p></sec></sec></sec><sec id="s9"><title>5. 结论</title><p>本文提出了一种新的跨模态检索方法，即基于一致性哈希的跨模态检索算法。该算法不仅利用原始数据的信息，还将原始数据中标签信息加入到哈希码的学习过程中，同时，本文利用正交旋转矩阵来学习哈希函数，从而降低产生哈希码时带来的量化误差。本文在三种常用的数据集上进行了大量的实验，并与相关的跨模态哈希算法相比，该方法能够更好的提出检索性能。</p></sec><sec id="s10"><title>文章引用</title><p>刘志虎. 基于标签一致性哈希的跨模态检索算法Label Consistency Hashing for Cross-Modal Retrieval[J]. 计算机科学与应用, 2021, 11(04): 1104-1112. https://doi.org/10.12677/CSA.2021.114114</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41957-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Ding, G., Guo, Y. and Zhou, J. (2014) Collective Matrix Factorization Hashing Formultimodal Data. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Columbus, 23-28 June 2014, 2083-2090.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2014.267</mixed-citation></ref><ref id="hanspub.41957-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Zhou, J., Ding, G. and Guo, Y. (2014) Latent Semantic Sparse Hashing for Cross-Modal Similarity Search. ACM SIGIR International Conference Research Development in Infor-mation Retrieval, Queensland, July 2014, 415-424.  
&lt;br&gt;https://doi.org/10.1145/2600428.2609610</mixed-citation></ref><ref id="hanspub.41957-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Song, J.K., Yang, Y., Yang, Y., et al. (2013) Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources. Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data, New York, June 2013, 785-796. &lt;br&gt;https://doi.org/10.1145/2463676.2465274</mixed-citation></ref><ref id="hanspub.41957-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, D. and Li, W.J. (2014) Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization. Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI), 28, 2177-2183.</mixed-citation></ref><ref id="hanspub.41957-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Tang, J., Wang, K. and Shao, L. (2016) Supervised Matrix Factorization Hashing for Cross-Modal Retrieval. IEEE Transactions on Image Processing, 25, 3157-3166. &lt;br&gt;https://doi.org/10.1109/TIP.2016.2564638</mixed-citation></ref><ref id="hanspub.41957-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Z., Ding, G., Hu, M., et al. (2015) Semantics-Preserving Hashing for Cross-View Retrieval. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 7-12 June 2015, 3864-3872.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2015.7299011</mixed-citation></ref><ref id="hanspub.41957-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Schonemann, P.H. (1966) A Generalized solution of the Or-thogonal Procrustes Problem. Psychometrika, 31, 1-10.  
&lt;br&gt;https://doi.org/10.1007/BF02289451</mixed-citation></ref><ref id="hanspub.41957-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Lu, X., Zhang, H., Sun, J., et al. (2018) Discriminative Correlation Hashing for Supervised Cross-Modal Retrieval. Signal Processing: Image Communication, 65, 221-230. &lt;br&gt;https://doi.org/10.1016/j.image.2018.04.009</mixed-citation></ref><ref id="hanspub.41957-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Chua, T.S., Tang, J., Hong, R., et al. (2009) Nus-Wide: A Re-al-World Web Image Database from National University of Singapore. ACM International Conference on Image and Video Retrieval, Article No. 48, 1-9.  
&lt;br&gt;https://doi.org/10.1145/1646396.1646452</mixed-citation></ref><ref id="hanspub.41957-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Huiskes, M.J. and Lew, M.S. (2008) The MIR Flickr Retrieval Evaluation. Proceedings of the 1st ACM international conference on Multimedia information retrieval, New York, Octo-ber 2008, 39-43.  
&lt;br&gt;https://doi.org/10.1145/1460096.1460104</mixed-citation></ref></ref-list></back></article>