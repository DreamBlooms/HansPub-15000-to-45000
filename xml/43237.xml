<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">PM</journal-id><journal-title-group><journal-title>Pure  Mathematics</journal-title></journal-title-group><issn pub-type="epub">2160-7583</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/PM.2021.116131</article-id><article-id pub-id-type="publisher-id">PM-43237</article-id><article-categories><subj-group subj-group-type="heading"><subject>PM20210600000_93932413.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  Karcher均值算法在动作识别上的应用
  The Application of Karcher Mean Algorithm in Action Recognition
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>赵</surname><given-names>倩</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>叶</surname><given-names>震</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>周</surname><given-names>朝政</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>上海电气集团股份有限公司中央研究院，上海</addr-line></aff><aff id="aff2"><addr-line>上海大学，上海</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>03</day><month>06</month><year>2021</year></pub-date><volume>11</volume><issue>06</issue><fpage>1166</fpage><lpage>1180</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    本文提出Karcher均值算法与测地距离相结合的平均轨迹计算方法，并将其应用到动作识别任务中。具体地说，本文通过基于李群的骨架表示方法，将动作骨架序列表示为流形上的轨迹。为了解决轨迹的时间错位问题，本文将Karcher均值算法与李群上测地距离的定义相结合，计算所有动作轨迹的平均轨迹，然后采用传输平方根向量场表示方法，将所有动作轨迹与平均轨迹进行时间对齐。此外，本文在特征提取阶段，提出对特征进行加权融合，实验结果验证了融合特征的有效性。
    In this paper, we propose an average trajectory calculation method based on the combination of Karcher mean algorithm and geodesic distance, and apply it to the action recognition task. Specif-ically, the skeletal sequences of actions are represented as trajectories on manifolds by a skeleton representation method based on Lie groups. In order to solve the temporal misalignment problem of trajectories, this paper combines the Karcher mean algorithm with the definition of geodesic distance on Lie group, calculates the average trajectories of all action trajectories, and then uses the Transported Square-Root Vector Field representation method to align all action trajectories with the average trajectories in time. In addition, the weighted fusion of features is proposed in the feature extraction stage, and the experimental results verify the effectiveness of the fusion features. 
  
 
</p></abstract><kwd-group><kwd>动作识别，Karcher均值算法，骨架表示，轨迹时间对齐, Action Recognition</kwd><kwd> Karcher Mean Algorithm</kwd><kwd> Skeleton Representation</kwd><kwd> Trajectory Temporal Misalignment</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>本文提出Karcher均值算法与测地距离相结合的平均轨迹计算方法，并将其应用到动作识别任务中。具体地说，本文通过基于李群的骨架表示方法，将动作骨架序列表示为流形上的轨迹。为了解决轨迹的时间错位问题，本文将Karcher均值算法与李群上测地距离的定义相结合，计算所有动作轨迹的平均轨迹，然后采用传输平方根向量场表示方法，将所有动作轨迹与平均轨迹进行时间对齐。此外，本文在特征提取阶段，提出对特征进行加权融合，实验结果验证了融合特征的有效性。</p></sec><sec id="s2"><title>关键词</title><p>动作识别，Karcher均值算法，骨架表示，轨迹时间对齐</p></sec><sec id="s3"><title>The Application of Karcher Mean Algorithm in Action Recognition<sup> </sup></title><p>Qian Zhao<sup>1</sup>, Zhen Ye<sup>2</sup>, Chaozheng Zhou<sup>2</sup></p><p><sup>1</sup>Shanghai University, Shanghai</p><p><sup>2</sup>Shanghai Electric Central Research Institute, Shanghai</p><p><img src="//html.hanspub.org/file/20-1251309x4_hanspub.png" /></p><p>Received: May 12<sup>th</sup>, 2021; accepted: Jun. 14<sup>th</sup>, 2021; published: Jun. 21<sup>st</sup>, 2021</p><p><img src="//html.hanspub.org/file/20-1251309x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In this paper, we propose an average trajectory calculation method based on the combination of Karcher mean algorithm and geodesic distance, and apply it to the action recognition task. Specifically, the skeletal sequences of actions are represented as trajectories on manifolds by a skeleton representation method based on Lie groups. In order to solve the temporal misalignment problem of trajectories, this paper combines the Karcher mean algorithm with the definition of geodesic distance on Lie group, calculates the average trajectories of all action trajectories, and then uses the Transported Square-Root Vector Field representation method to align all action trajectories with the average trajectories in time. In addition, the weighted fusion of features is proposed in the feature extraction stage, and the experimental results verify the effectiveness of the fusion features.</p><p>Keywords:Action Recognition, Karcher Mean Algorithm, Skeleton Representation, Trajectory Temporal Misalignment</p><disp-formula id="hanspub.43237-formula18"><graphic xlink:href="//html.hanspub.org/file/20-1251309x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/20-1251309x7_hanspub.png" /> <img src="//html.hanspub.org/file/20-1251309x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>动作识别算法作为计算机视觉领域热门的研究课题之一，被广泛地应用于人机交互 [<xref ref-type="bibr" rid="hanspub.43237-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref2">2</xref>]、监控安全 [<xref ref-type="bibr" rid="hanspub.43237-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref4">4</xref>] 以及医疗健康 [<xref ref-type="bibr" rid="hanspub.43237-ref5">5</xref>] 等领域。由于RGB图像数据存在遮挡、光线变化以及视角变换等挑战，基于骨架的动作识别算法为人们打开了新的思路。</p><p>骨架数据的一个重要优势在于它显式地记录了关节点的坐标信息，避免背景信息的干扰。但挑战也是显而易见的，动作的执行速率不同，导致采样的骨架序列存在时间错位的问题。尤其是，当以不同的演变速率执行相同的动作时，数据对应的采样帧并不完全相似。解决这个问题的常用方法是找到一个合适的模板动作，将所有动作序列样本与模板对齐。形状分析都在对齐后的骨架序列上进行，从而避免由时间错位引起的度量扭曲。</p><p>在动作识别的黎曼框架下，骨架序列被表示成流形上的轨迹，而流形上模板轨迹的计算方法，目前大致分为两种。第一种是在轨迹的切空间寻找模板。Vemulapalli等人 [<xref ref-type="bibr" rid="hanspub.43237-ref1">1</xref>] 将动作骨架序列表示为李群上的轨迹，随后将轨迹上的点映射到李代数上。由于李代数为向量空间，因此，作者首先随机选择一条轨迹的李代数来初始化模板，并将其他轨迹的李代数与之对齐。然后直接求取对齐后的所有轨迹李代数的线性平均，将该平均向量作为新的对齐模板。在切空间寻找对齐模板，方法简单，计算高效，但是一旦将轨迹映射到了切空间，它就不再具有轨迹的非欧结构。这在对齐轨迹时，可能会损失一些结构信息。第二种常用方法就是基于Karcher均值的概念，计算所有样本轨迹的平均轨迹，并将该平均轨迹作为模板轨迹。平均轨迹包含所有样本轨迹的统计信息，是合适的轨迹对齐模板。Su等人 [<xref ref-type="bibr" rid="hanspub.43237-ref7">7</xref>] 提出将传输平方根向量场(Transported Square-Root Vector Field, TSRVF)与Karcher均值概念相结合的平均轨迹计算方法，将平均轨迹定义为一条积分曲线，积分起点固定，沿着平均传输平方根向量场方向进行积分。Anirudh等人 [<xref ref-type="bibr" rid="hanspub.43237-ref8">8</xref>] 将Su等人 [<xref ref-type="bibr" rid="hanspub.43237-ref7">7</xref>] 提出的平均轨迹计算方法与基于李群的骨架表示方法进行结合，用于解决李群上轨迹的时间错位问题。Amor等人 [<xref ref-type="bibr" rid="hanspub.43237-ref9">9</xref>] 将骨架序列表示成Kendall形状空间上的轨迹，然后结合轨迹在形状空间上的测地距离以及Karcher均值的概念来计算平均轨迹。</p><p>本文基于Karcher均值算法的概念，提出将Karcher均值算法与李群上测地距离相结合的平均轨迹计算方法。该方法不同于上述工作中的平均轨迹计算方法，其基本思想是平均轨迹到所有样本轨迹的测地距离平方和最小。根据测地距离，将平均轨迹逐步向样本轨迹中心平行移动，因此，计算所得的平均轨迹更靠近样本轨迹中心。本文将该方法应用于动作识别中，构建一个新的动作识别框架。本文首先采用基于李群的骨架表示方法 [<xref ref-type="bibr" rid="hanspub.43237-ref6">6</xref>]，对骨架上身体部位之间的相对几何位置(旋转和平移)进行编码。随后，本文运用提出的平均轨迹计算方法，计算所有样本轨迹的平均轨迹，并利用传输平方根向量场表示方法 [<xref ref-type="bibr" rid="hanspub.43237-ref7">7</xref>] 将所有样本轨迹与平均轨迹对齐。在特征提取阶段，本文提取两种分类特征，即平均轨迹到样本轨迹的射击向量以及样本轨迹的李代数。为了解决数据噪声问题，我们采用傅里叶时间金字塔表示方法 [<xref ref-type="bibr" rid="hanspub.43237-ref10">10</xref>] 来处理两类特征，滤掉高频系数，并提出对两类特征的傅里叶系数进行加权融合，以获得更优的动作识别结果。最后，我们训练一个one-vs-all线性支持向量机分类器来实现动作分类。本文的主要贡献可以总结为以下两点：</p><p>1) 提出了Karcher均值与测地距离相结合的平均轨迹计算方法，并将其应用于动作识别任务，构建出一个新的动作识别框架。</p><p>2) 提出对两种动作特征的傅里叶系数进行加权融合。本文实验结果验证了融合特征的有效性。</p></sec><sec id="s6"><title>2. 预备知识</title><sec id="s6_1"><title>2.1. 特殊欧氏群SE(3)的基础知识介绍</title><p>特殊欧氏群SE(3)是 4 &#215; 4 矩阵的集合，矩阵形式为</p><p>P = [ R d → 0 1 ] ,</p><p>其中 R ∈ ℝ 3 &#215; 3 表示旋转矩阵，它是特殊正交群SO(3)上的元素。 d → ∈ ℝ 3 表示平移向量。从几何的角度看，特殊欧氏群SE(3)中的元素形成了一个弯曲的6维流形，并且具有李群结构 [<xref ref-type="bibr" rid="hanspub.43237-ref11">11</xref>]。恒等矩阵 I 4 是SE(3)中的元素，同时也是该李群的单位元。</p><p>SE(3)在单位元 I 4 处的切平面称为SE(3)的李代数，记为 s e ( 3 ) 。对任意元素</p><p>ξ ^ = [ ω ^ v → 0 0 ] = [ 0 − ω 3 ω 2 v 1 ω 3 0 − ω 1 v 2 − ω 2 ω 1 0 v 3 0 0 0 0 ] ∈ s e ( 3 ) .</p><p>其中 v → ∈ ℝ 3 ， ω ^ 是 3 &#215; 3 的反对称矩阵。它的等价向量表示为 ξ = [ ω T , v → T ] T = [ ω 1 , ω 2 , ω 3 , v 1 , v 2 , v 3 ] T ∈ ℝ 6 ，是一个6维的向量空间。</p><p>对于SE(3)上的指数映射和对数映射，本文沿用 [<xref ref-type="bibr" rid="hanspub.43237-ref12">12</xref>] 中给出的定义。对任意 ξ ^ ∈ s e ( 3 ) ，当 ‖ ω ‖ = 0 时，指数映射定义为</p><p>e x p ξ ^ = [ 1 v → 0 1 ] , (1)</p><p>当 ‖ ω ‖ ≠ 0 时，指数映射定义为</p><p>exp ξ ^ = [ e ω ^ A v → 0 1 ] . (2)</p><p>其中 e ω ^ 和A可由Rodrigue公式显式给出，</p><p>e ω ^ = I + ω ^ ‖ ω ‖ sin ‖ ω ‖ + ω ^ 2 ‖ ω ‖ 2 ( 1 − cos ‖ ω ‖ ) ,</p><p>A = I + ω ^ ‖ ω ‖ 2 ( 1 − cos ‖ ω ‖ ) + ω ^ 2 ‖ ω ‖ 3 ( ‖ ω ‖ − sin ‖ ω ‖ ) .</p><p>对任意 P ∈ S E ( 3 ) ，对数映射的定义为</p><p>ξ ^ = log [ R d → 0 1 ] = [ ω ^ A − 1 d → 0 0 ] , (3)</p><p>其中 ω ^ = log R ，当 ‖ ω ‖ ≠ 0 时，</p><p>A − 1 = I − 1 2 ω ^ + 2 sin ‖ ω ‖ − ‖ ω ‖ ( 1 + cos ‖ ω ‖ ) 2 ‖ ω ‖ 2 sin ‖ ω ‖ ω ^ 2 ,</p><p>当 ‖ ω ‖ = 0 时， A = I 。</p><p>图1. 成对插值法。红点为待插值点，橘色实线为李群SE(3)测地线</p><p>对SE(3)上的数据进行插值有多种方法 [<xref ref-type="bibr" rid="hanspub.43237-ref13">13</xref>]。本文主要采用基于螺旋运动的成对插值法 [<xref ref-type="bibr" rid="hanspub.43237-ref14">14</xref>]。如图1所示，给定 P ( t 1 ) , P ( t 2 ) , ⋯ , P ( t n ) ∈ S E ( 3 ) ，分别对应时刻 t 1 , t 2 , ⋯ , t n ，然后定义以下曲线进行插值：</p><p>β ( t ) = P ( t i ) exp S E ( 3 ) ( t − t i t i + 1 − t i B i ) ,</p><p>其中 t ∈ [ t i , t i + 1 ] ， B i = log S E ( 3 ) ( P ( t i ) − 1 P ( t i + 1 ) ) ， i = 1 , 2 , ⋯ , n − 1 。指数映射exp和对数映射log通过(1)、(2)、(3)式进行估计。</p></sec><sec id="s6_2"><title>2.2. 骨架表示的具体方法</title><p>令 S = ( L , E ) 表示一副骨架，其中 L = { l 1 , l 2 , ⋯ , l N } 表示关节点集合， E = { e 1 , e 2 , ⋯ , e M } 表示带有方向的身体骨骼位置集合。如图2展示了一副包含15个关节点和14块身体骨骼的骨架。</p><p>给定一对身体骨骼 e m 和 e n ，如图3所示， e n 1 , e n 2 ∈ ℝ 3 分别表示身体骨骼的 e n 起点和终点。为了表述它们之间的相对几何关系，我们首先对其中一块骨骼构建局部坐标系，然后描述将其移动到另一块骨骼位置处的旋转和平移。在图3中，骨骼 e n 的局部坐标系是通过旋转一平移全局坐标系所得。令 e n 为 x 轴， e n 1 是坐标原点， e m 1 n ( t ) , e m 2 n ( t ) ∈ ℝ 3 分别为时刻 t 时，骨骼 e m 在 e n 局部坐标系下的起始关节点。于是，骨骼 e m 和 e n 在时刻 t 下的相对几何可以描述为</p><p>P m , n ( t ) = [ R m , n ( t ) d → m , n ( t ) 0 1 ] ∈ S E ( 3 ) .</p><p>图2. 人体骨架示意图</p><p>图3. e<sub>m</sub>在e<sub>n</sub>的局部坐标系下的表示<sub> </sub></p><p>考虑到单个 P m , n ( t ) 在一些特定的相对运动下会保持不变，因此，本文同时采用 P m , n ( t ) 和 P n , m ( t ) 来表示骨骼 e m 和 e n 之间的相对几何。只有当骨骼间不存在相对运动时， P m , n ( t ) 和 P n , m ( t ) 才会同时保持不变 [<xref ref-type="bibr" rid="hanspub.43237-ref6">6</xref>]。</p><p>通过上述身体骨骼之间的相对几何，我们可以把时刻 t 时的一副完整骨架 S 表示为</p><p>β ( t ) = ( P 1 , 2 ( t ) , P 2 , 1 ( t ) , ⋯ , P M − 1 , M ( t ) , P M , M − 1 ( t ) ) ∈ S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) ,</p><p>其中 M 为身体骨骼的数量。一副完整的骨架共有 M ( M − 1 ) 个骨骼对，本文为了记号简洁，将骨骼对 P m , n ( t ) 下标 ( m , n ) 用骨骼对数目的序号表示，记为 P k ( t ) ， k = 1 , ⋯ , M ( M − 1 ) ，即完整的骨架表示又可以写为</p><p>β ( t ) = ( P 1 ( t ) , P 2 ( t ) , ⋯ , P M ( M − 1 ) ( t ) ) ∈ S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) .</p><p>依据这种骨架表示方法，一个完整的动作骨架序列就可以表示为 S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) 上的轨迹 { β ( t ) , t ∈ [ 1 , T ] } .</p></sec></sec><sec id="s7"><title>3. Karcher均值与测地距离相结合的平均轨迹计算方法</title><p>本文采用基于李群的骨架表示方法，将动作骨架序列表示为特殊欧氏群积空间 S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) 上的轨迹，记为 β ( t ) , t = 1 , ⋯ , T 。 T 为骨架序列的帧数。假设每副骨架包含 M 块身体骨骼，则每条轨迹的尺寸为 ，其中 4 &#215; 4 是相对几何的尺寸。</p><p>Karcher均值这个概念首先由Grove和 Karcher [<xref ref-type="bibr" rid="hanspub.43237-ref15">15</xref>] 提出，之后被推广到黎曼流形上表示样本中心。本文提出Karcher均值与测地距离相结合的平均轨迹计算方法，其基本思想是样本轨迹中心(平均轨迹)到所有样本轨迹的测地距离平方和最小。给定 S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) 上 n 条样本轨迹 β 1 , ⋯ , β n ，根据 Karcher均值的概念，平均轨迹 μ 的计算公式可以描述为</p><p>μ = arg min β ∈ S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) ∑ i = 1 n d ( β i , β ) 2 , (4)</p><p>其中 d ( β i , β ) 为动作轨迹 β i 和 β 之间的测地距离。如图4所示，红点代表流形上的轨迹，黄点表示潜在的轨迹中心。红色实线表示轨迹，黄色虚线表示两点之间的测地线。</p><p>图4. Karcher均值与测地距离相结合的平均轨迹计算方法</p><p>令 M ′ = M ( M − 1 ) ，那么每条动作轨迹应当包含 M ′ &#215; T 个相对几何位置(旋转和平移)，每个相对几何位置记为 P k i ( t ) ∈ S E ( 3 ) ， i = 1 , ⋯ , n 是样本轨迹的序号， k = 1 , ⋯ , M ′ 表示每副骨架上的相对几何位置的序号， t = 1 , ⋯ , T 为骨架帧数的序号。因此，动作轨迹之间的测地距离平方和可以写为两条动作轨迹上，所有对应相对几何位置之间的测地距离平方和，即</p><p>μ = arg min P k μ ∈ S E ( 3 ) ∑ i = 1 n ∑ k = 1 M ′ ∑ t = 1 T δ ( P k i ( t ) , P k μ ( t ) ) 2 , (5)</p><p>其中 δ ( P k i ( t ) , P k μ ( t ) ) 表示的是 t 时刻，第 i 条动作轨迹的第 k 个相对几何到平均轨迹 μ 的第 k 个相对几何的测地距离。由于相对几何位置位于李群 S E ( 3 ) 上，因此，距离 δ 定义在李群 S E ( 3 ) 上。令 P k i ( t ) , P k j ( t ) ∈ S E ( 3 ) ，则第 i 条动作轨迹的第 k 个相对几何到第 j 条动作轨迹的第 k 个相对几何的测地距离可以描述为</p><p>δ ( P k i ( t ) , P k j ( t ) ) = ∫ 0 1 γ ˙ ( s ) d s = ‖ v i j , k ( t ) ‖ ,</p><p>其中 v i j , k ( t ) = log P k i ( t ) ( P k j ( t ) ) 。对数映射 log 的具体计算形式可以通过(3)式所得。</p><p>在求解目标函数(5)式的过程中，要求平均轨迹到所有动作轨迹的测地距离平方和最小，即要求轨迹上每个对应相对几何之间的测地距离最小。因此，(5)式的具体求解步骤可总结如下算法1所示。</p></sec><sec id="s8"><title>4. 平均轨迹计算方法在动作识别方向上的应用</title><sec id="s8_1"><title>4.1. 特征的提取和融合</title><p>由第二章可知，本文首先将骨架序列表示为李群 S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) 上的轨迹。为了解决轨迹时间错位问题，本文采用Karcher均值与测地距离相结合的平均轨迹计算方法，计算所有样本的平均轨迹，并将其作为轨迹时间对齐的模板。随后，本文采用TSRVF表示方法 [<xref ref-type="bibr" rid="hanspub.43237-ref7">7</xref>]，用于对齐平均轨迹和样本轨迹。然而，对齐后的轨迹仍位于流形空间，在这个空间上进行轨迹分类并不容易。为了解决这个问题，我们针对时间对齐后的轨迹提取两类动作特征：一类是平均轨迹到样本轨迹的射击向量(Shooting Vector，SV)，另一类是样本轨迹的李代数(Lie Algebra, LA)。射击向量可以理解为单位时间内，序列空间上一点到另一点的切向 [<xref ref-type="bibr" rid="hanspub.43237-ref8">8</xref>]。在本文中，轨迹时间对齐之后，我们在 τ = 0 时刻，从平均轨迹 μ ( t ) 的某一点出发，在 τ = 1 时刻，到达另一动作轨迹的对应点，计算两点之间的射击向量，如图5所示。具体计算过程如算法2所示。</p><p>图5. 特征射击向量的示意图</p><p>由于轨迹位于李群 S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) 上，同时轨迹的李代数为向量空间，因此，本文提取轨迹的李代数进行动作表征。李代数的提取方法总结在算法3中</p><p>为了克服数据噪声问题，获得更加有效、鲁棒的动作表征，本文采用傅里叶时间金字塔方法 [<xref ref-type="bibr" rid="hanspub.43237-ref10">10</xref>] 处理两类特征，分别获得射击向量的傅里叶时间金字塔系数 V S C 和李代数的傅里叶时间金字塔系数 V L C 。此外，为了获得更优的动作识别结果，本文提出对两类特征的傅里叶时间金字塔系数进行加权融合，融合特征 V F C = α V S C + ( 1 − α ) V L C ，其中 α 为特征的权重系数。融合特征既包含轨迹之间的距离信息，也包含单个样本轨迹的几何信息。</p></sec><sec id="s8_2"><title>4.2. 动作识别框架</title><p>本文将Karcher 均值和测地距离相结合的平均轨迹计算方法应用于动作识别中，构建一个新的动作识别框架，如图6所示。</p><p>图6. 本文的动作识别框架</p><p>第一步，采用基于李群的骨架表示方法，对3D骨架形状的相对几何进行编码，从而得到位于特殊欧氏群积空间 S E ( 3 ) &#215; ⋯ &#215; S E ( 3 ) 上的动作轨迹。</p><p>第二步，针对每一个动作类别，运用本文提出的平均轨迹计算方法，计算每个动作的平均轨迹。该平均轨迹将用作轨迹时间对齐的对齐模板。</p><p>第三步，采用TSRVF表示方法将每条样本轨迹与平均轨迹进行对齐。</p><p>第四步，得到时间对齐后的轨迹，可以进一步实现特征提取。图 6中第一行为特征射击向量的提取过程，计算的是平均轨迹到所有对齐后的动作轨迹的切向量。第二行为特征李代数的提取过程，计算的是每条轨迹在单位元 I 4 处的切向量。</p><p>第五步，为了解决数据噪声的问题，本文采用傅里叶时间金字塔表示方法处理两类特征，滤去高频系数。由于我们对特征的每一维都用傅里叶时间金字塔进行处理，因此最终获得的特征是所有傅里叶系数的级联。</p><p>第六步，在分类之前，我们对两种分类特征的傅里叶系数进行加权融合，然后训练一个one-vs-all 线性支持向量机来分类特征。</p></sec></sec><sec id="s9"><title>5. 实验评估</title><sec id="s9_1"><title>5.1. 数据集</title><p>本文采用了两个动作识别数据集，分别是UTKinect Action数据集 [<xref ref-type="bibr" rid="hanspub.43237-ref16">16</xref>] 和MSR-Action3D数据集 [<xref ref-type="bibr" rid="hanspub.43237-ref17">17</xref>]。</p><p>UTKinect Action数据集：UTKinect Action是2012年使用单个静态Kinect相机采集的数据集。它由199个动作序列组成，分别由10位演员执行10个动作：“walk”，“sit down”，“stand up”，“pick up”，“carry”，“throw”，“push”，“pull”，“wave hands”以及“clap hands”。每个动作重复执行两次。每帧骨架上包含20个关节点。这个数据集的挑战性在于，所有的动作序列并不是在同一视角下捕捉到的。如图7所示，(a)行和(b)行虽然都是动作“walk”的骨架序列，但是视角完全相反。因此在处理该数据集时，必须克服视角变换的差异。</p><p>MSR-Action3D数据集：MSR-Action3D是2010年由一个与Kinect类似的深度传感器采集的数据集。它包含557个动作序列，分别由10位演员执行20个动作：“high arm wave”，“horizontal arm wave”，“hammer”，“hand catch”，“forward punch”，“high throw”，“draw X”，“draw tick”，“draw circle”，“hand clap”，“two hand wave”，“side boxing”，“bend”，“forward kick”，“side kick”，“jogging”，“tennis swing”，“tennis serve”，“golf swing”以及“pick up and throw”。每个动作重复执行两到三次。每帧骨架上包含20个关节点。如图8所示，骨架序列从左到右，展示了动作“high arm wave”的部分骨架序列。</p><p>图7. UTKinect数据集中，动作“walk”在两个视角下的部分骨架序列</p><p>图8. MSR-Action3D数据集中，动作“high arm wave”的部分骨架序列</p></sec><sec id="s9_2"><title>5.2. 实验设计</title><p>为了保证实验的丰富性，本文对两种分类特征以及它们的融合特征分别求取动作识别结果。假设每个动作有 T 帧骨架，每副骨架有 N 个关节点和 M 个身体部位。由于基于李群的骨架表示是对骨架骨骼之间的相对几何进行编码，因此每条动作轨迹的尺寸为 [ M ( M − 1 ) , 4 &#215; 4 , T ] ，其中 4 &#215; 4 是刚体变换矩阵的维度。令 j = 1 , 2 , ⋯ , J 为不同动作类别的下标， μ j 是根据本文提出的平均轨迹计算方法得到的第 j 类动作的平均轨迹。对任意动作轨迹 β i ( i 是样本轨迹的下标)，令 β i j 表示样本轨迹 β i 到平均轨迹 μ j 的最优对齐结果。接下来，我们计算射击向量 V S 及其傅里叶系数 V S C ，李代数 V L 及其傅里叶系数 V L C ，并进行加权融合。由于有 J 类动作，因此，可以计算得到 J 个特征射击向量 V S 和特征李代数 V L 。</p></sec><sec id="s9_3"><title>5.3. 实验参数设置</title><p>针对这两个动作数据集，本文采用 [<xref ref-type="bibr" rid="hanspub.43237-ref18">18</xref>] 中提出的目标交叉实验设置(cross-subject test setting)。一半演员所做的动作用于训练分类器，剩下一半演员所做的动作用于测试。我们选择五种不同的训练和测试组合，最终的分类结果为五种组合结果的平均值。我们将算法1中的超参数 ϵ 设置为0.1。在对两类分类特征进行加权融合时，我们分别测试了从0.1到0.9共9组参数，然后选择其中最佳的参数组合。</p></sec><sec id="s9_4"><title>5.4. 实验结果及讨论</title><p>UTKinect Action数据集上的动作分类结果如表1所示。特征射击向量和李代数的分类结果分别为96.80%和97.20%，李代数分类结果比射击向量高出0.4%。特征融合的结果展示在图9中。由图可以看出，共有四组融合参数[0.7, 0.3]，[0.6, 0.4]，[0.5, 0.5]以及[0.4, 0.6]，使得融合特征取得最高的分类结果98.20%。参数组合的前者为特征射击向量的权重系数，后者为李代数的权重系数。该融合特征的结果比射击向量和李代数的分类结果分别高出1.4%和1%。可以看出，在UTKinect Action数据集上，特征融合的操作能够有效提高动作识别的正确率。图10展示了UTKinect Action数据集上各类动作的分类正确率。10类动作中，有8类动作达到了100%的识别正确率，动作“throw”识别正确率最低，为86%，动作“pick up”识别正确率为96%。本文提出的动作识别框架在UTKinect Action数据集上取得了良好表现。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Action classification results on UTKinect Action dataset. SV: shooting vector, LA: Lie algebr</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >平均轨迹计算方法</th><th align="center" valign="middle"  colspan="3"  >分类特征(%)</th></tr></thead><tr><td align="center" valign="middle" >SV</td><td align="center" valign="middle" >LA</td><td align="center" valign="middle" >SV + LA</td></tr><tr><td align="center" valign="middle" >Karcher均值与测地距离相结合的算法1</td><td align="center" valign="middle" >96.80</td><td align="center" valign="middle" >97.20</td><td align="center" valign="middle" >98.20</td></tr></tbody></table></table-wrap><p>表1. UTKinect Action数据集上的动作分类结果。SV：射击向量，LA：李代数</p><p>图9. UTKinect Action数据集上的特征加权融合</p><p>MSR-Action3D数据集上的动作分类结果如表2所示。特征射击向量和特征李代数的动作分类结果分别为89.05%和91.40%。李代数的分类正确率比射击向量高出2.35%。最佳特征融合参数如图11所示，在参数组合为[0.2, 0.8]或[0.1, 0.9]时，融合特征取得最高的动作分类结果91.48%。该融合特征结果比两个单类特征的分类结果分别高出2.43%和0.08%。图12展示了MSR-Action3D数据集中各项动作的识别正确率。大部分动作类别都能达到80%以上的识别正确率。由于动作“hand catch”和“draw circle”相似度较高，因此误分类情况较多，两类动作的识别正确率仅有31.67%和69.33%。</p><p>通过对比三类特征的实验结果，可以发现，特征李代数的实验结果明显优于特征射击向量。这是因为在李群SE(3)上，当两点之间的距离较远时，对数映射容易发生扭曲，从而使得特征射击向量的有效性低于特征李代数。其次，在两个数据集上，融合特征的分类正确率也要高于两个单类特征，验证了特征融合操作的有效性。</p><p>本文的动作识别实验结果也与采用了相同实验设置 [<xref ref-type="bibr" rid="hanspub.43237-ref18">18</xref>] 的现有方法进行了比较。我们将现有方法的分类结果展示在了表格第一栏，表格第二栏为本文的最优分类结果。</p><p>图10. UTKinect Action数据集上的每类动作的分类正确率</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Action classification results on the MSR-Action3D dataset. SV: shooting vector, LA: Lie algebr</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >平均轨迹计算方法</th><th align="center" valign="middle"  colspan="3"  >分类特征(%)</th></tr></thead><tr><td align="center" valign="middle" >SV</td><td align="center" valign="middle" >LA</td><td align="center" valign="middle" >SV + LA</td></tr><tr><td align="center" valign="middle" >Karcher均值与测地距离相结合的算法1</td><td align="center" valign="middle" >89.05</td><td align="center" valign="middle" >91.40</td><td align="center" valign="middle" >91.48</td></tr></tbody></table></table-wrap><p>表2. MSR-Action3D数据集上的动作分类结果。SV：射击向量，LA：李代数</p><p>图11. MSR-Action3D数据集上的特征加权融合</p><p>本文方法与现有其他方法在UTKinect Action数据集上的动作识别正确率展示在表3中。由表格可知，在目标交叉实验设置下，本文实验取得了最优的动作识别结果98.2%，比采用了相同骨架表示方法的黎曼方法Lie Group [<xref ref-type="bibr" rid="hanspub.43237-ref1">1</xref>] 和TSRVF on Lie Group [<xref ref-type="bibr" rid="hanspub.43237-ref8">8</xref>] 分别高出1.12%和3.33%。与非黎曼方法JL-distance LSTM [<xref ref-type="bibr" rid="hanspub.43237-ref19">19</xref>] 的实验结果相比，也要高出2.24%。与最近的SCDL [<xref ref-type="bibr" rid="hanspub.43237-ref20">20</xref>] 方法相比，本文实验结果略高出0.81%。本文的动作识别框架在UTKinect Action数据集上取得了最优的实验结果。</p><p>图12. MSR-Action3D数据集上的每类动作的分类正确率</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Comparison of the results of the action recognition method in this paper and other methods on the UTKinect Action datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >动作识别方法</th><th align="center" valign="middle" >UTKinect Action数据集(%)</th></tr></thead><tr><td align="center" valign="middle" >Lie Group [<xref ref-type="bibr" rid="hanspub.43237-ref2014">2014</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref1">1</xref>]</td><td align="center" valign="middle" >97.08</td></tr><tr><td align="center" valign="middle" >TSRVF on Lie Group [<xref ref-type="bibr" rid="hanspub.43237-ref2016">2016</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref8">8</xref>]</td><td align="center" valign="middle" >94.87</td></tr><tr><td align="center" valign="middle" >JL-distance LSTM [<xref ref-type="bibr" rid="hanspub.43237-ref2017">2017</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref19">19</xref>]</td><td align="center" valign="middle" >95.96</td></tr><tr><td align="center" valign="middle" >SCDL [<xref ref-type="bibr" rid="hanspub.43237-ref2020">2020</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref20">20</xref>]</td><td align="center" valign="middle" >97.39</td></tr><tr><td align="center" valign="middle" >Karcher均值与测地距离相结合的算法1 (SV + LA)</td><td align="center" valign="middle" >98.20</td></tr></tbody></table></table-wrap><p>表3. 本文动作识别方法与其他方法在UTKinect Action数据集上的结果比较</p><p>表4展示了本文动作识别方法与其他动作识别算法在MSR-Action3D数据集上的动作分类结果。可以看出，在目标交叉实验设置下，本文实验的动作识别正确率比采用了相同骨架表示方法的Lie Group [<xref ref-type="bibr" rid="hanspub.43237-ref1">1</xref>] 和TSRVF on Lie Group [<xref ref-type="bibr" rid="hanspub.43237-ref8">8</xref>] 分别高出2%和6.32%。比TSRVF on S [<xref ref-type="bibr" rid="hanspub.43237-ref9">9</xref>] 工作高出2.48%，该方法将骨架序列表示为Kendall形状空间上的轨迹，并且同样采用TSRVF表示方法进行时间对齐。与SCDL [<xref ref-type="bibr" rid="hanspub.43237-ref20">20</xref>] 相比，本文实验结果仍高出1.47%。本文的动作识别框架在MSR-Action3D数据集上仍取得最优的实验结果。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> The comparison of the results of the action recognition method in this paper with other methods on the MSR- Action3D datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >动作识别方法</th><th align="center" valign="middle" >MSR-Action3D数据集(%)</th></tr></thead><tr><td align="center" valign="middle" >Lie Group [<xref ref-type="bibr" rid="hanspub.43237-ref2014">2014</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref1">1</xref>]</td><td align="center" valign="middle" >89.48</td></tr><tr><td align="center" valign="middle" >TSRVF on Lie Group [<xref ref-type="bibr" rid="hanspub.43237-ref2016">2016</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref8">8</xref>]</td><td align="center" valign="middle" >85.16</td></tr><tr><td align="center" valign="middle" >TSRVF on S [<xref ref-type="bibr" rid="hanspub.43237-ref2016">2016</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref9">9</xref>]</td><td align="center" valign="middle" >89.00</td></tr><tr><td align="center" valign="middle" >SCDL [<xref ref-type="bibr" rid="hanspub.43237-ref2020">2020</xref>] [<xref ref-type="bibr" rid="hanspub.43237-ref20">20</xref>]</td><td align="center" valign="middle" >90.01</td></tr><tr><td align="center" valign="middle" >Karcher均值与测地距离相结合的算法1 (SV + LA)</td><td align="center" valign="middle" >91.48</td></tr></tbody></table></table-wrap><p>表4. 本文动作识别方法与其他方法在MSR-Action3D数据集上的结果比较</p></sec></sec><sec id="s10"><title>6. 结论</title><p>本文主要针对轨迹时间对齐过程中，模板轨迹的求取问题，提出Karcher均值与测地距离相结合的平均轨迹计算方法。该方法计算所有样本轨迹的平均轨迹，然后将平均轨迹当作轨迹对齐的模板。随后，本文将该平均轨迹计算方法应用于动作识别框架中。本文动作识别框架流程清晰简洁，同时，通过实验结果可以看出，本文根据提出算法求得的平均轨迹是合适的对齐模板。然而，Karcher均值算法存在初始值依赖的问题，可能会为结果引入偏差。因此，无偏的平均轨迹计算方法仍是一个值得研究的方向。</p><p>本文还针对动作识别框架中的特征提取，提出将特征射击向量和李代数的傅里叶系数进行加权融合。融合特征既包含了轨迹间的距离信息，也包含了轨迹自身的几何信息。实验在两个动作识别数据集上进行。通过对比三类特征的实验结果，验证了特征融合的有效性。同时，与现有的动作识别方法相比，本文提出的动作识别框架也取得了最优的实验结果。</p></sec><sec id="s11"><title>基金项目</title><p>上海市“科技创新行动计划”(18441909000)。</p></sec><sec id="s12"><title>文章引用</title><p>赵 倩,叶 震,周朝政. Karcher均值算法在动作识别上的应用The Application of Karcher Mean Algorithm in Action Recognition[J]. 理论数学, 2021, 11(06): 1166-1180. https://doi.org/10.12677/PM.2021.116131</p></sec><sec id="s13"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.43237-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Polat, E., Yeasin, M. and Sharma, R. (2003) Robust Tracking of Human Body Parts for Collaborative Human Computer Interaction. Computer Vision &amp; Image Understanding, 89, 44-69. &lt;br&gt;https://doi.org/10.1016/S1077-3142(02)00031-0</mixed-citation></ref><ref id="hanspub.43237-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Mokhber, A., Achard, C. and Milgram, M. (2008) Recogni-tion of Human Behavior by Space-Time Silhouette Characterization. Pattern Recognition Letters, 29, 81-89. &lt;br&gt;https://doi.org/10.1016/j.patrec.2007.08.016</mixed-citation></ref><ref id="hanspub.43237-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Chang, E. and Wang, Y.F. (2004) Introduction to the Special Issue on Video Surveillance. Multimedia Systems, 10, 116-117. &lt;br&gt;https://doi.org/10.1007/s00530-004-0144-5</mixed-citation></ref><ref id="hanspub.43237-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">房菲. 基于核方法的支持向量机在人体动作识别中的应用研究[D]: [硕士学位论文]. 青岛: 中国海洋大学, 2013.</mixed-citation></ref><ref id="hanspub.43237-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Y., Duff, M., Lehrer, N., et al. (2011) A Computational Framework for Quantitative Evaluation of Movement during Rehabilitation. AIP Conference Proceedings, 1371, 317-326.</mixed-citation></ref><ref id="hanspub.43237-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Vemulapalli, R., Arrate, F. and Chellappa, R. (2014) Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group. In: Proceed-ings of the IEEE Conference on Computer Vision and Pattern Recognition, IEEE, Columbus, 588-595. &lt;br&gt;https://doi.org/10.1109/CVPR.2014.82</mixed-citation></ref><ref id="hanspub.43237-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Su, J., Kurtek, S., Klassen, E., et al. (2014) Statistical Analysis of Trajectories on Riemannian Manifolds: Bird Migration, Hurricane Tracking and Video Surveillance. Annals of Applied Statistics, 8, 530-552.  
&lt;br&gt;https://doi.org/10.1214/13-AOAS701</mixed-citation></ref><ref id="hanspub.43237-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Anirudh, R., Turaga, P., Su, J., et al. (2016) Elastic Functional Coding of Riemannian Trajectories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 922-936. &lt;br&gt;https://doi.org/10.1109/TPAMI.2016.2564409</mixed-citation></ref><ref id="hanspub.43237-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Amor, B.B., Su, J. and Srivastava, A. (2015) Action Recogni-tion Using Rate-Invariant Analysis of Skeletal Shape Trajectories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 1-13.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2015.2439257</mixed-citation></ref><ref id="hanspub.43237-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Wang, J., Liu, Z., Wu, Y., et al. (2012) Mining Actionlet En-semble for Action Recognition with Depth Cameras. 2012 IEEE Conference on Computer Vision and Pattern Recogni-tion, Providence, 16-21 June 2012, 1290-1297.</mixed-citation></ref><ref id="hanspub.43237-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Hall, B. (2015) Lie Groups, Lie Algebras, and Representations: An Elementary Introduction. Springer, Berlin.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-13467-3</mixed-citation></ref><ref id="hanspub.43237-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Murray, R.M., Li, Z., Sastry, S.S., et al. (1994) A Mathematical Introduction to Robotic Manipulation. CRC Press, Boca Raton.</mixed-citation></ref><ref id="hanspub.43237-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Zefran, M. and Kumar, V. (1998) Two Methods for Interpolating Rigid Body Motions. Proceedings 1998 IEEE International Conference on Robotics and Automation, Vol. 4, 2922-2927.</mixed-citation></ref><ref id="hanspub.43237-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Zefran, M., Kumar, V. and Croke, C. (1996) Choice of Riemannian Metrics for Rigid Body Kinematics. International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, V02BT02A030.</mixed-citation></ref><ref id="hanspub.43237-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Grove, K. and Karcher, H. (1973) How to Conjugate C1-Close Group Actions. Mathematische Zeitschrift, 132, 11-20.  
&lt;br&gt;https://doi.org/10.1007/BF01214029</mixed-citation></ref><ref id="hanspub.43237-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Xia, L., Chen, C.C. and Aggarwal, J.K. (2012) View Invariant Human Action Recognition Using Histograms of 3D Joints. 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, Providence, 16-21 June 2012, 20-27. &lt;br&gt;https://doi.org/10.1109/CVPRW.2012.6239233</mixed-citation></ref><ref id="hanspub.43237-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Li, W., Zhang, Z. and Liu, Z. (2010) Action Recognition Based on a Bag of 3d Points. 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, 13-18 June 2010, 9-14.  
&lt;br&gt;https://doi.org/10.1109/CVPRW.2010.5543273</mixed-citation></ref><ref id="hanspub.43237-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, Y., Chen, W. and Guo, G. (2013) Fusing Spatiotemporal Features and Joints for 3d Action Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, Portland, 23-28 June 2013, 486-491. &lt;br&gt;https://doi.org/10.1109/CVPRW.2013.78</mixed-citation></ref><ref id="hanspub.43237-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, S., Liu, X. and Xiao, J. (2017) On Geometric Features for Skeleton-Based Action Recognition Using Multilayer LSTM Networks. 2017 IEEE Winter Conference on Applications of Computer Vision, Santa Rosa, 24-31 March 2017, 148-157. &lt;br&gt;https://doi.org/10.1109/WACV.2017.24</mixed-citation></ref><ref id="hanspub.43237-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Tanfous, A.B., Drira, H. and Amor, B.B. (2019) Sparse Coding of Shape Trajectories for Facial Expression and Action Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42, 2594-2607.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2019.2932979</mixed-citation></ref></ref-list></back></article>