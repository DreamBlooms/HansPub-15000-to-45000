<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AIRR</journal-id><journal-title-group><journal-title>Artificial Intelligence and Robotics Research</journal-title></journal-title-group><issn pub-type="epub">2326-3415</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AIRR.2018.73013</article-id><article-id pub-id-type="publisher-id">AIRR-26402</article-id><article-categories><subj-group subj-group-type="heading"><subject>AIRR20180300000_59265380.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject><subject> 工程技术</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于单目视觉的高精度三维场景重建技术研究
  High Precision 3D Scene Reconstruction Based on Monocular Vision
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>金</surname><given-names>家梁</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>朱</surname><given-names>孟飞</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>姚</surname><given-names>拓中</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>宋</surname><given-names>加涛</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>宁波工程学院，浙江 宁波</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>27</day><month>06</month><year>2018</year></pub-date><volume>07</volume><issue>03</issue><fpage>112</fpage><lpage>121</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   近年来，随着计算机硬件的不断快速更新，计算机的处理能力也不断变强。同时场景三维模型的获取技术越来越成熟，我们获得场景的三维模型数据的方式更多也更加方便了。而目前在基于单目和双目的三维重建技术中，单目技术较双目操作简单、而且取材方面更有利于推向市场。本文主要讲述基于单相机的三维重建，然后通过基于累积图的快速NCC匹配的种子扩张算法来进行高精度的三维场景重建。本章对经典的NCC相似度量函数进行优化，以此减少计算时间。而种子像素扩张算法即先选择初始的种子像素，利用视差图进行窗口比较从而获得高置信度的种子像素，因此大大降低了视差图的误匹配点。试验表明，该方法能够得到高质量的三维场景模型。 In recent years, along with the rapid updating of computer hardware, the processing capability of computer is also increasing. At the same time, 3d scene reconstruction technique has become more and more mature and we can get 3d model data for scenarios more easily than ever before. Now, in the 3d reconstruction technology based on monocular and binocular, monocular technology is simpler to operate than binocular technology and more convenient to acquire materials and more favorable to the market. This paper focuses on monocular based 3D reconstruction, the algorithm is used to reconstruct the 3d scene with a fast NCC algorithm based on the cumulative diagram. This paper improves the classic NCC similarity measures to reduce the computation time. Seed pixel expansion algorithm is presented to choose the initial seed pixels, use parallax to make window comparisons to obtain high confidence seed pixels, therefore, the mismatches of the parallax figure are greatly reduced. Experiments show that the method can reconstruct precise and clear 3d scenarios. 
 
</p></abstract><kwd-group><kwd>累积图，单目视觉，视差图，三维场景重建，NCC匹配，种子像素, Cumulative Diagram</kwd><kwd> Monocular Vision</kwd><kwd> Parallax Figure</kwd><kwd> 3D Scene Reconstruction</kwd><kwd> NCC Algorithm</kwd><kwd> Seed Pixels</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于单目视觉的高精度三维场景重建 技术研究<sup> </sup></title><p>金家梁，朱孟飞，姚拓中<sup>*</sup>，宋加涛</p><p>宁波工程学院，浙江 宁波</p><disp-formula id="hanspub.26402-formula23"><graphic xlink:href="//html.hanspub.org/file/4-2610122x5_hanspub.png"  xlink:type="simple"/></disp-formula><p>收稿日期：2018年7月24日；录用日期：2018年8月7日；发布日期：2018年8月14日</p><disp-formula id="hanspub.26402-formula24"><graphic xlink:href="//html.hanspub.org/file/4-2610122x6_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>近年来，随着计算机硬件的不断快速更新，计算机的处理能力也不断变强。同时场景三维模型的获取技术越来越成熟，我们获得场景的三维模型数据的方式更多也更加方便了。而目前在基于单目和双目的三维重建技术中，单目技术较双目操作简单、而且取材方面更有利于推向市场。本文主要讲述基于单相机的三维重建，然后通过基于累积图的快速NCC匹配的种子扩张算法来进行高精度的三维场景重建。本章对经典的NCC相似度量函数进行优化，以此减少计算时间。而种子像素扩张算法即先选择初始的种子像素，利用视差图进行窗口比较从而获得高置信度的种子像素，因此大大降低了视差图的误匹配点。试验表明，该方法能够得到高质量的三维场景模型。</p><p>关键词 :累积图，单目视觉，视差图，三维场景重建，NCC匹配，种子像素</p><disp-formula id="hanspub.26402-formula25"><graphic xlink:href="//html.hanspub.org/file/4-2610122x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2018 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/4-2610122x8_hanspub.png" /> <img src="//html.hanspub.org/file/4-2610122x9_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>近几年来，随着计算机领域的各个方面的快速发展，人类的生产生活中处处可见计算机的影子，尤其是3D打印技术的发展促使三维重建技术快速发展，这使得三维重建技术逐渐成为计算机领域的一个研究热点。不同领域的研究人员都致力于研究解决如何快速且便捷地获取三维信息。三维信息也被广泛应用于不同的领域，比如，机器人领域的自动导航以及计算机视觉中的自动识别物体，这两者都需要获取目标物体的精准的三维数据；建筑学中需要通过高精度的三维重建来实现预知建成效果；考古学中需要通过三维重建来实现对文物的保护以及维修工作。来自多个领域的大量需求要求三维重建需要有较高的精度。</p><p>依据使用的传感器的不同可以将三维重建的方法进行分类。第一类利用主动距离传感器，比如激光扫描仪 [<xref ref-type="bibr" rid="hanspub.26402-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.26402-ref2">2</xref>] ，激光扫描仪通过激光扫描目标物体能够获得精准的三维信息从而构建出高精度的三维模型，但是该设备比较笨重而且价格昂贵，因而主要应用于科研领域。结构光法 [<xref ref-type="bibr" rid="hanspub.26402-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.26402-ref4">4</xref>] 也能够构建高精度三维模型，但是需要借助投影仪的帮忙，而且在室外高亮度环境下难以工作，从而使得其只能局限于室内环境的应用。另一类方法用到被动距离传感器——摄像机。近年来，立体视觉 [<xref ref-type="bibr" rid="hanspub.26402-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.26402-ref6">6</xref>] 被广泛地运用。不过该类方法一般需要固定相对位置且拍摄同步的两台摄像机来构成整个系统，同样使用较为复杂且价格不菲。相比之下研究仅利用一台手持式数码摄像机实现基于立体视觉技术的三维重建的方法，具有设备简易、使用方便以及成本低廉等长处，并且非常容易在其他领域得到推广应用。</p></sec><sec id="s4"><title>2. 相关工作</title><p>自多视觉几何被提出以来，基于视觉的三维重建发展快速。Pollefeys [<xref ref-type="bibr" rid="hanspub.26402-ref7">7</xref>] 等人的三维重建系统使用自带的单目摄像机绕着目标拍摄一系列的连续图像，通过处理这些图像来重建出三维模型，获取的图片越多重建出效果就越真实。但是该算法比较复杂，耗时较长。Snavely [<xref ref-type="bibr" rid="hanspub.26402-ref8">8</xref>] 等人提出了一种基于无序互联网图像序列的视觉重建方法，但是其并不能获得一个致密的三维重建结果。A Geiger [<xref ref-type="bibr" rid="hanspub.26402-ref9">9</xref>] 等人发明了一种基于双目立体视觉的实时三维重建方法，从而获得致密的三维重建结果，但是立体视觉系统结构比较复杂。中国科学院自动化研究所机器视觉课题组致力于视觉重建的发展和研究，他们利用多视角的高分辨率的建筑物图像就可以自动获取建筑物三维模型 [<xref ref-type="bibr" rid="hanspub.26402-ref10">10</xref>] ，并将该技术应用在建筑保护方面。微软研究院(Microsoft Research)在2013年推出了Kinect Fusion项目 [<xref ref-type="bibr" rid="hanspub.26402-ref11">11</xref>] ，Kinect Fusion利用Kinect传感器(深度相机)可以直接得到目标场景的深度图像数据，将传感器绕着目标物体或场景移动就可以获得不同角度下的多组深度图像数据并实时地构建场景三维模型。但是一开始传感器获取的数据会有很大的误差从而导致开头的重建模型很不精确。</p><p>本章针对现有三维重建算法的运算时间和精度两大研究热点提出了基于累积图和种子像素提取算法的高精度的三维重建系统。通过基于累积图快速NCC匹配算法来降低图像匹配的算法复杂度，从而大大降低了运算时间。利用种子像素提取算法获得的高置信度的种子像素，有效地减少了误配点，使得重建结果更加精准和真实。</p></sec><sec id="s5"><title>3. 场景高精度三维模型重建算法</title><sec id="s5_1"><title>3.1. 算法概述</title><p>首先，获取同一个目标的两张图片。然后，计算两幅图对应的特征点，利用这些特征点计算获取两幅图像位置关系。接着，利用位置关系通过图像校正使得极线和扫描线的位置一致。最后，用一种基于种子像素点扩张技术的立体匹配算法：随机选择任意一个像素点，视为初始种子像素，运用初始种子像素通过计算可获得两幅视差图，再从两幅视差图中各自选取两个尺寸相同的窗口，通过比较两个窗口内相同位置的像素值就可以获得高置信度的种子像素，我们对这个高置信度的种子像素进行扩张，可以大大降低视差错误区域出现的可能性。同时提出一种基于累积图的快速NCC匹配代价计算方法从而高效地获得两幅图像对应的视差图，这样我们能得到快速准确的视差图。最后利用视差图和摄像机参数信息就可以得到目标场景的高精度三维模型重建结果。算法流程见图1。</p></sec><sec id="s5_2"><title>3.2. 摄像机内参数标定</title><p>首先要获得单目摄像机的内参以及畸变系数，然后利用平面模板方法 [<xref ref-type="bibr" rid="hanspub.26402-ref12">12</xref>] 计算相机内参数矩阵和镜头非线性畸变系数即通过摄像机标定过程计算未知的摄像机内部参数矩阵K和非线性畸变系数k<sub>c</sub><sub>1</sub>、k<sub>c</sub><sub>2</sub>。该方法利用了位于同一高度上的特征点通过计算可以获得摄像机内参的二个约束等式这个方法。通过拍摄不同位置的同一块标定板即可实现。</p><p>图1. 算法流程框图</p></sec><sec id="s5_3"><title>3.3. 图像采集和特征匹配</title><p>首先用摄像机对目标区域拍摄一幅图像；然后要求摄像机向与光轴指向基本垂直的方向运动一段距离，然后再次拍摄目标区域。两次拍摄只要求都能看到目标区域。</p><p>由于两幅图像之间视角相差较大，需要使用SIFT特征匹配算法 [<xref ref-type="bibr" rid="hanspub.26402-ref13">13</xref>] ，对两幅图片进行特征匹配。然而一般的SIFT匹配比较粗糙，存在不少误配点，需要利用诸如RANSAC技术以减少错误点。特征匹配原图见图2。</p></sec><sec id="s5_4"><title>3.4. 图像间相对位置计算</title><p>通过计算图像对对应的SIFT特征点得到一个齐次坐标集合，结合摄像机内参数K，通过8点算法以及RANSAC技术 [<xref ref-type="bibr" rid="hanspub.26402-ref14">14</xref>] 可得到本质矩阵E。最终利用矩阵E可以获得旋转矩阵R和平移矢量T。</p><p>上述方法计算旋转和平移运动基于对代数误差的最小化，虽然速度较快但是缺乏几何意义。因此估计得到的运动参数精度不高，影响下一步的图像校正过程。所以需要对运动参数进行优化，这里可以采取非线性优化算法。本章使用的非线性优化算法基于极线约束关系，也就是说通过最小化SIFT特征点到其所在的极线的距离来获得优化的运动参数。</p></sec><sec id="s5_5"><title>3.5. 图像校正</title><p>每幅图像都有极线和扫浙描线，为了使这两者对齐需要进行图像校正。当图像中的某一个像素点与对应图像中对应的像素点在同一扫描行上时，就使得图像匹配的问题得到简化，将匹配的复杂度从二维搜索降低到一维搜索。利用已知的相机内参和图像间的位置关系，我们设置两幅图使其相互平行，最后分别投影成新的平面图像。</p><p>图3显示了原图中的两幅图像进行图像校正并且进行了SIFT特征匹配的结果。从匹配结果可以看出，</p><p>图2. 实验原图</p><p>图3. 校正后的两幅图像SIFT特征匹配的结果</p><p>左右两幅图片匹配上的特征点在同一高度上，因此可以做立体匹配。</p></sec><sec id="s5_6"><title>3.6. 立体匹配</title><p>为了获得两幅图中对应像素点之间的位移(通常称作视差)，我们需要在两幅图像的每一个对应的像素点之间建立一种关系，通过立体匹配可以得到这种关系。然而，基于窗口的局部匹配运算速度较快但是产生误匹配区较多，而基于优化预设能量函数的全局优化匹配算法得出的视差图质量较高但是算法复杂。为了能解决算法功能和复杂度之间的矛盾，为此提出了一种同时具备上述两种算法优点的方法----基于种子像素扩张的图像匹配算法。</p><p>首先在立体图对中寻找一些匹配可信度较高的种子像素点，然后从这些种子像素点出发利用视差梯度约束来确定周围像素点的视差值。因为该算法利用置信度较高的种子像素点的视差值来约束周围像素点的视差搜索范围，从而降低了图中平滑区域误匹率；同时算法利用视差梯度约束保证了最终视差图的平滑性。该算法不运用能量函数并且不进行全局优化以此保证运算复杂度较低；该算法在扩张过程中兼顾了像素周围四邻域视差值产生的影响，因而不会像动态规划算法那样产生扫描行之间视差值不统一。</p><p>其次本章还提出了一种基于累积图的NCC匹配方法，该算法的计算复杂度和匹配窗口大小无关，因此加快了运算速度。</p><sec id="s5_6_1"><title>3.6.1. 基于累积图的快速NCC匹配代价计算</title><p>基于累积图的快速NCC匹配在经典的NCC匹配算法的基础上去除了大量的不必要的计算，简化了匹配搜索过程从而实现快速匹配，大大降低了计算复杂度。</p><p>NCC匹配代价为：</p><p>C ( p , d ) = ∑ ( x , y ) ∈ W p ( I 1 ( x , y ) − I &#175; 1 ( p x , p y ) ) ⋅ ( I 2 ( x + d , y ) − I &#175; 2 ( p x + d , p y ) ) ∑ ( x , y ) ∈ W p ( I 1 ( x , y ) − I &#175; 1 ( p x , p y ) ) 2 ⋅ ∑ ( x , y ) ∈ W p ( I 2 ( x + d , y ) − I &#175; 2 ( p x + d , p y ) ) 2 (1)</p><p>其中W<sub>p</sub>为以像素 p = ( p x , p y ) 为中心的匹配窗口。 I &#175; 1 ( x , y ) 为I<sub>1</sub>中以 ( x , y ) 为中心的窗口中所有像素平均的亮度。 I &#175; 2 ( x + d , y ) 为I<sub>2</sub>中以 ( x + d , y ) 为中心的窗口中所有像素平均的亮度。若直接利用上述式子计算I<sub>1</sub>中每一个像素在每一个可能视差值处的匹配代价，则运算复杂度为O (WHD<sub>r</sub> |W<sub>p</sub>|)。定义视差搜索范围为D<sub>r</sub>且|W<sub>p</sub>|为匹配窗口内像素的个数。通过分析可以知道，上式的计算复杂度受匹配窗口大小的影响。所以当立体图像对分辨率高时，需要比较大的匹配窗口，如果直接利用上式计算NCC匹配代价会很慢。</p><p>优化后的计算方式如下：</p><p>令I<sub>k</sub>对应的累积图为ii<sub>k</sub>，该累积图可以通过 i i ( x , y ) = i ( x , y ) + i i ( x − 1 , y ) + i i ( x , y − 1 ) − i i ( x − 1 , y − 1 ) 快速计算。我们定义：</p><p>i i k ( W p ) = ∑ ( x , y ) ∈ W p I k ( x , y ) (2)</p><p>对于校正过后的立体图像对，计算方法有以下3步：</p><p>步骤1：根据立体图像对的分辨率设定匹配窗口W<sub>p</sub>。</p><p>步骤2：计算累积图ii<sub>1</sub>和ii<sub>2</sub>。我们计算：</p><p>I 22 ( x , y ) = I 2 ( x , y ) &#215; I 2 ( x , y ) (3)</p><p>利用上新产生的两幅图像计算其分别对应的累积图ii<sub>11</sub>和ii<sub>22</sub>。</p><p>步骤3：设最小和最大视差搜索级分别为D<sub>min</sub>和D<sub>max</sub>。在仅考虑整数级别的视差级的情况下，可得：</p><p>D r = D max − D min + 1 (4)</p><p>对于视差搜索范围内的每一个视差搜索级d，其满足 D min ≤ d ≤ D max ，我们计算：</p><p>I 12 ( x , y ) = I 1 ( x , y ) &#215; I 2 ( x + d , y ) (5)</p><p>然后计算其对应的累积图ii<sub>12</sub>。最后计算视差搜索级为d时像素点p处的NCC匹配代价：</p><p>C ( p , d ) = i i 12 ( W p ) − 1 | W p | ⋅ i i 1 ( W p ) ⋅ i i 2 ( W p + d ) [ i i 11 ( W p ) − 1 | W p | ⋅ i i 1 ( W p ) 2 ] ⋅ [ i i 22 ( W p + d ) − 1 | W p | ⋅ i i 2 ( W p + d ) 2 ] (6)</p><p>其中 W p + d 是以坐标 ( p x + d , p y ) 的像素作为为中心像素的匹配窗口。</p><p>分析该算法复杂度：在步骤2中可知每一幅累积图的运算复杂度均为O (WH)。根据I<sub>11</sub>和I<sub>22</sub>的定义可知计算其中每一幅图像的运算复杂度也为O (WH)。为了得到它们对应的累积图需要额外的O (WH)运算复杂度。所以总运算复杂度为：</p><p>O ( W H ) + O ( W H ) + ( O ( W H ) + O ( W H ) ) + ( O ( W H ) + O ( W H ) ) = O ( W H ) (7)</p><p>在步骤3中，当d固定时，计算I<sub>12</sub>的复杂度为O (WH)。利用I<sub>12</sub>，当d固定时，整幅图像的NCC匹配代价计算复杂度为O (WH)。因此对于某一固定的视差搜索级d，步骤3总的运算复杂度为 O ( W H ) + O ( W H ) = O ( W H ) 。由于需要遍历视差搜索范围中的每一个视差级，因此步骤3总的运算复杂度为O (WHD<sub>r</sub>)。本算法总的运算复杂度为步骤2和步骤3运算复杂度之和： O ( W H ) + O ( W H D r ) = O ( W H D r ) 在前文中我们提到直接用定义式计算NCC匹配代价运算复杂度为O (WHD<sub>r</sub> |W<sub>p</sub>|)。可见本算法的运算复杂度大大减小。</p></sec><sec id="s5_6_2"><title>3.6.2. 种子像素提取算法</title><p>种子像素提取算法优化了种子像素的视差值，降低了其误匹配点，从而使得三维重建更加精确。该算法包括三个步骤：</p><p>步骤1：初始种子像素。因为初始种子像素不影响最终的种子像素。所以初始种子像素的数量以及视差值对最终结果的影响较小。当 ( p x , p y , d i s p ) 满足下述下条件时认定为种子像素：</p><p>{ k &#215; C ( p x , p y , d i s p ) ≥ C ( p x , p y , d ) , ∀ d ≠ d i s p k &#215; C ( p x , p y , d i s p ) ≥ C ( p ′ x , p y , d ′ ) , p ′ x + d ′ = p x + d i s p , d ′ ≠ d i s p (8)</p><p>其中， k ∈ ( 0 , 1 ] ，由需要的种子像素点的多少和种子像素视差值的错误率决定。</p><p>步骤2：初始视差图生成。H为摄像机高度。 ( p x , p y , d i s p ) 为初始种子像素，可得两个集合，分别为：</p><p>S u = { ( p x , p y , d i s p ) | p y &lt; H 2 } (9)</p><p>S d = { ( p x , p y , d i s p ) | p y ≥ H 2 } (10)</p><p>利用S<sub>u</sub>中的种子像素通过视差曲面扩张算法获得视差图D<sub>u</sub>，同理得出视差图D<sub>d</sub>。通过这两幅视差图可以得出最后中种子像素。</p><p>步骤3：获取种子像素。定义视差图D<sub>u</sub>中位置 ( p x , p y ) 处的视差值为 D u ( p x , p y ) ，同理定义视差图D<sub>d</sub>的视差值。选择各个边长均为2w + 1的矩形为窗口。如果公式(11)，则 ( p x , p y , D u ( p x , p y ) ) 就是最终的种子像素：</p><p>∑ i = p x − w p x + w ∑ j = p y − w p y + w ( D u ( i , j ) − D d ( i , j ) ) 2 = 0 (11)</p><p>当获取到所有的种子像素后，需要进行基于视差置信度的扩张。其扩张算法如下：</p><p>定义 ( p x , p y ) 处像素视差值为d，置信度为 P ( p x , p y , d ) ，利用相似度衡量函数反映匹配点对的置信度，则取：</p><p>P ( p x , p y , d ) ∝ C ( p x , p y , d ) (12)</p><p>步骤1：创建像素的队列，对所有获取到的种子像素计算 P ( p x , p y , d ) 并按照P的值从大到小填入队列中。</p><p>步骤2：上述队列不为空时，对队列开始的第一个种子像素做扩张计算。对于该种子像素的四邻域像素 ( p ′ x , p ′ y ) ，若该像素的视差值尚不存在，则视差值为：</p><p>d ′ = arg d i s p max ( C ( p ′ x , p ′ y , d i s p ) ) d i s p ∈ { d − 1 , d , d + 1 } (13)</p><p>若该像素的视差值已经存在且为d<sub>1</sub>，则新的视差值为：</p><p>d ′ = arg d i s p max ( C ( p ′ x , p ′ y , d i s p ) ) d i s p ∈ { d − 1 , d , d + 1 , d 1 } (14)</p><p>如果队列为空则结束，输出视差图。</p><p>步骤3：若步骤2中没有生成新的种子像素时，删除队列开头的数据并再次做步骤2。否则计算每一个种子像素的 P ( p x , p y , d ) ，并根据其值的大小按从大到小的顺序插入到队列相应位置，删除队列头部种子像素再重复步骤2。</p></sec></sec></sec><sec id="s6"><title>4. 实验结果</title><p>本章使用的两组图像的分辨率为均716 &#215; 591。数据集来自KITTI (http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo)。实验所用的电脑配置为：英特尔i5-6300核心处理器，8G内存，64位操作系统。</p>累积图的NCC匹配代价计算复杂度分析<p>我们比较本章提出的算法和直接利用NCC的定义式计算匹配代价。所有的实验均对两种算法使用相同的立体图像对，图4显示了实验结果。</p><p>在第一个实验中，视察搜索范围是90个视差级。我们使匹配窗口大小在4 &#215; 4到24 &#215; 24之间进行变化且我们仅分析计算NCC匹配代价消耗的时间。从图4(a)可以看到本章的算法不随匹配窗口大小的变化而变化，基本保持在3秒以内。然而直接使用定义式的NCC匹配算法法耗时随着匹配窗口尺寸的增大而迅速成二次曲线增长。图4(a)的结果符合我们在3.6.1中的分析。上述结果对于任意的矩形匹配窗口都是正确的。</p><p>在第2个实验中我们将匹配窗口的尺寸固定在17 &#215; 17。让视差搜索范围在10到100个视差级之间变化，仅分析计算NCC匹配代价消耗的时间。从图4(b)可以看到两个算法的消耗时间都是随着视差级的增长而线性增加，但是本章算法的增长速度远慢于使用定义式的方法。</p><p>图4. NCC匹配代价计算方法运算时间比较。(a) 当匹配窗口大小不同时的运算时间。(b) 仅分析NCC匹配代价计算时的运算时间</p><p>通过上述的2个实验，可以验证3.6.1中的理论分析：本章提出的NCC匹配代价计算方法运算复杂度为O (WHDr)。该运算复杂度远低于使用定义式的NCC匹配代价计算方法。</p></sec><sec id="s7"><title>5. 重建结果</title><p>图5以及图6分别为两组室内场景的重建最终结果。</p><p>以上两组图片的第一行为原图，第二行第一幅为GRAPH CUT的左视差图结果，第二幅为本章算法立体匹配后的左视差图，第三行为重建结果。</p><p>从以上重建结果可以看出，利用GRAPH CUT得出的结果相对于本章提出的算法得出的结果来说图像的边缘部分以及细节部分像素值丢失严重(见图中红色方框标注处)，并且图像本身噪声较多，这会导致最后的重建结果不够精准。而运用本章算法得出的视差图像素点饱满而且图像本身较平滑。所以本章提出的重建系统可以获得精度较高的三维场景。</p><p>图5. 室内场景重建结果1</p><p>图6. 室内场景重建结果2</p></sec><sec id="s8"><title>6. 结论</title><p>本章我们提出了一种利用单台摄像机拍摄两幅图像从而获得场景高精度三维模型的算法。为简化算法的复杂度，本章使用差分求和定理对NCC相似度量函数进行了改进。利用种子像素提取算法自适应生成线状特征的直线模板，以此降低视差图的误配点。本章的方法对于获取两幅图像的方式要求不高，因此可以广泛用于多种场合。重建三维模型可以用于局部地形重建、考古学、无人机驾驶、机器人的自主导航等领域。所以，基于图像的三维重建既有理论意义也有广泛的应用价值。此外，本章提出一种基于种子像素扩张的立体匹配算法，降低了误匹配的概率。实验结果证明本章提出的方法能够适应多种不同的环境，产生真实的三维模型。</p></sec><sec id="s9"><title>基金项目</title><p>浙江省公益技术应用研究计划项目(2016C33255)；浙江省重点研发计划项目(2018C01086)，2017年度浙江省大学生科技创新活动计划（2017R424018）。</p></sec><sec id="s10"><title>文章引用</title><p>金家梁,朱孟飞,姚拓中,宋加涛. 基于单目视觉的高精度三维场景重建技术研究High Precision 3D Scene Reconstruction Based on Monocular Vision[J]. 人工智能与机器人研究, 2018, 07(03): 112-121. https://doi.org/10.12677/AIRR.2018.73013</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.26402-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Cyberware (2008) Cyberware Web Page. &lt;br&gt;http://www.cyberware.com</mixed-citation></ref><ref id="hanspub.26402-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Konica (2008) Konica Minolta 3-D Digitizer Web Page. &lt;br&gt;http://www.minolta3d.com</mixed-citation></ref><ref id="hanspub.26402-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Scharstein, D. and Szeliski, R. (2003) High-Accuracy Stereo Depth Maps Using Structured Light. 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Madison, WI, 18-20 June 2003, 195-202.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2003.1211354</mixed-citation></ref><ref id="hanspub.26402-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Neugebauer, P. and Klein, K. (1999) Texturing 3D Models of Real World Objects from Multiple Unregistered Photographic Views. Eurographics, 18, 245-256.</mixed-citation></ref><ref id="hanspub.26402-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Scharstein, D. and Szeliski, R. (2002) A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms. International Journal of Computer Vision, 47, 7-42. &lt;br&gt;https://doi.org/10.1023/A:1014573219977</mixed-citation></ref><ref id="hanspub.26402-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Ogale, A. and Aloimonos, Y. (2007) A Roadmap to the Integration of Early Visual Modules. International Journal of Computer Vision, 72, 9-25. &lt;br&gt;https://doi.org/10.1007/s11263-006-8890-9</mixed-citation></ref><ref id="hanspub.26402-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Pollefeys, M., Van Gool, L. and Vergauwen, M. (2004) Visual Modeling with a Hand-Held Camera. International Journal of Computer Vision, 59, 207-232. &lt;br&gt;https://doi.org/10.1023/B:VISI.0000025798.50602.3a</mixed-citation></ref><ref id="hanspub.26402-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Snavely, N., Seitz, S.M. and Szeliski, R. (2008) Modeling the World from Internet Photo Collections. International Journal of Computer Vision, 80, 189-210. &lt;br&gt;https://doi.org/10.1007/s11263-007-0107-3</mixed-citation></ref><ref id="hanspub.26402-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Geiger, A., Ziegler, J. and Stiller, C. (2011) StereoScan: Dense 3d Reconstruc-tion in Real-Time. Intelligent Vehicles Symposium, Baden-Baden, 5-9 June 2011, 963-968. &lt;br&gt;https://doi.org/10.1109/IVS.2011.5940405</mixed-citation></ref><ref id="hanspub.26402-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Shen, S.H. (2013) Accurate Multiple View 3D Reconstruction Using Patch-Based Stereo for Large-Scale Scenes. IEEE Transactions on Image Processing, 22, 1901-1914. &lt;br&gt;https://doi.org/10.1109/TIP.2013.2237921</mixed-citation></ref><ref id="hanspub.26402-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Newcombe, R.A., Izadi, S. and Hilliges, O. (2013) Kinect Fusion: Real-Time Dense Surface Mapping and Tracking. Mixed and Augmented Reality, 233, 430-436.</mixed-citation></ref><ref id="hanspub.26402-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Z. (2000) A Flexible New Technique for Camera Calibration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22, 1330-1334. &lt;br&gt;https://doi.org/10.1109/34.888718</mixed-citation></ref><ref id="hanspub.26402-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Lowe, D. (2004) Distinctive Image Features from Scale-Invariant Key points. Interna-tional Journal of Computer Vision, 60, 91-110. &lt;br&gt;https://doi.org/10.1023/B:VISI.0000029664.99615.94</mixed-citation></ref><ref id="hanspub.26402-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Fischler, M. and Bolles, R. (1981) Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Communications, 24, 381-395. &lt;br&gt;https://doi.org/10.1145/358669.358692</mixed-citation></ref></ref-list></back></article>