<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2021.108286</article-id><article-id pub-id-type="publisher-id">AAM-44453</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20210800000_30083752.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  多元线性回归的中心化和标准化实验结果比较
  Comparison of Centralized and Standardized Experimental Results of Multiple Linear Regression
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>艳玲</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>华北电力大学，北京</addr-line></aff><pub-date pub-type="epub"><day>03</day><month>08</month><year>2021</year></pub-date><volume>10</volume><issue>08</issue><fpage>2748</fpage><lpage>2758</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  在讨论一元线性回归模型的时候，我们可以看出，对数据进行中心化处理后，推导计算过程会简化许多。由此想到，对于多元线性回归模型，能否也对数据进行中心化处理，或者进一步的标准化处理，以期简化计算？实际上，经过中心化和标准化处理，可得到均值为0，标准差为1的数据，从而在进行多元线性回归拟合时消除了因量纲不同或数值差异较大而引起的误差。
   When discussing the unary linear regression model, we can see that the derivation and calculation process will be much simplified after centralized data processing. Therefore, for the multiple linear regression model, can the data also be processed centrally or further standardized to simplify the calculation? In fact, data with a mean value of 0 and a standard deviation of 1 can be obtained after centralized and standardized processing, so that errors caused by different dimensions or large numerical differences can be eliminated when performing multiple linear regression fitting.
 
</p></abstract><kwd-group><kwd>多元线性回归，中心化，标准化，数据处理, Multiple Linear Regression</kwd><kwd> Centralization</kwd><kwd> Standardization</kwd><kwd> Data Processing</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>在讨论一元线性回归模型的时候，我们可以看出，对数据进行中心化处理后，推导计算过程会简化许多。由此想到，对于多元线性回归模型，能否也对数据进行中心化处理，或者进一步的标准化处理，以期简化计算？实际上，经过中心化和标准化处理，可得到均值为0，标准差为1的数据，从而在进行多元线性回归拟合时消除了因量纲不同或数值差异较大而引起的误差。</p></sec><sec id="s2"><title>关键词</title><p>多元线性回归，中心化，标准化，数据处理</p></sec><sec id="s3"><title>Comparison of Centralized and Standardized Experimental Results of Multiple Linear Regression<sup> </sup></title><p>Yanling Zhang</p><p>North China Electric Power University, Beijing</p><p><img src="//html.hanspub.org/file/14-2621757x4_hanspub.png?20210809175040519" /></p><p>Received: Jul. 4<sup>th</sup>, 2021; accepted: Jul. 23<sup>rd</sup>, 2021; published: Aug. 9<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/14-2621757x5_hanspub.png?20210809175040519" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>When discussing the unary linear regression model, we can see that the derivation and calculation process will be much simplified after centralized data processing. Therefore, for the multiple linear regression model, can the data also be processed centrally or further standardized to simplify the calculation? In fact, data with a mean value of 0 and a standard deviation of 1 can be obtained after centralized and standardized processing, so that errors caused by different dimensions or large numerical differences can be eliminated when performing multiple linear regression fitting.</p><p>Keywords:Multiple Linear Regression, Centralization, Standardization, Data Processing</p><disp-formula id="hanspub.44453-formula39"><graphic xlink:href="//html.hanspub.org/file/14-2621757x6_hanspub.png?20210809175040519"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/14-2621757x7_hanspub.png?20210809175040519" /> <img src="//html.hanspub.org/file/14-2621757x8_hanspub.png?20210809175040519" /></p></sec><sec id="s5"><title>1. 实验方法与步骤</title><p>首先在一元线性模型的基础上，推导多元线性模型相关系数的求解公式；之后利用产生的随机数分别按不做处理、做中心化处理、做标准化处理计算相应的未知参数；最后通过比较计算得到的未知参数，分析中心化和标准化处理的好处。</p></sec><sec id="s6"><title>2. 实验过程</title><sec id="s6_1"><title>2.1. 公式推导</title><sec id="s6_1_1"><title>2.1.1. 一元线性回归未中心 [<xref ref-type="bibr" rid="hanspub.44453-ref1">1</xref>]</title><p>对 y = β 0 + β 1 x 1 + ε</p><p>{ y 1 = β 0 + β 1 x 1 + ε 1                             ⋮ y n = β 0 + β 1 x n + ε n</p><p>记 y = ( y 1 ⋮ y n ) , X = ( 1 x 1 ⋮ ⋮ 1 x n ) , ε = ( ε 1 ⋮ ε n ) ，</p><p>则有 L = X ′ X = ( n n x &#175; n x &#175; ∑ t = 1 n x t 2 ) 。</p><p>L − 1 = 1 n ∑ t = 1 n ( x t − x &#175; ) 2 ( ∑ t = 1 n x t 2 n x &#175; − n x &#175; n )</p><p>其中， x &#175; = 1 n ∑ t = 1 n x t ， y &#175; = 1 n ∑ t = 1 n y t 。</p><p>β ^ = L − 1 X ′ y = 1 n ∑ t = 1 n ( x t − x ) 2 ( n y &#175; ∑ t = 1 n x t 2 − n x &#175; ∑ t = 1 n x t y t n ∑ t = 1 n x t y t − n 2 x &#175; y &#175; )</p><p>σ 2 = 1 n ( y − X β ^ ) ′ ( y − X β ^ ) = s 2 2 − β ^ 1 2 s 1 2</p><p>其中 s 1 2 = 1 n ∑ t = 1 n ( x t − x &#175; ) 2 , s 2 2 = 1 n ∑ t = 1 n ( y t − y &#175; ) 2 。</p><p>则 y ^ = β ^ 0 + β ^ 1 x 。</p></sec><sec id="s6_1_2"><title>2.1.2. 一元线性回归的中心化</title><p>对 y = β 0 + β 1 ( x − x &#175; ) + ε</p><p>即 { y 1 = β 0 + β 1 ( x − x &#175; ) + ε 1                                   ⋮ y n = β 0 + β 1 ( x n − x &#175; ) + ε n</p><p>其中 x &#175; = 1 n ∑ t = 1 n x t 。</p><p>对数据 x 1 , x 2 , ⋯ , x n 作中心化处理，利用新的n组数据 ( y t , x t − x ) ， t = 1 , 2 , ⋯ , n 。</p><p>建立线性回归方程 y ^ = β ^ 0 + β ^ 1 ( x − x &#175; ) 。</p><p>记 y = ( y 1 ⋮ y n ) , X = ( 1 x 1 − x &#175; ⋮ ⋮ 1 x n − x &#175; )</p><p>L = X ′ X = ( n 0 0 ∑ t = 1 n ( x t − x &#175; ) 2 )</p><p>L − 1 = 1 n ∑ t = 1 n ( x t − x &#175; ) 2 ( ∑ t = 1 n ( x t − x &#175; ) 2 0 0 n )</p><p>记 s 1 2 = 1 n ∑ t = 1 n ( x t − x &#175; ) 2 , s 2 2 = 1 n ∑ t = 1 n ( y t − y &#175; ) 2</p><p>β ^ = L − 1 X ′ y = 1 n ∑ t = 1 n ( x t − x ) 2 ( n y &#175; ∑ t = 1 n ( x t − x &#175; ) 2 n ∑ t = 1 n ( x t − x &#175; ) y t ) = 1 n s 1 2 ( n s 1 2 y n s 12 )</p><p>σ 2 = 1 n ( y − X β ^ ) ′ ( y − X β ^ ) = s 2 2 − β ^ 1 2 s 1 2</p><p>记 R = s 12 s 1 ⋅ s 2 ，</p><p>则有 β ^ 1 = s 12 s 1 2 = R s 2 s 1 。</p><p>σ ^ 2 = s 2 2 − ( R s 2 s 1 ) 2 s 1 2 = ( 1 − R 2 ) s 2 2</p></sec><sec id="s6_1_3"><title>2.1.3. 多元线性回归的中心化</title><p>样本数据的中心化公式：</p><p>x ˙ i t = x i t − x &#175; i ( i = 1 , 2 , ⋯ , k ; t = 1 , 2 , ⋯ , n ) ,   y ˙ t = y t − y &#175; ( t = 1 , 2 , ⋯ , n )</p><p>其中： x &#175; i = 1 n ∑ t = 1 n x i t ,   y &#175; = 1 n ∑ t = 1 n y t</p><p>∴ Y ˙ = X ˙ B + e</p><p>其中</p><p>Y ˙ = ( y ˙ 1 y ˙ 2 ⋮ y ˙ n ) ,   X ˙ = ( x ˙ 11 x ˙ 21 x ˙ 12 x ˙ 22 ⋯ x ˙ k 1 ⋯ x ˙ k 2 ⋮ ⋮ x ˙ 1 n x ˙ 2 n ⋱ ⋮ ⋯ x ˙ k n ) ,   B = ( b 1 b 2 ⋮ b k ) ,   e = ( e 1 e 2 ⋮ e n )</p><p>用最小二乘原理求出参数B的估计量 B ^ ：</p><p>根据最小二乘原理，需寻找一组参数估计值 B ^ ，使残差平方和 e ′ e = ( Y ˙ − X ˙ B ^ ) ′ ( Y ˙ − X ˙ B ^ ) = Y ˙ ′ Y ˙ − 2 Y ˙ ′ X ˙ B ^ + B ^ ′ X ˙ ′ X ˙ B ^ 最小。</p><p>于是参数的最小二乘估计值为</p><p>B ^ = ( X ˙ ' X ˙ ) − 1 X ˙ ′ Y ˙</p><p>中心化回归模型只包含k个参数估计值 b ^ 1 , b ^ 2 , ⋯ , b ^ k 。</p><p>对于(10)：</p><p>X ˙ ′ X ˙ = ( x ˙ 11 x ˙ 12 x ˙ 21 x ˙ 22 ⋯ x ˙ 1 n ⋯ x ˙ 2 n ⋮ ⋮ x ˙ k 1 x ˙ k 2 ⋱ ⋮ ⋯ x ˙ k n ) ( x ˙ 11 x ˙ 21 x ˙ 12 x ˙ 22 ⋯ x ˙ k 1 ⋯ x ˙ k 2 ⋮ ⋮ x ˙ 1 n x ˙ 2 n ⋱ ⋮ ⋯ x ˙ k n ) = ( ∑ t = 1 n x ˙ 1 t ∑ t = 1 n x ˙ 1 t x ˙ 2 t ⋯ ∑ t = 1 n x ˙ 1 t x ˙ k t ∑ t = 1 n x ˙ 2 t x ˙ 1 t ∑ t = 1 n x ˙ 2 t ⋯ ∑ t = 1 n x ˙ 2 t x ˙ k t ⋮ ⋮ ⋱ ⋮ ∑ t = 1 n x ˙ k t x ˙ 1 t ∑ t = 1 n x ˙ k t x ˙ 2 t ⋯ ∑ t = 1 n x ˙ k t )</p><p>X ˙ ′ Y ˙ = ( x ˙ 11 x ˙ 12 x ˙ 21 x ˙ 22 ⋯ x ˙ 1 n ⋯ x ˙ 2 n ⋮ ⋮ x ˙ k 1 x ˙ k 2 ⋱ ⋮ ⋯ x ˙ k n ) ( y ˙ 1 y ˙ 2 ⋮ y ˙ n ) = ( ∑ t = 1 n x ˙ 1 t y ˙ t ∑ t = 1 n x ˙ 2 t y ˙ t ⋮ ∑ t = 1 n x ˙ k t y ˙ t )</p></sec><sec id="s6_1_4"><title>2.1.4. 多元线性回归的标准化</title><p>样本数据的标准化公式：</p><p>x i t ∗ = x i t − x &#175; i ∑ ​ ( x i t − x &#175; i ) 2 / ( n − 1 ) = x ˙ i t ∑ ​     x ˙ i t 2 / ( n − 1 ) = x ˙ i t σ x i   ( i = 1 , 2 , ⋯ , k )</p><p>y t ∗ = y t − y &#175; ∑ ​ ( y t − y &#175; ) 2 / ( n − 1 ) = y ˙ t ∑ ​ ( y t − y &#175; ) 2 / ( n − 1 ) = y ˙ t σ y   ( t = 1 , 2 , ⋯ , n )</p><p>同中心化类似，用最小二乘方法，求出标准化的样本数据 ( y ˜ t , x ˜ 1 t , x ˜ 2 t , ⋯ , x ˜ k t ) 的经验回归方程，记为 y ^ t ∗ = b ^ 1 ∗ x 1 t ∗ + b ^ 2 ∗ x 2 t ∗ + ⋯ + b ^ k ∗ x k t ∗ 。</p><p>其中： b ^ 1 ∗ , b ^ 2 ∗ , ⋯ , b ^ k ∗ 为y对自变量 x 1 , x 2 , ⋯ , x k 的标准化回归系数，标准化包括了中心化。</p><p>标准化回归系数与最小二乘回归系数之间存在关系式 b ^ i ∗ = σ x i σ y b ^ i   ( i = 1 , 2 , ⋯ , k ) 。</p><p>其中： σ x i , σ y 为 x i , y 的样本标准差，普通最小二乘估计 b ^ i (或中心化回归系数)表示在其他变量不变的情况下，自变量 x i 的每单位的绝对变化引起的因变量均值的绝对变化量。</p><p>标准化回归系数 b ^ t ∗ 表示自变量 x i 的1%相对变化(相对于标准差)引起的因变量均值的相对变化百分数(相对于标准差)。</p><p>由样本观测值 x i 1 , x i 2 , ⋯ , x i k   ( i = 1 , 2 , ⋯ , n ) ，分别计算 x i 与 x j 的简单相关系数 r i j ，得自变量样本相关系</p><p>数矩阵 R = ( 1 r 12 ⋯ r 1 k r 21 1 ⋯ r 2 k ⋮ ⋮ ⋱ ⋮ r k 1 r k 2 ⋯ 1 ) 。</p><p>若记： X ∗ = ( x i j ∗ ) n &#215; k ，表示标准化的设计阵，则相关系数矩阵可以表示为</p><p>R = ( X ∗ ) ′ X ∗ = ( 1 r 12 ⋯ r 1 k r 21 1 ⋯ r 2 k ⋮ ⋮ ⋱ ⋮ r k 1 r k 2 ⋯ 1 )</p><p>相关系数矩阵R是对称矩阵，若 X ∗ 满秩，则R为对称正定矩阵。</p></sec></sec><sec id="s6_2"><title>2.2. 实验数据运行</title><sec id="s6_2_1"><title>2.2.1. 一元线性模型 不做处理</title><p>matlab程序：</p><p>x_1=[143 144 145 147 148 150 153 154 155 156 157 158 159 160 161 162]';</p><p>X=[ones(16,1),x_1];</p><p>Y=[87 85 88 91 92 90 93 95 98 98 97 95 97 99 100 102]';</p><p>[b,bint,r,rint,stats]=regress(Y,X)</p><p>t=1:16;</p><p>figure(1);</p><p>y_fitting=X(t,:)*b;</p><p>plot(t,y_fitting,'r-',t,Y(t,:),'b-',t,abs(y_fitting-Y(t,:)),'k-');</p><p>legend('红--拟合值','蓝--实际值','黑--误差值');</p><p>text(8,50,strcat('相关系数R=',num2str(stats(1,1))));</p><p>text(8,40,strcat('F=',num2str(stats(1,2))));</p><p>text(8,30,strcat('P=',num2str(stats(1,3),'%f')));</p><p>nhfcsl=strcat('拟合方程式Y1=',num2str(b(1,1)),'+',num2str(b(2,1)),'*x1');</p><p>text(8,20,nhfcsl);</p><p>title('线性回归方程拟合结果');</p><p>xlabel('样本点');ylabel('y');</p><p>figure(2);</p><p>u1=rint(:,1);</p><p>I1=rint(:,2);</p><p>plot(t,I1,'b-',t,r,'R*',t,u1,'g-');</p><p>legend('蓝--残差95%置信区间的上限','红--残差值','绿--残差95%置信区间下限');</p><p>xlabel('样本点');ylabel('残差值');</p><p>运行结果如图1和图2：</p><p>stats = 0.9047132.8768 0.0000 2.5357</p><p>图1. 一元线性回归方程拟合结果</p><p>图2. 样本残差图</p><p>由以上运行结果可得到：</p><p>参数的估计： b ^ 0 = − 20.7500 , b ^ 1 = 0.7500</p><p>∴ 回归方程为： y ^ = − 20.7500 + 0.7500 x 1 (1)</p><p>b 0 的区间估计： ( − 42.1526 , 0.6526 )</p><p>b 1 的区间估计： ( 0.6105 , 0.8895 ) 。</p><p>拟合优度(回归平方和和总离差平方和的比值) R 2 = 0.9047 ，表示回归值对观测值的拟合程度，值越接近1，说明回归直线对观测值的拟合程度越好。</p><p>F值(方差检验量) F = 132.8768 ，是整个模型的整体检验，值越大，说明回归方程越显著。</p><p>p值 p = 0.000000 ，其值小于0.05或0.01时说明系数通过检验。</p><p>结论：将残差的置信上下限和实际残差值绘制出来后可看到，残差值都在区间内，回归模型正常；将实际数据值和拟合值分别绘制成折线图之后可看到，两条曲线非常接近，可从直观上说明拟合程度较好。从数值上，拟合优度接近1，方差检验量也较大，p值也说明系数通过了检验。故上述回归方程拟合较好。</p></sec><sec id="s6_2_2"><title>2.2.2. 一元线性模型 做中心化处理(减均值)</title><p>x_1=[143 144 145 147 148 150 153 154 155 156 157 158 159 160 161 162]';</p><p>a_1=mean(x_1')</p><p>X=[ones(16,1),x_1-a_1];</p><p>Y=[87 85 88 91 92 90 93 95 98 98 97 95 97 99 100 102]';</p><p>b_1=mean(Y')</p><p>Y=Y-b_1;</p><p>[b,bint,r,rint,stats]=regress(Y,X)</p><p>t=1:16;</p><p>类似地，可得到：</p><p>参数的估计： b ^ 0 = 0.0000 , b ^ 1 = 0.7500</p><p>∴ 回归方程为： y ˙ ^ = 0.7500 x ˙ 1</p><p>又 x &#175; 1 = 153.2500 , y &#175; = 94.1875</p><p>代回原始数据得： y ^ − y &#175; = 0.7500 ( x 1 − x &#175; )</p><p>即</p><p>y ^ = − 20.7500 + 0.7500 x 1 (2)</p><p>b 0 的区间估计： ( − 0.8538 , 0.8538 )</p><p>b 1 的区间估计： ( 0.6105 , 0.8895 )</p><p>拟合优度 R 2 = 0.9047 方差检验量 F = 132.8768 p值 p = 0.0000 。</p></sec><sec id="s6_2_3"><title>2.2.3. 一元线性模型 做标准化处理(减均值再除以标准差)</title><p>x_1=[143 144 145 147 148 150 153 154 155 156 157 158 159 160 161 162]';</p><p>a_1=mean(x_1')</p><p>s_x=std(x_1)</p><p>X=[ones(16,1),(x_1-a_1)/s_x];</p><p>Y=[87 85 88 91 92 90 93 95 98 98 97 95 97 99 100 102]';</p><p>b_1=mean(Y')</p><p>s_y=std(Y')</p><p>Y=(Y-b_1)/s_y;</p><p>[b,bint,r,rint,stats]=regress(Y,X)</p><p>t=1:16;</p><p>参数的估计： b ^ 0 = 0.0000 , b ^ 1 = 0.9511</p><p>∴ 回归方程为： y ˙ ^ = 0.9511 x 1 *</p><p>又 x &#175; 1 = 153.2500 , y &#175; = 94.1875 , s x = 6.3193 , s y = 4.9829</p><p>代回原始数据得： y ^ − y &#175; s y = 0.9511 ( x 1 − x &#175; ) s x</p><p>即</p><p>y ^ = − 20.7500 + 0.7500 x 1 (3)</p><p>b 0 的区间估计： ( − 0.1714 , 0.1714 ) 。</p><p>b 1 的区间估计： ( 0.7742 , 1.1281 ) 。</p><p>拟合优度 R 2 = 0.9047 方差检验量 F = 132.8768 p值 p = 0.0000 。</p></sec><sec id="s6_2_4"><title>2.2.4. 多元线性模型 不做处理</title><p>matlab程序：</p><p>x_1=[143 144 145 147 148 150 153 154 155 156 157 158 159 160 161 162]';</p><p>x_2=unifrnd(2,4,16,1);</p><p>x_3=rand(16,1);</p><p>X=[ones(16,1),x_1,x_2,x_3*10];</p><p>Y=[87 85 88 91 92 90 93 95 98 98 97 95 97 99 100 102]';</p><p>[b,bint,r,rint,stats]=regress(Y,X)</p><p>t=1:16;</p><p>figure(1);</p><p>y_fitting=X(t,:)*b;</p><p>plot(t,y_fitting,'r-',t,Y(t,:),'b-',t,abs(y_fitting-Y(t,:)),'k-');</p><p>legend('红--拟合值','蓝--实际值','黑--误差值');</p><p>text(2,50,strcat('相关系数R=',num2str(stats(1,1))));</p><p>text(2,50,strcat('F=',num2str(stats(1,2))));</p><p>text(2,50,strcat('P=',num2str(stats(1,3),'%f')));</p><p>nhfcsl=strcat('拟合方程式Y1=',num2str(b(1,1)),'+',num2str(b(2,1)),'*x1','+',num2str(b(3,1)),'*x2','+', num2str(b(4,1)),'*x3');</p><p>text(2,50,nhfcsl);</p><p>title('线性回归方程拟合结果');</p><p>xlabel('样本点');ylabel('y');</p><p>figure(2);</p><p>u1=rint(:,1);</p><p>I1=rint(:,2);</p><p>plot(t,I1,'b-',t,r,'R*',t,u1,'g-');</p><p>legend('蓝--残差95%置信区间的上限','红--残差值','绿--残差95%置信区间下限');</p><p>xlabel('样本点');ylabel('残差值');</p><p>运行结果如图3：</p><p>stats = 0.9069 38.9804 0.0000 2.8884</p><p>图3. 多元线性回归方程拟合结果</p><p>由以上运行结果可得到：</p><p>参数的估计： b ^ 0 = − 23.2385 , b ^ 1 = 0.7629 , b ^ 2 = 0.0390 , b ^ 3 = 0.0781</p><p>∴ y ^ = − 23.3285 + 0.7629 x 1 + 0.0390 x 2 + 0.0781 x 3 (4)</p><p>b 0 的区间估计： ( − 48.7709 , 2.1139 ) 。</p><p>b 1 的区间估计： ( 0.6014 , 0.9244 ) 。</p><p>b 2 的区间估计： ( − 1.4414 , 1.5195 ) 。</p><p>b 3 的区间估计： ( − 0.2778 , 0.4340 ) 。</p><p>拟合优度(回归平方和和总离差平方和的比值) R 2 = 0.9069 ，表示回归值对观测值的拟合程度，值越接近1，说明回归直线对观测值的拟合程度越好。</p><p>F值(方差检验量) F = 38.9804 ，是整个模型的整体检验，值越大，说明回归方程越显著。</p><p>p值 p = 0.000002 ，其值小于0.05或0.01时说明系数通过检验。</p><p>结论：将残差的置信上下限和实际残差值绘制出来后可看到，残差值都在区间内，回归模型正常；将实际数据值和拟合值分别绘制成折线图之后可看到，两条曲线非常接近，可从直观上说明拟合程度较好。从数值上，拟合优度接近1，方差检验量也较大，p值也说明系数通过了检验。故上述回归方程拟合较好。</p></sec><sec id="s6_2_5"><title>2.2.5. 多元线性模型 做中心化处理(减均值)</title><p>x_1=[143 144 145 147 148 150 153 154 155 156 157 158 159 160 161 162]';</p><p>x_2=unifrnd(2,4,16,1);</p><p>x_3=rand(16,1)*10;</p><p>a_1=mean(x_1')</p><p>a_2=mean(x_2')</p><p>a_3=mean(x_3')</p><p>X=[ones(16,1),x_1-a_1,x_2-a_2,x_3-a_3];</p><p>Y=[87 85 88 91 92 90 93 95 98 98 97 95 97 99 100 102]';</p><p>b_1=mean(Y')</p><p>Y=Y-b_1;</p><p>[b,bint,r,rint,stats]=regress(Y,X)</p><p>t=1:16;</p><p>类似地，可得到：</p><p>参数的估计： b ^ 0 = 0.0000 , b ^ 1 = 0.7629 , b ^ 2 = 0.0390 , b ^ 3 = 0.0781</p><p>∴ y ^ = 0.7629 x ˙ 1 + 0.0390 x ˙ 2 + 0.0781 x ˙ 3</p><p>又 x &#175; 1 = 153.2500 , x &#175; 2 = 3.2189 , x &#175; 3 = 6.0626 , y &#175; = 94.1875</p><p>代回原数据： y ^ − y &#175; = 0.7629 ( x 1 − x &#175; 1 ) + 0.0390 ( x 2 − x &#175; 2 ) + 0.0781 ( x 3 − x &#175; 3 )</p><p>即</p><p>y ^ = − 23.3260 + 0.7629 x 1 + 0.0390 x 2 + 0.0781 x 3 (5)</p><p>b 0 的区间估计： ( − 0.9257 , 0.9257 ) 。</p><p>b 1 的区间估计： ( 0.6014 , 0.9244 ) 。</p><p>b 2 的区间估计： ( − 1.4414 , 1.5195 ) 。</p><p>b 3 的区间估计： ( − 0.2778 , 0.4340 ) 。</p><p>拟合优度 R 2 = 0.9069 ，方差检验量 F = 38.9804 ，p值 p = 0.0000 。</p></sec><sec id="s6_2_6"><title>2.2.6. 多元线性模型 做标准化处理(减均值再除以标准差)</title><p>x_1=[143 144 145 147 148 150 153 154 155 156 157 158 159 160 161 162]';</p><p>x_2=unifrnd(2,4,16,1);</p><p>x_3=rand(16,1)*10;</p><p>a_1=mean(x_1')</p><p>a_2=mean(x_2')</p><p>a_3=mean(x_3')</p><p>s_1=std(x_1)</p><p>s_2=std(x_2)</p><p>s_3=std(x_3)</p><p>X=[ones(16,1),(x_1-a_1)/s_1,(x_2-a_2)/s_2,(x_3-a_3)/s_3];</p><p>Y=[87 85 88 91 92 90 93 95 98 98 97 95 97 99 100 102]';</p><p>b_1=mean(Y')</p><p>s_y=std(Y)</p><p>Y=(Y-b_1)/s_y;</p><p>[b,bint,r,rint,stats]=regress(Y,X)</p><p>t=1:16;</p><p>参数的估计： b ^ 0 = 0.0000 , b ^ 1 = 0.9827 , b ^ 2 = − 0.1857 , b ^ 3 = 0.0260</p><p>∴ y ^ = 4.7335 x 1 * − 0.1463 x 2 * + 0.1246 x 3 *</p><p>又 x &#175; 1 = 153.2500 , x &#175; 2 = 2.8550 , x &#175; 3 = 5.1213 , y &#175; = 94.1875</p><p>s 1 = 6.3193 , s 2 = 0.4895 , s 3 = 3.6571 , s y = 4.9829</p><p>代回原数据： y ^ − y &#175; s y = 0.7629 ( x 1 − x &#175; 1 ) s 1 + 0.0390 ( x 2 − x &#175; 2 ) s 2 + 0.0781 ( x 3 − x &#175; 3 ) s 3</p><p>即</p><p>y ^ = 0.3140 + 0.6016 x 1 + 0.3970 x 2 + 0.1064 x 3 (6)</p><p>b 0 的区间估计： ( − 0.1580 , 0.1580 ) 。</p><p>b 1 的区间估计： ( 0.8137 , 1.1517 ) 。</p><p>b 2 的区间估计： ( − 0.4060 , 0.0345 ) 。</p><p>b 3 的区间估计： ( − 0.1916 , 0.2435 ) 。</p><p>拟合优度 R 2 = 0.9327 方差检验量 F = 55.3995 p值 p = 0.0000 。</p></sec></sec></sec><sec id="s7"><title>3. 实验结论</title><p>对一元线性回归模型，由以上的数据模拟可看出，不做处理，做中心化处理和做标准化处理均没有改变 R 2 , F 的值；未代回原始数据之前，和不做处理相比，中心化处理后的线性回归方程少了常数项，相当于是做了坐标轴的平移，标准化处理后的线性回归方程不仅少了常数项，系数也发生了改变，相当于改变了坐标的分度值；但代回原始数据之后，三个线性方程相同。</p><p>对多元线性回归，可以看出，做了中心化处理之后，得到的方程不含常数项，其他系数和 R 2 , F 值均相同，且代回原始数据之后得到和不做处理相同的回归方程；做了标准化处理后，各系数的值均有不同程度的变化，由于有随机数的参与，此时无法准确比较，但观察数据，不做任何处理时计算出来的结果会由于值的大小，比如第一行数据和第二行数据相差近100倍，产生的相应的估计有较大的差距。但透过数据的变化情况，两个因素对响应变量的影响又很接近，这也就告诉我们，在比较影响时不能直接比较，需要做标准化处理。</p><p>总之，数据的中心化处理相当于将坐标轴的原点移至样本中心，数据的标准化处理相当于是将不同指标化为同一尺度标准和数量级，方便比较。</p><p>特别地，在用多元线性回归方程描述某种经济现象时，由于自变量所用的单位大都不同，数据的大小差异也往往很大，这就不利于放在同一标准上进行比较。将样本数据作标准化处理后就消除了量纲不同和数量级的差异所带来的影响。</p></sec><sec id="s8"><title>文章引用</title><p>张艳玲. 多元线性回归的中心化和标准化实验结果比较Comparison of Centralized and Standardized Experimental Results of Multiple Linear Regression[J]. 应用数学进展, 2021, 10(08): 2748-2758. https://doi.org/10.12677/AAM.2021.108286</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.44453-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">邓集贤, 等. 概率论与数理统计(下册) [M]. 北京: 高等教育出版社, 2009.</mixed-citation></ref></ref-list></back></article>