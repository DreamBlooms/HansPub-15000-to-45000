<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.109163</article-id><article-id pub-id-type="publisher-id">CSA-37468</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200900000_29352872.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于正负类边界距离的多标签数据属性约简
  Attribute Reduction for Multi-Label Data Based on Boundary Distance between Positive and Negative Classes
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>纪</surname><given-names>思南</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>渤海大学数学系，辽宁 锦州</addr-line></aff><pub-date pub-type="epub"><day>02</day><month>09</month><year>2020</year></pub-date><volume>10</volume><issue>09</issue><fpage>1549</fpage><lpage>1558</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   本文首先将多标签分类问题分解为一系列单标签二分类子问题，每一个子问题对应一个标签。子问题的正类定义为具有该标签的样本，负类定义为不具有该标签的样本。在给定的属性子集下，计算出正负类样本之间的最小距离，即分类边界的最小距离。将子问题分类边界最小距离求和定义为依赖度函数，并将此依赖度函数作为属性子集重要度评价指标。然后建立了所提出依赖度函数关于属性子集的单调性，并通过最大化依赖度函数给出了属性约简的定义。最后，设计了一种基于正负类边界距离的属性约简算法，并在实际的多标签数据集上进行了实验，实验结果表明，所提约简算法能够建立合理的属性重要度排序，有效地去除冗余属性。 In this paper, the multi-label classification problem is decomposed into a series of single-label bi-nary classification sub-problems, each of which corresponds to one label. The positive class of the sub-problem is defined as the sample with the label, and the negative class is defined as the sample without the label. Under the given attribute subset, the minimum distance between positive and negative class samples is calculated, that is, the minimum distance of classification boundary. The sum of the boundary distances for all sub-problems is defined as dependency function, which is used as the evaluation of the importance of the attribute subset. The monotonicity of the proposed dependence function with respect to the attribute subset is established, and the definition of the attribute reduction is given by maximizing the dependence function. Finally, an attribute reduction algorithm based on positive and negative class boundary distance is designed, and the experiments are carried out on actual multi-label data sets. The experimental results show that the proposed algorithm can establish a reasonable ranking of attribute importance and effectively remove redundant attributes. 
  
 
</p></abstract><kwd-group><kwd>多标签分类，属性约简，边界距离，依赖度函数, Multi-Label Classification</kwd><kwd> Attribute Reduction</kwd><kwd> Boundary Distance</kwd><kwd> Dependence Function</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>本文首先将多标签分类问题分解为一系列单标签二分类子问题，每一个子问题对应一个标签。子问题的正类定义为具有该标签的样本，负类定义为不具有该标签的样本。在给定的属性子集下，计算出正负类样本之间的最小距离，即分类边界的最小距离。将子问题分类边界最小距离求和定义为依赖度函数，并将此依赖度函数作为属性子集重要度评价指标。然后建立了所提出依赖度函数关于属性子集的单调性，并通过最大化依赖度函数给出了属性约简的定义。最后，设计了一种基于正负类边界距离的属性约简算法，并在实际的多标签数据集上进行了实验，实验结果表明，所提约简算法能够建立合理的属性重要度排序，有效地去除冗余属性。</p></sec><sec id="s2"><title>关键词</title><p>多标签分类，属性约简，边界距离，依赖度函数</p></sec><sec id="s3"><title>Attribute Reduction for Multi-Label Data Based on Boundary Distance between Positive and Negative Classes</title><p>Sinan Ji</p><p>Department of Mathematics, Bohai University, Jinzhou Liaoning</p><p><img src="//html.hanspub.org/file/2-1541854x4_hanspub.png" /></p><p>Received: Aug. 18<sup>th</sup>, 2020; accepted: Aug. 28<sup>th</sup>, 2020; published: Sep. 4<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/2-1541854x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In this paper, the multi-label classification problem is decomposed into a series of single-label binary classification sub-problems, each of which corresponds to one label. The positive class of the sub-problem is defined as the sample with the label, and the negative class is defined as the sample without the label. Under the given attribute subset, the minimum distance between positive and negative class samples is calculated, that is, the minimum distance of classification boundary. The sum of the boundary distances for all sub-problems is defined as dependency function, which is used as the evaluation of the importance of the attribute subset. The monotonicity of the proposed dependence function with respect to the attribute subset is established, and the definition of the attribute reduction is given by maximizing the dependence function. Finally, an attribute reduction algorithm based on positive and negative class boundary distance is designed, and the experiments are carried out on actual multi-label data sets. The experimental results show that the proposed algorithm can establish a reasonable ranking of attribute importance and effectively remove redundant attributes.</p><p>Keywords:Multi-Label Classification, Attribute Reduction, Boundary Distance, Dependence Function</p><disp-formula id="hanspub.37468-formula7"><graphic xlink:href="//html.hanspub.org/file/2-1541854x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/2-1541854x7_hanspub.png" /> <img src="//html.hanspub.org/file/2-1541854x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>近年来，在数据挖掘和机器学习领域中，多标签分类问题引起了广泛的关注。多标签分类和单标签分类的区别就是单标签分类中的样本只能与一个标签相关联，而多标签分类中的样本可能同时与多个类别标签相关联 [<xref ref-type="bibr" rid="hanspub.37468-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.37468-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.37468-ref3">3</xref>]。</p><p>在多标签数据中，有些属性可能是冗余的或与分类任务不相关的，冗余或不相关的属性会导致多标签分类器的分类性能较差。因此，在设计分类器前，需要减少冗余或不相关的属性 [<xref ref-type="bibr" rid="hanspub.37468-ref4">4</xref>]。属性约简就是在分类能力不变的前提下，删除其中冗余或不相关的属性，选择一个最优属性子集来提高分类器的分类性能，保留了数据集最有用的信息，使分类模型简洁，具有更好的泛化能力 [<xref ref-type="bibr" rid="hanspub.37468-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.37468-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.37468-ref7">7</xref>]。</p><p>属性约简是多标签分类重要的数据预处理过程，其目的是减少数据的维数，来提高数据处理速度 [<xref ref-type="bibr" rid="hanspub.37468-ref8">8</xref>]。2018年，Lin等人提出了一种新的模糊粗糙集模型，采用局部采样技术构造样本间的鲁棒距离，并建立了一种适用于多标签学习的属性约简算法 [<xref ref-type="bibr" rid="hanspub.37468-ref9">9</xref>]。2019年，Wang等人提出了一种基于信息论的多标签学习特征选择算法，他先定义了多标签信息熵和多标签互信息的新概念，然后，建立了一种缺失标签的多标签特征选择方法 [<xref ref-type="bibr" rid="hanspub.37468-ref10">10</xref>]。2020年，Liu等人通过设计类间判别和类内邻域识别来选择每个新到达的标签特征，提出了一种在流标签环境下的多标签特征选择方法 [<xref ref-type="bibr" rid="hanspub.37468-ref11">11</xref>]。</p><p>现有的属性约简算法，计算复杂度较高，而样本间距离运算简单，直观，所以我们在这篇论文中利用样本间距离，定义依赖度函数，使模型更加简洁，运算速度更快。我们首先将多标签分类问题转化为二分类问题，并介绍了正负类样本集，然后，求出正负类样本间距离，从而定义了依赖度函数。通过正负类样本间距离函数，证明了依赖度函数相对于属性子集是单调递增的。接着给出了属性约简定义，最后，设计了一种基于正负类边界距离的属性约简算法，来选择最优的多标签属性子集。为了验证该方法的性能，我们将其与现有的四种多标签特征选择算法进行了比较。实验结果表明，该方法能够有效地去除冗余属性。</p><p>论文的其余部分结构如下。在第一节中，回顾了多标签分类的基础知识、点到点的距离、集到集的距离和点到集的距离的定义。在第二节中，我们给出了属性约简的定义，设计了一种基于正负类边界距离的属性约简算法。在第三节中，汇报了我们的算法在9个多标签数据集上实验的结果。在第四节中，对本文所得结论和实验结果进行总结，并提出了今后要研究的问题。</p></sec><sec id="s6"><title>2. 基础知识</title><p>多标签数据表示为 S = ( U , A , L ) ，其中样本集 U = { x 1 , x 2 , ⋯ , x n } 是一个非空有限集，表示有n个样本实例。属性集 A = { a 1 , a 2 , ⋯ , a p } 是一个非空有限集，每个属性 a ∈ A 是从样本集U到实数集的函数，其中 a ( x ) 表示为样本 x ∈ U 在属性a上的值。标签集 L = { l 1 , l 2 , ⋯ , l q } 是一个非空有限集，表示有q个标签。每个标签 l ∈ L 被定义为从样本集U到集合 { 0,1 } 的函数。假设样本x与标签l相关联，则 l ( x ) = 1 ；否则， l ( x ) = 0 。</p><p>对于任意非空属性子集 B ⊆ A ，假设样本点x和点y都是样本集U中的点，则点x和点y之间的距离定义为</p><p>d B ( x , y ) = ∑ a ∈ B | a ( x ) − a ( y ) | .</p><p>对于每个非空属性子集 B ⊆ A ，设集合X和集合Y是样本集U的子集，则集合X和集合Y之间的距离定义为</p><p>d B ( X , Y ) = min x ∈ X , y ∈ Y d B ( x , y ) .</p><p>当集合X是单点集 { x } 时，点x和点集Y之间的距离为</p><p>d B ( x , Y ) = min y ∈ Y d B ( x , y ) .</p><p>任意两个非空集X和Y都有 d B ( X , Y ) ≥ 0 。</p></sec><sec id="s7"><title>3. 多标签数据属性约简</title><p>在给定标签 l i ∈ L 的情况下，关于 l i 的一对样本集被定义为</p><p>E i = { x ∈ U : l i ( x ) = 1 } , i = 1 , ⋯ , q .</p><p>F i = { x ∈ U : l i ( x ) = 0 } , i = 1 , ⋯ , q .</p><p>样本集 E i 是有该标签的样本集合，样本集 F i 是没有该标签的样本集合， E i 是正类样本集， F i 是负类样本集。每个标签具有一对样本集 E i 和 F i 。对于给定的属性子集 B ⊂ A ，下面的函数 d B ( E i , F i ) 可以测量正类样本集 E i 和负类样本集 F i 之间的距离，函数定义为</p><disp-formula id="hanspub.37468-formula8"><graphic xlink:href="//html.hanspub.org/file/2-1541854x41_hanspub.png"  xlink:type="simple"/></disp-formula><p>这个距离函数 d B ( E i , F i ) 测量了正类样本集 E i 和负类样本集 F i 的边界距离，函数值越大，分类器的分类能力越好。也就是说，标签相同的样本距离越近越好，标签不同的样本距离越远越好。</p><p>定义1：对于任意非空属性子集 B ⊆ A ，我们将每个标签的正负类样本间距离求和定义为依赖度函数，依赖函数为</p><p>γ ( B ) = ∑ i = 1 q d B ( E i , F i )</p><p>依赖度函数 γ ( B ) 用于评估属性子集的重要性，依赖度函数值的大小取决于正负类样本间距离函数 d B ( E i , F i ) 值的大小。</p><p>引理1：对于多标签数据 S = ( U , A , L ) ，距离函数 d B ( E i , F i ) 和依赖度函数 γ ( B ) 相对于属性集B是单调递增的。任意选择两个属性子集 B 1 和 B 2 ，有 B 1 ⊂ B 2 ⊂ A ，则存在 d B 1 ( E i , F i ) &lt; d B 2 ( E i , F i ) ， γ ( B 1 ) ≤ γ ( B 2 ) 。</p><p>证明：由距离的定义，对于任意 B 1 ⊂ B 2 ，有</p><p>d B 1 ( x , y ) = ∑ a ∈ B 1 | a ( x ) − a ( y ) | ≤ ∑ a ∈ B 2 | a ( x ) − a ( y ) | = d B 2 ( x , y )</p><p>根据集合到集合间距离的定义有</p><p>d B 1 ( E i , F i ) = min x ∈ E i , y ∈ F i d B 1 ( x , y ) ≤ d B 1 ( x , y )</p><p>d B 2 ( E i , F i ) = min x ∈ E i , y ∈ F i d B 2 ( x , y ) ≤ d B 2 ( x , y )</p><p>因此</p><p>d B 2 ( E i , F i ) − d B 1 ( E i − F i ) &gt; 0</p><p>d B 1 ( E i , F i ) &lt; d B 2 ( E i , F i )</p><p>另外，根据定义1有</p><p>γ ( B 1 ) = ∑ i = 1 q d B 1 ( E i , F i )</p><p>γ ( B 2 ) = ∑ i = 1 q d B 2 ( E i , F i )</p><p>得到</p><p>∑ i = 1 q d B 2 ( E i , F i ) − ∑ i = 1 q d B 1 ( E i , F i ) ≥ 0</p><p>因此</p><p>γ ( B 1 ) ≤ γ ( B 2 )</p><p>引理得证。</p><p>从引理1看出，随着属性数目的增加，样本间距离函数值越大，依赖度函数值也越大。这意味着在属性子集中添加新属性从而提高了分类器的分类能力。现在，我们通过最大化依赖度函数给出属性约简的定义。</p><p>定义2：对于多标签数据 S = ( U , A , L ) ，如果属性子集 B ⊆ A 满足以下条件，则称集合B是A的依赖约简：</p><p>1) γ ( B ) = γ ( A ) ；</p><p>2) 对于任意的 B ′ ⊂ B ，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/2-1541854x71_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>我们可以看到约简的是A的一个极小子集，它保持了分类器的分类能力。然而，在现实应用中，上述定义过于严格，因此我们引入一个参数 ε ，并给出基于正负类边界距离的属性约简定义。</p><p>定义3：对于多标签数据 S = ( U , A , L ) ，如果属性子集 B ⊆ A 满足以下条件，则称集合B是A基于正负类边界距离的属性约简：</p><p>1) | γ ( B ) − γ ( A ) | &lt; ε ；</p><p>2) 对于任意的 B ′ ⊂ B ， | γ ( B ′ ) − γ ( A ) | ≥ ε 。</p><p>在这里，我们通过引入参数 ε 来扩大属性约简的范围，并尽可能减少冗余属性，同时将正负类样本间距离的变化保持在较小的范围内。例1更详细地描述了约简过程。</p><p>例1：给出多标签数据集 S = ( U , A , L ) ，如表1所示，样本集为 U = { x 1 , x 2 , x 3 , x 4 , x 5 , x 6 } ，属性集为 A = { a , b , c } ，标签集为 L = { l 1 , l 2 , l 3 } 。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Multi-label dat</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >U</th><th align="center" valign="middle" >a</th><th align="center" valign="middle" >b</th><th align="center" valign="middle" >c</th><th align="center" valign="middle" >l 1</th><th align="center" valign="middle" >l 2</th><th align="center" valign="middle" >l 3</th></tr></thead><tr><td align="center" valign="middle" >x 1</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >x 2</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" ><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/2-1541854x88_hanspub.png" xlink:type="simple"/></inline-formula></td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >x 4</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >x 5</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >x 6</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td></tr></tbody></table></table-wrap><p>表1. 多标签数据</p><p>正负类样本集如表2所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Sample se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >L</th><th align="center" valign="middle" >E</th><th align="center" valign="middle" >F</th></tr></thead><tr><td align="center" valign="middle" >l 1</td><td align="center" valign="middle" >E 1 = { x 1 , x 3 , x 4 }</td><td align="center" valign="middle" >F 1 = { x 2 , x 5 , x 6 }</td></tr><tr><td align="center" valign="middle" >l 2</td><td align="center" valign="middle" >E 2 = { x 2 , x 6 }</td><td align="center" valign="middle" >F 2 = { x 1 , x 3 , x 4 , x 5 }</td></tr><tr><td align="center" valign="middle" >l 3</td><td align="center" valign="middle" >E 3 = { x 3 , x 4 , x 5 }</td><td align="center" valign="middle" >F 3 = { x 1 , x 2 , x 6 }</td></tr></tbody></table></table-wrap><p>表2. 样本集</p><p>根据距离定义计算出正负类样本间距离，然后得到依赖度函数值，结果如表3所示。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Distanc</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >B</th><th align="center" valign="middle" >{a}</th><th align="center" valign="middle" >{b}</th><th align="center" valign="middle" >{c}</th><th align="center" valign="middle" >{a,b}</th><th align="center" valign="middle" >{a,c}</th><th align="center" valign="middle" >{b,c}</th><th align="center" valign="middle" >A</th></tr></thead><tr><td align="center" valign="middle" >d B</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >2.236</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2.236</td></tr><tr><td align="center" valign="middle" >d B</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >2.236</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2.236</td></tr><tr><td align="center" valign="middle" >d B</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2.828</td><td align="center" valign="middle" >3.605</td><td align="center" valign="middle" >2.236</td><td align="center" valign="middle" >4.123</td></tr><tr><td align="center" valign="middle" >γ ( B )</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >7.3</td><td align="center" valign="middle" >7.605</td><td align="center" valign="middle" >4.236</td><td align="center" valign="middle" >8.595</td></tr></tbody></table></table-wrap><p>表3. 距离</p><p>根据定义3计算得到表4。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Dependence degre</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >{a}</th><th align="center" valign="middle" >{b}</th><th align="center" valign="middle" >{c}</th><th align="center" valign="middle" >{a,b}</th><th align="center" valign="middle" >{a,c}</th><th align="center" valign="middle" >{b,c}</th><th align="center" valign="middle" >A</th></tr></thead><tr><td align="center" valign="middle" >| γ ( B ) − γ ( A ) |</td><td align="center" valign="middle" >2.595</td><td align="center" valign="middle" >4.595</td><td align="center" valign="middle" >7.595</td><td align="center" valign="middle" >1.295</td><td align="center" valign="middle" >0.99</td><td align="center" valign="middle" >4.359</td><td align="center" valign="middle" >0</td></tr></tbody></table></table-wrap><p>表4. 依赖度</p><p>根据定义3中的参数 ε = 1 ，则从表4中可看出属性集 { a , c } 是正负类边界距离的属性约简。</p><p>现在，我们设计了一种基于正负类边界距离的属性约简算法。把定义3中的参数 ε 看作算法的停止条件，当依赖度的增加小于参数 ε 时，算法终止，完成属性约简。</p><p>变量 A k 是剩余的属性集，变量 B k 是选择的属性集。最初的剩余属性集就是全部的属性集A，从 A k 中任意选择一个 a j ，把它并到 B k 里，计算依赖度 γ ( B k ∪ { a j } ) ，有多少个属性就算多少个依赖度。然后选择依赖度最大的属性，在剩余属性 A k 里去掉，选择的属性 B k 中加上。选择最大的依赖度作为新的依赖度，当依赖增量小于参数 ε 时，算法终止，属性约简完成。当依赖增量大于参数 ε 时，回到第2步，选择了一个属性后还剩多少，然后循环上述过程，循环到满足不等式时，属性约简完成。在算法中， | U | 是样本数， | A | 是属性数，步骤2到步骤5是一个循环过程，循环 | A | 次。因此，时间复杂度为 O ( | A | | U | 2 ) 。</p></sec><sec id="s8"><title>4. 实验</title><p>为了评估PNBR算法的性能，本文在九个多标签数据集上实现了PNBR算法，并将实验结果与四种多标签属性约简算法进行了性能比较。这些算法包括PR、MDMR、FSSL和NLD，其中PR代表经典的正域约简算法 [<xref ref-type="bibr" rid="hanspub.37468-ref12">12</xref>]、MDMR代表标签最大依赖和最小冗余算法 [<xref ref-type="bibr" rid="hanspub.37468-ref13">13</xref>]、FSSL代表基于流标签的多标签学习的特征选择算法 [<xref ref-type="bibr" rid="hanspub.37468-ref11">11</xref>]、NLD代表邻域标签依赖度约简算法 [<xref ref-type="bibr" rid="hanspub.37468-ref14">14</xref>] 和PNBR代表我们提出的正负类边界距离的多标签数据属性约简算法。九个多标签数据集如表5所示。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Multi-label data set</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Data set</th><th align="center" valign="middle" >Type</th><th align="center" valign="middle" >Samples</th><th align="center" valign="middle" >Attributes</th><th align="center" valign="middle" >Label</th></tr></thead><tr><td align="center" valign="middle" >Flags</td><td align="center" valign="middle" >Hybrid</td><td align="center" valign="middle" >194</td><td align="center" valign="middle" >19</td><td align="center" valign="middle" >7</td></tr><tr><td align="center" valign="middle" >Yeast</td><td align="center" valign="middle" >Numerical</td><td align="center" valign="middle" >2417</td><td align="center" valign="middle" >103</td><td align="center" valign="middle" >14</td></tr><tr><td align="center" valign="middle" >Emotion</td><td align="center" valign="middle" >Numerical</td><td align="center" valign="middle" >593</td><td align="center" valign="middle" >72</td><td align="center" valign="middle" >6</td></tr><tr><td align="center" valign="middle" >CAL500</td><td align="center" valign="middle" >Numerical</td><td align="center" valign="middle" >500</td><td align="center" valign="middle" >62</td><td align="center" valign="middle" >174</td></tr><tr><td align="center" valign="middle" >Scene</td><td align="center" valign="middle" >Numerical</td><td align="center" valign="middle" >2407</td><td align="center" valign="middle" >294</td><td align="center" valign="middle" >6</td></tr><tr><td align="center" valign="middle" >Enron</td><td align="center" valign="middle" >Nominal</td><td align="center" valign="middle" >1702</td><td align="center" valign="middle" >1001</td><td align="center" valign="middle" >53</td></tr><tr><td align="center" valign="middle" >Medical</td><td align="center" valign="middle" >Nominal</td><td align="center" valign="middle" >978</td><td align="center" valign="middle" >1449</td><td align="center" valign="middle" >45</td></tr><tr><td align="center" valign="middle" >Core15k</td><td align="center" valign="middle" >Nominal</td><td align="center" valign="middle" >5000</td><td align="center" valign="middle" >499</td><td align="center" valign="middle" >374</td></tr><tr><td align="center" valign="middle" >Genbase</td><td align="center" valign="middle" >Nominal</td><td align="center" valign="middle" >662</td><td align="center" valign="middle" >1185</td><td align="center" valign="middle" >27</td></tr></tbody></table></table-wrap><p>表5. 多标签数据集</p><p>在我们的实验中，采用十折交叉验证来评估不同方法的有效性。十折交叉验证是将原始数据集随机分成10个大小相同的样本子集，轮流将其中9个子集当作多标签训练集，一个子集当作多标签测试集，进行实验。每次实验都会得出相应的正确率或差错率，10次实验结果的正确率或差错率的平均值作为对算法精度的估计。表6给出了实验后所选属性的平均数量，并用下划线突显了两个最佳结果。通过比较所选属性的平均数量可知，MDMR算法和PNBR算法更好，可以去除更多的冗余属性。而PR算法和FSSL算法相对差一点。</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Average numbers of the selected attribute</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Data set</th><th align="center" valign="middle" >Raw data</th><th align="center" valign="middle" >PR</th><th align="center" valign="middle" >MDMR</th><th align="center" valign="middle" >FSSL</th><th align="center" valign="middle" >NLD</th><th align="center" valign="middle" >PNBR</th></tr></thead><tr><td align="center" valign="middle" >Flags</td><td align="center" valign="middle" >19</td><td align="center" valign="middle" >8.3</td><td align="center" valign="middle" >5.0</td><td align="center" valign="middle" >11.9</td><td align="center" valign="middle" >9.4</td><td align="center" valign="middle" >4.0</td></tr><tr><td align="center" valign="middle" >Yeast</td><td align="center" valign="middle" >103</td><td align="center" valign="middle" >7.7</td><td align="center" valign="middle" >7.3</td><td align="center" valign="middle" >9.6</td><td align="center" valign="middle" >9.4</td><td align="center" valign="middle" >8.6</td></tr><tr><td align="center" valign="middle" >Emotion</td><td align="center" valign="middle" >72</td><td align="center" valign="middle" >5.9</td><td align="center" valign="middle" >5.8</td><td align="center" valign="middle" >5.7</td><td align="center" valign="middle" >6.1</td><td align="center" valign="middle" >5.6</td></tr><tr><td align="center" valign="middle" >CAL500</td><td align="center" valign="middle" >62</td><td align="center" valign="middle" >8.7</td><td align="center" valign="middle" >8.5</td><td align="center" valign="middle" >8.2</td><td align="center" valign="middle" >3.7</td><td align="center" valign="middle" >3.3</td></tr><tr><td align="center" valign="middle" >Scene</td><td align="center" valign="middle" >294</td><td align="center" valign="middle" >8.9</td><td align="center" valign="middle" >8.6</td><td align="center" valign="middle" >11.9</td><td align="center" valign="middle" >8.8</td><td align="center" valign="middle" >11.8</td></tr><tr><td align="center" valign="middle" >Enron</td><td align="center" valign="middle" >1001</td><td align="center" valign="middle" >100.9</td><td align="center" valign="middle" >39.7</td><td align="center" valign="middle" >63.8</td><td align="center" valign="middle" >36.4</td><td align="center" valign="middle" >37.3</td></tr><tr><td align="center" valign="middle" >Medical</td><td align="center" valign="middle" >1449</td><td align="center" valign="middle" >63.9</td><td align="center" valign="middle" >34.4</td><td align="center" valign="middle" >63.3</td><td align="center" valign="middle" >31.9</td><td align="center" valign="middle" >31.2</td></tr><tr><td align="center" valign="middle" >Core15k</td><td align="center" valign="middle" >499</td><td align="center" valign="middle" >168.5</td><td align="center" valign="middle" >34.9</td><td align="center" valign="middle" >31.8</td><td align="center" valign="middle" >159.9</td><td align="center" valign="middle" >155.1</td></tr><tr><td align="center" valign="middle" >Genbase</td><td align="center" valign="middle" >1185</td><td align="center" valign="middle" >18.9</td><td align="center" valign="middle" >15.7</td><td align="center" valign="middle" >17.9</td><td align="center" valign="middle" >15.9</td><td align="center" valign="middle" >15.1</td></tr></tbody></table></table-wrap><p>表6. 所选属性的平均数量</p><p>实验结果分析，对于数值型数据而言，PNBR算法和MDMR算法选择的属性最少。在Flags数据集中，它只保留了19个属性中的4或5个，这两种算法相对其他算法较差。然而在数据集Enron、Medical和Genbase中，随着数据属性数量的增加，属性减少的就不多了，这种影响就可以忽略不计了。</p><p>十折交叉验证的平均运行时间如表7所示，可以看出，MDMR算法、NLD算法和PNBR算法的运行时间比其他算法长，PR算法和FSSL算法的运行时间比其他算法短。</p><table-wrap id="table7" ><label><xref ref-type="table" rid="table7">Table 7</xref></label><caption><title> Average running tim</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Data set</th><th align="center" valign="middle" >PR</th><th align="center" valign="middle" >MDMR</th><th align="center" valign="middle" >FSSL</th><th align="center" valign="middle" >NLD</th><th align="center" valign="middle" >PNBR</th></tr></thead><tr><td align="center" valign="middle" >Flags</td><td align="center" valign="middle" >0.10</td><td align="center" valign="middle" >0.18</td><td align="center" valign="middle" >0.12</td><td align="center" valign="middle" >0.25</td><td align="center" valign="middle" >0.37</td></tr><tr><td align="center" valign="middle" >Yeast</td><td align="center" valign="middle" >107.34</td><td align="center" valign="middle" >255.62</td><td align="center" valign="middle" >329.41</td><td align="center" valign="middle" >757.11</td><td align="center" valign="middle" >967.71</td></tr><tr><td align="center" valign="middle" >Emotion</td><td align="center" valign="middle" >0.49</td><td align="center" valign="middle" >0.64</td><td align="center" valign="middle" >0.43</td><td align="center" valign="middle" >0.56</td><td align="center" valign="middle" >0.53</td></tr><tr><td align="center" valign="middle" >CAL500</td><td align="center" valign="middle" >0.37</td><td align="center" valign="middle" >3.96</td><td align="center" valign="middle" >1.52</td><td align="center" valign="middle" >5.31</td><td align="center" valign="middle" >9.38</td></tr><tr><td align="center" valign="middle" >Scene</td><td align="center" valign="middle" >320.68</td><td align="center" valign="middle" >318.62</td><td align="center" valign="middle" >848.60</td><td align="center" valign="middle" >841.88</td><td align="center" valign="middle" >821.58</td></tr><tr><td align="center" valign="middle" >Enron</td><td align="center" valign="middle" >337.68</td><td align="center" valign="middle" >373.22</td><td align="center" valign="middle" >208.66</td><td align="center" valign="middle" >337.81</td><td align="center" valign="middle" >324.85</td></tr><tr><td align="center" valign="middle" >Medical</td><td align="center" valign="middle" >193.33</td><td align="center" valign="middle" >129.91</td><td align="center" valign="middle" >150.05</td><td align="center" valign="middle" >92.38</td><td align="center" valign="middle" >88.69</td></tr><tr><td align="center" valign="middle" >Core15k</td><td align="center" valign="middle" >349.89</td><td align="center" valign="middle" >3768.82</td><td align="center" valign="middle" >264.80</td><td align="center" valign="middle" >2925.93</td><td align="center" valign="middle" >2815.86</td></tr><tr><td align="center" valign="middle" >Genbase</td><td align="center" valign="middle" >67.81</td><td align="center" valign="middle" >67.92</td><td align="center" valign="middle" >57.69</td><td align="center" valign="middle" >60.78</td><td align="center" valign="middle" >67.89</td></tr></tbody></table></table-wrap><p>表7. 平均运行时间</p><p>实验结果分析，对于数值型数据而言，FSSL算法和NLD算法的计算时间大约是PR算法和MDMR算法的2倍，NLD算法和PNBR算法的计算时间大约是PR算法和MDMR算法的3倍。在Flags数据集中，由于数据集较小，所以算法的计算速度的差异不明显。</p><p>本文用分类器ML-9NN评估了五种属性约简算法的分类性能，同时，采用Hamming Loss、F<sub>1</sub> score和Coverage三种多标签分类的评价指标，来衡量约简数据的分类精度 [<xref ref-type="bibr" rid="hanspub.37468-ref15">15</xref>]，其中下划线突显了两个最佳结果。</p><table-wrap id="table8" ><label><xref ref-type="table" rid="table8">Table 8</xref></label><caption><title> Hamming los</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Data set</th><th align="center" valign="middle" >Raw data</th><th align="center" valign="middle" >PR</th><th align="center" valign="middle" >MDMR</th><th align="center" valign="middle" >FSSL</th><th align="center" valign="middle" >NLD</th><th align="center" valign="middle" >PNBR</th></tr></thead><tr><td align="center" valign="middle" >Flags</td><td align="center" valign="middle" >0.284 &#177; 0.051</td><td align="center" valign="middle" >0.285 &#177; 0.049</td><td align="center" valign="middle" >0.276 &#177; 0.057</td><td align="center" valign="middle" >0.269 &#177; 0.029</td><td align="center" valign="middle" >0.293 &#177; 0.031</td><td align="center" valign="middle" >0.289 &#177; 0.035</td></tr><tr><td align="center" valign="middle" >Yeast</td><td align="center" valign="middle" >0.198 &#177; 0.011</td><td align="center" valign="middle" >0.213 &#177; 0.010</td><td align="center" valign="middle" >0.202 &#177; 0.011</td><td align="center" valign="middle" >0.209 &#177; 0.012</td><td align="center" valign="middle" >0.199 &#177; 0.011</td><td align="center" valign="middle" >0.198 &#177; 0.010</td></tr><tr><td align="center" valign="middle" >Emotion</td><td align="center" valign="middle" >0.189 &#177; 0.019</td><td align="center" valign="middle" >0.238 &#177; 0.029</td><td align="center" valign="middle" >0.235 &#177; 0.029</td><td align="center" valign="middle" >0.198 &#177; 0.019</td><td align="center" valign="middle" >0.217 &#177; 0.019</td><td align="center" valign="middle" >0.179 &#177; 0.019</td></tr><tr><td align="center" valign="middle" >CAL500</td><td align="center" valign="middle" >0.167 &#177; 0.013</td><td align="center" valign="middle" >0.169 &#177; 0.011</td><td align="center" valign="middle" >0.165 &#177; 0.011</td><td align="center" valign="middle" >0.168 &#177; 0.013</td><td align="center" valign="middle" >0.159 &#177; 0.012</td><td align="center" valign="middle" >0.157 &#177; 0.013</td></tr><tr><td align="center" valign="middle" >Scene</td><td align="center" valign="middle" >0.118 &#177; 0.011</td><td align="center" valign="middle" >0.115 &#177; 0.010</td><td align="center" valign="middle" >0.123 &#177; 0.012</td><td align="center" valign="middle" >0.115 &#177; 0.011</td><td align="center" valign="middle" >0.131 &#177; 0.010</td><td align="center" valign="middle" >0.112 &#177; 0.011</td></tr><tr><td align="center" valign="middle" >Enron</td><td align="center" valign="middle" >0.065 &#177; 0.010</td><td align="center" valign="middle" >0.066 &#177; 0.011</td><td align="center" valign="middle" >0.057 &#177; 0.015</td><td align="center" valign="middle" >0.069 &#177; 0.011</td><td align="center" valign="middle" >0.059 &#177; 0.010</td><td align="center" valign="middle" >0.068 &#177; 0.012</td></tr><tr><td align="center" valign="middle" >Medical</td><td align="center" valign="middle" >0.029 &#177; 0.011</td><td align="center" valign="middle" >0.027 &#177; 0.010</td><td align="center" valign="middle" >0.025 &#177; 0.011</td><td align="center" valign="middle" >0.026 &#177; 0.013</td><td align="center" valign="middle" >0.019 &#177; 0.010</td><td align="center" valign="middle" >0.020 &#177; 0.011</td></tr><tr><td align="center" valign="middle" >Core15k</td><td align="center" valign="middle" >0.013 &#177; 0.000</td><td align="center" valign="middle" >0.014 &#177; 0.000</td><td align="center" valign="middle" >0.017 &#177; 0.000</td><td align="center" valign="middle" >0.015 &#177; 0.000</td><td align="center" valign="middle" >0.012 &#177; 0.000</td><td align="center" valign="middle" >0.011 &#177; 0.000</td></tr><tr><td align="center" valign="middle" >Genbase</td><td align="center" valign="middle" >0.019 &#177; 0.000</td><td align="center" valign="middle" >0.010 &#177; 0.000</td><td align="center" valign="middle" >0.019 &#177; 0.000</td><td align="center" valign="middle" >0.017 &#177; 0.000</td><td align="center" valign="middle" >0.021 &#177; 0.000</td><td align="center" valign="middle" >0.022 &#177; 0.000</td></tr></tbody></table></table-wrap><p>表8. 汉明损失</p><p>汉明损失反应的是错误分类的样本标签占总样本标签的比率，该指标取值越小，算法的性能越好，当值为0时达到最优。从表8可以看出，NLD算法和PNBR算法优于其他算法，MDMR算法和FSSL算法的性能略优于PR算法，结合表6和表7中的属性约简数量和运行时间，NLD算法和PNBR算法保留的属性更多，即使耗时长，但它的分类效果更好，特别是对于符号型数据。</p><table-wrap id="table9" ><label><xref ref-type="table" rid="table9">Table 9</xref></label><caption><title> F1 scor</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Data set</th><th align="center" valign="middle" >Raw data</th><th align="center" valign="middle" >PR</th><th align="center" valign="middle" >MDMR</th><th align="center" valign="middle" >FSSL</th><th align="center" valign="middle" >NLD</th><th align="center" valign="middle" >PNBR</th></tr></thead><tr><td align="center" valign="middle" >Flags</td><td align="center" valign="middle" >0.686 &#177; 0.059</td><td align="center" valign="middle" >0.695 &#177; 0.059</td><td align="center" valign="middle" >0.703 &#177; 0.079</td><td align="center" valign="middle" >0.687 &#177; 0.038</td><td align="center" valign="middle" >0.683 &#177; 0.049</td><td align="center" valign="middle" >0.679 &#177; 0.039</td></tr><tr><td align="center" valign="middle" >Yeast</td><td align="center" valign="middle" >0.627 &#177; 0.018</td><td align="center" valign="middle" >0.589 &#177; 0.011</td><td align="center" valign="middle" >0.597 &#177; 0.013</td><td align="center" valign="middle" >0.589 &#177; 0.018</td><td align="center" valign="middle" >0.625 &#177; 0.018</td><td align="center" valign="middle" >0.633 &#177; 0.011</td></tr><tr><td align="center" valign="middle" >Emotion</td><td align="center" valign="middle" >0.636 &#177; 0.047</td><td align="center" valign="middle" >0.627 &#177; 0.029</td><td align="center" valign="middle" >0.613 &#177; 0.026</td><td align="center" valign="middle" >0.631 &#177; 0.038</td><td align="center" valign="middle" >0.623 &#177; 0.057</td><td align="center" valign="middle" >0.615 &#177; 0.057</td></tr><tr><td align="center" valign="middle" >CAL500</td><td align="center" valign="middle" >0.448 &#177; 0.013</td><td align="center" valign="middle" >0.453 &#177; 0.013</td><td align="center" valign="middle" >0.446 &#177; 0.011</td><td align="center" valign="middle" >0.443 &#177; 0.011</td><td align="center" valign="middle" >0.435 &#177; 0.015</td><td align="center" valign="middle" >0.439 &#177; 0.016</td></tr><tr><td align="center" valign="middle" >Scene</td><td align="center" valign="middle" >0.669 &#177; 0.028</td><td align="center" valign="middle" >0.657 &#177; 0.039</td><td align="center" valign="middle" >0.661 &#177; 0.047</td><td align="center" valign="middle" >0.659 &#177; 0.028</td><td align="center" valign="middle" >0.657 &#177; 0.018</td><td align="center" valign="middle" >0.673 &#177; 0.015</td></tr><tr><td align="center" valign="middle" >Enron</td><td align="center" valign="middle" >0.447 &#177; 0.049</td><td align="center" valign="middle" >0.453 &#177; 0.058</td><td align="center" valign="middle" >0.451 &#177; 0.059</td><td align="center" valign="middle" >0.436 &#177; 0.088</td><td align="center" valign="middle" >0.439 &#177; 0.059</td><td align="center" valign="middle" >0.443 &#177; 0.059</td></tr><tr><td align="center" valign="middle" >Medical</td><td align="center" valign="middle" >0.367 &#177; 0.089</td><td align="center" valign="middle" >0.355 &#177; 0.117</td><td align="center" valign="middle" >0.359 &#177; 0.108</td><td align="center" valign="middle" >0.353 &#177; 0.117</td><td align="center" valign="middle" >0.373 &#177; 0.169</td><td align="center" valign="middle" >0.371 &#177; 0.168</td></tr><tr><td align="center" valign="middle" >Core15k</td><td align="center" valign="middle" >0.087 &#177; 0.018</td><td align="center" valign="middle" >0.083 &#177; 0.048</td><td align="center" valign="middle" >0.090 &#177; 0.017</td><td align="center" valign="middle" >0.082 &#177; 0.029</td><td align="center" valign="middle" >0.088 &#177; 0.017</td><td align="center" valign="middle" >0.081 &#177; 0.017</td></tr><tr><td align="center" valign="middle" >Genbase</td><td align="center" valign="middle" >0.873 &#177; 0.018</td><td align="center" valign="middle" >0.868 &#177; 0.018</td><td align="center" valign="middle" >0.869 &#177; 0.018</td><td align="center" valign="middle" >0.879 &#177; 0.018</td><td align="center" valign="middle" >0.878 &#177; 0.018</td><td align="center" valign="middle" >0.883 &#177; 0.018</td></tr></tbody></table></table-wrap><p>表9. F<sub>1</sub>分数</p><p>F<sub>1</sub> score相对于汉明损失是一个综合版本，F<sub>1</sub> score衡量分类的准确性，并忽略错误分类的样本。它是精确率和召回率的调和平均值，该指标取值越大，算法的性能越好，通常最大值为1，最小值为0，当值为1时达到最优。从表9可知，除数据集Core15k外，五种算法在F<sub>1</sub> score方面没有显著差异。对于Core15k数据集，NLD算法和MDMR算法的分类精度与原始数据差不多。</p><table-wrap id="table10" ><label><xref ref-type="table" rid="table1">Table 1</xref>0</label><caption><title> Coverag</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Data set</th><th align="center" valign="middle" >Raw data</th><th align="center" valign="middle" >PR</th><th align="center" valign="middle" >MDMR</th><th align="center" valign="middle" >FSSL</th><th align="center" valign="middle" >NLD</th><th align="center" valign="middle" >PNBR</th></tr></thead><tr><td align="center" valign="middle" >Flags</td><td align="center" valign="middle" >0.919 &#177; 0.019</td><td align="center" valign="middle" >0.927 &#177; 0.019</td><td align="center" valign="middle" >0.936 &#177; 0.028</td><td align="center" valign="middle" >0.923 &#177; 0.019</td><td align="center" valign="middle" >0.900 &#177; 0.010</td><td align="center" valign="middle" >0.899 &#177; 0.019</td></tr><tr><td align="center" valign="middle" >Yeast</td><td align="center" valign="middle" >0.869 &#177; 0.010</td><td align="center" valign="middle" >0.839 &#177; 0.010</td><td align="center" valign="middle" >0.840 &#177; 0.010</td><td align="center" valign="middle" >0.827 &#177; 0.010</td><td align="center" valign="middle" >0.836 &#177; 0.010</td><td align="center" valign="middle" >0.843 &#177; 0.010</td></tr><tr><td align="center" valign="middle" >Emotion</td><td align="center" valign="middle" >0.678 &#177; 0.027</td><td align="center" valign="middle" >0.543 &#177; 0.019</td><td align="center" valign="middle" >0.517 &#177; 0.019</td><td align="center" valign="middle" >0.658 &#177; 0.029</td><td align="center" valign="middle" >0.647 &#177; 0.03</td><td align="center" valign="middle" >0.639 &#177; 0.019</td></tr><tr><td align="center" valign="middle" >CAL500</td><td align="center" valign="middle" >0.987 &#177; 0.010</td><td align="center" valign="middle" >0.987 &#177; 0.010</td><td align="center" valign="middle" >0.989 &#177; 0.010</td><td align="center" valign="middle" >0.986 &#177; 0.010</td><td align="center" valign="middle" >0.975 &#177; 0.010</td><td align="center" valign="middle" >0.973 &#177; 0.010</td></tr><tr><td align="center" valign="middle" >Scene</td><td align="center" valign="middle" >0.634 &#177; 0.019</td><td align="center" valign="middle" >0.228 &#177; 0.010</td><td align="center" valign="middle" >0.243 &#177; 0.010</td><td align="center" valign="middle" >0.298 &#177; 0.010</td><td align="center" valign="middle" >0.339 &#177; 0.078</td><td align="center" valign="middle" >0.223 &#177; 0.010</td></tr><tr><td align="center" valign="middle" >Enron</td><td align="center" valign="middle" >0.717 &#177; 0.067</td><td align="center" valign="middle" >0.778 &#177; 0.100</td><td align="center" valign="middle" >0.789 &#177; 0.069</td><td align="center" valign="middle" >0.637 &#177; 0.078</td><td align="center" valign="middle" >0.725 &#177; 0.083</td><td align="center" valign="middle" >0.710 &#177; 0.063</td></tr><tr><td align="center" valign="middle" >Medical</td><td align="center" valign="middle" >0.418 &#177; 0.087</td><td align="center" valign="middle" >0.379 &#177; 0.087</td><td align="center" valign="middle" >0.419 &#177; 0.087</td><td align="center" valign="middle" >0.369 &#177; 0.119</td><td align="center" valign="middle" >0.423 &#177; 0.119</td><td align="center" valign="middle" >0.421 &#177; 0.119</td></tr><tr><td align="center" valign="middle" >Core15k</td><td align="center" valign="middle" >0.349 &#177; 0.039</td><td align="center" valign="middle" >0.336 &#177; 0.068</td><td align="center" valign="middle" >0.362 &#177; 0.059</td><td align="center" valign="middle" >0.351 &#177; 0.048</td><td align="center" valign="middle" >0.213 &#177; 0.029</td><td align="center" valign="middle" >0.227 &#177; 0.029</td></tr><tr><td align="center" valign="middle" >Genbase</td><td align="center" valign="middle" >0.953 &#177; 0.010</td><td align="center" valign="middle" >0.947 &#177; 0.019</td><td align="center" valign="middle" >0.964 &#177; 0.010</td><td align="center" valign="middle" >0.957 &#177; 0.028</td><td align="center" valign="middle" >0.953 &#177; 0.010</td><td align="center" valign="middle" >0.947 &#177; 0.010</td></tr></tbody></table></table-wrap><p>表10. 覆盖率</p><p>覆盖率表示算法生成的排序底部的真实标签的排序平均值。排序越大越不相关，所以这个覆盖率越小，算法性能越好。从表10看出，PR算法和PNBR算法优于其他算法。从数值上看，约简前后数据的分类精度并没有明显差异。实验结果表明所提约简算法能够建立合理的属性重要度排序，有效地去除冗余属性。</p></sec><sec id="s9"><title>5. 总结</title><p>本文受基于集合到集合距离的启发，设计了一种基于正负类边界距离的属性约简算法，通过实验对所提的约简方法进行了评估，结果表明，所提约简算法能够建立合理的属性重要度排序，有效地去除冗余属性，并保持较高的分类精度。</p><p>实验研究表明，我们这个方法在处理大规模数据时，计算复杂度还是有点高，如何降低时间复杂度是我们今后要做的工作。另外在本文，我们将多标签数据分解中的标签看作为独立的，不相关的，但是在现实生活中，标签之间应该是有相关性的，所以，今后的研究工作中要将标签的相关性考虑进去，再做研究。</p></sec><sec id="s10"><title>文章引用</title><p>纪思南. 基于正负类边界距离的多标签数据属性约简Attribute Reduction for Multi-Label Data Based on Boundary Distance between Positive and Negative Classes[J]. 计算机科学与应用, 2020, 10(09): 1549-1558. https://doi.org/10.12677/CSA.2020.109163</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.37468-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, M.L. and Zhou, Z.H. (2014) A Review on Multi-Label Learning Algorithms. IEEE Transactions on Knowledge and Data Engineering, 26, 1819-1837. https://doi.org/10.1109/TKDE.2013.39</mixed-citation></ref><ref id="hanspub.37468-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Xu, S.P., Yang, X.B., Yu, H.L., Yu, D.J., Yang, J.Y., Eric, C.C. and Tsang, J. (2016) Multi-Label Learning with Label-Specific Feature Reduction. Knowledge-Based Systems, 104, 52-61. https://doi.org/10.1016/j.knosys.2016.04.012</mixed-citation></ref><ref id="hanspub.37468-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, P.F., Xu, Q., Hu, Q.H., Zhang, C.Q. and Zhao, H. (2018) Multi-Label Feature Selection with Missing Labels. Pattern Recognition, 74, 488-502. https://doi.org/10.1016/j.patcog.2017.09.036</mixed-citation></ref><ref id="hanspub.37468-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Li, H., Li, D.Y., Zhai, Y.H., Wang, S.G. and Zhang, J. (2016) A Novel Attribute Reduction Approach for Multi-Label Data Based on Rough Set Theory. Information Sciences, 367-368, 827-847. https://doi.org/10.1016/j.ins.2016.07.008</mixed-citation></ref><ref id="hanspub.37468-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Li, Y.W., Lin, Y.J., Liu, J.H., Weng, W., Shi, Z.K. and Wu, S.X. (2018) Feature Selection for Multi-Label Learning Based on Kernelized Fuzzy Rough Sets. Neurocomputing, 318, 271-286. https://doi.org/10.1016/j.neucom.2018.08.065</mixed-citation></ref><ref id="hanspub.37468-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Liu, J.H., Lin, Y.J., Li, Y.W., Weng, W. and Wu, S.X. (2018) Online Multi-Label Streaming Feature Selection Based on Neighborhood Rough Set. Pattern Recognition, 84, 273-287. https://doi.org/10.1016/j.patcog.2018.07.021</mixed-citation></ref><ref id="hanspub.37468-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Y.J., Hu, Q.H., Zhang, J. and Wu, X.D. (2016) Mul-ti-Label Feature Selection with Streaming Labels. Information Sciences, 372, 256-275. https://doi.org/10.1016/j.ins.2016.08.039</mixed-citation></ref><ref id="hanspub.37468-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Fan, X.D., Zhao, W.D., Wang, C.Z. and Huang, Y. (2018) Attribute Re-duction Based on Max-Decision Neighborhood Rough Set Model. Knowledge-Based Systems, 151, 16-23. https://doi.org/10.1016/j.knosys.2018.03.015</mixed-citation></ref><ref id="hanspub.37468-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Y.J., Li, Y.W., Wang, C.X. and Chen, J.K. (2018) Attribute Re-duction for Multi-Label Learning with Fuzzy Rough Set. Knowledge-Based Systems, 152, 51-61. https://doi.org/10.1016/j.knosys.2018.04.004</mixed-citation></ref><ref id="hanspub.37468-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Wang, C.X., Lin, Y.J. and Liu, J.H. (2019) Feature Selection for Multi-Label Learning with Missing Labels. Applied Intelligence, 49, 3027-3042. &lt;br&gt;https://doi.org/10.1007/s10489-019-01431-6</mixed-citation></ref><ref id="hanspub.37468-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Liu, J.H., Li, Y.W., Weng, W., Zhang, J., Chen, B.H. and Wu, S.X. (2020) Feature Selection for Multi-Label Learning with Streaming Label. Neurocomputing, 387, 268-278. &lt;br&gt;https://doi.org/10.1016/j.neucom.2020.01.005</mixed-citation></ref><ref id="hanspub.37468-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">段洁, 胡清华, 张灵均, 钱宇华, 李德玉. 基于邻域粗糙集的多标记分类特征选择算法[J]. 计算机研究与发展, 2015, 52(1): 56-65.</mixed-citation></ref><ref id="hanspub.37468-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Y.J., Hu, Q.H., Liu, J.H. and Duan, J. (2015) Multi-Label Feature Selection Based on Max-Dependency and Min-Redundancy. Neurocomputing, 168, 92-103. &lt;br&gt;https://doi.org/10.1016/j.neucom.2015.06.010</mixed-citation></ref><ref id="hanspub.37468-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Fan, X.D., Chen, Q., Qiao, Z.J., Wang, C.Z. and Ten, M.Y. (2020) Attribute Reduction for Multi-Label Classification Based on Labels of Positive Region. Soft Computing, 1-11. &lt;br&gt;https://doi.org/10.1007/s00500-020-04780-4</mixed-citation></ref><ref id="hanspub.37468-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, M.L. and Zhou, Z.H. (2006) ML-KNN: A Lazy Learning Approach to Multi-Label Learning. Pattern Recognition, 40, 2038-2048. &lt;br&gt;https://doi.org/10.1016/j.patcog.2006.12.019</mixed-citation></ref></ref-list></back></article>