<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">JISP</journal-id><journal-title-group><journal-title>Journal of Image and Signal Processing</journal-title></journal-title-group><issn pub-type="epub">2325-6753</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/JISP.2019.83017</article-id><article-id pub-id-type="publisher-id">JISP-31144</article-id><article-categories><subj-group subj-group-type="heading"><subject>JISP20190300000_87919749.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于目标检测的海上舰船图像超分辨率研究
  Research on Super-Resolution of Marine Ship Image Based on Target Detection
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>坤</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>天伟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>海军大连舰艇学院航海系，辽宁 大连</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>02</day><month>07</month><year>2019</year></pub-date><volume>08</volume><issue>03</issue><fpage>121</fpage><lpage>129</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    针对海上舰船图像有效像素在整体像素中占比小的问题，提出一种基于目标检测网络的超分辨率方法。该方法包含两个阶段，结合bicubic变换，逐步地将图像的清晰度从粗到细地进行恢复。首先，第一阶段通过目标检测网络，检测出原图像中需要超分辨率的区域，然后，第二阶段将对应区域通过bicubic变换调整至指定分辨率，而后通过生成对抗网络增强图像细节。最终在自建数据集上的实验结果表明，与传统方法和现有基于深度神经网路的超分辨率重建算法相比，该算法不仅图像视觉效果最好，而且在数据集上的峰值信噪比(PSNR)平均提高了0.79 dB，结构相似性(SSIM)平均提高了0.04，证明了该算法的有效性。
    Aiming at the problem that the effective pixels in the image of marine ships account for a small proportion in the total pixels, a super-resolution method based on target detection network is proposed. The method consists of two stages, combining with the bicubic transform, to restore the sharpness of the image from coarse to fine step by step. Firstly, in the first stage, super-resolution regions in the original image are detected through the target detection network. Then, in the se-cond stage, the corresponding regions are adjusted to the specified resolution by bicubic trans-formation, and then the image details are enhanced by generating the countermeasure network. Finally, the experimental results on the self-built dataset show that compared with the traditional method and the existing super-resolution reconstruction algorithm based on deep neural network, this algorithm not only has the best visual effect, but also improves the peak signal-to-noise ratio (PSNR) of the dataset by an average of 0.79 dB and the structural similarity (SSIM) by an average of 0.04, which proves the effectiveness of the algorithm. 
  
 
</p></abstract><kwd-group><kwd>目标检测，生成对抗网络，超分辨率，U-Net，PatchGAN, Target Detection</kwd><kwd> Generation of Countermeasure Network</kwd><kwd> Super-Resolution</kwd><kwd> U-Net</kwd><kwd> PatchGAN</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于目标检测的海上舰船图像超分辨率研究<sup> </sup></title><p>张坤，李天伟</p><p>海军大连舰艇学院航海系，辽宁 大连</p><p><img src="//html.hanspub.org/file/3-2670192x1_hanspub.png" /></p><p>收稿日期：2019年6月11日；录用日期：2019年6月28日；发布日期：2019年7月4日</p><disp-formula id="hanspub.31144-formula31"><graphic xlink:href="//html.hanspub.org/file/3-2670192x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>针对海上舰船图像有效像素在整体像素中占比小的问题，提出一种基于目标检测网络的超分辨率方法。该方法包含两个阶段，结合bicubic变换，逐步地将图像的清晰度从粗到细地进行恢复。首先，第一阶段通过目标检测网络，检测出原图像中需要超分辨率的区域，然后，第二阶段将对应区域通过bicubic变换调整至指定分辨率，而后通过生成对抗网络增强图像细节。最终在自建数据集上的实验结果表明，与传统方法和现有基于深度神经网路的超分辨率重建算法相比，该算法不仅图像视觉效果最好，而且在数据集上的峰值信噪比(PSNR)平均提高了0.79 dB，结构相似性(SSIM)平均提高了0.04，证明了该算法的有效性。</p><p>关键词 :目标检测，生成对抗网络，超分辨率，U-Net，PatchGAN</p><disp-formula id="hanspub.31144-formula32"><graphic xlink:href="//html.hanspub.org/file/3-2670192x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-2670192x7_hanspub.png" /> <img src="//html.hanspub.org/file/3-2670192x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>近年来，图像超分辨率技术得到了长足的发展，结合比较有效的深度学习技术，该方法的精度已经得到了较大的提高，并已经广泛应用于各种领域，如视频监控、医学成像、高清晰度电视、遥感、手机与数码相机等 [<xref ref-type="bibr" rid="hanspub.31144-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.31144-ref2">2</xref>]。本文算法的应用背景为提高海上远距离舰船目标图像的分辨率，主要应用于单幅海上舰船图像的超分辨率，该问题是一个不适定的逆问题，旨在从低分辨率(Low-Resolution, LR)图像中恢复出一个高分辨率(High-Resolution, HR)图像。如图1。</p><p>图1. 海上舰船图像</p><p>目前传统超分辨率算法在运行时存在以下问题：1) 由于远距离舰船图像的像素在整体图像所占比例较少，大部分像素为用户并不关心的海浪天空等区域，因此在运行传统算法的时，大部分运算时间应用到了无关的区域，不符合使用者的使用意图，而且运行效率低下。2) 在应用深度学习技术的超分辨率算法中，如果对整张图像进行学习，则算法使用的卷积神经网络不能有效学习舰船目标的特征，而很有可能把海浪天空的特征误认为舰船的特征，从而无法有效对舰船图像进行重建。基于以上两点原因，本文提出一种基于目标检测网络的超分辨率算法。</p></sec><sec id="s4"><title>2. 研究背景及现状</title><sec id="s4_1"><title>2.1. 目标检测算法</title><p>自从AlexNet在比赛中使用卷积神经网络进而大幅度提高了图像分类的准确率，便有学者尝试将深度学习应用到目标检测中。在这方面，主要有两种主流的算法：一类是结合region proposal、CNN网络的，基于分类的R-CNN系列目标检测框架(two stage)；另一类则是将目标检测转换为回归问题的算法(single stage) [<xref ref-type="bibr" rid="hanspub.31144-ref3">3</xref>]。虽然FasterRCNN算法是目前主流的目标检测算法之一，但是速度上并不能满足实时的要求。随后出现像YOLO，SDD这一类的算法逐渐凸显出其在速度上的优势。YOLO [<xref ref-type="bibr" rid="hanspub.31144-ref4">4</xref>] 算法的网络设计策略延续了GoogleNet [<xref ref-type="bibr" rid="hanspub.31144-ref5">5</xref>] 的核心思想，真正意义上实现了端到端的目标检测，且发挥了速度快的优势。YOLO采用以cell为中心的多尺度区域取代region proposal，舍弃了一些精确度以换取检测速度的大幅提升，检测速度可以达到45 f/s，足以满足实时要求。</p></sec><sec id="s4_2"><title>2.2. 超分辨率算法</title><p>传统的SISR的方法包括基于插值的算法，基于凸集投影法的算法等，但由于深度学习在计算机视觉领域的突破性进展，人们尝试在超分辨率问题中引入深度神经网络，通过构建深层次的网络进行端到端的训练来解决图像超分辨率重建问题 [<xref ref-type="bibr" rid="hanspub.31144-ref6">6</xref>]。SRCNN (super-resolution convolutional neural network) [<xref ref-type="bibr" rid="hanspub.31144-ref7">7</xref>] 是最早运用深度学习方法在LR与HR之间建立端到端映射的SISR模型，其输入图像采用了插值预处理的方法。Ledig等人 [<xref ref-type="bibr" rid="hanspub.31144-ref8">8</xref>] 基于GAN提出了一种用于图像超分辨率的生成对抗网络SRGAN，通过生成式和判别器的交替执行，充分提取高频信息。由于海上舰船图片成对采集，因此适用于使用建立像素到像素映射关系的超分辨率算法 [<xref ref-type="bibr" rid="hanspub.31144-ref9">9</xref>]，论文借鉴pix2pix [<xref ref-type="bibr" rid="hanspub.31144-ref10">10</xref>] 的算法设计。</p></sec></sec><sec id="s5"><title>3. 基于目标检测网络的超分辨率重建模型</title><p>本文所设计的模型是一种基于目标检测算法的模型，目的在于检测出整幅中用户感兴趣的区域，而后再对指定区域进行超分辨率放大，从而减少算法运行的时间，并更好的重建目标区域图像的边缘和纹理，本文的方法由两阶段组成，如图2所示。</p><p>图2. 模型结构</p><p>X为原图， X ′ 为X的退化图像，Y为X中用户感兴趣的区域，x为 中用户感兴趣的区域，y为生成网络生成的图像。T为目标检测网络，用于获取X中的(x, y, h, w, confidence)信息，G为图像生成网络，D为鉴别网络。</p><sec id="s5_1"><title>3.1. 目标检测卷积神经网络结构设计</title><p>目标检测网络T的结构设计借鉴了yoloV3中Darknet-53的神经网络设计，其候选框基于anchor候选框机制，其原理图如图3：</p><p>网络实际的预测值为，t<sub>x</sub>、t<sub>y</sub>、t<sub>w</sub>、b<sub>h</sub>根据上图中的四个公式计算得到预测框的中心点坐标和宽高b<sub>x</sub>，b<sub>y</sub>，b<sub>w</sub>，b<sub>h</sub>。其中c<sub>x</sub>、c<sub>y</sub>为当前grid相对于左上角grid偏移的grid数量。</p><p>图3所示σ(t)函数为logistic函数，将坐标归一化到0~1之间。最终得到的b<sub>x</sub>，b<sub>y</sub>为归一化后的相对于gridcell的值。p<sub>w</sub>，p<sub>h</sub>与groundtruth重合度最大的anchor框的宽和高。实际在使用中，将b<sub>w</sub>，b<sub>h</sub>也归一化到0~1，实际程序中的p<sub>w</sub>，p<sub>h</sub>为anchor的宽，高和feature map的宽，高的比值。最终得到的b<sub>w</sub>，b<sub>h</sub>为归一化后相对于anchor的值。</p><p>图3. Anchor候选框</p><p>卷积神经网络结构如图4：</p><p>图4. 卷积神经网络结构</p><p>该网络的特点在于使用了连续的3 &#215; 3和1 &#215; 1的卷积基层，简化了resnet神经网络，减少了检测时间。</p><p>在yoloV3损失函数中，需要关注4个信息，分别为位置信息：(x, y)，选中框的长宽：(w, h)，识别的置信度：confidence，识别出物体的类别：class。由于在舰船的超分辨率任务中，识别的目标只有舰船一类，所以为了简化算法，提高运算速度，本文算法只是用前三项作为算式函数内容，因此的到如下公式：</p><p>l o s s = λ c o o r d ∑ i = 0 l . w ∗ l . h ∑ j = 0 l . n 1 i j o b j [ ( x i − x ^ i ) 2 + ( y i − y ^ i ) 2 ]     + λ c o o r d ∑ i = 0 l . w ∗ l . h ∑ j = 0 l . n 1 i j o b j [ ( w i − w ^ i ) 2 + ( h i − h ^ i ) 2 ]     + λ n o o b j ∑ i = 0 l . h ∗ l . w ∑ j = 0 l . n 1 i j n o o b j ( C i − C ^ i ) 2 + λ o b j ∑ i = 0 l . h ∗ l . w ∑ j = 0 l . n 1 i j o b j ( C i − C ^ i ) 2</p><p>其中 λ c o o r d 为位置错误的权重， λ n o o b j 为没有object的候选框的置信度权值， λ o b j 为有object的候选框的置信度权值， 1 i j o b j 判断第i个栅格中的第j个候选框是否包含物体，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-2670192x20_hanspub.png" xlink:type="simple"/></inline-formula>判断是否有物体的中心落在i中， x i , y i 为实际的坐标值， x ^ i , y ^ i 为预测的坐标值， w i , h i 为实际候选框的宽和高， w ^ i , h ^ i 为预测候选框的宽和高， C i 为实际的类别，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-2670192x26_hanspub.png" xlink:type="simple"/></inline-formula>为预测的类别。</p></sec><sec id="s5_2"><title>3.2. 图像生成卷积神经网络结构设计</title><p>图像生成网络G的设计为U-Net结构，U-Net [<xref ref-type="bibr" rid="hanspub.31144-ref11">11</xref>] 是德国Freiburg大学模式识别和图像处理组提出的一种全卷积结构。和常见的先降采样到低维度，再升采样到原始分辨率的编解码(Encoder-Decoder)结构的网络相比，U-Net的区别是加入skip-connection，对应的featuremaps和decode之后的同样大小的featuremaps按通道拼(concatenate)一起，用来保留不同分辨率下像素级的细节信息，U-Net对提升细节的效果非常明显。</p></sec><sec id="s5_3"><title>3.3. 图像判别卷积神经网络结构设计</title><p>图像判别网络G为PatchGAN [<xref ref-type="bibr" rid="hanspub.31144-ref12">12</xref>]，该网络不是以整个图像的作为输入，而是以小的patch来进行的。把一副图像划分为N &#215; N个patch后，对于每一块进行上述的那个操作。可以发现当N = 1的时候，相当于逐像素进行了，当N = 256 (图像大小为256的话)，就是一幅图像的操作。最后将一张图片所有patch的结果取平均作为最终的判别器输出。实验发现当N = 70的时候，效果最好 [<xref ref-type="bibr" rid="hanspub.31144-ref10">10</xref>]。</p></sec><sec id="s5_4"><title>3.4. 超分辨网络的目标函数</title><p>构建好生成网络和判别网络结构后，设定目标函数。生成器G不断的尝试最小化下面的目标函数，而D则通过不断的迭代去最大化这个目标函数。目标函数分为两个部分，第一部分为生成对抗网络的损失函数，公式为：</p><p>L c G A N ( G , D ) = E x , y [ log D ( x , y ) ] + E x , z [ log ( 1 − D ( x , G ( x , z ) ) ) ]</p><p>第二部分为L1正则化补偿函数，公式为：</p><p>L L 1 ( G ) = E x , y , z [ ‖ y − G ( x , z ) ‖ 1 ]</p><p>所以最终的目标函数是：</p><p>G = arg min G max D L c G A N ( G , D ) + λ L L 1 (G)</p></sec></sec><sec id="s6"><title>4. 实验结果与分析</title><p>由于目前并没有针对舰船的图片数据集，本文实验使用项目自行采集的数据集作为训练集，该数据集由实际采集的舰船照片和网络爬取的舰船照片组成。其中实际采集的舰船照片为同一目标不同分辨率的图像，经测算高分辨率图像经双线性差值与高斯模糊函数相叠加的方式作为退化函数能有效的模拟低分辨率图像，而后将此退化函数应用到采集的舰船照片上，得到本文使用的数据集。舰船图像的原始像素大小的256 &#215; 256，经退化函数处理得到不同分辨率的低分辨率图像。在4倍缩小尺度下，低分辨率图像像素为128 &#215; 128，在9倍缩小尺度下，低分辨率图像像素为85 &#215; 85，在16倍缩小尺度下，低分辨率图像像素为64 &#215; 64。一张高分辨舰船图像和一张对应的低分辨率图像为一组图像。数据集分为训练图像、验证图像和测试图像三个子集，其中训练图像包含3000组图像，验证图像包含50组图像，测试图像包含50组图像。</p><p>本文实验通过Adam优化方法对网络进行训练。利用“步长”(step)策略调整学习率，初始学习率权重为2e−5，指数衰减率为0.999，调整系数gamma为0.5，最大迭代次数为10000。训练网络用机器配置为：i7-7700，16G内存，GTX1070Ti8G。</p><p>本文所提出的算法与现有的5种超分辨率的方法相比较，分别为bilinear，bicubic [<xref ref-type="bibr" rid="hanspub.31144-ref13">13</xref>]，discogan [<xref ref-type="bibr" rid="hanspub.31144-ref14">14</xref>]，pix2pix [<xref ref-type="bibr" rid="hanspub.31144-ref10">10</xref>]，pocs [<xref ref-type="bibr" rid="hanspub.31144-ref15">15</xref>]。由于本文使用的算法只对选定的图像位置进行超分比率，因此进行图像质量评价时为保证评价的公平性，也仅对选定区域进行比较评价。利用本文算法得到的位置信息，对各类算法得的图像进行截取，而后利用峰值信噪比(PSNR)和结构相似度(Structual Similarity, SSIM)对截取的原图像和生成图像进行比较，得到重建图像的评价值，比较&#215;4，&#215;9，&#215;16不同尺度下的效果，结果如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Test results on datasets using different super-resolution method</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >重建倍数</th><th align="center" valign="middle" >Psnr</th><th align="center" valign="middle" >Ssim</th><th align="center" valign="middle" >Time (ms)</th></tr></thead><tr><td align="center" valign="middle" >Yolo2pix</td><td align="center" valign="middle"  rowspan="6"  >&#215;4</td><td align="center" valign="middle" >24.549</td><td align="center" valign="middle" >0.791</td><td align="center" valign="middle" >77.8</td></tr><tr><td align="center" valign="middle" >discogan</td><td align="center" valign="middle" >23.401</td><td align="center" valign="middle" >0.724</td><td align="center" valign="middle" >13.2</td></tr><tr><td align="center" valign="middle" >pix2pix</td><td align="center" valign="middle" >23.891</td><td align="center" valign="middle" >0.731</td><td align="center" valign="middle" >12.9</td></tr><tr><td align="center" valign="middle" >pocs</td><td align="center" valign="middle" >18.948</td><td align="center" valign="middle" >0.631</td><td align="center" valign="middle" >2034</td></tr><tr><td align="center" valign="middle" >bilinear</td><td align="center" valign="middle" >23.266</td><td align="center" valign="middle" >0.752</td><td align="center" valign="middle" >8.8</td></tr><tr><td align="center" valign="middle" >bicubic</td><td align="center" valign="middle" >23.864</td><td align="center" valign="middle" >0.777</td><td align="center" valign="middle" >13.6</td></tr><tr><td align="center" valign="middle" >Yolo2pix</td><td align="center" valign="middle"  rowspan="6"  >&#215;9</td><td align="center" valign="middle" >22.791</td><td align="center" valign="middle" >0.715</td><td align="center" valign="middle" >78.2</td></tr><tr><td align="center" valign="middle" >discogan</td><td align="center" valign="middle" >22.031</td><td align="center" valign="middle" >0.647</td><td align="center" valign="middle" >13.3</td></tr><tr><td align="center" valign="middle" >pix2pix</td><td align="center" valign="middle" >22.154</td><td align="center" valign="middle" >0.652</td><td align="center" valign="middle" >13.1</td></tr><tr><td align="center" valign="middle" >pocs</td><td align="center" valign="middle" >17.549</td><td align="center" valign="middle" >0.581</td><td align="center" valign="middle" >2154</td></tr><tr><td align="center" valign="middle" >bilinear</td><td align="center" valign="middle" >21.815</td><td align="center" valign="middle" >0.682</td><td align="center" valign="middle" >8.8</td></tr><tr><td align="center" valign="middle" >bicubic</td><td align="center" valign="middle" >22.148</td><td align="center" valign="middle" >0.699</td><td align="center" valign="middle" >14.8</td></tr><tr><td align="center" valign="middle" >Yolo2pix</td><td align="center" valign="middle"  rowspan="6"  >&#215;16</td><td align="center" valign="middle" >21.746</td><td align="center" valign="middle" >0.675</td><td align="center" valign="middle" >78.3</td></tr><tr><td align="center" valign="middle" >discogan</td><td align="center" valign="middle" >21.050</td><td align="center" valign="middle" >0.627</td><td align="center" valign="middle" >13.3</td></tr><tr><td align="center" valign="middle" >pix2pix</td><td align="center" valign="middle" >21.047</td><td align="center" valign="middle" >0.636</td><td align="center" valign="middle" >13.2</td></tr><tr><td align="center" valign="middle" >pocs</td><td align="center" valign="middle" >16.758</td><td align="center" valign="middle" >0.532</td><td align="center" valign="middle" >2253</td></tr><tr><td align="center" valign="middle" >bilinear</td><td align="center" valign="middle" >20.958</td><td align="center" valign="middle" >0.629</td><td align="center" valign="middle" >9.6</td></tr><tr><td align="center" valign="middle" >bicubic</td><td align="center" valign="middle" >21.241</td><td align="center" valign="middle" >0.645</td><td align="center" valign="middle" >15.2</td></tr></tbody></table></table-wrap><p>表1. 使用不同超分辨率方法在数据集上的测试结果</p><p>从表1中可以看出本文算法在测试集上的PSNR，SSIM超过了其他超分辨率方法，但是相比于传统算法有一步选定区域的运算，因此相比于其他超分辨率算法，运行时间稍长。但这种劣势会随着舰船在所述图像中所占像素的减小而抵消。</p><p>为了直观的比较成像效果，将本算法的生成图像和其他算法生成的图像排列对比，实验结果如图5、图6、图7所示。</p><p>图5. 4倍缩放比率下不同超分辨率方法的结果比对</p><p>图6. 9倍缩放比率下不同超分辨率方法的结果比对</p><p>图7. 16倍缩放比率下不同超分辨率方法的结果比对</p><p>从数据上看，排除效果较差的凸集投影(pocs)算法，本文算法相比于其他算法，在重建倍数4下，PSNR最少提高0.658 dB，平均提高0.94 dB，SSIM最少提高0.014，平均提高0.045。在重建倍数9下，PSNR最少提高0.637 dB，平均提高0.75 dB，SSIM最少提高0.016，平均提高0.045。在重建倍数16下，PSNR最少提高0.505 dB，平均提高0.67 dB，SSIM最少提高0.03，平均提高0.04。总体来说，在数据集上的峰值信噪比(PSNR)平均提高了0.79 dB，结构相似性(SSIM)平均提高了0.04。人工观察效果来看，本文方法的整体视觉效果更好，在重建倍数4下，本文算法恢复了更多的船艇结构细节，其船体部分棱角更加分明，在重建倍数16下，船体上部分窗户细节恢复的比较真实。但是船体的侧面细节纹理没有有效重建。</p></sec><sec id="s7"><title>5. 结语</title><p>本文提出基于目标检测网络的图像超分辨率重建方法，该方法首先框选出需要超分辨率的区域，而后对选定区域使用基于GAN的超分辨率算法，从而达到提升图像清晰度的目的。第一阶段利用Darknet-53网络快速检测目标，获得目标的位置尺寸和置信度信息。第二阶段利用生成对抗模型，利用U-Net结构和PatchGAN构建了端对端的超分辨率模型。结合自建舰船数据库，让网络更有针对性的学习舰船结构特征，从而从低分辨率图像中恢复出拥有更多纹理信息的高分辨率图像。本文方法与其他超分辨率算法相比，不论是在主观重建效果还是客观评价标准上都有所提高的，且重建出的图像具有更高的质量并显示更精细的细节。在实际应用中，一幅画面中可能出现不止一艘船艇，本文使用的Darknet-53网络对图像不同部分一次性给出全部置信度参数，可以一次检测多艘船艇，因此本文算法可以满足实际使用的需求，但在检测小目标时算法效果有待考证，此为下一步研究方向。</p></sec><sec id="s8"><title>文章引用</title><p>张 坤,李天伟. 基于目标检测的海上舰船图像超分辨率研究 Research on Super-Resolution of Marine Ship Image Based on Target Detection[J]. 图像与信号处理, 2019, 08(03): 121-129. https://doi.org/10.12677/JISP.2019.83017</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.31144-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">李欣, 崔子冠, 朱秀昌. 超分辨率重建算法综述[J]. 电视技术, 2016, 40(9): 1-9.</mixed-citation></ref><ref id="hanspub.31144-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Glasner, D., Bagon, S. and Irani, M. (2009) Super-Resolution from a Single Image. In: Proceedings of the 2009 IEEE 12th International Conference on Computer Vision, IEEE, Piscataway, 349-356.  
https://doi.org/10.1109/ICCV.2009.5459271</mixed-citation></ref><ref id="hanspub.31144-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">郑伟成. 基于深度学习的目标检测算法综述[C]//中国计算机用户协会网络应用分会. 中国计算机用户协会网络应用分会2018年第二十二届网络新技术与应用年会论文集. 北京: 中国计算机用户协会网络应用分会, 北京联合大学北京市信息服务工程重点实验室, 2018: 5.</mixed-citation></ref><ref id="hanspub.31144-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J., Divvala, S., Girshick, R., et al. (2015) You Only Look Once: Unified, Real-Time Object Detection. Proceedings of CVPR 2015, Boston, 7-12 June 2015, 779-788. https://doi.org/10.1109/CVPR.2016.91</mixed-citation></ref><ref id="hanspub.31144-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Zhong, Z., Jin, L. and Xie, Z. (2015) High Performance off Line Handwritten Chinese Character Recognition Using Google Net and Directional Feature Maps. International Conference on Document Analysis and Recognition, Nancy, 23-26 August 2015, 846-850. https://doi.org/10.1109/ICDAR.2015.7333881</mixed-citation></ref><ref id="hanspub.31144-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">南方哲, 钱育蓉, 行艳妮, 赵京霞. 基于深度学习的单图像超分辨率重建研究综述[J/OL]. 计算机应用研究, 1-7.  
http://kns.cnki.net/kcms/detail/51.1196.TP.20181219.1953.003.html, 2019-04-03.</mixed-citation></ref><ref id="hanspub.31144-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Dong, C., Loy, C.C., He, K., et al. (2014) Learning a Deep Convolutional Network for Image Super-Resolution. In: Computer Vision ECCV 2014, Springer International Pub-lishing, Berlin, 184-199.  
https://doi.org/10.1007/978-3-319-10593-2_13</mixed-citation></ref><ref id="hanspub.31144-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Ledig, C., Theis, L., Huszar, F., et al. (2016) Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 105-114.  
https://doi.org/10.1109/CVPR.2017.19</mixed-citation></ref><ref id="hanspub.31144-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. (2014) Generative Adversarial Nets. In: Advances in Neural Information Processing Systems, The MIT Press, Cambridge, 2672-2680.</mixed-citation></ref><ref id="hanspub.31144-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Isola, P., Zhu, J.Y., Zhou, T., et al. (2016) Image-to-Image Translation with Conditional Adver-sarial Networks. Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 5967-5976.  
https://doi.org/10.1109/CVPR.2017.632</mixed-citation></ref><ref id="hanspub.31144-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Ronneberger, O., Fischer, P. and Brox, T. (2015) U-Net: Convolutional Networks for Biomedical Image Segmentation.</mixed-citation></ref><ref id="hanspub.31144-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Li, C. and Wand, M. (2016) Pre-Computed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks. European Conference on Computer Vision, Amsterdam, 8-16 October 2016, 702-716.  
https://doi.org/10.1007/978-3-319-46487-9_43</mixed-citation></ref><ref id="hanspub.31144-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Keys, R. (2003) Cubic Convolution Interpolation for Digital Image Pro-cessing. IEEE Transactions on Acoustics, Speech, and Signal Processing, 29, 1153-1160. https://doi.org/10.1109/TASSP.1981.1163711</mixed-citation></ref><ref id="hanspub.31144-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Kim, T. (2017) Learning to Discover Cross-Domain Relations with Gener-ative Adversarial Networks.</mixed-citation></ref><ref id="hanspub.31144-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">杨庆怡, 黄灿, 张琼. 基于POCS算法的超分辨率图像重建技术[J]. 计算机光盘软件与应用, 2012(16): 61-62.</mixed-citation></ref></ref-list></back></article>