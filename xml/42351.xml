<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.115132</article-id><article-id pub-id-type="publisher-id">CSA-42351</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210500000_10387785.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  多维注意力与部件关注的无监督行人重识别
  Unsupervised Person Re-Identification Based on Multi-Dimensional Attention and Part Focus Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>麻</surname><given-names>可可</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>薛</surname><given-names>丽霞</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>汪</surname><given-names>荣贵</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>杨</surname><given-names>娟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>合肥工业大学计算机与信息学院，安徽 合肥</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>08</day><month>05</month><year>2021</year></pub-date><volume>11</volume><issue>05</issue><fpage>1301</fpage><lpage>1304</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   行人重识别的目的是通过将人物的探测图像与图像库中的所有图像进行比较，从而在图像库中找到感兴趣的人。大多数的行人重识别算法都是在一些小的带标签的数据集上进行监督训练，直接将这些训练好的模型部署到真实世界的大型摄像机网络中可能会由于拟合不足而导致性能低下。因此，有必要在没有明确监督的情况下，自主地对模型进行训练。因此本文提出了一个多维注意力网络和部件关注网络联合学习的无监督行人重识别方法。首先多维注意力网络对行人图像复杂的高阶统计信息进行建模和利用，其次使用部件关注网络关注不同的部件，最后是一系列的损失函数来引导部件关注网络学习未标记数据集上的部件特征。在Market-1501和DukeMTMC-reID两个数据集上的实验结果表明，本文提出的方法有效并取得了显著的效果。 Person re-identification (Re-ID) aims at finding a person of interest in the image gallery by comparing the probe image of this person with all the gallery images. Most of the Re-ID algorithms conduct supervised training in some small labeled datasets, so directly deploying these trained models to the real-world large camera networks may lead to a poor performance due to underfitting. Therefore, it is necessary to train models without explicit supervision in an autonomous manner, and propose an unsupervised Re-ID method based on Multi-dimensional Attention Network (MDAN) and Part Focus Network (PFN). MDAN can model and utilize the complex higher-order statistics in-formation in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then there is a PFN, which is deployed into an improved spatial transform network (STN) so that each branch can focus on different parts of the pedestrian. We evaluate the proposed method on two public datasets, including Market-1501 and DukeMTMC-reID. Extensive experimental results show that the proposed method is effective and achieves impressive results. 
  
 
</p></abstract><kwd-group><kwd>行人重识别，多维注意力网络，部件关注网络, Person Re-Identification</kwd><kwd> Multi-Dimensional Attention Network</kwd><kwd> Part Focus Network</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>行人重识别的目的是通过将人物的探测图像与图像库中的所有图像进行比较，从而在图像库中找到感兴趣的人。大多数的行人重识别算法都是在一些小的带标签的数据集上进行监督训练，直接将这些训练好的模型部署到真实世界的大型摄像机网络中可能会由于拟合不足而导致性能低下。因此，有必要在没有明确监督的情况下，自主地对模型进行训练。因此本文提出了一个多维注意力网络和部件关注网络联合学习的无监督行人重识别方法。首先多维注意力网络对行人图像复杂的高阶统计信息进行建模和利用，其次使用部件关注网络关注不同的部件，最后是一系列的损失函数来引导部件关注网络学习未标记数据集上的部件特征。在Market-1501和DukeMTMC-reID两个数据集上的实验结果表明，本文提出的方法有效并取得了显著的效果。</p></sec><sec id="s2"><title>关键词</title><p>行人重识别，多维注意力网络，部件关注网络</p></sec><sec id="s3"><title>Unsupervised Person Re-Identification Based on Multi-Dimensional Attention and Part Focus Network</title><p>Keke Ma, Lixia Xue, Ronggui Wang, Juan Yang</p><p>School of Computer and Information, Hefei University of Technology, Hefei Anhui</p><p><img src="//html.hanspub.org/file/9-1542152x4_hanspub.png" /></p><p>Received: Apr. 17<sup>th</sup>, 2021; accepted: May 11<sup>th</sup>, 2021; published: May 18<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/9-1542152x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Person re-identification (Re-ID) aims at finding a person of interest in the image gallery by comparing the probe image of this person with all the gallery images. Most of the Re-ID algorithms conduct supervised training in some small labeled datasets, so directly deploying these trained models to the real-world large camera networks may lead to a poor performance due to underfitting. Therefore, it is necessary to train models without explicit supervision in an autonomous manner, and propose an unsupervised Re-ID method based on Multi-dimensional Attention Network (MDAN) and Part Focus Network (PFN). MDAN can model and utilize the complex higher-order statistics information in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then there is a PFN, which is deployed into an improved spatial transform network (STN) so that each branch can focus on different parts of the pedestrian. We evaluate the proposed method on two public datasets, including Market-1501 and DukeMTMC-reID. Extensive experimental results show that the proposed method is effective and achieves impressive results.</p><p>Keywords:Person Re-Identification, Multi-Dimensional Attention Network, Part Focus Network</p><disp-formula id="hanspub.42351-formula45"><graphic xlink:href="//html.hanspub.org/file/9-1542152x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/9-1542152x7_hanspub.png" /> <img src="//html.hanspub.org/file/9-1542152x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>行人重识别(person re-identification, Re-ID)由于在视频监控、人机交互领域的重要应用，近年来受到了越来越多的关注。在视频监控中，由于相机分辨率和拍摄角度的缘故，通常无法得到高质量的人脸图片，此时人脸识别 [<xref ref-type="bibr" rid="hanspub.42351-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref3">3</xref>] 失效。行人重识别就成了一个非常重要的替代技术，它广泛被认为是一个图像检索的子任务，给定目标人物的图像，行人重识别的目标是找到不同相机或同一相机在不同时间捕捉到的同一人的其他图像。</p><p>最近几年，越来越多的研究者尝试将行人重识别的研究与深度学习 [<xref ref-type="bibr" rid="hanspub.42351-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref6">6</xref>] 结合在一起，并取得了很好的效果。现有的行人重识别的工作大部分聚焦于监督学习 [<xref ref-type="bibr" rid="hanspub.42351-ref7">7</xref>] - [<xref ref-type="bibr" rid="hanspub.42351-ref14">14</xref>]，它们假设可以为每一对相机视图提供大量手动标记的匹配对，来学习该相机相对优化的特征表示或匹配度量函数。然而，这种规模的手动标签不仅在现实世界中收集起来非常困难，而且在许多情况下是不可行。例如可能没有足够的训练人员在每对相机视图中重新出现。这限制了它在真实应用场景中的扩展性和可用性。</p><p>针对上述问题，一种通用的解决方案是设计无监督模型 [<xref ref-type="bibr" rid="hanspub.42351-ref15">15</xref>] - [<xref ref-type="bibr" rid="hanspub.42351-ref26">26</xref>]。虽然一些无监督行人重识别算法已经被提出，但是与监督学习方法相比，它们的识别效果较弱。一个主要的原因是，如果没有跨视图的标记数据，则无监督方法由于不同的视角、背景和照明而缺乏跨视图下相同身份视觉特征变化所需要的必要知识。对此，我们提出了一个多维注意力模块来解决行人图片中的视角、背景等的干扰，该模块可以去除图片中杂乱无章的背景噪声，从而提取出具有鲁棒性的嵌入特征。</p><p>此外，在行人重识别中一些基于部件生成的方法 [<xref ref-type="bibr" rid="hanspub.42351-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref14">14</xref>] 被提出来并超过了基于全局特征的方法。例如，Rahul等人 [<xref ref-type="bibr" rid="hanspub.42351-ref12">12</xref>] 提出了基于图像切块的方法，将图像块按顺序送到一个LSTM网络中，最后进行特征融合。Wei等人 [<xref ref-type="bibr" rid="hanspub.42351-ref14">14</xref>] 提出了一种全局–局部对齐的特征描述子，来解决行人姿态变化的问题。虽然基于部件匹配效果超过了基于全局特征的方法，但是我们也发现了部件匹配存在的问题。那就是部件无法进行很好地对齐，从而影响了效果。为此，我们提出了部件关注模块来解决部件对齐的问题。该模块可以使部件能够很好地进行匹配，从而提取出具有鲁棒性的局部特征。</p><p>基于以上分析，本文提出了一个多维注意力网络和部件关注网络联合学习的无监督行人重识别方法。首先通过一个多维注意力模块来解决行人图片中的背景噪声的干扰；然后是一个部件关注网络，将它部署到一个改进的STN后，使得每个分支可以关注人体不同的部件，包括头部特征、上半身特征和下半身特征。</p><p>分别在两个已知的行人重识别的数据集上对本文提出的方法进行了评估，包括Market-151和DukeMTMC-reID。结果表明本文提出的方法取得了非常好的效果。</p></sec><sec id="s6"><title>2. 相关工作</title><p>在本节中，简要叙述行人重识别之前的工作，包括监督的行人重识别、无监督的行人重识别和行人重识别中的注意力机制。</p><sec id="s6_1"><title>2.1. 监督行人重识别</title><p>监督学习方法是解决行人重识别问题最常用的方法，包括基于表征的学习 [<xref ref-type="bibr" rid="hanspub.42351-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref8">8</xref>]、基于度量学习 [<xref ref-type="bibr" rid="hanspub.42351-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref11">11</xref>]、基于局部特征学习 [<xref ref-type="bibr" rid="hanspub.42351-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref14">14</xref>] 的行人重识别，它们主要得益于卷积神经网络的快速发展。</p><p>表征学习的方法是一种非常常见的行人重识别方法，它主要得益于卷积神经网络的快速发展。由于卷积神经网络可以自动从原始图像数据中根据任务需求自动提取表征特征，所以一些研究者把行人重识别问题看作分类或验证问题。例如，Zheng等人 [<xref ref-type="bibr" rid="hanspub.42351-ref7">7</xref>] 提出使用两种模型来学习行人描述符，分别是分类模型和验证模型。其中分类模型对输入图片进行预测，根据预测的ID来计算分类误差损失。验证模型融合两张图片的特征，判断两张图片是否为同一身份。Lin等人 [<xref ref-type="bibr" rid="hanspub.42351-ref8">8</xref>] 提出了基于属性和身份学习的方法，提出的模型不仅会学习到行人的ID特征，还可以学习到行人的属性特征，这大大增强了模型的泛化能力。</p><p>度量学习旨在通过网络学习出两张图片的相似度。在行人重识别问题上，具体表现为同一个人的不同图片相似度大于不同人图片的相似度。例如，Rahul等人 [<xref ref-type="bibr" rid="hanspub.42351-ref9">9</xref>] 提出了一个基于Siamese网络的方法，将一对图片输入到孪生网络中，使用交叉熵损失判断两张图片的相似度，使得正样本对之间的距离逐渐变小，负样本对之间的距离逐渐变大。Cheng等人 [<xref ref-type="bibr" rid="hanspub.42351-ref10">10</xref>] 提出了基于三元组损失的网络，需要输入三张图片(一对正样本对，一对负样本对)，使用三元组损失拉近正样本对的距离，推开负样本对的距离。Alexander等人 [<xref ref-type="bibr" rid="hanspub.42351-ref11">11</xref>] 提出了一个难样本采样三元组损失，主要解决的是三元组损失采样抽取出来的都是简单易区分样本对的问题。</p><p>早期行人重识别的研究大家只关注全局特征，就是用整张图片得到一个特征向量进行图像检索。但是仅仅使用全局特征达不到理想的效果，所以一些研究者开始关注局部特征。例如，Rahul等人 [<xref ref-type="bibr" rid="hanspub.42351-ref12">12</xref>] 提出了基于图像切块的方法，将图片垂直切割为若干块，按顺序送到一个LSTM网络中，最后进行特征融合。为了解决图像切块无法对齐问题，Zheng等人 [<xref ref-type="bibr" rid="hanspub.42351-ref13">13</xref>] 提出了基于姿态估计与关节点定位的方法，首先使用姿态估计模型估计出行人的14个关键点，然后使用仿射变换使得相同的关键点对齐，这些关键点可以将人体分为不同的部位。Wei等人 [<xref ref-type="bibr" rid="hanspub.42351-ref14">14</xref>] 提出了一种全局–局部对齐的特征描述子，来解决行人姿态变化的问题。</p></sec><sec id="s6_2"><title>2.2. 无监督行人重识别</title><p>近年来一些研究者提出了具有手工特征的非监督方法 [<xref ref-type="bibr" rid="hanspub.42351-ref15">15</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref16">16</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref17">17</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref18">18</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref19">19</xref>]。然而，与监督学习方法相比，它们的再识别性能较差。例如，Zheng等人 [<xref ref-type="bibr" rid="hanspub.42351-ref15">15</xref>] 提出了一种无监督的BOW描述符，受图像搜索系统的启发，将每个行人图片表示成视觉词汇直方图，并支持全局快速匹配。Liao等人 [<xref ref-type="bibr" rid="hanspub.42351-ref17">17</xref>] 提出一个特征提取方法LOMO，主要着眼于光照和视角问题，基于HSV颜色直方图和XQDA度量学习。Tetsu等人 [<xref ref-type="bibr" rid="hanspub.42351-ref19">19</xref>] 提出了一种基于像素特征的层次分布描述符，通过分层高斯分布描述图像中的局部区域。</p><p>由于卷积神经网络在监督行人重识别领域中取得了无可比拟的性能，因此一些研究者在无监督行人重识别中也使用了深度学习的方法，并取得了一定的效果。目前基于深度学习的无监督方法主要分为基于聚类的无监督行人重识别和基于跨域的无监督行人重识别。</p><p>聚类分析 [<xref ref-type="bibr" rid="hanspub.42351-ref20">20</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref21">21</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref22">22</xref>] 是一种长期存在的无监督深度学习方法，近几年，基于聚类的无监督行人重识别被提出。例如，Fan等人 [<xref ref-type="bibr" rid="hanspub.42351-ref20">20</xref>] 提出了基于聚类和微调的无监督学习方法，采用K-means聚类进行标签估计，逐步选取可靠的图像，并利用这些图像对深度神经网络进行微调，学习识别特征。Yu等人 [<xref ref-type="bibr" rid="hanspub.42351-ref21">21</xref>] 提出了无监督非对称距离度量学习的方法，基于非对称的K-means聚类来实现试图不变性。Lin等人 [<xref ref-type="bibr" rid="hanspub.42351-ref22">22</xref>] 提出了一种自底向上的聚类框架，该框架根据预定义的标准将聚类分层组合，基于一个非常简单的最小距离准则和一个集群大小正则化项。然而，聚类得到的图像的伪标签可能是有噪声的，因为它可能会将相同的标签分配给具有不同身份的相似图片，使得区分相似的人更加困难。</p><p>最近，一些研究者提出了跨域迁移学习的无监督行人重识别 [<xref ref-type="bibr" rid="hanspub.42351-ref23">23</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref24">24</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref25">25</xref>] [<xref ref-type="bibr" rid="hanspub.42351-ref26">26</xref>] 的方法，主要利用带有标签的数据集来提高模型在目标数据集上的性能。例如，Peng等人 [<xref ref-type="bibr" rid="hanspub.42351-ref23">23</xref>] 提出了无监督跨数据集的迁移学习，通过字典学习机制将人的外观视图不变表示从带有标签的源数据集转移到无标签的目标数据集中，并获得了更好的性能。Zhong等人 [<xref ref-type="bibr" rid="hanspub.42351-ref24">24</xref>] 引入了摄像机样式转换方法来处理多个视图中的图像样式变化，并学习了一个摄像机不变的描述子空间。Wang等人 [<xref ref-type="bibr" rid="hanspub.42351-ref25">25</xref>] 首先利用源域上的属性进行训练，然后学习身份和属性的联合特征表示。</p><p>Gao等人 [<xref ref-type="bibr" rid="hanspub.42351-ref26">26</xref>] 首先提出了“相机感知”域适应，以减少源域和目标域之间的差异，然后利用目标域中每个摄像机的时间连续性来创建有区别的信息。</p></sec><sec id="s6_3"><title>2.3. 行人重识别中的注意力机制</title><p>最近几年，注意力机制在计算机视觉 [<xref ref-type="bibr" rid="hanspub.42351-ref27">27</xref>] 领域取得了巨大的成功，例如目标检测 [<xref ref-type="bibr" rid="hanspub.42351-ref28">28</xref>]、图像分割 [<xref ref-type="bibr" rid="hanspub.42351-ref29">29</xref>] 和姿态估计 [<xref ref-type="bibr" rid="hanspub.42351-ref30">30</xref>]。对于行人重识别 [<xref ref-type="bibr" rid="hanspub.42351-ref31">31</xref>] - [<xref ref-type="bibr" rid="hanspub.42351-ref39">39</xref>]，也存在着许多基于注意力机制的方法。</p><p>这些方法的共同策略是将注意力机制整合到深层模型中，以解决行人重识别的定位和不对齐问题。Li等人 [<xref ref-type="bibr" rid="hanspub.42351-ref34">34</xref>] 提出了一种协调注意力模型(harmonious attention, HA)，该模型从全身图像中定位身体部位，同时学习多尺度特征图。Song等人 [<xref ref-type="bibr" rid="hanspub.42351-ref31">31</xref>] 提出了MGCAM移除背景、视点和姿态等因素对行人的影响，使网络更关注于前景区域图像。Wang等人 [<xref ref-type="bibr" rid="hanspub.42351-ref32">32</xref>] 提出了一种基于新的无参数空间注意力，将特征图上激活之间的空间关系引入到模型中。Cheng等人 [<xref ref-type="bibr" rid="hanspub.42351-ref35">35</xref>] 提出了ABD-Net，该模型将注意力模块和多样性正则化融入整个网络，其中注意力机制结合了通道和空间信息，避免注意力机制过度集中于前景，正则化可以增强隐藏激活和权重的多样性。Zhao等人 [<xref ref-type="bibr" rid="hanspub.42351-ref37">37</xref>] 提出了一种基于部件映射检测器的部件对齐表示方法，用于每个预定义的身体部件。Si等人 [<xref ref-type="bibr" rid="hanspub.42351-ref39">39</xref>] 提出了一种基于类间和类内注意模块的双重注意匹配网络，用于捕获视频序列的上下文信息。</p></sec></sec><sec id="s7"><title>3. 方法</title><p>在本节中，详细介绍了改进的无监督行人重识别学习框架，该框架如图1所示，它是基于ResNet-50的卷积神经网络。</p><p>首先，是一个多维注意力模块，将ResNet-50分解为两个部分，即P1 (from conv1 to layer2)与P2 (from layer3 to GAP)。P1用于将给定的图像从原始像素空间编码为中层特征空间，P2用于将注意信息编码为高层特征空间，可以对数据进行分类。在P1与P2之间放置不同维度的注意力模块，生成多样化的注意力特征，强化所学知识的丰富性。在Sec.3.1中详细介绍。</p><p>其次，使用一个改进的STN形成三个分支，在每个分支后加入一个部件关注模块，并使用带有标签的数据集来预训练部件关注模块，使得每个分支可以关注人体不同的部件，包括头部特征，上半身特征和下半身特征。在本节中的Sec.3.2详细介绍。</p><p>最后是损失函数模块，首先使用了一个存储器来存储部件的更新特征，然后是一系列的损失函数来引导部件关注网络学习未标记数据集上的部件特征。在本节的Sec.3.3中详细介绍。</p><p>图1. 网络结构图</p><sec id="s7_1"><title>3.1. 多维注意力模块</title><p>注意力机制在行人重识别中变得更有吸引力，因为它能够将可用资源分配给输入信号中信息最丰富的部分，同时我们可以得到注意力方法的一般情况。具体的说，对于给定的三维张量X，我们可以得到一个卷积激活输出。其中 X ∈ R C &#215; H &#215; W ，C、H、W分别表示通道数，高度和宽度。我们的目标是将卷积激活输出进行重新加权，将此过程表述为：</p><p>y = F ( X ) ⊙ X (1)</p><p>其中 y ∈ R C &#215; H &#215; W 是注意力模块的激活输出， ⊙ 表示两个矩阵对应元素相乘， F ( X ) 是一个权重项，每个元素的值在区间 [ 0 , 1 ] 之间。</p><p>然而，目前这些常用的注意力方法要么是粗糙的，要么是一维的，如图2中 D = 1 所示，仅限于挖掘简单而粗糙的信息。为此我们提出了高维注意力模块，如图2所示。以 D = 3 为例，它有三个分支，对于第r个分支，我们使用r组通道数相同的1 &#215; 1的卷积核 { U s r } s = 1 , ⋯ , r 生成通道为 D r 的一组特征图 { Z s r } s = 1 , ⋯ , r 。操作如下：</p><p>Z s r = W s r ∗ X (2)</p><p>其中 W s r ∈ ℝ 1 &#215; 1 &#215; C &#215; D r 为卷积核的权重，X为输入的一个三维张量。</p><p>将这组特征图结合在一起可得第r个分支的特征图为：</p><p>Z r = Z r 1 ⊙ ⋯ ⊙ Z r r (3)</p><p>然后再使用一组1 &#215; 1的卷积核 α r 来生成通道数为C的特征图，可以得到：</p><p>F ( X ) = sigmoid ( ∑ r = 1 3 α r T δ ( z r ) ) (4)</p><p>其中 δ 代表ReLU激活函数， F ( X ) 作为一个权重项，使用sigmoid激活函数将其中的每个元素值映射到 [ 0 , 1 ] 的区间，将 F ( X ) 代入公式(1)就可以得到高维注意力激活特征。</p><p>最后，将不同维度的注意力模块组合在一起，共同学习更具鲁棒性的特征。</p><p>图2. 多维注意力模块</p></sec><sec id="s7_2"><title>3.2. 部件关注模块</title><p>我们首先使用一个改进的STN形成三个分支，在每个分支后加入一个部件关注模块，使得每个分支可以关注人体不同的部件。该模块由两部分组成，如图3所示。对于第一个分支，它关注部件的空间信息，对于第二个分支，它关注着部件的通道信息。</p><p>图3. 部件关注网络</p><p>对于第一个分支，假设输入层的特征图为U，令 U = [ u 1 , 1 , u 1 , 2 , ⋯ , u i , j , ⋯ , u H , W ] ， u i , j ∈ ℝ 1 &#215; 1 &#215; C 对应着 ( i , j ) 的空间位置，其中 i ∈ { 1 , 2 , ⋯ , H } ， j ∈ { 1 , 2 , ⋯ , W } 。通过通道数为1的1&#215;1卷积实现空间压缩操作，如下： q = W s q ∗ U ，其中 W s q ∈ ℝ 1 &#215; 1 &#215; C &#215; 1 为权重，生成一个映射张量 q ∈ ℝ H &#215; W 。映射的每一个 q i , j 表示空间位置 ( i , j ) 的所有通道C的线性组合，再经过sigmod对其进行归一化操作到 [ 0 , 1 ] 。操作如下：</p><p>U ^ 1 = [ σ ( q 1 , 1 ) u 1 , 1 , ⋯ , σ ( q i , j ) u i , j , ⋯ , σ ( q H , W ) u H , W ] (5)</p><p>每一个 σ ( q i , j ) 的值代表着特征图中空间位置坐标 ( i , j ) 的重要性。</p><p>对于第二个分支，将输入层特征图 U = [ u 1 , u 2 , ⋯ , u C ] 看作是信道 u i ∈ ℝ H &#215; W 的组合，经过全局池化层之后得到向量 z ∈ ℝ 1 &#215; 1 &#215; C ，每个位置k的值为：</p><p>z k = 1 H &#215; W ∑ i H ∑ j W u k ( i , j ) (6)</p><p>然后经过两次全连接层，过程如下：</p><p>z ^ = W 1 ( δ ( W 2 z ) ) (7)</p><p>其中 W 1 ∈ ℝ C &#215; C 2 ， W 2 ∈ ℝ C 2 &#215; C ， W 1 ， W 2 分别为全连接层的权重， δ ( ⋅ ) 代表着RelU激活操作。再经过sigmod对 z ^ 进行归一化操作到 [ 0 , 1 ] 。操作如下：</p><p>U ^ 2 = [ σ ( z ^ 1 ) u 1 , σ ( z ^ 2 ) u 2 , ⋯ , σ ( z ^ i ) u i , ⋯ , σ ( z ^ C ) u C ] (8)</p><p>σ ( z ^ i ) 代表的信息就是第i个通道 u i 的重要性程度。</p><p>最后，将上述的两个模块结合从而得到此时的部件特征图 U ^ = U ^ 1 + U ^ 2 。</p><p>为了更好地学习到行人的部件特征，我们使用带有标签的数据集MSMT17 [<xref ref-type="bibr" rid="hanspub.42351-ref43">43</xref>] 对部件关注网络进行预训练。对于每一张标签为t的训练图像I，其中t为目标类的索引。对于每个分支i，可以得到部件特征图，即， M i ∈ ℝ h &#215; w &#215; c ，为了使部件关注模块能够更好的关注我们感兴趣的部件，我们在每一个分支后添加1 &#215; 1卷积和一个Sigmoid函数将值映射到(0,1)来获得每个部件的掩码 a i ∈ ℝ h &#215; w 。</p><p>a i ( x , y ) = 1 1 + exp ( − ( W s q ∗ M i ) ) (9)</p><p>其中W为1 &#215; 1卷积的权重。由于目标是引导不同的分支来关注不同的身体区域，因此不同分支 a i 的非零区域应该是不重叠的。因此，我们提出了一个损失函数来对重叠区域进行惩罚，定义如下：</p><p>L o = 1 N ∑ x , y ( a 1 ⊙ a 2 ⊙ ⋯ ⊙ a N ) (10)</p><p>其中 ⊙ 表示逐元素乘法，N表示分支数。</p><p>对部件特征 M i 进行全局平均池化后，可以得到部件的类分数 S i ，通过Softmax函数进一步归一化为概率分布 y i ∈ ℝ C ，因此第i个分支的分类损失被计算为预测概率 y i 与真实值之间的交叉熵。</p><p>L i d i = − log ( y t i ) (11)</p><p>其中t是目标类的索引，所有分支的损失相加构成了识别损失， L i d = ∑ i = 1 N L i d i 。</p><p>最终的损失函数是 L i d 和 L o 的加权和。</p><p>L t o t a l = L i d + α L o (12)</p><p>其中 α 是权重，并且在后面的实验中我们令 α = 1 。</p></sec><sec id="s7_3"><title>3.3. 损失函数</title><sec id="s7_3_1"><title>3.3.1. 特征存储</title><p>在此我们使用存储器来存储部件的最新特征，存储器是一个键值对结构(key-value, K-V)。K用来存储每个图像的索引。V用来存储每一张图片的部件特征，包括三部分，分别用来存储行人的头部特</p><p>征，上半身特征和下半身特征。如图所示，每个部件特征的存储特征可以表示为： V m = { v j m } j = 1 N 其中 m ∈ { 1 , 2 , 3 } 分别代表着三个部件特征，N表示训练图片的个数。我们通过以下公式对存储体进行更新：</p><p>v j , t m = { ( 1 − l ) &#215; v j , t − 1 m + l &#215; x j , t m ,         t &gt; 0 x j , t m ,                                                       t = 0 (13)</p><p>其中t是训练轮回数(epoch)， t = 0 表示存储器的初始状态。l是 v j , t m 的学习率， x j , t m 是第t个轮回时第j张图片的第m个部件的特征。</p></sec><sec id="s7_3_2"><title>3.3.2. 基于部件的损失</title><p>我们提出了一种基于部件的损失函数(PBL)，通过将相似部件的特征拉在一起，将不同的部件推开，从而引导部件提取网络学习未标记数据集上的部件特征。我们可以使用式13对存储器进行不断的更新，在此过程中，我们计算每个x与每个v的差异。详细的说，对于每一个 x i m ，通过计算 x i m 与存储体</p><p>V m = { v j m } j = 1 N 之间的 l 2 距离从而获得 x i m 的k个最近的集合 K i m ，为了拉近特征相似的部件，推远特征不</p><p>相似的部件，我们提出了一个基于部件的损失函数，通过下式计算PBL：</p><p>L p m = − log ∑ v j m ∈ K i m     e − s 2 ‖ x i m − v j m ‖ 2 2 ∑ j = 1 , j ≠ i N     e − s 2 ‖ x i m − v j m ‖ 2 2 (14)</p><p>其中s是比例数，最小化 L p m 会促使模型将与 x i m 相似的部件 K i m 拉近，同时在特征空间中将与 x i m 不相似的部件 { v j m | v j m ∉ K i m } 推离。</p><p>为了对所有的部件进行约束，我们提出了基于部件约束的损失(BC)，它包含三部分，头部与上半身的约束，上半身与下半身的约束及整体的约束。这些约束就是将人体的部件进行融合，从而得到部</p><p>件的约束特征。此时的样本约束特征可以表示为 { x i n } ，存储体可以表示为 { v j n } j = 1 N 。可以得到基于部件</p><p>约束的损失函数为：</p><p>L p c n = − log ∑ v j ∈ K i     e − s 2 ‖ x i n − v j n ‖ 2 2 ∑ j = 1 , j ≠ i N     e − s 2 ‖ x i n − v j n ‖ 2 2 (15)</p><p>其中s是比例数， n = 1 表示头部与上半身的约束， n = 2 表示上半身与下半身的约束， n = 3 表示整体的约束。</p></sec><sec id="s7_3_3"><title>3.3.3. 基于图像级的损失</title><p>为了进一步挖掘图像间的潜在信息并最小化类内差异同时最大化类间差异，我们提出了一个三元组损失对全局特征进一步的约束。在我们的实验中，我们定义了一系列随机变换来生成代理正样本，包括图像的裁剪、缩放、旋转、亮度、对比度和饱和度。然后为每个真实样本生成一个代理正样本。</p><p>同时我们通过循环排序挖掘到负样本对，如图4所示：给定一小批量样本特征 { x i } i = 1 B ，每个样本 x i 的排序结果可以基于成对相似度度量来得到。我们使用 l 2 距离来度量成对的相似性，从而得到图像 x i 的排序列表 N i 。然后我们按顺序遍历排序列表 N i 。对于每个候选样本 x j ∈ N i ，我们使用相同的方法来计算排序列表。最后，如果 x i 不是 x j 的top-r最近邻，那么我们认为 x j 很可能是 x i 的负样本。此外，由于难分的负样本对可以更有效地学习判别特征，因此我们只考虑第一个符合上述条件的候选样本 x j 。我们把这个负的候选项表示成负样本 n i 。在这里我们给定三元组损失的定义：</p><p>L v = max { ‖ x i − p i ‖ 2 − ‖ x i − n i ‖ 2 + m , 0 } (16)</p><p>其中m代表着三元组损失的间隔(margin)， p i 代表正样本， n i 代表负样本。</p><p>图4. 循环排序模块</p></sec><sec id="s7_3_4"><title>3.3.4. 最终的损失</title><p>因此，我们模型中的每一幅图像的总体损失函数可以表示为：</p><p>L = L v + λ 1 M ∑ i = 1 M     L p m + μ 1 N ∑ i = 1 N     L p c n (17)</p><p>其中 λ 是部件损失的权重系数， μ 是部件约束损失的权重系数。</p></sec></sec></sec><sec id="s8"><title>4. 实验</title><p>在这一部分中，主要是对所提出的方法进行评估。首先在第4.1节中介绍使用的数据集与评估指标，其次在4.2节中的介绍了实验细节。然后在4.3节中，进行消融实验来验证组件的有效性。最后将本文提出的方法与其他最先进的算法进行比较。</p><sec id="s8_1"><title>4.1. 数据集与评估指标</title><p>Market-1501 [<xref ref-type="bibr" rid="hanspub.42351-ref40">40</xref>] 是一个大型的行人重识别数据集，它包含1501个id的32,688张图像。这些图片是在校园里被6台摄像机拍了下来。数据集分为两部分：其中751个id的12,936张图像用于训练，750个id的19,732张图像用于测试。</p><p>DukeMTMC-reID [<xref ref-type="bibr" rid="hanspub.42351-ref41">41</xref>] 是行人跟踪数据集DukeMTMC [<xref ref-type="bibr" rid="hanspub.42351-ref42">42</xref>] 的子集，它包含了从8个相机中收集到的36,411张1812个id的图片。与Market-1501数据集的划分类似，数据集包含来自702个ID的16,522个训练图像样本，来自其他1,110个ID的2228个查询图像样本和17,661个待匹配的图像库样本。</p><p>MSMT17 [<xref ref-type="bibr" rid="hanspub.42351-ref43">43</xref>] 是目前最大的行人重识别数据集，它包含了15个摄像头中4101个id的126,441张图像。与Market-1501和DukeMTMC-reID类似，数据集被分为两部分：用于训练的1041个id的32,621张图像，用于查询的3060个id的82,161张图像和11,659个待匹配的图像库样本。</p><p>Evaluation Metrics：本文使用 rank1、rank5、rank10和平均精度(mAP)来评估这三个数据集。</p></sec><sec id="s8_2"><title>4.2. 实验细节</title><p>实验基于Linux环境下的开源Pytorch框架 [<xref ref-type="bibr" rid="hanspub.42351-ref36">36</xref>]，硬件基础为NVIDIA GeForce RTX 2080GPU。使用ResNet-50 [<xref ref-type="bibr" rid="hanspub.42351-ref44">44</xref>] 作为基本CNN模型，并在ImageNet数据集 [<xref ref-type="bibr" rid="hanspub.42351-ref45">45</xref>] 上进行预训练，首先将ResNet-50 [<xref ref-type="bibr" rid="hanspub.42351-ref44">44</xref>] 分解成两部分P1和P2，在P1和P2之间放置不同维数的注意力模块，生成不同维度的注意图，每个注意力模块的输出特征的维数为256。然后将不同的注意力模块特征相融合并使用一个改进的空间变换网络(STN)形成三个分支，在每个分支后加入一个部件关注网络，并使用带有标签的数据集MSMT17 [<xref ref-type="bibr" rid="hanspub.42351-ref43">43</xref>] 对部件关注网络进行预训练，使之关注人体的不同部件，即生成3个256维的部件特征(头部，上半身，下半身)。对于生成的每个部件，使用特征存储器来进行部件特征存储更新，更新率 设置为0.1。同时将DukeMTMC-reID [<xref ref-type="bibr" rid="hanspub.42351-ref41">41</xref>] 数据集上的比例系数s设置为10，Market-1501 [<xref ref-type="bibr" rid="hanspub.42351-ref40">40</xref>] 上的设置为30。在对未标记的数据集进行训练时，随机采样图像，并将其大小调整为384 &#215; 128。每个mini-batch由8个真实样本组成，另外还有8个代理正样本用于计算三元组损失。使用SGD [<xref ref-type="bibr" rid="hanspub.42351-ref32">32</xref>] 作为优化算法，初始的学习率设置为0.0001，每20个轮回数(epoch)衰减0.1。在未标记的数据集上对模型进行了60个轮回数(epoch)的训练。测试时，将同一幅图像的部件特征拼接在一起，计算出两两距离。</p></sec><sec id="s8_3"><title>4.3. 消融实验</title><p>为了对本文提出的方法性能进行验证，在Market-1501 [<xref ref-type="bibr" rid="hanspub.42351-ref40">40</xref>] 和DukeMTMC-reID [<xref ref-type="bibr" rid="hanspub.42351-ref41">41</xref>] 数据集上进行了大量的消融实验，来分析每个组件的有效性。实验结果如表所示。</p><p>注意力维度个数对实验的影响：从表1可以看出，维度个数为4实验效果最好。</p><p>部件个数对实验的影响：从表2可以看出，部件个数对实验有一定的影响。多次实验结果表明，部件个数为3时，效果最好。</p><p>不同模块对实验的影响：从表3可以看出，“MDAN”的结果比“基本网络”的结果要好；“PFN”的结果比“基本网络”的结果要好，“MDAN”与“PFN”联合学习的效果最好。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The influence of the number of attention dimensions on the experiment (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >维度个数</th><th align="center" valign="middle"  colspan="2"  >Market-1501</th><th align="center" valign="middle"  colspan="2"  >DukeMTMC-reID</th></tr></thead><tr><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >mAP</td><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >mAP</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >70.9</td><td align="center" valign="middle" >40.7</td><td align="center" valign="middle" >70.2</td><td align="center" valign="middle" >50.1</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >72.1</td><td align="center" valign="middle" >41.5</td><td align="center" valign="middle" >71.8</td><td align="center" valign="middle" >52.0</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >73.2</td><td align="center" valign="middle" >42.3</td><td align="center" valign="middle" >73.5</td><td align="center" valign="middle" >53.8</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >71.5</td><td align="center" valign="middle" >40.9</td><td align="center" valign="middle" >71.1</td><td align="center" valign="middle" >51.8</td></tr></tbody></table></table-wrap><p>表1. 注意力维度个数对实验的影响</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> The influence of different parts number on experiment (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >部件个数</th><th align="center" valign="middle"  colspan="2"  >Market-1501</th><th align="center" valign="middle"  colspan="2"  >DukeMTMC-reID</th></tr></thead><tr><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >mAP</td><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >mAP</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >70.3</td><td align="center" valign="middle" >39.7</td><td align="center" valign="middle" >69.2</td><td align="center" valign="middle" >49.3</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >72.1</td><td align="center" valign="middle" >41.9</td><td align="center" valign="middle" >72.6</td><td align="center" valign="middle" >52.1</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >71.4</td><td align="center" valign="middle" >40.3</td><td align="center" valign="middle" >71.1</td><td align="center" valign="middle" >50.6</td></tr></tbody></table></table-wrap><p>表2. 不同部件个数对实验的影响</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> The influence of different modules (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >模块</th><th align="center" valign="middle"  colspan="2"  >Market-1501</th><th align="center" valign="middle"  colspan="2"  >DukeMTMC-reID</th></tr></thead><tr><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >mAP</td><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >mAP</td></tr><tr><td align="center" valign="middle" >基本网络</td><td align="center" valign="middle" >64.3</td><td align="center" valign="middle" >36.5</td><td align="center" valign="middle" >66.9</td><td align="center" valign="middle" >45.7</td></tr><tr><td align="center" valign="middle" >MDAN</td><td align="center" valign="middle" >73.2</td><td align="center" valign="middle" >42.3</td><td align="center" valign="middle" >73.5</td><td align="center" valign="middle" >53.8</td></tr><tr><td align="center" valign="middle" >PFN</td><td align="center" valign="middle" >72.1</td><td align="center" valign="middle" >41.9</td><td align="center" valign="middle" >72.6</td><td align="center" valign="middle" >52.1</td></tr><tr><td align="center" valign="middle" >MDAN + PFN</td><td align="center" valign="middle" >76.2</td><td align="center" valign="middle" >45.1</td><td align="center" valign="middle" >75.6</td><td align="center" valign="middle" >55.9</td></tr></tbody></table></table-wrap><p>表3. 不同模块对在两个数据集上对实验的影响</p></sec><sec id="s8_4"><title>4.4. 对比实验</title><p>将本文提出的模型与其他先进的无监督行人重识别模型进行了比较，包括：1) 基于手工制作的模型LOMO [<xref ref-type="bibr" rid="hanspub.42351-ref17">17</xref>]、BOW [<xref ref-type="bibr" rid="hanspub.42351-ref40">40</xref>] 和UDML [<xref ref-type="bibr" rid="hanspub.42351-ref24">24</xref>]；2) 基于伪标签学习的模型PUL [<xref ref-type="bibr" rid="hanspub.42351-ref20">20</xref>]、DBC [<xref ref-type="bibr" rid="hanspub.42351-ref21">21</xref>] 和BUC [<xref ref-type="bibr" rid="hanspub.42351-ref22">22</xref>]；3) 基于无监督域自适应模型TJ-AIDL [<xref ref-type="bibr" rid="hanspub.42351-ref25">25</xref>]、HHL [<xref ref-type="bibr" rid="hanspub.42351-ref26">26</xref>]、UCDA-CCE [<xref ref-type="bibr" rid="hanspub.42351-ref46">46</xref>]、SSL-CCE [<xref ref-type="bibr" rid="hanspub.42351-ref47">47</xref>] 和ECN [<xref ref-type="bibr" rid="hanspub.42351-ref48">48</xref>]。</p><p>其中表4是Market-1501数据集的比较结果，表5是DukeMTMC-reID数据集的比较结果。可以看到，本文提出的方法在两个数据集上都明显优于其他方法。与手工制作的基于特征表示的模型比较时，性能差距最为显著。其主要原因是这些早期的作品大多基于启发式设计，因此无法学习到最优的判别特征；与基于伪标签学习模型的比较时，本文模型明显优于基于伪标签学习的无监督行人重识别模型。一个关键的原因是基于伪标签的方法可能会将不同身份的相似图片分成相同的伪标签，在本文中，即使将不同身份的某一部件拉近，仍然有图像的其他部件进行补充；与基于无监督域自适应的行人重识别模型相比，所提出的模型具有明显的优势，一个关键的原因是源域图像与目标域图像之间的差距大于图像部分之间的差距，所以基于图像的特征学习模型很难转移到目标域。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Comparison to the state-of-art unsupervised results in the Market-1501 datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >Rank-1</th><th align="center" valign="middle" >Rank-5</th><th align="center" valign="middle" >Rank-10</th><th align="center" valign="middle" >mAP</th></tr></thead><tr><td align="center" valign="middle" >LOMO [<xref ref-type="bibr" rid="hanspub.42351-ref17">17</xref>]</td><td align="center" valign="middle" >27.2</td><td align="center" valign="middle" >41.6</td><td align="center" valign="middle" >49.1</td><td align="center" valign="middle" >8.0</td></tr><tr><td align="center" valign="middle" >BOW [<xref ref-type="bibr" rid="hanspub.42351-ref40">40</xref>]</td><td align="center" valign="middle" >35.8</td><td align="center" valign="middle" >52.4</td><td align="center" valign="middle" >60.3</td><td align="center" valign="middle" >14.8</td></tr><tr><td align="center" valign="middle" >UMDL [<xref ref-type="bibr" rid="hanspub.42351-ref24">24</xref>]</td><td align="center" valign="middle" >34.5</td><td align="center" valign="middle" >52.6</td><td align="center" valign="middle" >59.6</td><td align="center" valign="middle" >12.4</td></tr><tr><td align="center" valign="middle" >PUL [<xref ref-type="bibr" rid="hanspub.42351-ref20">20</xref>]</td><td align="center" valign="middle" >45.5</td><td align="center" valign="middle" >60.7</td><td align="center" valign="middle" >66.7</td><td align="center" valign="middle" >20.5</td></tr><tr><td align="center" valign="middle" >BUC [<xref ref-type="bibr" rid="hanspub.42351-ref22">22</xref>]</td><td align="center" valign="middle" >66.2</td><td align="center" valign="middle" >79.6</td><td align="center" valign="middle" >84.5</td><td align="center" valign="middle" >38.3</td></tr><tr><td align="center" valign="middle" >DBC [<xref ref-type="bibr" rid="hanspub.42351-ref21">21</xref>]</td><td align="center" valign="middle" >69.2</td><td align="center" valign="middle" >83.0</td><td align="center" valign="middle" >87.8</td><td align="center" valign="middle" >41.3</td></tr><tr><td align="center" valign="middle" >HHL [<xref ref-type="bibr" rid="hanspub.42351-ref26">26</xref>]</td><td align="center" valign="middle" >62.2</td><td align="center" valign="middle" >78.8</td><td align="center" valign="middle" >84.0</td><td align="center" valign="middle" >31.4</td></tr><tr><td align="center" valign="middle" >TJ-AIDL [<xref ref-type="bibr" rid="hanspub.42351-ref25">25</xref>]</td><td align="center" valign="middle" >58.2</td><td align="center" valign="middle" >74.8</td><td align="center" valign="middle" >81.1</td><td align="center" valign="middle" >26.5</td></tr><tr><td align="center" valign="middle" >UCDA-CCE [<xref ref-type="bibr" rid="hanspub.42351-ref46">46</xref>]</td><td align="center" valign="middle" >64.3</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >34.5</td></tr><tr><td align="center" valign="middle" >SSL-CCE [<xref ref-type="bibr" rid="hanspub.42351-ref47">47</xref>]</td><td align="center" valign="middle" >71.7</td><td align="center" valign="middle" >83.8</td><td align="center" valign="middle" >87.4</td><td align="center" valign="middle" >37.8</td></tr><tr><td align="center" valign="middle" >ECN [<xref ref-type="bibr" rid="hanspub.42351-ref48">48</xref>]</td><td align="center" valign="middle" >75.1</td><td align="center" valign="middle" >87.6</td><td align="center" valign="middle" >91.6</td><td align="center" valign="middle" >43.0</td></tr><tr><td align="center" valign="middle" >Ours</td><td align="center" valign="middle" >76.2</td><td align="center" valign="middle" >89.1</td><td align="center" valign="middle" >93.4</td><td align="center" valign="middle" >45.1</td></tr></tbody></table></table-wrap><p>表4. 不同算法在 Market-1501 数据集上的性能比较</p><table-wrap-group id="5"><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Comparison to the state-of-art unsupervised results in the DukeMTMC-reID datase</title></caption><table-wrap id="5_1"><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >Rank-1</th><th align="center" valign="middle" >Rank-5</th><th align="center" valign="middle" >Rank-10</th><th align="center" valign="middle" >mAP</th></tr></thead><tr><td align="center" valign="middle" >LOMO [<xref ref-type="bibr" rid="hanspub.42351-ref17">17</xref>]</td><td align="center" valign="middle" >12.3</td><td align="center" valign="middle" >21.3</td><td align="center" valign="middle" >26.6</td><td align="center" valign="middle" >4.8</td></tr><tr><td align="center" valign="middle" >BOW [<xref ref-type="bibr" rid="hanspub.42351-ref40">40</xref>]</td><td align="center" valign="middle" >17.1</td><td align="center" valign="middle" >28.8</td><td align="center" valign="middle" >34.9</td><td align="center" valign="middle" >8.3</td></tr></tbody></table></table-wrap><table-wrap id="5_2"><table><tbody><thead><tr><th align="center" valign="middle" >UMDL [<xref ref-type="bibr" rid="hanspub.42351-ref24">24</xref>]</th><th align="center" valign="middle" >18.5</th><th align="center" valign="middle" >31.4</th><th align="center" valign="middle" >37.6</th><th align="center" valign="middle" >7.3</th></tr></thead><tr><td align="center" valign="middle" >PUL [<xref ref-type="bibr" rid="hanspub.42351-ref20">20</xref>]</td><td align="center" valign="middle" >45.5</td><td align="center" valign="middle" >60.7</td><td align="center" valign="middle" >66.7</td><td align="center" valign="middle" >20.5</td></tr><tr><td align="center" valign="middle" >BUC [<xref ref-type="bibr" rid="hanspub.42351-ref22">22</xref>]</td><td align="center" valign="middle" >47.4</td><td align="center" valign="middle" >62.6</td><td align="center" valign="middle" >68.4</td><td align="center" valign="middle" >27.5</td></tr><tr><td align="center" valign="middle" >DBC [<xref ref-type="bibr" rid="hanspub.42351-ref21">21</xref>]</td><td align="center" valign="middle" >51.5</td><td align="center" valign="middle" >64.6</td><td align="center" valign="middle" >70.1</td><td align="center" valign="middle" >30.0</td></tr><tr><td align="center" valign="middle" >HHL [<xref ref-type="bibr" rid="hanspub.42351-ref26">26</xref>]</td><td align="center" valign="middle" >46.9</td><td align="center" valign="middle" >61.0</td><td align="center" valign="middle" >66.7</td><td align="center" valign="middle" >27.2</td></tr><tr><td align="center" valign="middle" >TJ-AIDL [<xref ref-type="bibr" rid="hanspub.42351-ref25">25</xref>]</td><td align="center" valign="middle" >44.3</td><td align="center" valign="middle" >59.6</td><td align="center" valign="middle" >65.0</td><td align="center" valign="middle" >23.0</td></tr><tr><td align="center" valign="middle" >UCDA-CCE [<xref ref-type="bibr" rid="hanspub.42351-ref46">46</xref>]</td><td align="center" valign="middle" >55.4</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >36.7</td></tr><tr><td align="center" valign="middle" >SSL-CCE [<xref ref-type="bibr" rid="hanspub.42351-ref47">47</xref>]</td><td align="center" valign="middle" >52.5</td><td align="center" valign="middle" >63.5</td><td align="center" valign="middle" >68.9</td><td align="center" valign="middle" >28.6</td></tr><tr><td align="center" valign="middle" >ECN [<xref ref-type="bibr" rid="hanspub.42351-ref48">48</xref>]</td><td align="center" valign="middle" >63.3</td><td align="center" valign="middle" >75.8</td><td align="center" valign="middle" >80.4</td><td align="center" valign="middle" >40.4</td></tr><tr><td align="center" valign="middle" >Ours</td><td align="center" valign="middle" >75.6</td><td align="center" valign="middle" >84.4</td><td align="center" valign="middle" >90.7</td><td align="center" valign="middle" >55.9</td></tr></tbody></table></table-wrap></table-wrap-group><p>表5. 不同算法在DukeMTMC-reID数据集上的性能比较</p></sec></sec><sec id="s9"><title>5. 结论</title><p>在本文中，首先提出了多维注意力网络，对注意机制中复杂的高阶统计信息进行建模和利用，从而捕捉行人之间的细微差异，产生有区别的注意建议。其次，使用一个改进的STN形成三个分支，在每个分支后加入一个部件关注网络，并使用带有标签的数据集来预训练部件关注网络，使得每个分支可以关注人体不同的部件。最后是损失函数，使用了一个存储器来存储部件的更新特征，并通过一系列损失函数来引导部件关注网络学习未标记数据集上的部件特征。大量的实验验证了该方法的有效性。然而目前的行人重识别数据集基本上维持着几万张图片几千个ID的水平上，这大大阻碍了基于深度学习的行人重识别的研究。因此，在下一步的研究中，可以使用GAN来增加数据量的规模从而提高模型泛化的能力。</p></sec><sec id="s10"><title>文章引用</title><p>麻可可,薛丽霞,汪荣贵,杨 娟. 多维注意力与部件关注的无监督行人重识别Unsupervised Person Re-Identification Based on Multi-Dimensional Attention and Part Focus Network[J]. 计算机科学与应用, 2021, 11(05): 1301-1304. https://doi.org/10.12677/CSA.2021.115132</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.42351-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Kemelmacher-Shlizerman, I., Seitz, S.M., Miller, D. and Brossard, E. (2016) The Megaface Benchmark: 1 Million Faces for Recognition at Scale. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 4873-4882. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.527</mixed-citation></ref><ref id="hanspub.42351-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Liao, S., Lei, Z., Yi, D. and Li, S.Z. (2014) A Benchmark Study of Large-Scale Unconstrained Face Recognition. IEEE International Joint Conference on Biometrics, Clearwater, 29 September-2 Octoner 2014, 1-8. 
&lt;br&gt;https://doi.org/10.1109/BTAS.2014.6996301</mixed-citation></ref><ref id="hanspub.42351-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Taigman, Y., Yang, M., Ranzato, M. and Wolf, L. (2014) DeepFace: Closing the Gap to Human-Level Performance in Face Verification. Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus, 23-28 June 2014, 1701-1708. &lt;br&gt;https://doi.org/10.1109/CVPR.2014.220</mixed-citation></ref><ref id="hanspub.42351-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">LeCun, Y., Bengio, Y. and Hinton, G.E. (2015) Deep Learning. Nature, 521, 436-444.  
&lt;br&gt;https://doi.org/10.1038/nature14539</mixed-citation></ref><ref id="hanspub.42351-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky, A., Sutskever, I. and Hinton, G.E. (2012) Imagenet Classi-fication with Deep Convolutional Neural Networks. 25th International Conference on Neural Information Processing Systems, Stateline, December 2012, 1097-1105.</mixed-citation></ref><ref id="hanspub.42351-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Schmidhuber, J. (2015) Deep Learning in Neural Networks: An Overview. Neural Networks, 61, 85-117. 
&lt;br&gt;https://doi.org/10.1016/j.neunet.2014.09.003</mixed-citation></ref><ref id="hanspub.42351-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Zheng, Z., Zheng, L. and Yang, Y. (2017) A Discriminatively Learned CNN Embedding for Person Re-Identification. ACM Transactions on Multimedia Computing, Communications, and Applications, 14, Article No. 13.  
&lt;br&gt;https://doi.org/10.1145/3159171</mixed-citation></ref><ref id="hanspub.42351-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Y., Zheng, L., Zheng, Z., Wu, Y. and Yang, Y. (2016) Improving Per-son Re-Identification by Attribute and Identity Learning. IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, March 2017, 20-28.</mixed-citation></ref><ref id="hanspub.42351-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Rahul, V., Rama, Mrinal, H. and Gang, W. (2016) Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification. European Conference on Computer Vision, 8-16 October, Amsterdam, 791-808.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46484-8_48</mixed-citation></ref><ref id="hanspub.42351-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Cheng, D., Gong, Y., Zhou, S., Wang, J. and Zheng, N. (2016) Person Re-Identification by Multichannel Parts-Based CNN with Improved Triplet Loss Function. Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 1335-1344. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.149</mixed-citation></ref><ref id="hanspub.42351-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Hermans, A., Beyer, L. and Leibe, B. (2017) In Defense of the Tri-plet Loss for Person Re-Identification. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, March 2017, 5767-5782.</mixed-citation></ref><ref id="hanspub.42351-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Varior, R.R., Shuai, B., Lu, J., Xu, D. and Wang, G. (2016) A Siamese Long Short-Term Memory Architecture for Human Re-Identification. European Conference on Computer Vision, Amsterdam, 8-16 October, 135-153.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46478-7_9</mixed-citation></ref><ref id="hanspub.42351-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Zheng, L., Huang, Y., Lu, H. and Yang, Y. (2017) Pose-Invariant Embedding for Deep Person Re-Identification. IEEE Transactions on Image Processing, 28, 4500-4509. &lt;br&gt;https://doi.org/10.1109/TIP.2019.2910414</mixed-citation></ref><ref id="hanspub.42351-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Wei, L., Zhang, S., Yao, H., Gao, W. and Tian, Q. (2017) GLAD: Global-Local-Alignment Descriptor for Scalable Person Re-Identification. IEEE Transactions on Multimedia, 21, 986-999. &lt;br&gt;https://doi.org/10.1109/TMM.2018.2870522</mixed-citation></ref><ref id="hanspub.42351-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Farenzena, M., Bazzani, L., Perina, A., Murino, V. and Cristani, M. (2010) Person Re-Identification by Symmetry-Driven Accumulation of Local Features. 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, 13-18 June 2010, 2360-2367. &lt;br&gt;https://doi.org/10.1109/CVPR.2010.5539926</mixed-citation></ref><ref id="hanspub.42351-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Chen, D., Yuan, Z., Chen, B. and Zheng, N. (2016) Similarity Learning with Spatial Constraints for Person Re-Identification. 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 1268-1277. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.142</mixed-citation></ref><ref id="hanspub.42351-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Liao, S., Hu, Y., Zhu, X. and Li, S.Z. (2015) Person Re-Identification by Local Maximal Occurrence Representation and Metric Learning. 2015 IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 2197-2206. &lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298832</mixed-citation></ref><ref id="hanspub.42351-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Zhao, R., Ouyang, W. and Wang, X. (2013) Unsupervised Sa-lience Learning for Person Re-Identification. 2013 IEEE Conference on Computer Vision and Pattern Recognition, Port-land, 23-28 June 2013, 3586-3593.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2013.460</mixed-citation></ref><ref id="hanspub.42351-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Wang, H., Gong, S. and Xiang, T. (2014) Unsupervised Learning of Generative Topic Saliency for Person Re-Identification. Proceedings of 2014 British Machine Vision Conference, Not-tingham, 1-5 September 2014, 1-11.</mixed-citation></ref><ref id="hanspub.42351-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Fan, H., Zheng, L., Yan, C. and Yang, Y. (2018) Unsupervised Person Reidentification: Clustering and Fine-Tuning. ACM Transactions on Multimedia Computing, Communications, and Ap-plications, 14, Article No. 83. 
&lt;br&gt;https://doi.org/10.1145/3243316</mixed-citation></ref><ref id="hanspub.42351-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Ding, G., Khan, S., Tang, Z. and Zhang, J. (2019) Towards Better Validity: Dispersion Based Clustering for Unsupervised Person Re-Identification. IEEE Conference on Computer Vision and Pat-tern Recognition, Long Beach, June 2019, 1485-1494.</mixed-citation></ref><ref id="hanspub.42351-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Y., Dong, X., Zheng, L., Yan, Y. and Yang, Y. (2019) A Bottom-up Clustering Approach to Unsupervised Person Re-Identification. AAAI Conference on Artificial Intelligence, Honolulu, 27 January-1 February 2019, 8738-8745.</mixed-citation></ref><ref id="hanspub.42351-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Chen, H., Wang, Y., Shi, Y., Yan, K., Geng, M., Tian, Y., et al. (2018) Deep Transfer Learning for Person Re-Identification. IEEE 4th International Conference on Multimedia Big Data, Xi’an, 13-16 September 2018, 1-5. 
&lt;br&gt;https://doi.org/10.1109/BigMM.2018.8499067</mixed-citation></ref><ref id="hanspub.42351-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Peng, P., Xiang, T., Wang, Y., Pontil, M., Gong, S., Huang, T. and Tian, Y. (2016) Unsupervised Cross-Dataset Transfer Learning for Person Re-Identification. Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 1306-1315. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.146</mixed-citation></ref><ref id="hanspub.42351-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Wang, J., Zhu, X., Gong, S. and Li, W. (2018) Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification. IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 2275-2284. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00242</mixed-citation></ref><ref id="hanspub.42351-ref26"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Zhong, Z., Zheng, L., Li, S. and Yang, Y. (2018) Generalizing A Person Retrieval Model Hetero- and Homogeneously. Proceedings of the 2018 European Conference on Computer Vi-sion, Munich, 8-14 September 2018, 172-188.</mixed-citation></ref><ref id="hanspub.42351-ref27"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Li, G. and Yu, Y. (2016) Visual Saliency Detection Based on Mul-tiscale Deep CNN Features. IEEE Transactions on Image Processing, 25, 5012-5024. &lt;br&gt;https://doi.org/10.1109/TIP.2016.2602079</mixed-citation></ref><ref id="hanspub.42351-ref28"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W. and Chua, T.-S. (2017) SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning. 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 6298-6306. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.667</mixed-citation></ref><ref id="hanspub.42351-ref29"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">Chen, L.-C., Yang, Y., Wang, J., Xu, W. and Yuille, A.L (2016) Attention to Scale: Scale-Aware Semantic Image Segmentation. 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 3640-3649. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.396</mixed-citation></ref><ref id="hanspub.42351-ref30"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Liu, H., Feng, J., Qi, M., Jiang, J. and Yan, S. (2016) End-to-End Comparative Attention Networks for Person Re-Identification. IEEE Transactions on Image Processing, 26, 3492-3506. &lt;br&gt;https://doi.org/10.1109/TIP.2017.2700762</mixed-citation></ref><ref id="hanspub.42351-ref31"><label>31</label><mixed-citation publication-type="other" xlink:type="simple">Ba, J., Mnih, V. and Kavukcuoglu, K. (2014) Multiple Object Recognition with Visual Attention. arXiv:1412.7755.</mixed-citation></ref><ref id="hanspub.42351-ref32"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Chu, X., Yang, W., Ouyang, W., Ma, C., Yuille, A.L. and Wang, X. (2017) Multi-Context Attention for Human Pose Estimation. Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 1831-1840. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.601</mixed-citation></ref><ref id="hanspub.42351-ref33"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">Si, J., Zhang, H., Li, C.-G., Kuen, J., Kong, X., Kot, A. and Wang, G. (2018) Dual Attention Matching Network for Context-Aware Feature Sequence Based Person Re-Identification. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 5363-5372. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00562</mixed-citation></ref><ref id="hanspub.42351-ref34"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">Li, W., Zhu, X. and Gong, S. (2018) Harmo-nious Attention Network for Person Re-Identification. Proceedings of the 2018 IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, Salt Lake City, 18-23 June 2018, 2285-2294.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00243</mixed-citation></ref><ref id="hanspub.42351-ref35"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Xu, J., Zhao, R., Zhu, F., Wang, H. and Ouyang, W. (2018) At-tention-Aware Compositional Network for Personre-Identification. IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, Salt Lake City, 18-23 June 2018, 2119-2128. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00226</mixed-citation></ref><ref id="hanspub.42351-ref36"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">Chang, X., Hospedales, T. and Xiang, T. (2018) Multi-Level Factorisation Net for Person Re-Identification. IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 2109-2118. 
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00225</mixed-citation></ref><ref id="hanspub.42351-ref37"><label>37</label><mixed-citation publication-type="other" xlink:type="simple">Xu, J., Zhao, R., Zhu, F., Wang, H. and Ouyang, W. (2018) At-tention-Aware Compositional Network for Personre-Identification. IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, Salt Lake City, 18-23 June 2018, 2119-2128. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00226</mixed-citation></ref><ref id="hanspub.42351-ref38"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Hu, J., Shen, L. and Sun, G. (2018) Squeeze-and-Excitation Networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 7132-7141.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00745</mixed-citation></ref><ref id="hanspub.42351-ref39"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">Xiao, T., Li, S., Wang, B., Lin, L. and Wang, X. (2017) Joint De-tection and Identification Feature Learning for Person Search. 2017 IEEE International Conference on Computer, Hono-lulu, 21-26 July 2017, 3376-3385.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.360</mixed-citation></ref><ref id="hanspub.42351-ref40"><label>40</label><mixed-citation publication-type="other" xlink:type="simple">Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J. and Tian, Q. (2015) Scalable Person Re-Identification: A Benchmark. 2015 IEEE International Conference on Computer Vision, Santiago, 7-13 December 2015, 1116-1124. 
&lt;br&gt;https://doi.org/10.1109/ICCV.2015.133</mixed-citation></ref><ref id="hanspub.42351-ref41"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">Zheng, Z., Zheng, L. and Yang, Y. (2017) Unlabeled Samples Gen-erated by GAN Improve the Person Re-Identification Baseline in Vitro. 2017 IEEE International Conference on Com-puter, Venice, 22-29 October 2017, 3774-3782.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.405</mixed-citation></ref><ref id="hanspub.42351-ref42"><label>42</label><mixed-citation publication-type="other" xlink:type="simple">Ristani, E., Solera, F., Zou, R.S., Cucchiara, R. and Tomasi, C. (2016) Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking. 2016 ECCV Workshop on Benchmark-ing Multi-Target Tracking, Amsterdam, 8-16 October 2016, 17-35. &lt;br&gt;https://doi.org/10.1007/978-3-319-48881-3_2</mixed-citation></ref><ref id="hanspub.42351-ref43"><label>43</label><mixed-citation publication-type="other" xlink:type="simple">Wei, L., Zhang, S., Gao, W. and Tian, Q. (2018) Person Transfer GAN to Bridge Domain Gap for Person Re-Identification. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 79-88. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00016</mixed-citation></ref><ref id="hanspub.42351-ref44"><label>44</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S. and Sun, J. (2016) Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.42351-ref45"><label>45</label><mixed-citation publication-type="other" xlink:type="simple">Deng, J., Dong, W., Socher, R., Li, J., Li, K. and Li, F. (2009) ImageNet: A Large-Scale Hierarchical Imagedatabase. 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Miami, 20-25 June 2009, 248-255. &lt;br&gt;https://doi.org/10.1109/CVPR.2009.5206848</mixed-citation></ref><ref id="hanspub.42351-ref46"><label>46</label><mixed-citation publication-type="other" xlink:type="simple">Qi, L., Wang, L., Huo, J., Zhou, L., Shi, Y. and Gao, Y. (2019) A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-Identification. 2019 IEEE/CVF International Conference on Computer Vision, Seoul, 27 Oc-tober-2 November 2019, 8079-8088. &lt;br&gt;https://doi.org/10.1109/ICCV.2019.00817</mixed-citation></ref><ref id="hanspub.42351-ref47"><label>47</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Y., Xie, L., Wu, Y., Yan, C. and Tian, Q. (2020) Unsupervised Person Re-Identification via Softened Similarity Learning. 2020 IEEE/CVF Con-ference on Computer Vision and Pattern Recognitionn, Seattle, 13-19 June 2020, 3387-3396.  
&lt;br&gt;https://doi.org/10.1109/CVPR42600.2020.00345</mixed-citation></ref><ref id="hanspub.42351-ref48"><label>48</label><mixed-citation publication-type="other" xlink:type="simple">Zhong, Z., Zheng, L., Luo, Z., Li, S. and Yang, Y. (2019) Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-Identification. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, 15-20 June 2019, 598-607. &lt;br&gt;https://doi.org/10.1109/CVPR.2019.00069</mixed-citation></ref></ref-list></back></article>