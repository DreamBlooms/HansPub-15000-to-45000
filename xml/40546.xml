<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.112042</article-id><article-id pub-id-type="publisher-id">CSA-40546</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210200000_42343067.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  视频社会关系识别的多尺度图推理模型
  Multi-Scale Graph Reasoning Model for Video Social Relation Recognition
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>许</surname><given-names>飞</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>天雨</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>史</surname><given-names>俊彪</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>合肥工业大学计算机科学与信息工程学院，安徽 合肥</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>08</day><month>02</month><year>2021</year></pub-date><volume>11</volume><issue>02</issue><fpage>423</fpage><lpage>434</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   人类社会关系识别作为视频分类中的一个重要问题，逐渐成为计算机视觉领域的一个研究热点。由于视频信息较多，冗余信息过量，关键帧较少，因此如何准确的识别视频中的关键信息进行社会关系推理至关重要。为此，本文提出一种多尺度图推理模型来进行视频社会关系识别。首先我们提取视频中的时空特征和语义对象信息，获得丰富、鲁棒的社会关系表示。接着通过多尺度图卷积利用不同的感受野来进行时间推理，捕捉人物和语义对象间的交互。特别地，我们利用注意力机制来评估每个语义对象在不同场景的效果。在SRIV数据集上的实验结果表明，本文提出的方法优于大多数先进的方法。 As an important issue in video classification, human social relationship recognition has gradually become a research hotspot in the field of computer vision. Due to the large amount of video information, excessive redundant information and less key frames, how to accurately identify the key information in the video and carry out social relation reasoning is of great importance. To this end, this paper proposes a multi-scale graph reasoning model to identify video social relationships. First, we extract the temporal and spatial features and semantic object information in the video to obtain a rich and Lupin representation of social relations. Then use different receptive fields to perform temporal reasoning through multi-scale graph convolution, and capture the interaction between characters and semantic objects. In particular, we use the attention mechanism to evaluate the effect of each semantic object in different scenarios. The experimental results on SRIV dataset show that the method proposed in this paper is superior to most advanced methods. 
  
 
</p></abstract><kwd-group><kwd>社会关系识别，多尺度图卷积，注意力机制, Social Relation Recognition</kwd><kwd> Multi-Scale Graph Convolution</kwd><kwd> Attention Mechanism</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>人类社会关系识别作为视频分类中的一个重要问题，逐渐成为计算机视觉领域的一个研究热点。由于视频信息较多，冗余信息过量，关键帧较少，因此如何准确的识别视频中的关键信息进行社会关系推理至关重要。为此，本文提出一种多尺度图推理模型来进行视频社会关系识别。首先我们提取视频中的时空特征和语义对象信息，获得丰富、鲁棒的社会关系表示。接着通过多尺度图卷积利用不同的感受野来进行时间推理，捕捉人物和语义对象间的交互。特别地，我们利用注意力机制来评估每个语义对象在不同场景的效果。在SRIV数据集上的实验结果表明，本文提出的方法优于大多数先进的方法。</p></sec><sec id="s2"><title>关键词</title><p>社会关系识别，多尺度图卷积，注意力机制</p></sec><sec id="s3"><title>Multi-Scale Graph Reasoning Model for Video Social Relation Recognition</title><p>Fei Xu, Tianyu Zhang, Junbiao Shi</p><p>School of Computer Science and Information Engineering, Hefei University of Technology, Hefei Anhui</p><p><img src="//html.hanspub.org/file/17-1542042x4_hanspub.png" /></p><p>Received: Jan. 22<sup>nd</sup>, 2021; accepted: Feb. 17<sup>th</sup>, 2021; published: Feb. 24<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/17-1542042x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>As an important issue in video classification, human social relationship recognition has gradually become a research hotspot in the field of computer vision. Due to the large amount of video information, excessive redundant information and less key frames, how to accurately identify the key information in the video and carry out social relation reasoning is of great importance. To this end, this paper proposes a multi-scale graph reasoning model to identify video social relationships. First, we extract the temporal and spatial features and semantic object information in the video to obtain a rich and Lupin representation of social relations. Then use different receptive fields to perform temporal reasoning through multi-scale graph convolution, and capture the interaction between characters and semantic objects. In particular, we use the attention mechanism to evaluate the effect of each semantic object in different scenarios. The experimental results on SRIV dataset show that the method proposed in this paper is superior to most advanced methods.</p><p>Keywords:Social Relation Recognition, Multi-Scale Graph Convolution, Attention Mechanism</p><disp-formula id="hanspub.40546-formula22"><graphic xlink:href="//html.hanspub.org/file/17-1542042x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/17-1542042x7_hanspub.png" /> <img src="//html.hanspub.org/file/17-1542042x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>社会关系是多个个体之间的紧密联系，并构成我们社会的基本结构。从图像或视频中识别社交关系可以使机器更好地理解人类的行为或情感。然而，与基于图像的社会关系识别相比，基于视频的场景是一个重要但前沿的话题，常常被社会团体所忽视。它具有许多潜在的应用，例如帮助人们在手机中查找家庭视频 [<xref ref-type="bibr" rid="hanspub.40546-ref1">1</xref>]，或者向商店中的顾客群推荐合适的产品 [<xref ref-type="bibr" rid="hanspub.40546-ref2">2</xref>]。</p><p>现有的社会关系识别研究主要集中在基于图像的条件下，算法主要识别单个图像中人与人之间的社会关系。为了区分不同的社会关系，研究了人和语境对象的外观和面部属性。尽管在视频或电影中发现了社交网络 [<xref ref-type="bibr" rid="hanspub.40546-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref4">4</xref>]，社区，角色 [<xref ref-type="bibr" rid="hanspub.40546-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref6">6</xref>] 和群组行为 [<xref ref-type="bibr" rid="hanspub.40546-ref7">7</xref>]，但从视频片段中明确认识到社会关系的吸引力却远远不足注意。最近的方法仅将基于视频的社交关系识别视为一般的视频分类任务，该任务将RGB帧，光流或视频音频作为输入，并将视频片段分类为预定类型 [<xref ref-type="bibr" rid="hanspub.40546-ref8">8</xref>]。但是，这种模型显然过于简化，从而忽略了人的外观，人与语义对象之间的交互以及带有上下文对象的场景。如何解读场景中的众多特征，视频中的社会关系识别面临着独特的挑战。首先，与社会社区发现相比，在不同的场景中，社会关系更加细化和模糊。模型必须通过视觉内容来区分非常相似的社会关系，例如朋友和同事，即使对于人类来说，这也可能非常困难。此外，与基于图像的社会关系识别相反，人和语境对象可能出现在任意视频帧中，甚至出现在单独的视频帧中。这使得人和语境对象在连续帧中变化极大。因此，基于图像的方法不能直接用于基于视频的场景。此外，视频还提供了人和语义对象的时域特征。对人的动态变化与社会关系之间的潜在关联进行建模仍然具有很大的挑战性。</p><p>为此，我们提出了一个多尺度图推理模型(MSGRM)来解决视频中的社会关系理解问题。在特征提取阶段，利用特征提取网络提取场景的时空特征和语义对象特征。然后在多尺度图推理阶段，利用不同的感受野来学习长期和短期信息，以探索场景中人和语义对象之间的交互。此外，利用注意机制，通过测量每个节点的重要性，自适应地选择某一视频场景中最重要的节点进行识别。这样，MSGRM极大地提高了从视频中获取社会关系的能力。本文主要贡献如下：</p><p>1) 本文提出了一种多尺度图推理模型(MSGRM)来识别视频中的社会关系，在端到端的处理过程中，该方法可以准确地捕捉场景中角色的时空信息和交互信息。</p><p>2) 为了捕捉视频中的长期和短期时间线索，本文提出了一种基于多尺度时间感受野的MSGCN进行社会关系推理，以捕捉视频中的长期和短期线索。</p><p>3) 我们将该方法应用于SRIV数据集，并与一些优秀的研究工作进行了比较，取得了较好的识别效果。</p></sec><sec id="s6"><title>2. 相关工作</title><p>视频中的社会关系识别。在过去的十年中，社会学和计算机视觉的跨学科研究一直是热门领域。主要的研究主题包括社交网络发现 [<xref ref-type="bibr" rid="hanspub.40546-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref4">4</xref>]、关键角色检测 [<xref ref-type="bibr" rid="hanspub.40546-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref6">6</xref>]、多人跟踪 [<xref ref-type="bibr" rid="hanspub.40546-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref10">10</xref>] 和群体行为识别 [<xref ref-type="bibr" rid="hanspub.40546-ref7">7</xref>]。近年来，基于视觉内容的社会关系识别引起了研究者的关注 [<xref ref-type="bibr" rid="hanspub.40546-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref14">14</xref>]，现有的方法主要集中在静态图像上。例如，Zhang等提出通过卷积神经网络(CNN)从人脸图像中学习社会关系特征 [<xref ref-type="bibr" rid="hanspub.40546-ref12">12</xref>]。Sun等提出了一种基于社会领域理论的社会关系数据集 [<xref ref-type="bibr" rid="hanspub.40546-ref15">15</xref>]，并采用CNN从一组语义属性中识别社会关系 [<xref ref-type="bibr" rid="hanspub.40546-ref13">13</xref>]。Li等提出了一种用于社交关系识别的双视模型，其中第一眼聚焦感兴趣的人，第二眼应用注意力机制发现上下文线索 [<xref ref-type="bibr" rid="hanspub.40546-ref11">11</xref>]。Wang等人提出用图来表示图像中的人和物体，并用门控图神经网络 [<xref ref-type="bibr" rid="hanspub.40546-ref14">14</xref>] 进行社会关系推理。而对于基于视频的数据，社交关系识别仅被视为视频分类任务。例如，Lv等利用时间分段网络 [<xref ref-type="bibr" rid="hanspub.40546-ref16">16</xref>]，利用视频的RGB帧、光流和音频对视频进行分类 [<xref ref-type="bibr" rid="hanspub.40546-ref8">8</xref>]。他们还建立了一个视频社会关系(SRIV)数据集，其中包含约3000个带有多标签注释的视频片段。但是，该方法只考虑全局和粗糙特征，而忽略了视频中的人、对象和场景。因此，我们将视频中的人与对象的时空特征特征嵌入图模型，并在此基础上进行社会关系推理。</p><p>计算机视觉中的图模型。在计算机视觉领域，像素，区域，概念和先验知识可以表示为图形，以针对不同任务(例如目标检测 [<xref ref-type="bibr" rid="hanspub.40546-ref17">17</xref>]，图像分割 [<xref ref-type="bibr" rid="hanspub.40546-ref18">18</xref>]，图像搜索 [<xref ref-type="bibr" rid="hanspub.40546-ref19">19</xref>] 等)和对它们的关系进行建模。近年来，机器学习的研究人员研究了通过端到端可训练网络在图中进行消息传播，如图卷积网络(GCN) [<xref ref-type="bibr" rid="hanspub.40546-ref20">20</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref21">21</xref>] 和门控图神经网络(GGNN) [<xref ref-type="bibr" rid="hanspub.40546-ref22">22</xref>]。最近，这些模型已被用于计算机视觉任务 [<xref ref-type="bibr" rid="hanspub.40546-ref14">14</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref23">23</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref24">24</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref25">25</xref>]。例如，Liang等提出了一个“图形长短期记忆网络”在基于超像素的图形中传播信息，并用于语义对象解析 [<xref ref-type="bibr" rid="hanspub.40546-ref24">24</xref>]。Qi等提出了一种3D图神经网络在3D点云上建立一个k近邻图，并预测RGBD数据每个像素的语义类别 [<xref ref-type="bibr" rid="hanspub.40546-ref25">25</xref>]。Wang等提出用视频中的人物和对象将视频表示为时空图，并采用GCN来学习视频级特征以进行动作识别 [<xref ref-type="bibr" rid="hanspub.40546-ref26">26</xref>]。受以上研究的启发，我们建议将视频中人和物体之间的相互作用用图形来表示，并通过我们提出的多尺度图推理模型网络来进行社会关系识别。</p><p>注意力模型。人在观看某物时，总是关注感兴趣的视觉信息。一些研究发现，视觉注意力被信息含量最高的区域 [<xref ref-type="bibr" rid="hanspub.40546-ref27">27</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref28">28</xref>] 所吸引。在深度学习领域，注意力机制已应用于视频描述 [<xref ref-type="bibr" rid="hanspub.40546-ref29">29</xref>]，图像和动作分类 [<xref ref-type="bibr" rid="hanspub.40546-ref30">30</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref31">31</xref>] 以及文本中的实体歧义消除 [<xref ref-type="bibr" rid="hanspub.40546-ref26">26</xref>]，以学习数据的更多关键部分。一方面，基于CNN的注意力模型已经被提出并应用于不同的领域，这些方法比没有注意力模型取得了更优异的成绩。例如，Yu等人 [<xref ref-type="bibr" rid="hanspub.40546-ref32">32</xref>] 引入了注视编码注意网络(GEAN)，该网络可以利用注视跟踪信息为视频字幕提供时空关注。Zhu等 [<xref ref-type="bibr" rid="hanspub.40546-ref30">30</xref>] 提出了一种空间正则化网络，利用注意力机制学习不同标签的更多相关区域。另一方面，注意机制也被应用于序列学习模型中。如Pei等人 [<xref ref-type="bibr" rid="hanspub.40546-ref33">33</xref>] 提出了不同的注意力GRU模型，可以学习顺序数据的注意得分。然而，这些注意模型忽略了语义对象与特定视频时空特征之间的相关性。因此，在我们的社会关系识别模型中，提出了一种时空注意力机制，从视频中自适应地选择最有区别性的对象来理解社会关系。</p></sec><sec id="s7"><title>3. 多尺度图推理模型</title><p>我们的多尺度图推理模型的总体架构主要包含两部分，第一部分是从原始数据提取语义对象以构建图。该框架将一个视频帧作为输入，为了建模人和物体的时空特征和探究人和物体间的交互信息，我们构建了一个人—物图和上下文对象的共存，并用LSTM和ResNet [<xref ref-type="bibr" rid="hanspub.40546-ref34">34</xref>] 来提取人和物体的时空特征。第二个部分采用MSGCN来进行关系推理，在每个图中进行消息传播。在MSGCN中，我们探索多尺度的时间感受野来学习不同时间范围的相互作用。并利用注意力机制来探究场景中的语义对象对社会关系识别的重要性。图1给出了所提出模型的总体图解。</p><p>图1. 多尺度图推理模型框架</p><sec id="s7_1"><title>3.1. 特征提取模块</title><p>时空特征提取。为了从视频中学习时间特征，我们遵循 [<xref ref-type="bibr" rid="hanspub.40546-ref35">35</xref>] 采用LSTM单元对输入视频中采样的L个关键帧 I = { I i | i = 1 , 2 , ⋯ , L } 进行处理，来生成具有时间社会关系的特征序列，表示为 X t = { x t i | 1 , 2 , ⋯ , L } 。然后，将这些时空特征展平并连接起来以形成单个特征向量。</p><p>语义对象特征提取。使用预先训练的检测器捕获整个视频中的语义对象区域，并从相应的语义对象中提取特征，我们使用Faster R-CNN [<xref ref-type="bibr" rid="hanspub.40546-ref36">36</xref>] 检测器从采样的视频帧中检测视频中的人和物体对象 P = { p i } , i = 1 , 2 , ⋯ , N 和 O = { o j } , j = 1 , 2 , ⋯ , M ，该检测器是在COCO [<xref ref-type="bibr" rid="hanspub.40546-ref37">37</xref>] 数据集上训练的。COCO数据集是一种用于目标检测的大型数据集，涵盖了我们日常生活中经常出现的80个目标类别，用于从视频中收集语义对象。Faster R-CNN使用区域建议网络(RPN)处理输入关键帧I，生成一组具有高评分语义对象的区域建议。将检测到的置信度高于阈值ε的上下文区域 O I = { O I j | j = 1 , 2 , ⋯ , C } 作为语义对象，其中C表示检测到的类别。为了平衡准确性和效率，我们通过置信度得分将每个视频帧固定为N个人和M个目标对象。每个边界框的外观特征由VGG [<xref ref-type="bibr" rid="hanspub.40546-ref38">38</xref>] 网络来提取的。这些边界框被用作构建人-物图模型的节点，而每个结点的特征将在图卷积中用于社会关系推理。</p></sec><sec id="s7_2"><title>3.2. 人–物图模型</title><p>图形模型可以有效地表示空间视觉内容中对象的时间、空间、概念或者相似性关系 [<xref ref-type="bibr" rid="hanspub.40546-ref19">19</xref>] [<xref ref-type="bibr" rid="hanspub.40546-ref23">23</xref>]，为了捕获视频中不同人之间的交互和探究人物和上下文对象之间的互动，我们构建一个人–物图模型 G = ( V , E ) 来表示人际之间的交互和人与上下文对象之间的共存,其中 V = ( P , O ) 是我们场景中的人和目标对象节点，用不同的颜色表示，E表示节点之间的关系边。</p><p>对于建模人与人之间的交互，我们通过估计视频帧及其相邻帧中人的距离来构建图模型。对于人际之间的邻接矩阵 A p − p ∈ R N ∗ N ，如果人节点 P i 和 P j 是属于同一帧的，我们直接设置 A p − p ( p i , p j ) = 1 。如果人节点 P i 和 P j 属于相邻帧，我们设置</p><p>A p − p ( p i , p j ) = { 1       d i s t ( p i , p j ) ≥ τ 0           othersize (1)</p><p>其中 d i s t ( p i , p j ) = 1 − f ( p i ) τ f ( p j ) ‖ f ( p i ) ‖ ⋅ ‖ f ( p j ) ‖ 是人节点 P i 和 P j 之间的余弦距离，τ是我们设置的超参数。</p><p>同样，场景中的上下文对象是社交关系识别的重要信息，为了捕获视频中人物和上下文对象之间的互动。我们通过估计人物和上下文对象在视频帧中的共存来构建图模型。对于人和物之间的邻接矩阵 A p − o ∈ R ( N + M ) ∗ ( N + M ) ，如果 P i 和 O j 来自同一帧，则设置 A p − o ( p i , o j ) = 1 ，否则设置 A p − o ( p i , o j ) = 0 ，公式如下：</p><p>A p − o ( p i , o j ) = { 1             ∩ ( p i , o j ) 0           othersize (2)</p><p>其中 ∩ ( p i , o j ) 表示 P i 和 O j 来自同一个视频帧。为了方便我们更加直观的进行图推理，我们把人际交互图和人物共存图整合在一个图上，如图2所示。</p><p>图2. 人–物图模型结构</p></sec><sec id="s7_3"><title>3.3. 多尺度卷积网络</title><p>图卷积网络(GCN)通过在图中从节点到其邻居进行消息传播来进行关系推理 [<xref ref-type="bibr" rid="hanspub.40546-ref20">20</xref>]。因此，我们可以在人–物图模型中应用GCN来实现视频帧中的社会关系推理。给定一个有N人个节点的图，其中每个节点都有一个d长度的特征向量，一个图卷积层的运算可以表示为</p><p>X ( l + 1 ) = σ ( D ˜ − 1 2 A ˜ D ˜ − 1 2 X ( l ) W ( l ) ) (3)</p><p>其中 A ˜ ∈ R N * N 是人–物关系图的邻接矩阵， D ˜ ∈ R N * N 是 A ˜ ∈ R N * N 的度矩阵， X ( l ) ∈ R N ∗ d 是第 ( l − 1 ) 的输出结果， W ( L ) ∈ R d ∗ d 为可学习参数矩阵， σ ( • ) 是一个类似ReLU的非线性激活函数。特别说明，在我们的社会关系推理模型中，上式中的邻接矩阵为我们在3.2节中定义的 A p − p 和 A p − o 。邻接矩阵的索引是按照视频中节点的时间顺序排列的，通过这个顺序，时间信息被隐式地嵌入到构建的图中。初始特征矩阵可表示为 X ( 0 ) = [ x p ( i ) | i = 1 , 2 , ⋯ , N ; x o ( j ) | j = 1 , 2 , ⋯ , M ] T ，其中 x p ( i ) 和 x o ( j ) 是从视频中人和物体对象节点中提取的特征向量。GCNs的最终输出是图中节点的更新特征，这些特征可以聚合成视频级的特征向量用于社会关系预测。</p><p>GCN在一幅图中的所有节点上以及视频的整个时间范围上执行操作，这意味着GCN可以在时间域捕获全局视图。然而，社会关系识别的关键因素(如一个人的特定行为)可能出现在被不重要信息淹没的局部时间位置。因此，我们设计了一个多尺度图卷积网络(MSGCN)，通过不同的时间感受野来学习长期和短期信息。如图3所示为我们的多尺度卷积网络的一个块结构，每个块包含具有不同感受野的多个平行分支。Scale 1是标准GCN，它在整个相邻矩阵上执行图卷积并覆盖图中的所有节点。Scale 2给出了具有较小时间感受野的图卷积的示例，而Scale k是更一般的说明。对于每个Scale，所有滑动窗口的激活都汇总到一个特征矩阵中，该特征矩阵的形状与标准GCN的输出相同。通过沿着相邻矩阵的对角线滑动感受野，模型可以学习从视频的开始到结束的短期特征。最后，对多个尺度的输出进行平均池化合并，以生成下一个MSGCN层的特征矩阵X<sup>(l+1)</sup>。经过多次交互后，节点消息已经通过图进行传播，我们可以得到每个节点最终的状态为</p><p>Y = { y 1 , y 2 , ⋯ , y N + M } (4)</p><p>图3. 具有多尺度感受野的图卷积块</p></sec><sec id="s7_4"><title>3.4. 注意力机制</title><p>计算每个节点的特征后，我们可以直接将它们通过MSGCN聚合起来进行关系识别。然而，在不同的视频场景中，语义对象对区分关系的贡献并不相同。为了解决这一问题，我们引入了一种新的注意力机制，根据图形结构和视频特征自适应地推理出最相关的上下文对象。对于每一个社会关系和邻居对象对，该机制将它们的场景外观特征作为输入，并计算出这个对象对关系的重要性。我们首先将每个图中对象节点的外观特征和视频时空特征结合成一个向量 h i , j ∈ R d ∗ d</p><p>h i , j = R E L U ( x o + w t ⊗ x t ) (5)</p><p>其中 w t ∈ R d ∗ d 是一个权重矩阵， ⊗ 表示按矩阵元素相乘。</p><p>然后，我们通过sigmoid函数来计算每一个对象节点的注意力系数 a i , j ∈ [ 0 , 1 ] ，</p><p>a i , j = 1 1 + exp ( − ( W h , a h i , j + b a ) ) (6)</p><p>其中 W h , a ∈ R 1 ∗ k 是一个权重矩阵，根据节点j对节点i的重要性的不同，可将每个特征转换为可用的表达性更强的特征， b a 是一个偏置项。</p><p>对于关系 r i ，我们将其自人物节点的特征与上下文对象节点的加权特征向量连接起来作为其最终特征，</p><p>f i = [ y r i , a i 1 y o 1 , a y i 2 o 2 , ⋯ , a i M y i M ] (7)</p><p>然后由最后一层fc层对特征向量进行处理，生成关系得分：</p><p>s i = W f i + b (8)</p><p>表示视频场景具有社会关系 r i 的可能性。对所有关系节点重复此过程，计算得分向量 s = { s 1 , s 2 , ⋯ , s N } 。</p><p>整个网络通过交叉熵损失与地面真实标签 s ^ 一起训练，</p><p>F l o s s ( s ^ , s ) = ∑ i = 1 N s ^ i ∗ log ( s i ) + ( 1 − s ^ i ) ∗ log ( 1 − s i ) (9)</p><p>其中s是预测的类别概率。</p></sec></sec><sec id="s8"><title>4. 实验</title><sec id="s8_1"><title>4.1. 数据集</title><p>SRIV数据集：本文使用的数据集来自于电影和电视剧，名为SRIV [<xref ref-type="bibr" rid="hanspub.40546-ref8">8</xref>]。SRIV是第一个从视频中识别社会关系的视频数据集。它包含3124个带有多标签的视频，大约25个小时，这些视频来自69部电视剧和电影。数据集包含Sub-Relation和Obj-Relation类，其中包括16个子类，如表1所示。</p></sec><sec id="s8_2"><title>4.2. 实验细节和评价标准</title><p>在特征提取模块，从视频中随机采样的关键帧L的数量设置为128。类似于 [<xref ref-type="bibr" rid="hanspub.40546-ref11">11</xref>]，我们利用广泛使用的ResNet-101 [<xref ref-type="bibr" rid="hanspub.40546-ref34">34</xref>] 提取关键帧的特征，得出的特征向量为2048维。对于语义对象区域，我们使用VGG-16 [<xref ref-type="bibr" rid="hanspub.40546-ref38">38</xref>] 提取特征，从而得到4096维的特征向量。通常用于指导目标检测的阈值为0.5，而此处语义对象检测的结果将很大程度影响人–物关系图中的特征交互，所以我们的阈值ε提高为0.7，以获取更加准确的检测对象。在整个训练期间，除了MSGCN外，我们模型的所有组件使用SGD优化，MSGCN使用ADAM优化。对于SRIV数据集，学习率lr从0.01开始，每20个epochs乘以0.1，直到训练完80个epochs。</p><p>本文采用四个评价标准来评价我们所提出的方法的性能。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The statistics of the number for each class on SRI</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  colspan="4"  >Sub-Relation</th></tr></thead><tr><td align="center" valign="middle" >Dominant</td><td align="center" valign="middle" >Competitive</td><td align="center" valign="middle" >Trusting</td><td align="center" valign="middle" >Warm</td></tr><tr><td align="center" valign="middle" >770</td><td align="center" valign="middle" >840</td><td align="center" valign="middle" >1614</td><td align="center" valign="middle" >1482</td></tr><tr><td align="center" valign="middle" >Friendly</td><td align="center" valign="middle" >Attached</td><td align="center" valign="middle" >Inhibited</td><td align="center" valign="middle" >Assured</td></tr><tr><td align="center" valign="middle" >2221</td><td align="center" valign="middle" >600</td><td align="center" valign="middle" >594</td><td align="center" valign="middle" >810</td></tr><tr><td align="center" valign="middle"  colspan="4"  >Obj-Relation</td></tr><tr><td align="center" valign="middle" >Supervisor</td><td align="center" valign="middle" >Peer</td><td align="center" valign="middle" >Service</td><td align="center" valign="middle" >Parent</td></tr><tr><td align="center" valign="middle" >627</td><td align="center" valign="middle" >469</td><td align="center" valign="middle" >238</td><td align="center" valign="middle" >321</td></tr><tr><td align="center" valign="middle" >Mating</td><td align="center" valign="middle" >Sibling</td><td align="center" valign="middle" >Friendly</td><td align="center" valign="middle" >Hostile</td></tr><tr><td align="center" valign="middle" >600</td><td align="center" valign="middle" >141</td><td align="center" valign="middle" >1073</td><td align="center" valign="middle" >434</td></tr></tbody></table></table-wrap><p>表1. SRIV上每种类别的统计数量</p><p>F<sub>1</sub>_micro和F<sub>1</sub>_macro这两个评估基于是 F 1 分数的标签评估，第i类的 F 1 表示为</p><p>F i ( i ) = 2 ∗ T P ( i ) / ( 2 ∗ T P ( i ) + F P ( i ) + F N ( i ) ) (10)</p><p>其中TP(i)、FP(i)、FP(i)、FN(i)分别为第i类的正阳性、假阳性、真阴性、假阳性的个数，因此， F 1 _ micro 和 F 1 _ macro 的计算公式如下</p><p>F 1 _ macro = 1 C ∑ i = 1 C F 1 ( i ) (11)</p><p>F 1 _ micro = 2 ∗ ∑ i = 1 C T P ( i ) / ( 2 ∗ ∑ i = 1 C T P ( i ) + ∑ i = 1 C F P ( i ) + ∑ i = 1 C F N ( i ) ) (12)</p><p>其中C为类别数。</p><p>Accuracy 我们采用了Zhang等 [<xref ref-type="bibr" rid="hanspub.40546-ref12">12</xref>] 提出的平衡精度，与以往的accuracy计算有所区别，我们充分考虑了样本数据中的不平衡性，使得最终的预测更符合实际结果，具体计算公式如下：</p><p>Accuracy = 1 2 ( T P N p + T N N n ) (13)</p><p>其中N<sub>p</sub>和N<sub>n</sub>为阳性阴性样本数。</p><p>Subset Accuracy 由于我们的sub-relation类为主观感知的，分类标准更加严格细致，要求预测的标签集与样本真实标签集完全匹配，避免标签集中相似的标签干扰最终的预测，其具体公式如下：</p><p>Subaccuracy ( s i ) = 1 n ∑ i = 1 n I ( s i = s ^ i ) (14)</p></sec><sec id="s8_3"><title>4.3. 消融实验</title><p>这里我们探究了我们对尺度图模型中不同模块的效果，实验结果如表2所示。从结果中我们发现，MSGRM的整体准确率要高于GCN，这表明多尺度感受野能够从长期和短期范围捕捉到有用的特征。此外，在有Attention模块辅助下的实验结果要高于没有Attention模块的结果，这说明注意力模块可以关注与社会关系识别相关的关键帧。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> The effect of different feature modul</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >Method</th><th align="center" valign="middle"  colspan="2"  >Accuracy</th></tr></thead><tr><td align="center" valign="middle" >Sub-Relation</td><td align="center" valign="middle" >Obj-Relation</td></tr><tr><td align="center" valign="middle" >GCN</td><td align="center" valign="middle" >0.6725</td><td align="center" valign="middle" >0.6968</td></tr><tr><td align="center" valign="middle" >MSGRM</td><td align="center" valign="middle" >07154</td><td align="center" valign="middle" >0.7326</td></tr><tr><td align="center" valign="middle" >GCN + Attention</td><td align="center" valign="middle" >0.7369</td><td align="center" valign="middle" >0.7531</td></tr><tr><td align="center" valign="middle" >MSGRM + Attention</td><td align="center" valign="middle" >0.7756</td><td align="center" valign="middle" >0.7924</td></tr></tbody></table></table-wrap><p>表2. 不同功能模块的效果</p></sec><sec id="s8_4"><title>4.4. 与当前主流方法对比</title><p>为了验证所提出的多尺度图推理模型框架的有效性，我们在SRIV数据集上与几种最先进的方法进行了比较，实验结果如表3、表4所示。具体方法如下：</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Performance of different methods on sub-relation clas</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Method</th><th align="center" valign="middle" >F<sub>1_micro</sub></th><th align="center" valign="middle" >F<sub>1_macro</sub></th><th align="center" valign="middle" >Accuracy</th><th align="center" valign="middle" >Subaccuracy</th></tr></thead><tr><td align="center" valign="middle" >C3D [<xref ref-type="bibr" rid="hanspub.40546-ref39">39</xref>]</td><td align="center" valign="middle" >0.3958</td><td align="center" valign="middle" >0.3018</td><td align="center" valign="middle" >0.5568</td><td align="center" valign="middle" >0.1451</td></tr><tr><td align="center" valign="middle" >LSTM [<xref ref-type="bibr" rid="hanspub.40546-ref39">39</xref>]</td><td align="center" valign="middle" >0.4714</td><td align="center" valign="middle" >0.4193</td><td align="center" valign="middle" >0.6547</td><td align="center" valign="middle" >0.3792</td></tr><tr><td align="center" valign="middle" >TSN [<xref ref-type="bibr" rid="hanspub.40546-ref16">16</xref>]</td><td align="center" valign="middle" >0.6034</td><td align="center" valign="middle" >0.4894</td><td align="center" valign="middle" >0.5412</td><td align="center" valign="middle" >0.3045</td></tr><tr><td align="center" valign="middle" >Multi-stream [<xref ref-type="bibr" rid="hanspub.40546-ref8">8</xref>]</td><td align="center" valign="middle" >0.7019</td><td align="center" valign="middle" >0.6383</td><td align="center" valign="middle" >0.6136</td><td align="center" valign="middle" >0.5291</td></tr><tr><td align="center" valign="middle" >STMV [<xref ref-type="bibr" rid="hanspub.40546-ref35">35</xref>]</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >0.7535</td><td align="center" valign="middle" >0.5249</td></tr><tr><td align="center" valign="middle" >TSM [<xref ref-type="bibr" rid="hanspub.40546-ref41">41</xref>]</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >0.8274</td><td align="center" valign="middle" >0.5936</td></tr><tr><td align="center" valign="middle" >ASRN [<xref ref-type="bibr" rid="hanspub.40546-ref42">42</xref>]</td><td align="center" valign="middle" >0.7353</td><td align="center" valign="middle" >0.6812</td><td align="center" valign="middle" >0.6722</td><td align="center" valign="middle" >0.5392</td></tr><tr><td align="center" valign="middle" >MSGRM (Ours)</td><td align="center" valign="middle" >0.7124</td><td align="center" valign="middle" >0.6725</td><td align="center" valign="middle" >0.7756</td><td align="center" valign="middle" >0.5824</td></tr></tbody></table></table-wrap><p>表3. Sub-relation类上不同方法的性能</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Performance of different methods on Obj-relation clas</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Method</th><th align="center" valign="middle" >F<sub>1_micro</sub></th><th align="center" valign="middle" >F<sub>1_macro</sub></th><th align="center" valign="middle" >Accuracy</th><th align="center" valign="middle" >Subaccuracy</th></tr></thead><tr><td align="center" valign="middle" >C3D [<xref ref-type="bibr" rid="hanspub.40546-ref39">39</xref>]</td><td align="center" valign="middle" >0.4383</td><td align="center" valign="middle" >0.3886</td><td align="center" valign="middle" >0.0557</td><td align="center" valign="middle" >0.0347</td></tr><tr><td align="center" valign="middle" >LSTM [<xref ref-type="bibr" rid="hanspub.40546-ref40">40</xref>]</td><td align="center" valign="middle" >0.6780</td><td align="center" valign="middle" >0.5776</td><td align="center" valign="middle" >0.6667</td><td align="center" valign="middle" >0.2797</td></tr><tr><td align="center" valign="middle" >TSN [<xref ref-type="bibr" rid="hanspub.40546-ref16">16</xref>]</td><td align="center" valign="middle" >0.7142</td><td align="center" valign="middle" >0.6142</td><td align="center" valign="middle" >0.7089</td><td align="center" valign="middle" >0.3482</td></tr><tr><td align="center" valign="middle" >Multi-stream [<xref ref-type="bibr" rid="hanspub.40546-ref8">8</xref>]</td><td align="center" valign="middle" >0.8119</td><td align="center" valign="middle" >0.6683</td><td align="center" valign="middle" >0.7436</td><td align="center" valign="middle" >0.5213</td></tr><tr><td align="center" valign="middle" >STMV [<xref ref-type="bibr" rid="hanspub.40546-ref35">35</xref>]</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >0.6322</td><td align="center" valign="middle" >0.5311</td></tr><tr><td align="center" valign="middle" >TSM [<xref ref-type="bibr" rid="hanspub.40546-ref41">41</xref>]</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >0.7125</td><td align="center" valign="middle" >0.6032</td></tr><tr><td align="center" valign="middle" >ASRN [<xref ref-type="bibr" rid="hanspub.40546-ref42">42</xref>]</td><td align="center" valign="middle" >0.8141</td><td align="center" valign="middle" >0.6766</td><td align="center" valign="middle" >0.7692</td><td align="center" valign="middle" >0.5259</td></tr><tr><td align="center" valign="middle" >MSGRM(Ours)</td><td align="center" valign="middle" >0.7945</td><td align="center" valign="middle" >0.6941</td><td align="center" valign="middle" >0.7924</td><td align="center" valign="middle" >0.5762</td></tr></tbody></table></table-wrap><p>表4. Obj-relation类上不同方法的性能</p><p>C3D [<xref ref-type="bibr" rid="hanspub.40546-ref39">39</xref>]：提出了一种基于3D卷积的网络结构，该网络结构在视频特征提取中具有良好的性能。</p><p>LSTM [<xref ref-type="bibr" rid="hanspub.40546-ref39">39</xref>]：基本的LSTM模型是一种流行的序列建模技术，具有各种改进。</p><p>TSN [<xref ref-type="bibr" rid="hanspub.40546-ref16">16</xref>]：TSN是一种典型的双流CNN网络，在许多视频分类数据集上都取得了最先进的性能。</p><p>Multi-stream [<xref ref-type="bibr" rid="hanspub.40546-ref8">8</xref>]：使用代表社会关系的多个特征来提高识别性能。</p><p>STMV [<xref ref-type="bibr" rid="hanspub.40546-ref35">35</xref>]：基于多视角(即RGB，光流和面部)的融合模型，使用多个注意力单元来学习时空信息以进行社会关系理解。</p><p>TSM [<xref ref-type="bibr" rid="hanspub.40546-ref41">41</xref>]：将语义对象提取、上下文交互和注意机制相结合的模型。</p><p>ASRN [<xref ref-type="bibr" rid="hanspub.40546-ref42">42</xref>]：一种端到端的可训练模型，融合了多角度特征，如图像、运动、身体、人脸。</p><p>MSGRM (Ours)：这是我们所提出的多尺度图推理模型，它采用MSGCN学习场景中人物的多尺度动态，并融合了场景的时空注意力，实现社会关系推理。</p><p>表3和4显示了我们的模型与最先进的方法比较的结果。我们的MSGRM达到了比较领先的性能。这是因为通过不同尺度的图模型，学习了场景中不同感受野的信息，提取了视频中的关键序列特征，最后融合我们的时空注意力，促进了我们的社会关系识别。C3D、LSTM和TSN的性能很差，这表明这些方法虽然可以更好地描述视频的其他一些特征，但却无法提取社会关系的正确表示。Multi-stream和STMV都只关注视频的时空特征，因此很难获得更好的性能。TSM和ASRN因为融合了场景中的各个角度的特征，这些特征很大程度上能表示场景的社会关系，所以性能有很大提升。</p></sec><sec id="s8_5"><title>4.5. 实例可视化</title><p>注意力机制能为我们的多尺度图推理模型推理出最相关的上下文语义对象，如图4给出一些实例。图中左边为我们的原始采样帧，中间为我们的注意力机制生成的一系列热图，右边为我们的热图所对象的语义对象边界框。特征图显示了我们的注意力机制能过准确的捕获场景中重要的语义对象，因此能够进行有效的人–物特征交互，以提升社会关系识别的准确性。</p><p>图4. 场景注意力可视化实例</p></sec></sec><sec id="s9"><title>5. 结束语</title><p>在这项工作中，我们提出了一个多尺度图推理模型来解决视频中的社会关系识别问题，并引入特征提取模块以丰富视频中的时空特征表示。具体来说，我们的模型利用MSGCN来探索视频中人物与场景语义之间的交互，并通过不同的时间感受野来学习长期和短期信息。最后融合一种注意力机制，该机制测量场景中每个节点的重要性，以自适应地选择最重要的对象以提高社会关系的性能。在数据集SRIV上进行的大量实验证明，我们提出的多尺度图推理模型取得了优秀的表现。</p></sec><sec id="s10"><title>文章引用</title><p>许 飞,张天雨,史俊彪. 视频社会关系识别的多尺度图推理模型Multi-Scale Graph Reasoning Model for Video Social Relation Recognition[J]. 计算机科学与应用, 2021, 11(02): 423-434. https://doi.org/10.12677/CSA.2021.112042</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.40546-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Wang, G., Gallagher, A.C., Luo, J.B. and Forsyth, D.A. (2010) Seeing People in Social Context: Recognizing People and Social Relationships. European Conference on Computer Vision, Glasgow, 23-28 August 2010, 169-182.  
&lt;br&gt;https://doi.org/10.1007/978-3-642-15555-0_13</mixed-citation></ref><ref id="hanspub.40546-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Park, Y.-J. and Chang, K.-N. (2009) Individual and Group Behavior-Based Customer Profile Model for Personalized Product Recommendation. Expert Systems with Applications, 36, 1932-1939.  
&lt;br&gt;https://doi.org/10.1016/j.eswa.2007.12.034</mixed-citation></ref><ref id="hanspub.40546-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Ding, L. and Yilmaz, A. (2011) Inferring Social Relations from Visual Concepts. IEEE International Conference on Computer Vision, Barcelona, 6-13 November 2011, 699-706. &lt;br&gt;https://doi.org/10.1109/ICCV.2011.6126306</mixed-citation></ref><ref id="hanspub.40546-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Yu, T., Lim, S.-N., Patwardhan, K.A. and Krahnstoever, N. (2009) Monitoring, Recognizing and Discovering Social Networks. IEEE Conference on Computer Vision and Pattern Recognition, Miami, 20-25 June 2009, 1462-1469.  
&lt;br&gt;https://doi.org/10.1109/CVPRW.2009.5206526</mixed-citation></ref><ref id="hanspub.40546-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Ramanathan, V., Huang, J., Abu-El-Haija, S., Gorban, A.N., Murphy, K. and Li, F.-F. (2016) Detecting Events and Key Actors in Multi-Person Videos. IEEE Conference on Com-puter Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 3043-3053. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.332</mixed-citation></ref><ref id="hanspub.40546-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Ramanathan, V., Yao, B.P. and Li, F.-F. (2013) Social Role Discov-ery in Human Events. IEEE Conference on Computer Vision and Pattern Recognition, Portland, 23-28 June 2013, 2475-2482. &lt;br&gt;https://doi.org/10.1109/CVPR.2013.320</mixed-citation></ref><ref id="hanspub.40546-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Bagautdinov, T.M., Alahi, A., Fleuret, F., Fua, P. and Savarese, S. (2017) Social Scene Understanding: End-to-End Multi-Person Action Localization and Collective Activity Recognition. IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 3425-3434. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.365</mixed-citation></ref><ref id="hanspub.40546-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Lv, J.N., Liu, W., Zhou, L.L., Wu, B. and Ma, H.D. (2018) Mul-ti-Stream Fusion Model for Social Relation Recognition from Videos. International Conference on Multimedia Modeling, Bangkok, 5-7 February 2018, 355-368.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-73603-7_29</mixed-citation></ref><ref id="hanspub.40546-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Li, F.-F. and Savarese, S. (2016) Social LSTM: Human Trajectory Prediction in Crowded Spaces. IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 961-971. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.110</mixed-citation></ref><ref id="hanspub.40546-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Choi, W. and Savarese, S. (2012) A Unified Framework for Mul-ti-Target Tracking and Collective Activity Recognition. European Conference on Computer Vision, Florence, 7-13 Octo-ber 2012, 215-230.  
&lt;br&gt;https://doi.org/10.1007/978-3-642-33765-9_16</mixed-citation></ref><ref id="hanspub.40546-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Li, J.N., Wong, Y.K., Zhao, Q. and Kankanhalli, M.S. (2017) Dual-Glance Model for Deciphering Social Relationships. ICCV 2017, Palazzo del Cinema, 28 October 2017, 2669-2678.</mixed-citation></ref><ref id="hanspub.40546-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Z.P., Luo, P., Loy, C.C. and Tang, X.O. (2015) Learning Social Relation Traits from Face Im-ages. ICCV, Santiago, 7-13 December 2015, 3631-3639. &lt;br&gt;https://doi.org/10.1109/ICCV.2015.414</mixed-citation></ref><ref id="hanspub.40546-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Sun, Q.R., Schiele, B. and Fritz, M. (2017) A Domain Based Approach to Social Relation Recognition. IEEE Conference on Com-puter Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 435-444.</mixed-citation></ref><ref id="hanspub.40546-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Wang, Z.X., Chen, T.S., Ren, J.S.J., Yu, W.H., Cheng, H. and Lin, L. (2018) Deep Reasoning with Knowledge Graph for Social Relationship Understanding. International Joint Conference on Artificial Intelligence, 1021-1028.  
&lt;br&gt;https://doi.org/10.24963/ijcai.2018/142</mixed-citation></ref><ref id="hanspub.40546-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Bugental, D.B. (2000) Acquisition of the Algorithms of Social Life: A Domain-Based Approach. Psychological Bulletin, 126, 187. &lt;br&gt;https://doi.org/10.1037/0033-2909.126.2.187</mixed-citation></ref><ref id="hanspub.40546-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Wang, L.M., Xiong, Y.J., Wang, Z., Qiao, Y., Lin, D.H., Tang, X.O. and Van Gool, L. (2016) Temporal Segment Networks: Towards Good Practices for Deep Action Recognition. 14th European Conference, Amsterdam, 11-14 October 2016, 20-36. &lt;br&gt;https://doi.org/10.1007/978-3-319-46484-8_2</mixed-citation></ref><ref id="hanspub.40546-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Lin, L., Wang, X.L., Yang, W. and Lai, J.-H. (2015) Discrimi-natively Trained And-Or Graph Models for Object Shape Detection. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 37, 959-972.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2014.2359888</mixed-citation></ref><ref id="hanspub.40546-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Felzenszwalb, P.F. and Huttenlocher, D.P. (2004) Efficient Graph-Based Image Segmentation. International Journal of Computer Vision, 59, 167-181. &lt;br&gt;https://doi.org/10.1023/B:VISI.0000022288.19776.77</mixed-citation></ref><ref id="hanspub.40546-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Liu, W., Jiang, Y.-G., Luo, J.B. and Chang, S.-F. (2011) Noise Resistant Graph Ranking for Improved Web Image Search. IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, 20-25 June 2011, 849-856.</mixed-citation></ref><ref id="hanspub.40546-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Kipf, T.N. and Welling, M. (2016) Semi-Supervised Classification with Graph Convolutional Networks.</mixed-citation></ref><ref id="hanspub.40546-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Defferrard, M., Bresson, X. and Vandergh-eynst, P. (2016) Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. Advances in Neural Information Processing Systems, Barcelona, 5-10 December 2016, 3837- 3845.</mixed-citation></ref><ref id="hanspub.40546-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Li, Y.J., Tarlow, D., Brockschmidt, M. and Zemel, R.S. (2015) Gated Graph Sequence Neural Networks.</mixed-citation></ref><ref id="hanspub.40546-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Wang, X.L. and Gupta, A. (2018) Videos as Space-Time Region Graphs. European Conference on Computer Vision, Munich, 8-14 September 2018, 413-431. &lt;br&gt;https://doi.org/10.1007/978-3-030-01228-1_25</mixed-citation></ref><ref id="hanspub.40546-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Liang, X.D., Shen, X.H., Feng, J.S., Lin, L. and Yan, S.C. (2016) Semantic Object Parsing with Graph LSTM. European Conference on Computer Vision, Amsterdam, 8-16 Octo-ber 2016, 125-143.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46448-0_8</mixed-citation></ref><ref id="hanspub.40546-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Qi, X.J., Liao, R.J., Jia, J.Y., Fidler, S. and Urtasun, R. (2017) 3D Graph Neural Networks for RGBD Semantic Segmentation. IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 5209-5218.</mixed-citation></ref><ref id="hanspub.40546-ref26"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Phan, M.C., Sun, A.X., Tay, Y., Han, J.L. and Li, C.L. (2017) NeuPL: Attention-Based Semantic Matching and Pair- Linking for Entity Disambiguation. Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, Singapore, 6-10 November 2017, 1667-1676. &lt;br&gt;https://doi.org/10.1145/3132847.3132963</mixed-citation></ref><ref id="hanspub.40546-ref27"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Xu, J., Yao, T., Zhang, Y.D. and Mei, T. (2017) Learning Multi-modal Attention LSTM Networks for Video Captioning. Proceedings of the 25th ACM International Conference on Mul-timedia, Mountain View, 23-27 October 2017, 537-545. &lt;br&gt;https://doi.org/10.1145/3123266.3123448</mixed-citation></ref><ref id="hanspub.40546-ref28"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">Li, Y., Miao, Z., He, M., Zhang, Y.F. and Li, H. (2018) Deep Attention Residual Hashing. IEICE Transactions on Fundamen-tals of Electronics, Communications and Computer Sciences, 101-A, 654-657.  
&lt;br&gt;https://doi.org/10.1587/transfun.E101.A.654</mixed-citation></ref><ref id="hanspub.40546-ref29"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">Bin, Y., Yang, Y., Shen, F.M., Xie, N., Shen, H.T. and Li, X.L. (2019) Describing Video with Attention-Based Bidirectional LSTM. IEEE Transactions on Cybernetics, 49, 2631-2641. &lt;br&gt;https://doi.org/10.1109/TCYB.2018.2831447</mixed-citation></ref><ref id="hanspub.40546-ref30"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, F., Li, H.S., Ouyang, W.L., Yu, N.H. and Wang, X.G. (2017) Learning Spatial Regularization with Image-Level Supervisions for Multi-label Image Classification. IEEE Con-ference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 2027-2036. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.219</mixed-citation></ref><ref id="hanspub.40546-ref31"><label>31</label><mixed-citation publication-type="other" xlink:type="simple">Girdhar, R. and Ramanan, D. (2017) Attentional Pooling for Action Recognition. NIPS 2017, Long Beach, 4-9 December 2017, 34-45.</mixed-citation></ref><ref id="hanspub.40546-ref32"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Rao, T.R., Li, X.X., Zhang, H.M. and Xu, M. (2019) Multi-Level Region-Based Convolutional Neural Network for Image Emotion Classification. Neurocomputing, 333, 429-439. &lt;br&gt;https://doi.org/10.1016/j.neucom.2018.12.053</mixed-citation></ref><ref id="hanspub.40546-ref33"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">Pei, W.J., Baltrusaitis, T., Tax, D.M.J. and Morency, L.-P. (2016) Temporal Attention-Gated Model for Robust Sequence Classification.</mixed-citation></ref><ref id="hanspub.40546-ref34"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">He, K.M., Zhang, X.Y., Ren, S.Q. and Sun, J. (2016) Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778.</mixed-citation></ref><ref id="hanspub.40546-ref35"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Lv, J.N. and Wu, B. (2019) Spa-tio-Temporal Attention Model Based on Multi-View for Social Relation Understanding. 25th International Conference on Multi-Media Modeling, Thessaloniki, 8-11 January 2019, 1-12.</mixed-citation></ref><ref id="hanspub.40546-ref36"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">Ren, S.Q., He, K.M., Girshick, R.B. and Sun, J. (2015) Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Advances in Neural In-formation Processing Systems (NIPS 2015), Vol. 28, 91-99.</mixed-citation></ref><ref id="hanspub.40546-ref37"><label>37</label><mixed-citation publication-type="other" xlink:type="simple">Lin, T.-Y., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L. (2014) Microsoft COCO: Common Objects in Context. 13th European Conference, Zurich, 6-12 September 2014, 740-755.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-10602-1_48</mixed-citation></ref><ref id="hanspub.40546-ref38"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan, K. and Zisserman, A. (2014) Very Deep Convolu-tional Networks for Large-Scale Image Recognition.</mixed-citation></ref><ref id="hanspub.40546-ref39"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">Tran, D., Bourdev, L.D., Fergus, R., Torresani, L. and Paluri, M. (2015) Learning Spatiotemporal Features with 3D Convolutional Networks. ICCV, Santiago, 13-16 December 2015, 4489-4497. &lt;br&gt;https://doi.org/10.1109/ICCV.2015.510</mixed-citation></ref><ref id="hanspub.40546-ref40"><label>40</label><mixed-citation publication-type="other" xlink:type="simple">Findler, N.V. (1972) Short Note on a Heuristic Search Strategy in Long-Term Memory Networks. Information Processing Letters, 1, 191-196. &lt;br&gt;https://doi.org/10.1016/0020-0190(72)90037-3</mixed-citation></ref><ref id="hanspub.40546-ref41"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">Dai, P.L., Lv, J.N. and Wu, B. (2019) Two-Stage Model for Social Relationship Understanding from Videos. ICME 2019, Shanghai, 8-12 July 2019, 1132-1137. &lt;br&gt;https://doi.org/10.1109/ICME.2019.00198</mixed-citation></ref><ref id="hanspub.40546-ref42"><label>42</label><mixed-citation publication-type="other" xlink:type="simple">Lv, J.N., Wu, B., Zhang, Y.L. and Xiao, Y.P. (2019) Attentive Sequences Recurrent Network for Social Relation Recognition from Video. IEICE Transactions on Information and Systems, 102-D, 2568-2576.  
&lt;br&gt;https://doi.org/10.1587/transinf.2019EDP7104</mixed-citation></ref></ref-list></back></article>