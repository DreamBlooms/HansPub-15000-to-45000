<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2020.98146</article-id><article-id pub-id-type="publisher-id">AAM-37169</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20200800000_97112109.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  稀疏耦合神经网络的重构
  Reconstruction of Sparse Coupled Neural Networks
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>董</surname><given-names>国梨</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>肖</surname><given-names>玉柱</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>长安大学理学院，陕西 西安</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>04</day><month>08</month><year>2020</year></pub-date><volume>09</volume><issue>08</issue><fpage>1246</fpage><lpage>1254</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    结合实际中神经网络的稀疏耦合特性，基于等式约束的L1优化方法下，本文发展了一类描述神经网络不规则放电动力学模型中耦合矩阵的重构方法，并通过数值模拟验证了该方法的有效性。与先前的基于SVD分解的重构方法相比较，本文的方法能在更短的观测时间内达到更高的重构精度。
    Considering the sparse coupling characteristic of real neural network, a method based on the L1 optimization method with equality constraints is developed to reconstruct the coupling matrix of the dynamic model of neural networks with irregular firing rate. The effectiveness of proposed method is verified by numerical simulation. Compared with the previous reconstruction method based on SVD decomposition, the proposed method can achieve higher reconstruction accuracy in a shorter observation time. 
  
 
</p></abstract><kwd-group><kwd>神经网络，L1优化，网络重构, Neural Networks</kwd><kwd> L1 Optimization</kwd><kwd> Network Reconstruction</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>稀疏耦合神经网络的重构</title><p>董国梨<sup>*</sup>，肖玉柱<sup>#</sup></p><p>长安大学理学院，陕西 西安</p><p>收稿日期：2020年7月26日；录用日期：2020年8月13日；发布日期：2020年8月20日</p><disp-formula id="hanspub.37169-formula36"><graphic xlink:href="//html.hanspub.org/file/16-2621281x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>结合实际中神经网络的稀疏耦合特性，基于等式约束的L<sub>1</sub>优化方法下，本文发展了一类描述神经网络不规则放电动力学模型中耦合矩阵的重构方法，并通过数值模拟验证了该方法的有效性。与先前的基于SVD分解的重构方法相比较，本文的方法能在更短的观测时间内达到更高的重构精度。</p><p>关键词 :神经网络，L<sub>1</sub>优化，网络重构</p><disp-formula id="hanspub.37169-formula37"><graphic xlink:href="//html.hanspub.org/file/16-2621281x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/16-2621281x8_hanspub.png" /> <img src="//html.hanspub.org/file/16-2621281x9_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>基于观测数据的网络重构是一个具有挑战性的问题，它在物理学、生态学、流行病学、遗传调控、生物化学和生物动力学等许多学科中有着重要的应用 [<xref ref-type="bibr" rid="hanspub.37169-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.37169-ref2">2</xref>]。特别是在神经科学领域，比如：通过多通道脑电图信号 [<xref ref-type="bibr" rid="hanspub.37169-ref3">3</xref>] 重构各脑区之间的连接关系等，揭示各脑区间的协调工作机理是脑科学研究领域的重要课题之一。</p><p>网络重构方法主要有两类：一种是通过观察观测量间的统计相关性揭示网络各节点间的相互作用关系，如Luo等人利用Granger因果关系重构基因网络 [<xref ref-type="bibr" rid="hanspub.37169-ref4">4</xref>]；Yang等人利用Granger因果关系重构复杂定向网络等 [<xref ref-type="bibr" rid="hanspub.37169-ref5">5</xref>]。第二种是假设观测的量可由复杂动力系统描述，并试图基于此复杂动力系统描述重构网络各节点间的关系，如：Wang等人基于压缩感知理论从少量的数据中揭示了交互网络 [<xref ref-type="bibr" rid="hanspub.37169-ref6">6</xref>]；Huang等人基于贝叶斯压缩感知重构网络等 [<xref ref-type="bibr" rid="hanspub.37169-ref7">7</xref>]。</p><p>动力学模型是描述神经网络的重要模型，在计算神经学中有着重要应用 [<xref ref-type="bibr" rid="hanspub.37169-ref8">8</xref>]。在动力学模型中，神经网络由一组耦合的标量的微分方程描述。一些研究表明，当神经元间耦合强度较大时，神经元将呈现不规则放电行为 [<xref ref-type="bibr" rid="hanspub.37169-ref9">9</xref>]。在神经元处于不规则放电的情况下，基于描述神经网络的动力学模型，Pikovsky提出了一种重构神经网络的方法 [<xref ref-type="bibr" rid="hanspub.37169-ref9">9</xref>]。该方法的是通过相应的观测数据构造矩阵A，使得网络的耦合系数x满足 A x = 0 ，这样网络的重构问题则可转化为矩阵A的SVD (Singular Value Decomposition)分解方法求解。一般当观测数据充足的情况下，上述方法能够较准确的重构网络。然而，实际中获得系统的观测数据总是有限的，并且获得系统的观测数据成本昂贵。因此，在尽可能少的观测数据下实现系统的重构有着重要的现实意义和应用价值。考虑到实际中神经元间的连接具有稀疏性，本文将在Pikovsky方法的基础上，结合神经网络的稀疏耦合特性，利用等式约束的L<sub>1</sub>优化方法重构网络，进一步降低重构网络所需的数据量，提高重构精度。</p></sec><sec id="s4"><title>2. 模型与方法</title><sec id="s4_1"><title>2.1. 模型介绍</title><p>本文考虑计算神经科学中描述神经元放电的一般模型，模型中n个神经元的放电率 x i ( t ) 由如下微分方程描述 [<xref ref-type="bibr" rid="hanspub.37169-ref10">10</xref>]：</p><p>τ i d x i d t + x i = F i ( ∑ j = 1 n w i j x j ) ， i = 1 , ⋯ , n 。 (1)</p><p>其中 τ i 是节点i的神经元的松弛常数， F i 是节点i的增益函数，神经元之间的耦合由 n &#215; n 的矩阵 W = { w i j } n &#215; n</p><p>确定。当神经元间的耦合足够强时，神经网络(1)会表现出混沌状态。混沌状态确保了神经元的放电率 x i ( t ) 有足够的可变性，这使得能够从 x i ( t ) 的观测值重构网络耦合矩阵。如果系统不是混沌状态，将无法完成重构，如周期轨道，稳定状态等 [<xref ref-type="bibr" rid="hanspub.37169-ref9">9</xref>]。</p></sec><sec id="s4_2"><title>2.2. 重构方法</title><p>在上述模型(1)中，假设可以观测得到关于 x i ( t ) , x ˙ i ( t ) 所有的时间序列，则问题变成从观测到的时间序列重构耦合矩阵W。一般情况下，增益函数 F i 未知且不同，本文的方法不需要知道 F i 的具体形式，只要求增益函数 F i 是双射。为了更加方便的讨论问题，本文主要讨论函数 F 1 、常数 τ 1 以及耦合系数 w 1 j ，其他节点可类似的进行计算。</p><p>令 c j = w 1 j ( j = 1 , ⋯ , n ) ，则向量 c 表示耦合矩阵的第一行，即第一个神经元和其它神经元的耦合关系。假设参数 τ 1 已知，对一个给定的y值，找到 τ 1 x ˙ 1 + x 1 位于y的小邻域内的所有时刻，假设找到相对应的时</p><p>刻为 t 1 , t 2 , ⋯ , t m + 1 。令 x ( t k ) = [ x 1 ( t k ) , x 2 ( t k ) , ⋯ , x n ( t k ) ] ，则对任意 i , j ∈ [ 1 , m + 1 ] ， F 1 ( c ⋅ x ( t i ) ) ≈ y ，</p><p>F 1 ( c ⋅ x ( t j ) ) ≈ y 同时成立。由于本文采用的增益函数 F 1 为双射，所以近似的有</p><p>c ⋅ x ( t i ) ≈ c ⋅ x ( t j ) ，</p><p>即</p><p>c ⋅ [ x ( t i ) − x ( t j ) ] ≈ 0 。 (2)</p><p>又由 c ⋅ [ x ( t i ) − x ( t k ) ] ≈ 0 ， c ⋅ [ x ( t i ) − x ( t j ) ] ≈ 0 可以推出 c ⋅ [ x ( t k ) − x ( t j ) ] ≈ 0 ，所以向量 [ x ( t i ) − x ( t k ) ] ， [ x ( t i ) − x ( t j ) ] ， [ x ( t k ) − x ( t j ) ] 之间有较强的相关性。为了降低向量之间的相关系，我们进行如下计算形式的计算</p><p>z ( k ) = [ x ( t k + 1 ) − x ( t k ) ] ， k = 1 , ⋯ , m 。(3)</p><p>则方程(2)可写为</p><p>z ( k ) ⋅ c = 0 。 (4)</p><p>由于(4)式的构造不依赖y，所以可以选取不同的y值找到一系列满足(4)式的 z ( k ) ，把找到所有 z ( k ) 组成矩阵A，则可得如下方程</p><p>A ⋅ c = 0 。 (5)</p><p>于是重构神经网络的第一个节点的耦合系数转化为求方程(5)的非零解。一般情况下通过SVD对矩阵A进行分解，找矩阵A的最小奇异值 λ ，则酉矩阵中最小奇异值 λ 所的对应列为方程(5)的非零解。当观测数据充足的情况下，上述方法能够较准确的重构网络。然而，实际中获得系统的观测数据总是有限的，并且获得系统的观测数据成本昂贵，因此在尽可能少的数据条件下重构网络的连接有着重要的实际意义和应用价值。考虑到实际的神经网络常有稀疏耦合，所以对方程(5)增加稀疏限制条件，则变成</p><p>min ‖ c ‖ 1 s .t . 0 = A ⋅ c 。 (6)</p><p>其中 ‖ c ‖ 1 = ∑ i = 1 n | c i | 是向量 c 的L<sub>1</sub>范数。显然零向量是优化问题(6)的最优解，但我们要寻找优化问题(6)的非零最优解。令 c i ~ = c i / ∑ i = 1 n c i ，则求解优化问题(6)的非零最优解问题转化为</p><p>min ‖ c ~ ‖ 1 s .t . d = A &#175; ⋅ c ~ 。 (7)</p><p>其中矩阵 A &#175; = [ A a ] ，向量 a = [ 1 , ⋯ , 1 ] ，向量 d = [ 0 , 0 , ⋯ , 0 , 1 ] T 。上述问题中等式约束的L<sub>1</sub>优化问题，可在线性优化框架下求解 [<xref ref-type="bibr" rid="hanspub.37169-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.37169-ref12">12</xref>]。</p><p>上述讨论是在 τ 1 已知的情况下进行，但现实生活中 τ 1 常是未知的。在这种情况下，我们可以给定 τ 1 一个合理范围，在不同的 τ 1 值下求解式优化问题(7)，其解记为 b ⌢ 。显然，正确的 τ 1 值应该使 ‖ A &#175; b ^ − d ‖ 2 取得最小值。</p></sec><sec id="s4_3"><title>2.3. 数据处理</title><p>上一小节中，矩阵 A &#175; 的构造是在增益函数 F 1 取相近的值时所对应的自变量取值也相近的假设下进行的。然而，由于神经网络模型中常用sigmoid函数作为增益函数。对于sigmoid函数存在着 τ 1 x ˙ 1 + x 1 导数接近于0的点，在这些点导致增益函数 F 1 的反函数是近乎奇异的。因此，本文采用 | y ˙ | &gt; δ 的条件除去导数接近于0，其中 δ 为选定的阈值。</p><p>一般情况下，通过观测数据构造矩阵 A &#175; 后，我们并不直接求解优化问题(7)，而是对矩阵 A &#175; 作进一步的处理。这里假设 A &#175; 为 l &#215; n 维矩阵，对矩阵 A &#175; 做中心化和归一化处理，即</p><p>a ′ i j = ( a &#175; i j − ∑ j = 1 l a &#175; i j / l ) / ‖ a &#175; i j − ∑ j = 1 l a &#175; i j / l ‖ 2 。(8)</p><p>对向量 d 做中心化处理，即</p><p>d ′ j = d j − ∑ j = 1 l d j / l 。 (9)</p><p>这样可以降低矩阵的每一列的振幅对优化求解的影响。进而，代替优化问题(7)，我们求解</p><p>min ‖ c ′ ‖ 1 s .t . d ′ = A ′ ⋅ c ′ 。 (10)</p></sec></sec><sec id="s5"><title>3. 数值模拟</title><p>本节将以随机网络和小世界网络为例对本文的重构方法进行模拟验证，并和SVD分解方法进行比较。</p><sec id="s5_1"><title>3.1. 随机网络</title><p>在数值模拟中，令节点数 n = 100 ，松弛常数 τ i ∈ [ 0 .9 , 1.1 ] 通过均匀分布产生。增益函为sigmoid函数： F i ( u ) = α i / [ 1 + exp ( − u − ρ i ) ] ，其中 α i ∈ [ 0 .9 , 1.1 ] 通过均匀分布产生； ρ i = η i − 0.5 ⋅ ∑ j w i j ，其中 η i 通过标准正态分布 N ( 0 , 1 ) 产生。耦合系数矩阵W的元素以概率0.15生成，其取值通过正态分布 8 N ( 0 , 1 ) 产生。</p><p>图1展示了上述参数下生成的前10个 x i ( t ) 的混沌图像，此混沌状态是本文方法重构的前提。图2中的*表示根据 | y ˙ | &gt; δ 的条件取得的数据，其中 y = τ 1 x ˙ 1 + x 1 ，采样时间间隔0.01，阈值常数 δ = 0 .3 。从图中清晰的看到随着时间t变化较快的y值保留下来，而几乎为恒定的y值被舍弃。</p><p>在图2获得的数据下，选取不同的 τ 1 值，利用本文的方法对网络进行重构，并计算相应的 ‖ A ′ c ′ − d ′ ‖ 2 的值。图3绘制了 ‖ A ′ c ′ − d ′ ‖ 2 的值随松弛常数 τ 1 的变化曲线，其中从下到上的曲线对应的观测时间分别为 t = 100 , 120 , 140 , 160 ，竖直的直线表示正确的 τ 1 值。从图中看到 ‖ A ′ c ′ − d ′ ‖ 2 在正确 τ 1 附近取得最小值，因此可以通过寻找 ‖ A ′ c ′ − d ′ ‖ 2 最小值方法估计 τ 1 。在相同的数据下，利用SVD方法对网络进行重构，图4绘制了矩阵 A ′ 的最小奇异值 λ 随 τ 1 的变化曲线，其中从下到上的图形对应的观测时间分别为 t = 100 , 120 , 140 , 160 ，竖直的直线为正确的 τ 1 值。比较图3和图4，我们可以看出本文的方法在更短的观测时间给出 τ 1 值更为准确的估计。基于图3和图4确定的松弛常数 τ 1 ，我们分别利用本文中等式约束的L<sub>1</sub>优化方法和SVD方法对网络进行重构，图5展示了重构误差 ‖ w 1 − w ′ 1 ‖ 2 随观测时间t的变化曲线，其中 w 1 代表正确的耦合系数， w ′ 1 为重构的耦合系数。通过图5，我们容易看出本文的方法在更短的观测时间内达到了更高精度的重构。</p><p>图1. x i ( t ) ( i = 1 , ⋯ , 10 ) 随时间的变化图</p><p>图2. | y ˙ | &gt; δ 的条件下选取的数据</p><p>图3. ‖ A ′ c ′ − d ′ ‖ 2 随 τ 1 变化曲线</p><p>图4. 最小奇异值 λ 随 τ 1 变化曲线</p><p>图5. 两种方法的重构误差图</p></sec><sec id="s5_2"><title>3.2. 小世界网络</title><p>小世界网络最早由Duncan Watts和Steven Strogatz在1998年引进的，将高聚合系数与低平均路径长度作为特征，提出的一种新的复杂网络结构。其构造算法为：首先给定一个具有n个节点的环形规则网络，每个网络节点 V i 左右对称地连接它的k个邻居节点，即左右每侧各有k/2个连接点，然后以概率 P ( 0 &lt; P &lt; 1 ) 对其中任一个节点的每一条边进行断开重连，即保持该边一端不变，另一端重新选择一个新的节点连接，要避免自身连接和节点间的重复连接，以此节点开始，按顺序依次对每个节点进行边的重连，最后得到小世界网络 [<xref ref-type="bibr" rid="hanspub.37169-ref13">13</xref>]。一些研究表明，生物学、物理学、社会学中大多数网络都具有小世界网络的特征，因此研究小世界网络具有广泛的现实意义。</p><p>本文中以节点数 n = 100 ，每个节点的连接边数 k = 10 ，连接概率 P = 0 .3 产生小世界网络，网络中的其他参数设置与3.1中的随机网络相同，相应的数值模拟结果见图6~9。</p><p>图6展示了在随机网络的参数下生成的前10个 x i ( t ) 的小世界网络的混沌图像。</p><p>小世界网络的数据同样进行了图2方式的数据筛选。图7绘制了 ‖ A ′ c ′ − d ′ ‖ 2 的值随不同松弛常数 τ 1 的变化曲线，其中从下到上的曲线对应的观测时间分别为 t = 170 , 190 , 210 , 230 ，竖直的直线表示正确的 τ 1 值。从图中看到 ‖ A ′ c ′ − d ′ ‖ 2 在正确 τ 1 附近取得最小值，因此可以通过寻找 ‖ A ′ c ′ − d ′ ‖ 2 最小值方法估计 τ 1 。在相同的数据下，利用SVD方法对网络进行重构，图8绘制矩阵 A ^ 的最小奇异值 λ 随 τ 1 的变化曲线，其中从下到上的图形对应的观测时间分别为 t = 170 , 190 , 210 , 230 ，竖直的直线为正确的 τ 1 值。比较图7和图8，我们可以看出图8中在时间 t = 170 未能找到正确的 τ 1 ；当时间 t &gt; 170 ，找到的<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x154_hanspub.png" xlink:type="simple"/></inline-formula>存在误差，即在小世界网络中本文的方法在更短的观测时间给出<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x155_hanspub.png" xlink:type="simple"/></inline-formula>值更为准确的估计。基于图7和图8确定的松弛常数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x156_hanspub.png" xlink:type="simple"/></inline-formula>，分别利用本文中等式约束的L<sub>1</sub>优化方法和SVD方法对网络进行重构，图9展示了重构误差<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x157_hanspub.png" xlink:type="simple"/></inline-formula>随观测时间t的变化曲线，其中<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x158_hanspub.png" xlink:type="simple"/></inline-formula>代表正确的耦合系数，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x159_hanspub.png" xlink:type="simple"/></inline-formula>为重构的耦合系数。通过图9，我们容易看出在小世界网络中本文的方法在更短的观测时间内达到了更高精度的重构。</p><p>图6.<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x162_hanspub.png" xlink:type="simple"/></inline-formula>随时间的变化图</p><p>图7.<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x166_hanspub.png" xlink:type="simple"/></inline-formula>随<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x167_hanspub.png" xlink:type="simple"/></inline-formula>变化图</p><p>图8. 最小奇异值<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x171_hanspub.png" xlink:type="simple"/></inline-formula>随<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/16-2621281x172_hanspub.png" xlink:type="simple"/></inline-formula>变化图</p><p>图9. 两种方法的重构误差图</p></sec></sec><sec id="s6"><title>4. 结束语</title><p>基于观测数据的网络重构是一个具有挑战性的问题，特别在神经科学领域，通过多通道脑电图信号重构各脑区之间的连接关系，揭示各脑区间的协调工作机理是脑科学研究领域的重要课题之一。本文在先前Pikovsky工作的基础上，结合实际中神经网络的稀疏耦合特性，基于等式约束的L<sub>1</sub>优化方法，发展了重构一类描述神经网络不规则放电动力学模型中耦合矩阵的重构方法，并通过数值模拟验证了该方法的有效性。与Pikovsky的基于SVD分解的重构方法相比较，本文的方法能在更短的观测时间内达到更高的重构精度。由于实际的观测数据十分有限，因此发展在更少的数据量下重的网络重构方法是我们进一步的研究目标。此外，本文的方法和基于SVD分解的重构方法一般自适用于节点较少的小型网络系统，发展大型网络系统的重构方法是我们的另一研究目标。</p></sec><sec id="s7"><title>基金项目</title><p>长安大学高新技术培育项目(300102129202)，长安大学卓越青年项目(310812163504)。</p></sec><sec id="s8"><title>文章引用</title><p>董国梨,肖玉柱. 稀疏耦合神经网络的重构Reconstruction of Sparse Coupled Neural Networks[J]. 应用数学进展, 2020, 09(08): 1246-1254. https://doi.org/10.12677/AAM.2020.98146</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.37169-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Li, Z., Li, P., Krishnan, A. and Liu, J.D. (2011) Large-Scale Dynamic Gene Regulatory Network Inference Combining Differential Equation Models with Local Dynamic Bayesian Network Analysis. Bioinformatics, 27, 2686-2691.  
&lt;br&gt;https://doi.org/10.1093/bioinformatics/btr454</mixed-citation></ref><ref id="hanspub.37169-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Tirabassi, G., Sommerlade, L. and Masoller, C. (2017) Inferring Directed Climatic Interactions with Renormalized Partial Directed Coherence and Directed Partial Correlation. Chaos, 27, Article ID: 035815.  
&lt;br&gt;https://doi.org/10.1063/1.4978548</mixed-citation></ref><ref id="hanspub.37169-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Dickten, H. and Lehnertz, K. (2014) Identifying Delayed Directional Couplings with Symbolic Transfer Entropy. Physical Review E: Statistical, Nonlinear, and Soft Matter Physics, 90, Article ID: 062706.  
&lt;br&gt;https://doi.org/10.1103/PhysRevE.90.062706</mixed-citation></ref><ref id="hanspub.37169-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Luo, Q., Liu, X. and Yi, D.Y. (2009) Reconstructing Gene Networks from Microarray Time-Series Data via Granger Causality. In: Zhou, J., Ed., Complex Sciences, Springer, Berlin, 196-209.  
&lt;br&gt;https://doi.org/10.1007/978-3-642-02466-5_17</mixed-citation></ref><ref id="hanspub.37169-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Yang, G.X., Wang, L. and Wang, X.F. (2017) Reconstruction of Complex Directional Networks with Group Lasso Nonlinear Conditional Granger Causality. Scientific Reports, 7, Article No. 2991.  
&lt;br&gt;https://doi.org/10.1038/s41598-017-02762-5</mixed-citation></ref><ref id="hanspub.37169-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Wang, W.X., Lai, Y.C., Grebogi, C. and Ye, J.P. (2011) Network Reconstruction Based on Evolutionary-Game Data via Compressive Sensing. Physical Review X, 1, Article ID: 021021. &lt;br&gt;https://doi.org/10.1103/PhysRevX.1.021021</mixed-citation></ref><ref id="hanspub.37169-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Huang, K.K., Jiao, Y., Liu, C., Deng, W.F. and Wang, Z. (2019) Robust Network Structure Reconstruction Based on Bayesian Compressive Sensing. Chaos, 29, Article ID: 093119. &lt;br&gt;https://doi.org/10.1063/1.5109375</mixed-citation></ref><ref id="hanspub.37169-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Pikovsky, A. (2017) Reconstruction of a Scalar Voltage-Based Neural Field Network from Observed Times Series. EPL, 119, Article ID: 30004. &lt;br&gt;https://doi.org/10.1209/0295-5075/119/30004</mixed-citation></ref><ref id="hanspub.37169-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Pikovsky, A. (2016) Reconstruction of a Neural Network from a Time Series of Firing Rates. Physical Review E, 93, Article ID: 062313. &lt;br&gt;https://doi.org/10.1103/PhysRevE.93.062313</mixed-citation></ref><ref id="hanspub.37169-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Ermentrout, G.B. and Terman, D.H. (2010) Mathematical Foundations of Neuroscience (Interdisciplinary Applied Mathematics). Springer, New York, Vol. 35. &lt;br&gt;https://doi.org/10.1007/978-0-387-87708-2</mixed-citation></ref><ref id="hanspub.37169-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Boyd, S. and Vandenberghe, L. (2004) Convex Optimization. Cambridge University Press, Cambridge.  
&lt;br&gt;https://doi.org/10.1017/CBO9780511804441</mixed-citation></ref><ref id="hanspub.37169-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Wang, W.-X., Lai, Y.-C. and Grebogi, C. (2016) Data Based Identification and Prediction of Nonlinear and Complex Dynamical Systems. Physics Reports, 644, 1-76. &lt;br&gt;https://doi.org/10.1016/j.physrep.2016.06.004</mixed-citation></ref><ref id="hanspub.37169-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">石娟, 郑鹏, 徐凌峰, 常丁懿. 小世界网络中的大学生危机行为传播仿真研究[J]. 中国安全科学学报, 2019, 29(12): 21-27.</mixed-citation></ref></ref-list></back></article>