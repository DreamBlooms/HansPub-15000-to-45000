<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">IaE</journal-id><journal-title-group><journal-title>Instrumentation and Equipments</journal-title></journal-title-group><issn pub-type="epub">2332-6980</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/IaE.2021.91003</article-id><article-id pub-id-type="publisher-id">IaE-41183</article-id><article-categories><subj-group subj-group-type="heading"><subject>IaE20210100000_69786428.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>工程技术</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于深度学习的人机协同病理显微镜设计
  Design of Human-Computer Collaborative Pathology Microscope Based on Deep Learning
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>丁</surname><given-names>勇</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>倪</surname><given-names>爽</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>崔</surname><given-names>笑宇</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>魏</surname><given-names>然</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>东北大学医学与生物信息工程学院，辽宁 沈阳</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>05</day><month>03</month><year>2021</year></pub-date><volume>09</volume><issue>01</issue><fpage>15</fpage><lpage>21</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    本文结合传统的病理光学显微镜，提出了基于深度学习进行人机协同的病理显微镜设计方法，将病理图像倍率识别模块、病理图像拍摄模块、病理图像处理模块、分割结果投影模块等嵌入传统光学显微镜。其中在图像处理模块的算法部分，我们在医学图像分割网络Unet网络基础上加入Critic模块建立深度学习模型，该模型可以同时学习全局和局部的特征，分割癌变的区域。本文提出的显微镜设计方法可以实现病理医生在物镜下移动病理样本，显微镜可以实时分割图像并展示癌变区域。
    Combining with the traditional pathological optical microscope, a pathological microscope design method based on deep learning and man-machine collaboration is proposed. The pathological image shooting module, pathological image processing module and segmentation result projection module are embedded in the optical microscope. In the algorithm part of the image processing module, the deep learning model is established by adding the CRITIC module on the basis of the medical image segmentation network UNET. The model can learn global and local features at the same time. The microscope design method proposed in this paper can realize when the pathologist moves the sample under the objective lens, the microscope can segment the image in real time and show the cancerous area. 
  
 
</p></abstract><kwd-group><kwd>深度学习，人机交互，增强现实, Deep Learning</kwd><kwd> Human-Computer Interaction</kwd><kwd> Augmented Reality</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>本文结合传统的病理光学显微镜，提出了基于深度学习进行人机协同的病理显微镜设计方法，将病理图像倍率识别模块、病理图像拍摄模块、病理图像处理模块、分割结果投影模块等嵌入传统光学显微镜。其中在图像处理模块的算法部分，我们在医学图像分割网络Unet网络基础上加入Critic模块建立深度学习模型，该模型可以同时学习全局和局部的特征，分割癌变的区域。本文提出的显微镜设计方法可以实现病理医生在物镜下移动病理样本，显微镜可以实时分割图像并展示癌变区域。</p></sec><sec id="s2"><title>关键词</title><p>深度学习，人机交互，增强现实</p></sec><sec id="s3"><title>Design of Human-Computer Collaborative Pathology Microscope Based on Deep Learning<sup> </sup></title><p>Yong Ding, Shuang Ni, Xiaoyu Cui, Ran Wei</p><p>School of Medicine and Bioinformatics Engineering, Northeastern University, Shenyang Liaoning</p><p><img src="//html.hanspub.org/file/3-2990310x4_hanspub.png" /></p><p>Received: Feb. 15<sup>th</sup>, 2021; accepted: Mar. 16<sup>th</sup>, 2021; published: Mar. 24<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/3-2990310x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Combining with the traditional pathological optical microscope, a pathological microscope design method based on deep learning and man-machine collaboration is proposed. The pathological image shooting module, pathological image processing module and segmentation result projection module are embedded in the optical microscope. In the algorithm part of the image processing module, the deep learning model is established by adding the CRITIC module on the basis of the medical image segmentation network UNET. The model can learn global and local features at the same time. The microscope design method proposed in this paper can realize when the pathologist moves the sample under the objective lens, the microscope can segment the image in real time and show the cancerous area.</p><p>Keywords:Deep Learning, Human-Computer Interaction, Augmented Reality</p><disp-formula id="hanspub.41183-formula28"><graphic xlink:href="//html.hanspub.org/file/3-2990310x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-2990310x7_hanspub.png" /> <img src="//html.hanspub.org/file/3-2990310x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>临床上肿瘤良性与恶性诊断的金标准是病理诊断。 [<xref ref-type="bibr" rid="hanspub.41183-ref1">1</xref>] 病理学家通过研究疾病发生的原因以及疾病过程中机体的体形、结构、功能变化，来阐明疾病本质，从而掌握疾病的发生与发展规律。病理诊断是病理学家运用现代病理学知识、相关技术和个人专业实践经验，结合相关临床资料和其他临床检查，对提交标本的病变性质和特异性疾病进行判断的一种主观诊断 [<xref ref-type="bibr" rid="hanspub.41183-ref2">2</xref>] 因此，病理医生的经验对准确诊断疾病来说非常重要。常规的病理标本的制作过程中容易出现染色程度不均匀，染色漂洗过程中容易出现污点，病理观察没有客观可靠的指标。据统计，一位能够出具初步病理报告的病理学家需要阅读一万多个病例，一位能够审查初级医生报告的病理学家需要阅读三万个病例，阅读量达到五万例以上，才能解决疑难诊断 [<xref ref-type="bibr" rid="hanspub.41183-ref3">3</xref>]。我国目前只有2万多名病理科医生，但恶性肿瘤发病率达到10%以上，存在严重的医生短缺和分布不均的现象。</p><p>随着临床医学与计算机技术的发展，人工智能得到了广泛的研究，并在许多领域得到了应用。机器学习与医学相结合的应用将改变目前的医学模式。借助机器学习来处理临床上的大量数据可以减轻医生的工作量，提高医生的诊断准确性，提高工作效率，有助于医生在临床上指导治疗和评估预后，并且可以解决病理医生短缺的问题。目前，机器学习在有丝分裂检测、核分裂检测、组织分类等病理图像方面取得了良好的效果 [<xref ref-type="bibr" rid="hanspub.41183-ref4">4</xref>]。</p><p>CNN是一种利用卷积计算并且结构深度的前馈神经网络(Feedforward Neural Networks)，是机器学习中深度学习(deep learning)的关键算法 [<xref ref-type="bibr" rid="hanspub.41183-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.41183-ref5">5</xref>]。近年来，CNN被应用于视觉识别问题，并被证明能有效地从数据中学习多个尺度上的特征层次。在像素级语义分割方面，CNNs也取得了显著的成功。在医学图像分割领域，CNNs也得到了应用，分割准确度良好。Ronneberger等人的提出了一种U形网络结构用于语义分割，即U-net [<xref ref-type="bibr" rid="hanspub.41183-ref6">6</xref>]。U-net取得了非常好的性能，并已被应用于许多不同的任务，在医学图像种表现优异。</p><p>然而目前深度学习应用于病理图像方面的研究专注于数字化病理切片的癌变区域分割，由于术中冰冻病理往往不能制作数字化病理切片，传统的深度学习分割方法不适用于术中冰冻病理，也不符合医生传统使用光学显微镜的习惯，如何将深度学习算法与光学显微镜结合，设计一种仪器用于病理切片的实时观察，成为目前研究的热点与挑战。</p><p>本研究致力于将深度学习技术与传统的光学显微镜结合，设计了一种基于深度学习的人机协同的病理显微镜，随着医生在物镜下移动病理标本，显微镜目镜视野可以为医生实时展示深度学习算法的癌变分割结果，使得医生可以在显微镜下实时观察到深度学习算法标定的癌变区域。图像识别算法仅用于提示，最终决定权仍在医生，使人工智能真正运用于医生实践。</p></sec><sec id="s6"><title>2. 基于深度学习的病理图像分割</title><p>为实现深度学习技术对病理图像的自动分割，本文构建并训练了基于Unet嵌套Critic [<xref ref-type="bibr" rid="hanspub.41183-ref7">7</xref>] 的病理图像分割模型，该网络的构建借鉴了GAN的思想，使用Pytorch进行构建，训练数据集为皮肤癌病理图像，经数据扩增后数据大小为1000张。训练GPU资源为Nivida Titan XP。</p><sec id="s6_1"><title>2.1. 图像预处理</title><p>图像在在训练前先进行预处理，由于拍摄过程中可能会产生噪声，我们对图像进行了高斯降噪以去除大部分噪点。为了扩大数据集，将数据集进行镜像、翻转等数据扩充操作。</p></sec><sec id="s6_2"><title>2.2. 构建Unet嵌套Critic分割模型</title><p>本研究构建了一个深度学习模型，来分割病理图像中的癌变区域。在传统Unet模型的基础上，嵌套了Critic模块，网络结构见图1。整个模型由Unet和Critic两个模块构成，Unet用于初步分割病理图像中的癌变区域，Critic用于模型参数的优化。</p><p>图1. Unet嵌套Critic分割网络模型图</p><p>Unet模块的输入为572 * 572的显微镜相机拍摄的原始病理图像(Original image)，输出为该网络的预测分割(Predicted mask)。Unet模块先进行由卷积和最大池化(Max pooling)构成的一系列降采样操作——压缩路径。压缩路径是由4个block构成，其中每个block使用了3个有效卷积和1个Max Pooling降采样。对应的扩展路径由4个block组成，最后得到的Feature Map的尺寸是388 * 388。因为任务是一个二分类任务(癌变区域和非癌变区域)，所以网络有Feature Map为二输出。Unet模块用来产生中间过程的病理图像的预测分割。</p><p>Critic的输入是预测分割在原图的感兴趣区域(predicted mask)和标签在原图的感兴趣区域(Ground truth)。Critic模块被训练来最大化一个多尺度L1目标函数 [<xref ref-type="bibr" rid="hanspub.41183-ref6">6</xref>]，该函数考虑了CNN在多尺度上预测分割和基本事实分割之间的特征差异。通过对整个系统进行端到端的反向传播训练，以对抗性的方式交替进行Unet和Critic的优化，可以直接在多个尺度上学习病理图像的特征。Unet网络的训练目标是最小化多尺度L1损失，而Critic的训练目标是最大化相同的损失函数。最终研究构造出可以分割病理图像癌变区域的模型。</p><p>本文对该模型与传统的Unet分割模型进行比较，测试分割结果优于Unet网络模型。</p></sec></sec><sec id="s7"><title>3. 实验环境与结果</title><sec id="s7_1"><title>3.1. 硬件环境</title><p>为实现人机协同共同诊断，该病理显微镜主体包括显微成像系统、图像采集模块、图像处理模块、图像显示模块、图像倍率识别模块、观察透镜组、照明系统和物镜调节旋钮，按图2所示依次按序连接。</p><p>图2. 显微镜光路结构图</p><p>其中显微成像系统，使用尼康Ni-U的框架，包括一组物镜和一组目镜，物镜、目镜等按图示排列，同组的物镜和目镜在同一成像光轴上，对标本图像进行光学成像，生成一成像光束；目镜对物镜放大的标本图像进一步放大用于放大标本图像。</p><p>图像采集模块，型号为U3ISPM，用于采集标本图像；图像处理模块，采用图1所述深度学习模型，用于处理采集到的标本图像，标出癌变区域；图像显示模块包含投影光机，采用DLPC2010，用于将图像处理模块合成的图像进行显示；图像倍率识别模块，使用颜色传感器TCS3200，通过识别物镜上标记的不同颜色环，来识别具体的倍率。观察透镜，用于将观察通过显示模块显示的投影图像投射到光路中，与下方射入的标本图像重合，通过分束器的折射效果使重合的图像被目镜采集。图像采集模块的相机首先对物镜下的病理切片进行拍摄，图像倍率识别器会对倍率进行识别，图像处理模块的模型会对拍摄的病理图片进行分割，结果通过光路与图像显示模块投影到观察者的视野中。</p><p>该显微镜还包括照明系统、准焦螺旋。显微成像系统相连于照明系统，对显微成像系统进行照明，便于标本的观察。照明系统采用反射式，使用光源为冷光源。该显微镜还包历史审阅记忆系统，与图像处理模块相连，将图像处理模块生成的前10张图像进行存储，便于医生进行历史审阅。准焦螺旋与物镜相连，用于改变同组的物镜之间的距离，调整成像系统的放大倍数。</p><p>医生观察时，图像采集模块摄取来自物镜的组织病理图像，将采集的病理图像传输到图像处理模块器处理后，将标定癌变区域的图像传输到图像显示模块，图像显示模块中的OLED屏幕进行显示，并将各自显示的图像通过分光平镜投射到光路中，与来自物镜的组织病理图像重合，从而通过目镜被人眼观察到。最终显微镜外观和与显微镜实物图如图3所示。</p><p>图3. 显微镜外观图和与显微镜实物图</p></sec><sec id="s7_2"><title>3.2. 实验结果</title><p>我们将训练好的深度学习网络模型置于显微镜的图像处理模块进行测试，测试集为皮肤癌病理图像，测试集经数据增强后为120张。测试举例结果如图4所示。其中白色的轮廓内为算法标定的癌变区域。该基于深度学习的病理显微镜可以实现随着医生在物镜下移动标本，显微镜物镜视野实时标定出癌变区域。</p><p>研究对本文构建的深度学习模型与传统的Unet模型进行比较测试，Unet嵌套Critic的模型在病理图像表现优于Unet模型。如表1所示，本文构建的模型，在测试集上IOU达到65%，Dice达到72%，传统Unet模型在测试集上IOU为55%，Dice为68%。</p><p>该基于深度学习的病理显微镜可以实现随着医生在物镜下移动标本，显微镜物镜视野实时标定出癌变区域。</p><p>图4. 模型分割结果图</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison of deep learning model</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >评价指标</th><th align="center" valign="middle" >Unet</th><th align="center" valign="middle" >Unet嵌套Critic</th></tr></thead><tr><td align="center" valign="middle" >IOU</td><td align="center" valign="middle" >55%</td><td align="center" valign="middle" >65%</td></tr><tr><td align="center" valign="middle" >Dice</td><td align="center" valign="middle" >68%</td><td align="center" valign="middle" >72%</td></tr></tbody></table></table-wrap><p>表1. 深度学习模型比较结果</p></sec></sec><sec id="s8"><title>4. 总结与讨论</title><sec id="s8_1"><title>4.1. 总结</title><p>本文针对现有病理显微镜的智能需求与交互性的不足，开发了基于深度学习的病理显微镜，构建了分割病理图像癌变区域的深度学习模型，实现病理显微镜可自动识别出癌变区域，提示医生诊断。显微镜可以提高病理医生对病理切片的诊断效率和诊断质量，满足国内病理医生短缺和培养周期长的需求，通过显微镜观察切片是病理医生最习惯的诊断方式，将人工智能算法与病理医生最习惯的方式所结合，医生可以在目镜视野范围内实时观察算法标定的癌变区域，在人工智能算法只起提示作用，最终决断权在医生手中，人机协同的设计更容易被病理医生所接受，可用于医生的教学和阅片训练。目前病理的数字工作流程能与基于深度学习的人机协同病理显微镜使用，该显微镜已经在临床初步使用中体现了自己的价值。</p></sec><sec id="s8_2"><title>4.2. 不足与展望</title><p>深度学习模型计算资源消耗较大，未来将压缩模型，使得模型更为精巧。由于目前深度学习训练的模型过于依赖数据集大小，未来将加入视线追踪标记模块，利用视技术标记癌变区域，扩大训练集。未来将融入在线学习技术，使得视线追踪标记模块与现有的深度学习病理显微镜结合，在医生观察的同时显微镜可以实现在线学习。</p></sec></sec><sec id="s9"><title>基金项目</title><p>本文获得东北大学2019国家级大学生创新创业计划资助项目(201910145232)资助。</p></sec><sec id="s10"><title>文章引用</title><p>丁 勇,倪 爽,崔笑宇,魏 然. 基于深度学习的人机协同病理显微镜设计Design of Human-Computer Collaborative Pathology Microscope Based on Deep Learning[J]. 仪器与设备, 2021, 09(01): 15-21. https://doi.org/10.12677/IaE.2021.91003</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41183-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">朱子鹏. PTMC中BRAF V600E基因突变和Galectin-3, MC, CK19蛋白表达分析[D]: [硕士学位论文]. 福建: 福建医科大学, 2015.</mixed-citation></ref><ref id="hanspub.41183-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">王军, 张建功. 基层医院临床病理诊断中常见问题及解决方案[J]. 医学信息, 2012, 25(4): 351-352.</mixed-citation></ref><ref id="hanspub.41183-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Goodfellow, I., Bengio, Y. and Courville, A. (2016) Deep Learning. MIT Press, Cambridge, 326-366.</mixed-citation></ref><ref id="hanspub.41183-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">夏靖媛, 纪小龙. 计算机深度学习与智能图像诊断对胃高分化腺癌病理诊断的价值[J]. 世界华人消化杂志, 2017, 25(12): 1043-1049.</mixed-citation></ref><ref id="hanspub.41183-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, L., Wang, G. and Cai, J. (2015) Recent Advances in Convolutional Neural Networks. arXiv Preprint, arXiv:1512.07108.</mixed-citation></ref><ref id="hanspub.41183-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Ronneberger, O., Fischer, P. and Brox, T. (2015) U-Net: Convolutional Networks for Bio-medical Image Segmentation. International Conference on Medical Image Computing and Computer-Assisted Inter-vention, Springer, Cham234-241. 
&lt;br&gt;https://doi.org/10.1007/978-3-319-24574-4_28</mixed-citation></ref><ref id="hanspub.41183-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Xue, Y., Xu, T., Zhang, H., et al. (2017) SegAN: Adversarial Network with Multi-Scale L1 Loss for Medical Image Segmentation. Neuroinformatics, 16, 383-392. &lt;br&gt;https://doi.org/10.1007/s12021-018-9377-x</mixed-citation></ref></ref-list></back></article>