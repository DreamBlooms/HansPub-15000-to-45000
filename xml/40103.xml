<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.111018</article-id><article-id pub-id-type="publisher-id">CSA-40103</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210100000_78981556.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于改进EAST的文本检测算法
  Text Detection Algorithm Based on Improved EAST
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>俊</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>苗</surname><given-names>军</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>卿</surname><given-names>来云</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>乔</surname><given-names>元华</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>北京信息科技大学，网络文化与数字传播北京市重点实验室，北京</addr-line></aff><aff id="aff3"><addr-line>中国科学院大学，计算机科学与技术学院，北京</addr-line></aff><aff id="aff4"><addr-line>北京工业大学，数理学院，北京</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>08</day><month>01</month><year>2021</year></pub-date><volume>11</volume><issue>01</issue><fpage>167</fpage><lpage>175</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   自然场景文本定位检测是文本识别的研究热点之一。EAST算法是目前自然场景文本定位检测算法较为出色的算法之一，在ICDAR2015数据集上，有着较高的准确率和召回率。但EAST算法仍存在着感受野不够大、长文本检测效果不佳的问题。因此本实验对EAST算法进行改进，通过改进EAST算法的结构，加入了ASPP网络，扩大感受野，加入了BLSTM神经网络，增强了文本之间的关联，提高文本定位效果。实验结果表明，该算法在ICDAR2015文本定位任务上的召回率为77.84%，精确率为86.24%，F-score为81.82%，优于经典EAST算法。 Text location and detection in natural scenes is one of the research hotspots of text recognition. East algorithm is one of the most excellent algorithms for text location and detection in natural scenes. It has high accuracy and recall rate in ICDAR2015 Dataset. However, the sensitivity field of EAST is not large enough and the effect of long text detection is not good. Therefore, this experiment improves the EAST algorithm by improving the structure of the EAST algorithm, adding the ASPP network, expanding the receptive field, adding the BLSTM neural network, enhancing the relevance between texts, and improving the text location effect. Experimental results show that the recall rate, precision rate and F-score of ICDAR2015 are 77.84%, 86.24% and 81.82% respectively, which are better than the classical EAST algorithm. 
  
 
</p></abstract><kwd-group><kwd>文本识别，EAST，ASPP网络，BLSTM神经网络, Text Recognition</kwd><kwd> EAST</kwd><kwd> ASPP Network</kwd><kwd> BLSTM Neural Network</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>自然场景文本定位检测是文本识别的研究热点之一。EAST算法是目前自然场景文本定位检测算法较为出色的算法之一，在ICDAR2015数据集上，有着较高的准确率和召回率。但EAST算法仍存在着感受野不够大、长文本检测效果不佳的问题。因此本实验对EAST算法进行改进，通过改进EAST算法的结构，加入了ASPP网络，扩大感受野，加入了BLSTM神经网络，增强了文本之间的关联，提高文本定位效果。实验结果表明，该算法在ICDAR2015文本定位任务上的召回率为77.84%，精确率为86.24%，F-score为81.82%，优于经典EAST算法。</p></sec><sec id="s2"><title>关键词</title><p>文本识别，EAST，ASPP网络，BLSTM神经网络</p></sec><sec id="s3"><title>Text Detection Algorithm Based on Improved EAST</title><p>Jun Wang<sup>1</sup>, Jun Miao<sup>1*</sup>, Laiyun Qin<sup>2</sup>, Yuanhua Qiao<sup>3</sup></p><p><sup>1</sup>Beijing Key Laboratory of Internet Culture and Digital Dissemination Research, Beijing Information Science and Technology University, Beijing</p><p><sup>2</sup>School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing</p><p><sup>3</sup>College of Applied Sciences, Beijing University of Technology, Beijing</p><p><img src="//html.hanspub.org/file/18-1542010x5_hanspub.png" /></p><p>Received: Dec. 26<sup>th</sup>, 2020; accepted: Jan. 20<sup>th</sup>, 2021; published: Jan. 28<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/18-1542010x6_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Text location and detection in natural scenes is one of the research hotspots of text recognition. East algorithm is one of the most excellent algorithms for text location and detection in natural scenes. It has high accuracy and recall rate in ICDAR2015 Dataset. However, the sensitivity field of EAST is not large enough and the effect of long text detection is not good. Therefore, this experiment improves the EAST algorithm by improving the structure of the EAST algorithm, adding the ASPP network, expanding the receptive field, adding the BLSTM neural network, enhancing the relevance between texts, and improving the text location effect. Experimental results show that the recall rate, precision rate and F-score of ICDAR2015 are 77.84%, 86.24% and 81.82% respectively, which are better than the classical EAST algorithm.</p><p>Keywords:Text Recognition, EAST, ASPP Network, BLSTM Neural Network</p><disp-formula id="hanspub.40103-formula11"><graphic xlink:href="//html.hanspub.org/file/18-1542010x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/18-1542010x8_hanspub.png" /> <img src="//html.hanspub.org/file/18-1542010x9_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>随着互联网的发展和计算机技术的提高，光学字符识别(Optical Character Recognition, OCR)技术 [<xref ref-type="bibr" rid="hanspub.40103-ref1">1</xref>] 也得到了有效的提高。通过OCR识别技术可以有效地从图片上面提取到所需要的文字信息。但是目前通用的OCR算法只针对于简单的运用场景，一旦场景掺杂过大的因素，识别效率和召回率都会急剧下降。OCR识别技术主要分文本检测和文本识别两部分 [<xref ref-type="bibr" rid="hanspub.40103-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.40103-ref3">3</xref>]。文本检测作为文本识别的前提，在整个文本信息提取和理解过程中起着重要的作用。只有正确的定位到文本区域才能进行正确的文本识别。正确的文本区域检测对提高文本识别准确有着重要的作用，因此，如何提高文本检测是一个重要的课题。</p><p>国内外学者利用不同的方法解决了文本检测问题。Tian [<xref ref-type="bibr" rid="hanspub.40103-ref4">4</xref>] 提出的一种新型的连接主义文本提议网络(CTPN)，使用一种垂直锚定机制，共同预测每个固定宽度候选位置和文本/非文本的分数。通过最大分数确定文本行位置，这大大提高文字定位的准确度。但是CTPN只针对于水平文字检测有很高的效率。为此，Shi等人 [<xref ref-type="bibr" rid="hanspub.40103-ref5">5</xref>] 提出一种定向文本检测方法SegLink，在CTPN的基础上进行改进。主要的思想是将文本分为两个本地可以检测的元素，通过端对端训练的完全卷积神经网络在多个尺度上密集检测这两个元素。最终检测时通过连接段的组合。CTPN检测法、SegLink检测法是通过先预测proposals (预选框)、segment (切片)，然后再回归、合并等方式实现对文本的检测。由于CTPN模型过于冗余复杂，Xinyu Zhou [<xref ref-type="bibr" rid="hanspub.40103-ref6">6</xref>] 等人提出EAST检测法，将中间过程缩减为只有FCN (全卷积网络)、NMS (非极大值抑制) [<xref ref-type="bibr" rid="hanspub.40103-ref7">7</xref>] 两个阶段，而且输出结果支持文本行、单词的多个角度检测，既高效准确，又能适应多种自然应用场景。但是EAST算法仍然存在着感受野不够大，长文本检测效果不佳的问题。</p><p>因此，为了解决EAST算法的存在的问题，本文在EAST算法上进行改进，通过改进EAST算法的结构，利用ASPP网络替代EAST算法中的部分结构，引入BLSTM神经网络 [<xref ref-type="bibr" rid="hanspub.40103-ref8">8</xref>]，增加输出特征图之间的关联性，从而改善了EAST算法的文本检测效果，提高算法的性能。</p></sec><sec id="s6"><title>2. 改进EAST算法</title><sec id="s6_1"><title>2.1. EAST算法介绍</title><p>大部分传统的文本检测算法都是由多个阶段组成，在准确性和效率上面表现不是很好。EAST算法提出端到端的文本定位方法，消除多个中间的stage，直接预测文本行。它只有两个阶段，第一个阶段基于全卷积网络(FCN)模型，直接产生文本框预测；第二个阶段对生成的文本框进行非极大值抑制(NMS)以产生最终结果。该模型放弃了不必要的中间步骤，进行端到端的训练和优化。</p><p>如图1所示，EAST算法网络结构分为三个部分：特征提取层，特征合并和输出层。</p><p>图1. EAST算法网络结构</p><p>特征提取层：利用在ImageNet数据集上预训练的卷积网络参数初始化。基于VGG16模型 [<xref ref-type="bibr" rid="hanspub.40103-ref9">9</xref>] (或者ResNet-50模型 [<xref ref-type="bibr" rid="hanspub.40103-ref10">10</xref>] )作为主干神经网络提取文本的浅层和深层的纹理特征。提取模型的最后四个级别的特征图，其大小分别为输入图像的1/4，1/8，1/16，1/32。通过提取出不同尺度的特征图，实现对不同尺度文本行的检测(大的特征图擅长检测小物体，小的特征图擅长检测大物体)。</p><p>特征合并：特征合并主要采取U-net思想，通过上采样，将上一级别的特征图和上采用的特征图进行合并，再通过1 &#215; 1的卷积进行减少通道数量和计算量。接着通过3 &#215; 3的卷积核运算得到新的特征图。</p><p>输出层：输出的特征图进行不同的卷积操作，最后得到分数特征图和多通道几何图形特征图。</p><p>EAST算法的损失函数主要由两个部分组成，分类损失函数 L s 和几何损失函数 L g ：</p><p>L   =   L s   +   λ g L g (1)</p><p>上式中， L s 代表该像素是否存在文字的损失， L g 代表IOU和角度的损失， λ g 代表两个损失之间的重要性。原文的实验中将 λ g 设置为1 [<xref ref-type="bibr" rid="hanspub.40103-ref11">11</xref>]。</p><disp-formula id="hanspub.40103-formula12"><label>(2)</label><graphic position="anchor" xlink:href="//html.scirp.org/file/18-1542010x11_hanspub.png"  xlink:type="simple"/></disp-formula><p>上式中， Y ⌢ 代表分数图的预测， Y * 代表标注值。 β 代表正负样本之间的平衡因子。</p><p>L g 几何损失分为IOU损失和旋转角度的损失。公式如下：</p><p>L A A B B = − ln IOU ( R ⌢ , R ∗ ) =   −   ln | R ⌢ ∩ R * | | R ⌢ ∪ R ∗ | (3)</p><p>L θ ( θ ⌢ , θ ∗ ) = 1   −   cos ( θ ⌢   −   θ ∗ ) (4)</p><p>上式中， θ ⌢ 是对旋转角度的预测， θ * 表示标注值。总体的损失为：</p><p>L g = L A A B B   +   λ θ L θ (5)</p></sec><sec id="s6_2"><title>2.2. 加入ASPP网络</title><p>传统的卷积神经网络模型中，下采样过程是为了扩大感受野，使得每个卷积输出都包含较大范围的信息，对于提取抽象化信息有很大帮助，但在这个过程中，图像的分辨率不断下降，包含的信息越来越抽象，而图像的局部信息与细节信息会逐渐丢失，虽然现在也有通过线性插值上采样来恢复分辨率的手段存在，但在这个过程，还是不可避免的会造成信息的损失。而EAST算法采用的主干网络无论是VGG16还是resnet50都存在利用下采样用来增大感受野，但都不可避免的导致分辨率下降。而空洞卷积的出现解决了下采用带来的分辨率下降的问题。利用空洞卷积可以实现网络不进行下采样，同样能启到扩大感受野的目的。</p><p>本文利用空洞空间卷积池化金字塔ASPP (Atrous Spatial Pyramid Pooling)网络来替代EAST算法的主干网络VGG16 (或者resnet50)的stage 4模块。ASPP对所给定的输入以不同采样率的空洞卷积并行采样，相当于以多个比例捕捉图像的上下文。</p><p>如图2所示，本文EAST算法的主干网络采用的是resnet50，利用ASPP网络替代EAST算法的主干网络的stage 4模块，ASPP网络包括5个模块，通过将stage 3的输出特征图进行5种不同的操作，第一个模块是进行平均池化，1 &#215; 1的卷积层进行通道数变换，最后通过双线性插值恢复分辨率。第二个到第5个模型都是空洞卷积，但每个的卷积核的扩展率不同，分别取了1，6，12，18；之后将这五个模块的输出拼接到一起，通过一个1 &#215; 1的卷积层，降低通道数到需要的数值，作为下一步操作的输入。</p><p>图2. 加入ASPP网络结构</p></sec><sec id="s6_3"><title>2.3. 增加BLSTM网络</title><p>文本检测不只是检测某个位置是否是单个文字，同样检测文字之间是否存在连续性。文字存在特征信息，同时邻近的文字存在关联关系。CNN学习的是感受野内的空间信息，LSTM学习的是序列特征。对于文本序列检测，显然既需要CNN抽象空间特征，也需要序列特征。</p><p>本文在EAST网络的特征合并和输出层之间加入BLSTM网络。BLSTM是一种特殊的循环长短期记忆神经网络，由双向LSTM神经网络组成。</p><p>如图3所示，本文在EAST算法特征合并层和输出层之间插入BLSTM网络，BLSTM网络结构如图4所示。BLSTM网络能将每个特征的前后序列呈现为两个单独的隐藏状态，以分别捕获序列过去和未来的信息，然后再将两个隐藏的特征序列连接起来形成一个新的特征样本进行最终输出 [<xref ref-type="bibr" rid="hanspub.40103-ref12">12</xref>]。本文通过EAST算法特征合并后输出的特征图进行序列化关联，使得得到的序列样本更加合理均匀具有连续性。</p><p>图3. 加入BLSTM网络结构</p><p>图4. BLSTM网络结构</p></sec></sec><sec id="s7"><title>3. 实验结果</title><p>本实验在tensorflow深度学习框架上进行。采用GTX2080Ti的显卡进行改进的EAST算法实验。实验使用resnet-50网络模型作为EAST算法的主干网络进行预训练模型。使用的数据集是ICDAR2013和ICDAR2015的训练数据集。ICDAR2013训练集具有229张标注训练集，ICDAR2015训练集具有1000张标注训练集。以ICDAR2015测试集作为测试，ICDAR2015测试集有500张图片。这些数据集是由谷歌公司制作，数据集均是在自然场景下采集的，图中的文本是任意方向和位置的。</p><p>实验过程主要将图片送入resnet-50网络模型，经过4次下采用，将获取到特征图送入ASPP网络，ASPP网络在不损失图片分辨率的情况下，提高感受野。再通过3次上采用并且每次和原有对应的下采用特征图进行融合，最终获取特征图。特征图经过不同的卷积获取出来的数据和标注的数据集进行对比，通过ADAM优化器训练网络模型，获取较优的模型参数。本实验的优化器采用ADAM优化器，每个batch size等于8，ADAM的学习率从1E−3开始，每10,000批次衰减十分之一，训练到34万次获取到最优解。</p><p>将文中算法与其他算法 [<xref ref-type="bibr" rid="hanspub.40103-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.40103-ref14">14</xref>] [<xref ref-type="bibr" rid="hanspub.40103-ref15">15</xref>] 在ICDAR2015数据集上进行比较，结果如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison of different text detection algorithm</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >序号</th><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >Precision (%)</th><th align="center" valign="middle" >Recall (%)</th><th align="center" valign="middle" >F-score (%)</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >CTPN + VGG16</td><td align="center" valign="middle" >74.20</td><td align="center" valign="middle" >51.60</td><td align="center" valign="middle" >60.90</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Seglink + VGG16</td><td align="center" valign="middle" >76.80</td><td align="center" valign="middle" >73.10</td><td align="center" valign="middle" >75.00</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >EAST + VGG16</td><td align="center" valign="middle" >80.50</td><td align="center" valign="middle" >72.80</td><td align="center" valign="middle" >76.40</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >EAST + ResNet-50</td><td align="center" valign="middle" >81.66</td><td align="center" valign="middle" >77.32</td><td align="center" valign="middle" >79.43</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >EAST + PVANET2x</td><td align="center" valign="middle" >83.60</td><td align="center" valign="middle" >73.50</td><td align="center" valign="middle" >78.20</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >EAST + PVANET2x MS</td><td align="center" valign="middle" >84.64</td><td align="center" valign="middle" >77.23</td><td align="center" valign="middle" >80.77</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >86.24</td><td align="center" valign="middle" >77.84</td><td align="center" valign="middle" >81.82</td></tr></tbody></table></table-wrap><p>表1. 不同文本检测算法比较</p><p>其中本算法在ICDAR2015文本定位任务上的召回率(针对原样本而言的，它的含义是在实际为正的样本中被预测为正样本的概率)为77.84%，精准率(针对预测结果而言的，它的含义是在所有被预测为正的样本中实际为正的样本的概率)为86.24%，F-score为81.82%，优于经典EAST算法。</p><p>通过对比原有EAST算法以及改进EAST算法的检测效果，如图所示。</p><p>图5中包含两组自然场景文本图片，每组图片中的左侧为原始EAST算法的检测效果，右侧为改进EAST算法的检测效果。从图中可以看出原始EAST算法在检测长文本会遗漏部分文本，以及文本检测的边界过长导致部分没有联系的文本本框选，而本算法通过扩大感受野和增强文本之间的连续性，可以检测出跟多的文本，以及正确的框选文本的合理边界，更好地检测出自然场景文本区域，提高检测的准确率。</p><p>实验通过利用ASPP网络替代原来EAST算法的主干网络resnet-50的stage 4，再通过添加BLSTM网络增强特征图序列化关联。实验结果比经典的EAST算法具有更高的精确率和召回率，F-score到达81.82%。总体相对于经典的EAST算法有着一定的提高。</p><p>图5. 两种EAST算法检测效果</p></sec><sec id="s8"><title>4. 结论</title><p>本文在经典的EAST算法的基础上进行改进来实现文本检测。由于经典的EAST算法存在感受野不够大问题，通过利用ASPP网络来替代EAST算法的主干网络的stage 4模块，在不损失分辨率的情况下提高EAST算法的感受野。同时EAST算法也存在文本检测边框过长和过短的问题，通过添加BLSTM网络，增加文本特征图序列之间的关联，提高了文本检测分界线的效果。相比于经典的EAST算法，本文实现的算法在精确率和召回率上都提高了。同时本文仍存在不足，在后续的实验中可以通过调整参数和利用Dense ASPP网络来替代ASPP网络改进算法，进一步提高文本检测的效果。</p></sec><sec id="s9"><title>基金项目</title><p>北京市自然科学基金项目(4202025)，国家自然科学基金项目(61872333)，北京教委科技计划项目(KM201911232003)，北京未来芯片技术高精尖创新中心科研基金(KYJJ2018004)。</p></sec><sec id="s10"><title>文章引用</title><p>王 俊,苗 军,卿来云,乔元华. 基于改进EAST的文本检测算法Text Detection Algorithm Based on Improved EAST[J]. 计算机科学与应用, 2021, 11(01): 167-175. https://doi.org/10.12677/CSA.2021.111018</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.40103-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Mori, S. (1992) Historical Review of OCR Research and Development. Proceedings of the IEEE, 80, 1029-1058.  
&lt;br&gt;https://doi.org/10.1109/5.156468</mixed-citation></ref><ref id="hanspub.40103-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Goodfellow, I.J., Bulatov, Y., Ibarz, J., et al. (2013) Multi-Digit Number Recognition from Street View Imagery Using Deep Convolutional Neural Networks. Computer Science.</mixed-citation></ref><ref id="hanspub.40103-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Graves, A., Fern’andez, S., Gomez, F., et al. (2006) Labelling Unsegmented Sequence Data with Recurrent Neural Networks. Proceedings of the 23rd International Conference on Machine Learning, Pittsburgh, 25-29 June 2006, 369-376.</mixed-citation></ref><ref id="hanspub.40103-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Tian, Z., Huang, W., He, T., et al. (2016) Detecting Text in Natural Image with Connectionist Text Pro-posal Network. European Conference on Computer Vision, Amsterdam, 11-14 October 2016, 56-72.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46484-8_4</mixed-citation></ref><ref id="hanspub.40103-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Shi, B., Bai, X. and Belongie, S. (2017) Detecting Oriented Text in Natural Images by Linking Segments. 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 3482-3490.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.371</mixed-citation></ref><ref id="hanspub.40103-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Zhou, X.-Y., Yao, C., Wen, H., et al. (2017) EAST: An Efficient and Accurate Scene Text Detector. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 2642-2651.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.283</mixed-citation></ref><ref id="hanspub.40103-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">He, Y., Zhu, C., Wang, J., et al. (2019) Bounding Box Regression with Uncertainty for Accurate Object Detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, 15-20 June 2019, 2888-2897. &lt;br&gt;https://doi.org/10.1109/CVPR.2019.00300</mixed-citation></ref><ref id="hanspub.40103-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Hu, H., Zhang, C., Luo, Y., et al. (2017) WordSup: Exploiting Word Annotations for Character Based Text Detection. 2017 IEEE International Conference on Computer Vision (ICCV), Venice, 22-29 October 2017, 4950-4959.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.529</mixed-citation></ref><ref id="hanspub.40103-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J., Divwvala, S., Girshick, R., et al. (2016) You Only Look Once: Unified, Real-Time Object Detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 779-788. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.91</mixed-citation></ref><ref id="hanspub.40103-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S., et al. (2016) Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, 27-30 June 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.40103-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">杨飚, 杜晓宇. 基于改进EAST的自然场景文本定位算法[J]. 计算机工程与应用, 2019, 55(18): 161-165.</mixed-citation></ref><ref id="hanspub.40103-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">池凯, 赵逢禹. 改进EAST算法的游戏场景文本检测[J]. 小型微型计算机系统, 2020, 41(10): 2189-2193.</mixed-citation></ref><ref id="hanspub.40103-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Yang, P., Rong, G., Peng, G., et al. (2011) Research on Lip Detection Based on Opencv. 2011 International Conference on Transportation, Mechanical and Electrical Engineering (TMEE), Changchun, 16-18 December 2011, 1465-1468.</mixed-citation></ref><ref id="hanspub.40103-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Zhi, T., Huang, W., Tong, H., et al. (2016) Detecting Text in Nat-ural Image with Connectionist Text Proposal Network. In: European Conference on Computer Vision (ECCV), Springer, Cham, 56-72.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46484-8_4</mixed-citation></ref><ref id="hanspub.40103-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Deng, D., Liu, H., Li, X., et al. (2018) PixelLink: Detecting Scene Text via Instance Segmentation. 2018 the Association for the Advance of Artificial Intelligence (AAAI), New Orle-ans, 2-7 February 2018, 6773-6780.</mixed-citation></ref></ref-list></back></article>