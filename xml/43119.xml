<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.116171</article-id><article-id pub-id-type="publisher-id">CSA-43119</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210600000_93577154.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于多尺度顶帽变换的红外与可见光图像融合
  Fusion of Infrared and Visible Images Based on Multi-Scale Top-Hat Method
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>雪梅</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>立</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>汪</surname><given-names>君</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>琴</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>湖北省市场监督管理局行政许可技术评审中心，湖北 武汉</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>02</day><month>06</month><year>2021</year></pub-date><volume>11</volume><issue>06</issue><fpage>1662</fpage><lpage>1671</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对传统多尺度图像融合方法容易弱化红外目标信息和降低图像对比度的问题，本文借助于形态学的优势和模糊规则的特性，提出一种简单、高效的红外与可见光图像融合算法。首先，使用多尺度形态学分离源图像高频成分和低频成分；其次，利用模糊规则整合低频成分，使用均值法合理注入图像高频成分；最后，经过形态学逆变换得到融合图像。实验结果表明，与传统融合方法相比，本文算法能够较好地保留可见光图像中的细节信息，突出红外目标信息。 Aiming at the problem that the traditional multi-scale image fusion method is easy to weaken the infrared target information and reduce the image contrast, this paper proposes a fusion algorithm of infrared and visible image by virtue of the advantages of morphology and the characteristics of fuzzy rules. Firstly, multi-scale morphology was used to separate high-frequency and low-frequency components from source images. Secondly, the fuzzy rules were used to integrate the low frequency components, and the mean value method was used to inject the high frequency components reasonably. Finally, the fused image is obtained by morphological inverse transformation. Experimental results show that, compared with the traditional fusion method, the proposed algorithm can retain the detailed information in visible image and highlight the infrared target information. 
  
 
</p></abstract><kwd-group><kwd>图像融合，红外与可见光图像，形态学，模糊规则, Image Fusion</kwd><kwd> Infrared and Visible Images</kwd><kwd> Morphology</kwd><kwd> Fuzzy Logic</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>针对传统多尺度图像融合方法容易弱化红外目标信息和降低图像对比度的问题，本文借助于形态学的优势和模糊规则的特性，提出一种简单、高效的红外与可见光图像融合算法。首先，使用多尺度形态学分离源图像高频成分和低频成分；其次，利用模糊规则整合低频成分，使用均值法合理注入图像高频成分；最后，经过形态学逆变换得到融合图像。实验结果表明，与传统融合方法相比，本文算法能够较好地保留可见光图像中的细节信息，突出红外目标信息。</p></sec><sec id="s2"><title>关键词</title><p>图像融合，红外与可见光图像，形态学，模糊规则</p></sec><sec id="s3"><title>Fusion of Infrared and Visible Images Based on Multi-Scale Top-Hat Method<sup> </sup></title><p>Xuemei Wang, Li Chen, Jun Wang, Qin Wang</p><p>Administrative Licensing Technology Evaluation Center of Hubei Market Supervision Administration, Wuhan Hubei</p><p><img src="//html.hanspub.org/file/6-1542078x4_hanspub.png" /></p><p>Received: May 8<sup>th</sup>, 2021; accepted: Jun. 5<sup>th</sup>, 2021; published: Jun. 15<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/6-1542078x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Aiming at the problem that the traditional multi-scale image fusion method is easy to weaken the infrared target information and reduce the image contrast, this paper proposes a fusion algorithm of infrared and visible image by virtue of the advantages of morphology and the characteristics of fuzzy rules. Firstly, multi-scale morphology was used to separate high-frequency and low-frequency components from source images. Secondly, the fuzzy rules were used to integrate the low frequency components, and the mean value method was used to inject the high frequency components reasonably. Finally, the fused image is obtained by morphological inverse transformation. Experimental results show that, compared with the traditional fusion method, the proposed algorithm can retain the detailed information in visible image and highlight the infrared target information.</p><p>Keywords:Image Fusion, Infrared and Visible Images, Morphology, Fuzzy Logic</p><disp-formula id="hanspub.43119-formula22"><graphic xlink:href="//html.hanspub.org/file/6-1542078x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/6-1542078x7_hanspub.png" /> <img src="//html.hanspub.org/file/6-1542078x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>由于成像模式不同，同一场景的红外图像和可见光图像具有很好的互补性，将这两类图像信息整合成一幅图像，不仅有利于场景视觉效果优化，还有利于各类图像处理，如目标跟踪、分类、分割等。伴随着信息技术迅猛发展，种类繁多的图像融合算法不断涌现。其中，多尺度分析法由于具有简单、高效的优点，基于多尺度的图像融合算法得到了长足的发展，如小波分析算法(WT) [<xref ref-type="bibr" rid="hanspub.43119-ref1">1</xref>]、非下采样轮廓波算法(NSCT) [<xref ref-type="bibr" rid="hanspub.43119-ref2">2</xref>]、曲率变换算法(CVT) [<xref ref-type="bibr" rid="hanspub.43119-ref3">3</xref>]、双树复小波变换(DTCWT) [<xref ref-type="bibr" rid="hanspub.43119-ref4">4</xref>] 等。然而，传统多尺度算法中所需的滤波器不具有自适应性，这会导致对于不同的融合对象，其融合性能会存在一定的差异，甚至会由于滤波器的非理想性，产生频率或尺度混叠现象，进而影响图像融合效果 [<xref ref-type="bibr" rid="hanspub.43119-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.43119-ref6">6</xref>]。</p><p>数学形态学是一种数学逻辑严谨和算法简单的非线性分析方法，借助于特定结构元素可以捕捉图像中目标形状、边缘等细节特征，具有增强图像特征信息、提高图像对比度的能力 [<xref ref-type="bibr" rid="hanspub.43119-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.43119-ref4">4</xref>]。红外图像通常细节信息模糊、目标与背景对比度较低、灰度范围较窄，因此，红外与可见光图像融合目标不仅要保留丰富的可见光图像细节信息，还要提高目标与背景的对比度，以利于人眼视觉和图像处理。综上可知，利用形态学算法整合红外和可见光图像信息具有较好的优势。</p><p>顶帽变换(Top-Hat Transform)是一种重要的形态学运算，它能有效地抽取图像特征区域，“捕获”图像细节信息。结合结构元素扩张的多尺度技术，顶帽变换已被在目标探测、图像融合和图像增强等领域成功的得到应用 [<xref ref-type="bibr" rid="hanspub.43119-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.43119-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.43119-ref9">9</xref>]。然而，传统多尺度顶帽变换中的结构元素单一，在分析图像特征信息时容易遗漏或平滑图像细节信息，导致多图像整合效果不佳 [<xref ref-type="bibr" rid="hanspub.43119-ref10">10</xref>]。为了避免内容丰富的图像特征区域或细节信息平滑，Bai X设计了一种新颖的结构元素，并提出中心环绕顶帽变换(Center-surround Top-hat Transform)算法，它能很好地捕获图像特征和细节信息，使融合图像特征区域和细节信息丰富，并能增强图像对比度 [<xref ref-type="bibr" rid="hanspub.43119-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.43119-ref11">11</xref>]。然而，在这些算法中，作者主要关注新结构元素抽取信息的能力，忽略了不同尺度下信息的冗余关系，导致信息融合结果中仍存在细节信息平滑的问题。</p><p>高斯模糊逻辑能有效地区分红外图像的目标和背景信息，经过简单的隶属度标注不仅能压制红外背景信息，还能凸显目标信息 [<xref ref-type="bibr" rid="hanspub.43119-ref12">12</xref>]。因此，可利用高斯模糊逻辑捕获红外目标特征，并将可见光细节信息移植到红外背景中，可以在融合图像中既保留红外目标特征，又可呈现可见光等细节信息。</p><p>为解决现有形态学融合算法中存在的不足，本文基于中心环绕顶帽变换的信息捕获能力和高斯模糊逻辑的特点，提出一种适于红外和可见光图像的融合算法。实验结果表明本文提出方法的融合结果能较好地保留原图像的细节信息，凸显红外目标特征。</p></sec><sec id="s6"><title>2. 形态学</title><p>数学形态学在图像处理领域具有重要的应用价值。结合结构元素的多尺度膨胀，形态学序列运算可以构造各类型的多尺度形态学操作。</p><p>1) 中心环绕结构元素</p><p>由于传统形态学结构元素结构单一，因此对图像特征区域的差异性不敏感，存在捕获图像细节信息失效的问题。为提升形态学操作分析图像信息的能力，Bai X [<xref ref-type="bibr" rid="hanspub.43119-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.43119-ref11">11</xref>] 使用两个同形状但尺寸相近的结构元素构建中心环绕结构元素，如图1所示的圆盘中心环绕结构元素，其中，外部结构元素B<sub>o</sub>尺寸大于内部结构元素B<sub>in</sub>尺寸。</p><p>图1. 圆盘中心环绕结构元素 [<xref ref-type="bibr" rid="hanspub.43119-ref7">7</xref>]</p><p>中心环绕结构的有效区域可定义为边界结构元素可定义为：</p><p>Δ B = B o − B i n (2-1)</p><p>当ΔB区域小于特征区域时，环绕区域边界可以有效地捕获图像特征信息，并且根据不同需求，该结构通过尺度伸缩可以识别不同尺度的图像信息。根据不同的应用需求，可利用不同形状的结构元构建中心环绕结构元素，本文选取“圆盘”中心环绕结构为基本结构元素。</p><p>2) 结构元素多尺度扩展</p><p>图像通常包含丰富的尺度特征区域和细节，用单一尺度结构元素仅能抽取与其尺度对应的图像特征，不能有效地分析包含丰富特征信息的图像。为了有效地分析图像中的各种特征信息，多尺度形态学应运而生。形态学多尺度扩展是通过循序渐进地膨胀结构元素完成尺度扩展，使结构元素能在不同尺度下捕获图像特征信息。</p><p>以图1圆盘中心环绕结构元素为例，假设L和W分别是初始结构元素B<sub>b</sub>和ΔB的尺寸，ΔB中的边界宽度为M，中心环绕结构元素扩展n次，每次扩展的步长为Step，于是中心环绕结构元素在尺度s (1 &lt; s ≤ n)的尺寸可以表示为：</p><p>L s = L + s &#215; S t e p W s = W + s &#215; S t e p (2-2)</p><p>为了区分不同尺度下的中心环绕结构元素，在尺度s下的双结构元素分别表示为ΔB<sub>s</sub>和B<sub>bs</sub>，即，B<sub>bs</sub>的尺寸为L<sub>s</sub>，ΔB<sub>s</sub>的边界尺度为W<sub>s</sub>，边界宽度仍为M。经过多尺度扩展后，中心环绕结构元素就能抽取不同尺度下的特征区域及其特征环绕区域。</p><p>3) 顶帽变换</p><p>假设图像使用f (x, y)表示，基于中心环绕结构元素的开、闭运算使用f&#163;B<sub>oi</sub>和f■B<sub>oi</sub>表示为：</p><p>f □ B o i ( x , y ) = ( f Θ Δ B ) ⊕ B b (2-3)</p><p>f ■ B o i ( x , y ) = ( f ⊕ Δ B ) Θ B b (2-4)</p><p>B<sub>oi</sub>表示一组中心环绕双结构元素，它由B<sub>o</sub>和B<sub>in</sub>两个相关的结构元素组成。由于使用了双结构元素，开(闭)运算能更好地平滑图像的亮(暗)区域特征，包括亮(暗)特征区域的环绕区域。则白顶帽变换(NWTH)和暗顶帽变换(NBTH)可表示为：</p><p>N W T H ( x , y ) = f ( x , y ) − f □ B o i ( x , y ) (2-5)</p><p>N B T H ( x , y ) = f ■ B o i ( x , y ) − f ( x , y ) (2-6)</p><p>开运算能从图像中移去小于结构元素尺寸的亮特征，这就使得它能平滑图像的亮区域，因此白顶帽(NWTH)能用来抽取图像的亮特征区域。相反，闭运算从图像中移去小于结构元素尺寸的暗特征，这就使得它能平滑图像的暗图像区域，因此暗顶帽(NBTH)能用来抽取图像的暗特征区域。也就是说，一次顶帽变换能将图像拆分成一个基图像和两个表征亮、暗特征区域的图像。顶帽变换的性能对图像特征的尺寸很敏感，当使用不同尺寸的结构元素时，顶帽变换就会抽取图像中不同大小的亮、暗特征区域 [<xref ref-type="bibr" rid="hanspub.43119-ref6">6</xref>]。</p></sec><sec id="s7"><title>3. 基于多尺度顶帽变换的图像融合方法</title><p>顶帽变换对图像特征区域的尺寸大小极为敏感，结合多尺度中心环绕结构元素的信息捕获能力，本文提出一种新的红外与可见光图像融合方法。</p><sec id="s7_1"><title>3.1. 图像特征信息抽取</title><p>假设k幅源图像分别记为f<sub>1</sub>，……，f<sub>j</sub>，……，f<sub>k</sub>，(1 ≤ j ≤ k)。为抽取图像不同尺度的亮、暗特征区域，对中心环绕结构元素进行多尺度扩展，则多源图像在不同尺度下的顶帽变换可表示为 [<xref ref-type="bibr" rid="hanspub.43119-ref6">6</xref>]：</p><p>N W T H s j ( x , y ) = f j ( x , y ) − f j □ B o i , s ( x , y ) (3-1)</p><p>B W T H s j ( x , y ) = f j ■ B o i , s ( x , y ) − f j ( x , y ) (3-2)</p><p>上式中的标号j表示第j幅源图像，标号s表示在第s尺度下进行顶帽变换，B<sub>oi</sub><sub>,</sub><sub>s</sub>表示尺度s处的中心环绕结构元素。随着结构元素尺寸的增大，使用大尺寸结构元素抽取的特征区域会包含很多在小尺寸下抽取的特征区域。这就造成不同尺度下抽取的特征信息彼此会包含大量冗余信息，进而影响最后的融合结果。为了避免这一问题，并更好地表征图像特征，本文使用下面的方法对抽取的亮、暗特征区域进行精炼提取，以减小不同尺度层之间的信息冗余，并更好地表征特定尺度下的特征区域信息。</p><p>N W T H s , s + 1 j ( x , y ) = N W T H s + 1 j ( x , y ) − N W T H s j ( x , y ) (3-3)</p><p>N B T H s , s + 1 j ( x , y ) = N B T H s + 1 j ( x , y ) − N B T H s j ( x , y ) (3-4)</p><p>上式中， N W T H s j 抽取的是图像f<sub>j</sub>从尺度1到尺度s的亮特征区域，而 N W T H s + 1 j 抽取的是图像f<sub>j</sub>从尺度1到尺度s + 1的亮特征区域，通过式(3-3)，尺度s + 1下的亮特征区域信息就很好地在 N W T H s , s + 1 j 中得到了表征，且 N B T H s + 1 j 不包含其他尺度的信息。类似地， N B T H s , s + 1 j 能很好地表征尺度s + 1下的暗特征区域信息。</p></sec><sec id="s7_2"><title>3.2. 图像融合方法</title><p>本文使用恰当方式整合亮、暗特征信息，并得到融合图像。</p><sec id="s7_2_1"><title>3.2.1. 图像特征区域融合</title><p>本节对抽取的尺度区域特征进行归一化处理，以消除区域特征图像中可能出现的负值，获取更精确的亮、暗特征图像，有效地抽取每一尺度下的图像区域和感兴趣细节，并减小不同尺度层之间的冗余信息。</p><p>N W T H s , s + 1 j ( x , y ) = n o r m a l ( N W T H s + 1 j ( x , y ) − N W T H s j ( x , y ) ) (3-5)</p><p>N B T H s , s + 1 j ( x , y ) = n o r m a l ( N B T H s + 1 j ( x , y ) − N B T H s j ( x , y ) ) (3-6)</p><p>对不同源图像的同层特征区域信息进行以下操作得到相应的融合特征信息。</p><p>W F s , s + 1 = max 1 ≤ j ≤ k { N W T H s , s + 1 j } (3-7)</p><p>B F s , s + 1 = max 1 ≤ j ≤ k { N B T H s , s + 1 j } (3-8)</p><p>因为不同尺度层的特征和细节彼此不相同，因此，可以将各尺度层的特征区域信息进行如下整合，得到融合的亮、特征区域图像</p><p>W F = ∑ s = 0 k − 1 W F s , s + 1 (3-9)</p><p>B F = ∑ s = 0 k − 1 B F s , s + 1 (3-10)</p><p>WF和BF分别表示最后的融合亮、暗特征区域。</p></sec><sec id="s7_2_2"><title>3.2.2. 获取基图像</title><p>WF和BF主要表征的是多源图像的特征和细节信息，为了获取最后的融合图像，需要获得一幅包含多源图像主要能量信息的基图像。不同于传统的多尺度变换能将图像分解的剩余成分作为图像的低频信息，基于形态学的运算一般只能利用其自身的特性抽取图像的各种特征信息，而不能预留对应的低频信息。本文在尺度扩展过程中利用最大尺度的外结构元素获取的平滑图像表示图像的低频信息。</p><p>O j = f j ∘ B b n (3-11)</p><p>C j = f j • B b n (3-12)</p><p>在式(3-11)和(3-12)中，B<sub>bn</sub>表示结构元素扩展时，最大尺度中心环绕结构元素的外结构元素，O<sub>j</sub>和C<sub>j</sub>分别代表第j幅源图像的平滑亮、暗图像信息。为了获得融合的平滑亮图像信息，本节使用高斯模糊逻辑法将不同源图像的O<sub>j</sub>进行整合，得到融合的亮平滑图像O。使用同样的方法获得融合的暗平滑图像C，最后，使用均值法获取O和C的融合图像。下面是基于高斯模糊逻辑自适应地整合亮图像信息。</p><p>O F ( i , j ) = η T ( i , j ) O IR ( i , j ) + η B ( i , j ) O VI ( i , j ) (3-13)</p><p>式(3-13)中的下标IR，VI和F分别代表红外，可见光和融合图像。O (i, j)是亮图像信息。η<sub>T</sub>和η<sub>B</sub>分别是红外信息中目标和背景的隶属度(权值)。</p><p>根据红外低频成分直方图的特性，高斯隶属度函数能很好地描述红外图像的目标和背景信息 [<xref ref-type="bibr" rid="hanspub.43119-ref12">12</xref>]。在文献中 [<xref ref-type="bibr" rid="hanspub.43119-ref12">12</xref>]，作者获得了很好的融合结果，本文使用高斯函数确定信息整合的隶属度权值。</p><p>η B ( i , j ) = exp [ − ( C I R ( i , j ) − μ ) 2 2 ( k σ ) 2 ] (3-14)</p><p>η T ( i , j ) = 1 − η B ( i , j ) (3-15)</p><p>式(3-14)中的μ和σ分别是红外信息的均值和方差。k是一个用于优化融合结果的常数，其取值范围为1~3。在我们的试验中，k值均设置为1.5。</p></sec><sec id="s7_2_3"><title>3.2.3. 图像信息融合</title><p>分别获取了基图像和融合的亮、暗特征区域图像后，为了得到较好的融合结果，需要使用恰当的方式将亮、暗特征图像注入到基图像。实验结果表明，均值权重法能根据亮、暗特征图像的特性自适应地向基图像中注入有用的特征信息，并获得较好的融合结果 [<xref ref-type="bibr" rid="hanspub.43119-ref6">6</xref>]。本节采用均值权重法整合特征区域图像和基图像的信息。</p><p>f u ( x , y ) = A ( x , y ) + p w ( x , y ) &#215; W F ( x , y ) − p b ( x , y ) &#215; B F ( x , y ) (3-16)</p><p>p w ( x , y ) = m w ( x , y ) m w ( x , y ) + m b ( x , y ) p b ( x , y ) = m b ( x , y ) m w ( x , y ) + m b ( x , y ) (3-17)</p><p>式中，m<sub>w</sub>和m<sub>b</sub>分别为WF和BF在3&#215; 3窗口尺寸下的均值，A表示基图像，f<sub>u</sub>表示最后的融合图像。</p></sec></sec></sec><sec id="s8"><title>4. 实验与分析</title><p>为了验证提出基于多尺度顶帽变换的红外与可见光图像融合算法可行性和有效性，本文针对4组红外与可见光图像 [<xref ref-type="bibr" rid="hanspub.43119-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.43119-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.43119-ref11">11</xref>]，选取3种常用的性能较优多尺度融合算法DTCWT [<xref ref-type="bibr" rid="hanspub.43119-ref4">4</xref>]，NSCT [<xref ref-type="bibr" rid="hanspub.43119-ref2">2</xref>] 和CVT [<xref ref-type="bibr" rid="hanspub.43119-ref3">3</xref>] 与本文算法进行比较。使用信息熵、标准差、空间频率和平均梯度 [<xref ref-type="bibr" rid="hanspub.43119-ref6">6</xref>] 4种客观指标对融合结果进行评价。</p><p>由各组融合结果可知，传统的多尺度融合算法的性能比较相近，这些算法普遍损失了部分源图像的细节信息，导致融合结果趋于平滑，图像层次感较差，特别是在显著的目标区域边缘出现“振铃”、光晕、抖动等现象，导致图像对比度下降，目标和细节模糊，整体视觉效果不佳。从主观的视觉效果分析，本文算法的融合结果细节信息丰富，红外目标信息显著，目标边缘信息清晰，图像对比度更优，融合图像清晰自然(图2~5)</p><p>图2. 第1组可见光与红外图像融合结果对比</p><p>图3. 第2组可见光与红外图像融合结果对比</p><p>图4. 第3组可见光与红外图像融合结果对比</p><p>图5. 第4组可见光与红外图像融合结果对比</p><p>为了进一步验证本文算法的有效性，使用熵、平均梯度、空间频率和标准差4类图像质量指标对各算法融合结果进行客观评价，这些指标的值越大，表明图像融合效果越好。由表1~4的数据可知，本文算法的平均梯度、空间频率和标准差明显优于其他参考算法，说明本文算法具有更优的对比度和清晰度，图像层次分明。尽管本文算法的图像信息熵不总是最优，但从各表中数据可知，本文算法的熵值是较优的，表明图像具有丰富的信息。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Objective value comparison of the first fusion experimen</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Methods</th><th align="center" valign="middle" >熵</th><th align="center" valign="middle" >平均梯度</th><th align="center" valign="middle" >空间频率</th><th align="center" valign="middle" >标准差</th></tr></thead><tr><td align="center" valign="middle" >CVT</td><td align="center" valign="middle" >6.704</td><td align="center" valign="middle" >4.3735</td><td align="center" valign="middle" >10.9842</td><td align="center" valign="middle" >29.1967</td></tr><tr><td align="center" valign="middle" >DTCWT</td><td align="center" valign="middle" >6.6894</td><td align="center" valign="middle" >4.1929</td><td align="center" valign="middle" >10.9001</td><td align="center" valign="middle" >28.9651</td></tr><tr><td align="center" valign="middle" >NSCT</td><td align="center" valign="middle" >6.7165</td><td align="center" valign="middle" >4.4055</td><td align="center" valign="middle" >11.0619</td><td align="center" valign="middle" >29.4857</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >6.7552</td><td align="center" valign="middle" >5.1</td><td align="center" valign="middle" >14.0882</td><td align="center" valign="middle" >33.8282</td></tr></tbody></table></table-wrap><p>表1. 第1组融合实验客观评价值比较</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Objective value comparison of the second fusion experimen</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Methods</th><th align="center" valign="middle" >熵</th><th align="center" valign="middle" >平均梯度</th><th align="center" valign="middle" >空间频率</th><th align="center" valign="middle" >标准差</th></tr></thead><tr><td align="center" valign="middle" >CVT</td><td align="center" valign="middle" >6.1286</td><td align="center" valign="middle" >3.2834</td><td align="center" valign="middle" >8.8807</td><td align="center" valign="middle" >18.4183</td></tr><tr><td align="center" valign="middle" >DTCWT</td><td align="center" valign="middle" >6.1046</td><td align="center" valign="middle" >3.2224</td><td align="center" valign="middle" >8.8281</td><td align="center" valign="middle" >18.2672</td></tr><tr><td align="center" valign="middle" >NSCT</td><td align="center" valign="middle" >6.1175</td><td align="center" valign="middle" >3.3456</td><td align="center" valign="middle" >8.9637</td><td align="center" valign="middle" >18.4548</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >6.1234</td><td align="center" valign="middle" >3.7669</td><td align="center" valign="middle" >9.0296</td><td align="center" valign="middle" >18.7684</td></tr></tbody></table></table-wrap><p>表2. 第2组融合实验客观评价值比较</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Objective value comparison of the third fusion experimen</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Methods</th><th align="center" valign="middle" >熵</th><th align="center" valign="middle" >平均梯度</th><th align="center" valign="middle" >空间频率</th><th align="center" valign="middle" >标准差</th></tr></thead><tr><td align="center" valign="middle" >CVT</td><td align="center" valign="middle" >7.3116</td><td align="center" valign="middle" >8.5583</td><td align="center" valign="middle" >20.9401</td><td align="center" valign="middle" >42.6953</td></tr><tr><td align="center" valign="middle" >DTCWT</td><td align="center" valign="middle" >7.2957</td><td align="center" valign="middle" >8.5326</td><td align="center" valign="middle" >21.4188</td><td align="center" valign="middle" >43.0708</td></tr><tr><td align="center" valign="middle" >NSCT</td><td align="center" valign="middle" >7.3245</td><td align="center" valign="middle" >8.5118</td><td align="center" valign="middle" >21.6494</td><td align="center" valign="middle" >44.3428</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >7.3102</td><td align="center" valign="middle" >8.5782</td><td align="center" valign="middle" >23.6121</td><td align="center" valign="middle" >49.7966</td></tr></tbody></table></table-wrap><p>表3. 第3组融合实验客观评价值比较</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Objective value comparison of the four fusion experimen</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Methods</th><th align="center" valign="middle" >熵</th><th align="center" valign="middle" >平均梯度</th><th align="center" valign="middle" >空间频率</th><th align="center" valign="middle" >标准差</th></tr></thead><tr><td align="center" valign="middle" >CVT</td><td align="center" valign="middle" >6.7814</td><td align="center" valign="middle" >8.1701</td><td align="center" valign="middle" >20.8959</td><td align="center" valign="middle" >40.314</td></tr><tr><td align="center" valign="middle" >DTCWT</td><td align="center" valign="middle" >6.7571</td><td align="center" valign="middle" >8.1317</td><td align="center" valign="middle" >21.1552</td><td align="center" valign="middle" >40.6221</td></tr><tr><td align="center" valign="middle" >NSCT</td><td align="center" valign="middle" >6.7692</td><td align="center" valign="middle" >8.3571</td><td align="center" valign="middle" >21.4802</td><td align="center" valign="middle" >41.2466</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >6.768</td><td align="center" valign="middle" >12.6133</td><td align="center" valign="middle" >32.0822</td><td align="center" valign="middle" >54.8886</td></tr></tbody></table></table-wrap><p>表4. 第4组融合实验客观评价值比较</p></sec><sec id="s9"><title>5. 结论</title><p>本文针对传统多尺度图像融合方法容易弱化红外目标信息和降低图像对比度的问题，基于新颖的形态学结构元素和高斯模糊逻辑，提出一种简单、高效的红外与可见光图像融合算法。实验结果表明，本文提出的方法使融合结果拥有丰富的细节信息、显著的目标信息、更高的清晰度和对比度，其性能优于传统的多尺度融合方法。</p></sec><sec id="s10"><title>文章引用</title><p>王雪梅,陈 立,汪 君,王 琴. 基于多尺度顶帽变换的红外与可见光图像融合Fusion of Infrared and Visible Images Based on Multi-Scale Top-Hat Method[J]. 计算机科学与应用, 2021, 11(06): 1662-1671. https://doi.org/10.12677/CSA.2021.116171</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.43119-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">张雨晨, 李江勇. 基于小波变换的中波红外偏振图像融合[J]. 激光与红外, 2020, 50(5): 578-582.</mixed-citation></ref><ref id="hanspub.43119-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">颜正恕, 王璟. 基于非下采样轮廓波变换耦合对比度特征的遥感图像融合算法[J]. 电子测量与仪器学报, 2020, 34(3): 28-35.</mixed-citation></ref><ref id="hanspub.43119-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">蒋婷婷, 周洁静. 基于改进Curvelet变换的图像融合算法研究[J]. 信息化研究, 2020, 3(45): 23-27.</mixed-citation></ref><ref id="hanspub.43119-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">王亚杰, 李殿起, 徐心和. 基于双树复小波变换的夜视图像融合[J]. 系统仿真学报, 2008, 20(10): 2757-2761.</mixed-citation></ref><ref id="hanspub.43119-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Li, S., Yang, B. and Hu, J. (2011) Performance Comparison of Different Multi-Resolution Transforms for Image Fusion. Information Fusion, 12, 74-84. &lt;br&gt;https://doi.org/10.1016/j.inffus.2010.03.002</mixed-citation></ref><ref id="hanspub.43119-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, P., Ma, X. and Huang, Z. (2017) Fusion of Infrared-Visible Images Using Improved Multi-Scale Top-Hat Transform and Suitable Fusion Rules. Infrared Physics &amp; Technology, 81, 282-295.  
&lt;br&gt;https://doi.org/10.1016/j.infrared.2017.01.013</mixed-citation></ref><ref id="hanspub.43119-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Bai, X.Z., Zhou, F.G. and Xue, B.D. (2011) Infrared Image Enhancement through Contrast Enhancement by Using Multiscale New Top-Hat Transform. Infrared Physics &amp; Tech-nology, 54, 61-69.  
&lt;br&gt;https://doi.org/10.1016/j.infrared.2010.12.001</mixed-citation></ref><ref id="hanspub.43119-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Li, Y., Yong, B., Wu, H., et al. (2014) An Improved Top-Hat Filter with Sloped Brim for Extracting Ground Points from Airborne Lidar Point Clouds. Remote Sensing, 6, 12885-12908. &lt;br&gt;https://doi.org/10.3390/rs61212885</mixed-citation></ref><ref id="hanspub.43119-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Tan, X., Chen, M. and Jiang, C. (2008) The Small Target detection Base on Wavelet Transform and Mathematical Morphology. Electronics Optics &amp; Control, 9, 25-28.</mixed-citation></ref><ref id="hanspub.43119-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Li, Y., Feng, X. and Xu, M. (2012) Infrared and Visible Image Features Enhancement and Fusion Using Multi-Scale Top-Hat Decomposition, Infrared and Laser Engineering. Infrared and Laser Engineering, 41, 2824-2832.</mixed-citation></ref><ref id="hanspub.43119-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Bai, X.Z., Zhou, F.G. and Xue, B.D. (2011) Fusion of Infrared and Visual Images through Region Extraction by Using Multi Scale Center-Surround Top-Hat Transform. Optics Express, 19, 8444-8457.  
&lt;br&gt;https://doi.org/10.1364/OE.19.008444</mixed-citation></ref><ref id="hanspub.43119-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Yin, S.F., Cao, L.C., Tan, Q.F., et al. (2010) Infrared and Visible Im-age Fusion Based on NSCT and Fuzzy Logic. IEEE International Conference on Mechatronics and Automation (ICMA), Xi’an, 4-7 August 2010, 671-675.  
&lt;br&gt;https://doi.org/10.1109/ICMA.2010.5588318</mixed-citation></ref></ref-list></back></article>