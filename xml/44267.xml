<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">SEA</journal-id><journal-title-group><journal-title>Software Engineering and Applications</journal-title></journal-title-group><issn pub-type="epub">2325-2286</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/SEA.2021.104052</article-id><article-id pub-id-type="publisher-id">SEA-44267</article-id><article-categories><subj-group subj-group-type="heading"><subject>SEA20210400000_45810267.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>工程技术</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于LSTM的端到端声纹识别算法实现
  LSTM-Based End-to-End Voiceprint Recognition Algorithm Implementation
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>飞</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>徐</surname><given-names>颖捷</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>浙江理工大学信息学院，浙江 杭州</addr-line></aff><aff id="aff3"><addr-line>浙江理工大学启新学院，浙江 杭州</addr-line></aff><aff id="aff2"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>14</day><month>07</month><year>2021</year></pub-date><volume>10</volume><issue>04</issue><fpage>467</fpage><lpage>479</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  近年来，随着神经网络在语音识别领域应用中的快速发展，深度学习被应用到声纹识别领域，取得了很好的效果。本文先是介绍了声纹识别的基本理论，说明了语音信号预处理和特征识别的一般方法，而后又介绍了一种基于LSTM神经网络的端对端声纹识别算法，从理论上说明了这种算法的优越性。通过这种算法构建的说话人声纹识别模型，大大节省了模型训练的时间，训练效果较好。
   In recent years, with the rapid development of neural networks in the field of speech recognition applications, deep learning has been applied to the field of voiceprint recognition with good results. In this paper, we first introduce the basic theory of voiceprint recognition and illustrate the general methods of speech signal preprocessing and feature recognition, and then we introduce an end-to-end voiceprint recognition algorithm based on LSTM Neural Network to illustrate the theo-retical superiority of this algorithm. The speaker vocal pattern recognition model constructed by this algorithm greatly saves the time of model training and the training effect is better.
 
</p></abstract><kwd-group><kwd>声纹识别，端对端损失，LSTM神经网络, Voiceprint Recognition</kwd><kwd> End-to-End Losses</kwd><kwd> LSTM Neural Network</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>近年来，随着神经网络在语音识别领域应用中的快速发展，深度学习被应用到声纹识别领域，取得了很好的效果。本文先是介绍了声纹识别的基本理论，说明了语音信号预处理和特征识别的一般方法，而后又介绍了一种基于LSTM神经网络的端对端声纹识别算法，从理论上说明了这种算法的优越性。通过这种算法构建的说话人声纹识别模型，大大节省了模型训练的时间，训练效果较好。</p></sec><sec id="s2"><title>关键词</title><p>声纹识别，端对端损失，LSTM神经网络</p></sec><sec id="s3"><title>LSTM-Based End-to-End Voiceprint Recognition Algorithm Implementation<sup> </sup></title><p>Fei Wang<sup>1*</sup>, Yingjie Xu<sup>2</sup></p><p><sup>1</sup>Information College, Zhejiang Sci-Tech University, Hangzhou Zhejiang</p><p><sup>2</sup>Qixin College, Zhejiang Sci-Tech University, Hangzhou Zhejiang</p><p><img src="//html.hanspub.org/file/5-2690553x5_hanspub.png?20210913101732902" /></p><p>Received: Jun. 14<sup>th</sup>, 2021; accepted: Jul. 21<sup>st</sup>, 2021; published: Jul. 30<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/5-2690553x6_hanspub.png?20210913101732902" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In recent years, with the rapid development of neural networks in the field of speech recognition applications, deep learning has been applied to the field of voiceprint recognition with good results. In this paper, we first introduce the basic theory of voiceprint recognition and illustrate the general methods of speech signal preprocessing and feature recognition, and then we introduce an end-to-end voiceprint recognition algorithm based on LSTM Neural Network to illustrate the theoretical superiority of this algorithm. The speaker vocal pattern recognition model constructed by this algorithm greatly saves the time of model training and the training effect is better.</p><p>Keywords:Voiceprint Recognition, End-to-End Losses, LSTM Neural Network</p><disp-formula id="hanspub.44267-formula57"><graphic xlink:href="//html.hanspub.org/file/5-2690553x7_hanspub.png?20210913101732902"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/5-2690553x8_hanspub.png?20210913101732902" /> <img src="//html.hanspub.org/file/5-2690553x9_hanspub.png?20210913101732902" /></p></sec><sec id="s5"><title>1. 引言</title><sec id="s5_1"><title>1.1. 研究背景及意义</title><p>声纹识别是一种身份检测技术。由于声纹是携带信息的声波频谱，声纹同指纹一样，因此其具有独特的生物学特征，可用于身份识别。在文献 [<xref ref-type="bibr" rid="hanspub.44267-ref1">1</xref>] 中，作者举例说明了说话人识别技术可以应用的领域，包括司法领域可以用来固定刑事侦查证据，确认犯罪嫌疑人；银行金融等安全领域可以用来核验身份，确定人员访问权限；电子通信及互联网领域可以用来登录APP，减少了输入密码的麻烦。由此可见，说话人识别具有相当大的研究意义和使用价值，因此逐渐成为国内外研究学者关注的对象和研究热点。</p></sec><sec id="s5_2"><title>1.2. 研究概况、水平和发展趋势</title><p>1945年，Kersta提出了“声纹(Voiceprint)”的概念。1969年，J. E. Luck在对语音特征分析的基础上，首次提出将倒谱技术应用到声纹识别技术中 [<xref ref-type="bibr" rid="hanspub.44267-ref2">2</xref>]，其实验结果较为理想。B. S. Atal从中受到启发，他通过对声道进行分析建模提出了一系列参数，其中最著名的就是线性预测倒谱系数 [<xref ref-type="bibr" rid="hanspub.44267-ref3">3</xref>] (Linear Predictive Cepstrum Coefficients, LPCC)。同世纪的80年代S. B. Davis和Hermansky对人耳的听觉特性的分析和研究，并针对性地提出了Mel频谱的梅尔倒谱系数 [<xref ref-type="bibr" rid="hanspub.44267-ref4">4</xref>] (Mel Frequency Cepstral Coefficients, MFCC)。七十年代的矢量量化技术(Vector Quantization, VQ)在语音识别领域并取得了巨大的突破，随后VQ算法被应用于声纹识别领域 [<xref ref-type="bibr" rid="hanspub.44267-ref5">5</xref>]。为进一步提高识别结果，隐马尔科夫模型 [<xref ref-type="bibr" rid="hanspub.44267-ref6">6</xref>] (Hidden Markov Model, HMM)作为概率模型的代表被应用于声纹识别领域。随后，SVM、WCCN、NAP、LDA等被用于声纹识别领域。2005年，Kenny提出联合因子分析 [<xref ref-type="bibr" rid="hanspub.44267-ref7">7</xref>] (Joint Factor Analysis, JFA)，在建模过程中将 GMM 的均值超矢量所包含的信息分解为两部分：说话人与说话人之间的差异(Speaker Variability, SV)，和相同说话人不同语音段之间的差异(Session Variability/Channels Variability, CV)。随后基于这个思想提出了一系列的基于向量的i-vector算法和基于信道补偿的PLDA算法。近几年，随着计算能力的快速提高，深度学习被越来越多的应用到声纹识别领域 [<xref ref-type="bibr" rid="hanspub.44267-ref8">8</xref>]，成绩斐然。以ImageNet [<xref ref-type="bibr" rid="hanspub.44267-ref9">9</xref>] 为代表，深度学习神经网络在图像识别以及分类领域取得巨大成功，并诞生了一些经典的深度神经网络结构来解决通用问题，如GoogLeNet，VGG，ResNet等等。就语音识别领域来说，深度神经网络模型强大的拟合能力和泛化能力足以代替GMM模型，其模型建立和训练过程也变得足够简单。早期阶段有一些研究将DNN神经网络应用于说话人识别，用DNN代替GMM计算后验统计更改为高斯混合模型，延续了早期声纹识别的研究成果。</p></sec><sec id="s5_3"><title>1.3. 本文主要研究内容以及章节安排</title><p>本文先是介绍了声纹识别的基本理论，说明了语音信号预处理和特征识别的一般方法，而后又介绍了一种基于LSTM神经网络的端对端声纹识别算法，从理论上说明了这种算法的优越性。</p><p>本文的第一章是引言部分，主要介绍了说话人识别的研究背景及意义、研究概况，以及本文研究的主要内容。第二章介绍了声纹识别的基本理论，主要包括声纹识别的分类，语言信号的预处理以及语音信号的特征识别。第三章是论文的主要内容。主要介绍了循环神经网络的理论，端对端损失函数的原理，以及LSTM神经网络的结构。</p></sec></sec><sec id="s6"><title>2. 声纹识别基本理论</title><sec id="s6_1"><title>2.1. 声纹识别的分类</title><p>作为模式识别的一类，声纹识别的主要任务是通过待测试语音来判断对应说话人身份。声纹识别可以分为两类。若已知待测说话人的范围，需要通过随机或者特定的语音段来判断是否属于某个说话人，这属于声纹确认技术。这种问题是1对1的身份判别问题。若待测说话人的身份范围没有确定，需要通过随机或者特定的语音段来确定说话人的身份，这属于声纹辨认技术。这种问题是1对N的身份辨别问题。</p><p>声纹识别问题又可以分为与文本相关的声纹识别和文本无关的声纹识别。对于文本相关问题，待测试语音段的内容需要和系统中预先登记的内容相同。对于文本无关问题，待测试语音段的内容可以与系统中预先登记的内容不同，待测试说话人可以只说几个字来进行身份认证。本文涉及的是1对1的、文本无关的身份判别问题。</p></sec><sec id="s6_2"><title>2.2. 语音信号的预处理和特征识别</title><sec id="s6_2_1"><title>2.2.1. 语音信号的预处理</title><p>为了消除一些影响音频信号质量的因素，在进行声纹识别之前，一般要对语音信号进行预处理。语音信号的预处理一般包括预加重、分帧和加窗等。一般情况下，经过语音预处理后得到的信号会更加均匀、流畅，频谱更清晰。语音信号经过预处理，可以增强语音处理的效果。</p><p>1) 预加重</p><p>由于语音信号的高频部分在传输过程中会衰减，这种衰减严重影响声纹识别的效果，所以必须在传输线路的起始端对信号的高频部分进行增强。这种技术被称为预加重技术预加重技术可以有效地提高输出信噪比，因为其对噪声没有影响。一般说来，将信号通过一阶有限冲激响应高通数字滤波器就可以实现。数字滤波器的系统函数为：</p><p>H ( z ) = 1 − a z − 1 (1)</p><p>2) 分帧加窗</p><p>为了对语音信号进行频域分析，傅里叶变换是一种常见的工具。傅里叶变换的存在条件是信号经历的随机过程是平稳的，但是从宏观层面看，要求信号平稳的这个条件过于苛刻。在微观层面看，当信号被限定在一个较短时间内，就可以视为平稳信号。把信号的连续若干点设为一帧，这样的操作被称为分帧。分帧后的信号还不能马上进行傅里叶变换。由于分帧后的语音信号不光滑，信号的分辨率较差，因此在做傅里叶变换之前，还要先对信号乘以一个窗函数。这样的操作被称为加窗。加窗的目的是为了让分帧后的信号的两端无限接近于0。此项操作可以提高变换结果的分辨率。但加窗的会削弱信号两端的部分，所以在分帧时可以考虑相互重叠，重叠的部分一般是帧长的一半，这部分被称为帧移。</p><p>常见的窗函数有两种，分别是矩形窗和汉明窗。实验中采取的窗函数是汉宁窗，如图1所示。</p><p>图1. 汉宁窗函数</p><p>3) 语音活动检测</p><p>由于语音信号中即存在活动的部分又存在非活动的部分，为了区分这两种语音部分，语音活动检测技术应运而生。语音活动检测技术可以检测出原始语音信号中活动的语音，即存在人类语言的大部分，减少计算量。语音活动检测技术通常是与语音无关的。传统的语音活动检测的方法主要是提取语音信号的时域特征，例如短时能量、过0率、相关性和基音等 [<xref ref-type="bibr" rid="hanspub.44267-ref10">10</xref>]。在信号的信噪比较高的情况下，语音活动检测的效果较好。</p><p>实验采取的是根据短时能量的大小来分辨语音信号和非语音信号。实验中利用Python语言中librosa工具包中的librosa.effects.split函数，可以将信号分成非静音间隔，方便后续处理。</p></sec><sec id="s6_2_2"><title>2.2.2. 语音特征提取</title><p>为了将语音信号转换为计算机能够处理的特征向量，需要提取语音信号的参数特征，这样的操作被称为语音特征提取。常见的语音特征提取方法有以下几种。</p><p>1) 线性预测分析(Linear predictive coding, LPC)</p><p>线性预测分析最早由Weiner于1967年提出，后被应用于许多研究领域 [<xref ref-type="bibr" rid="hanspub.44267-ref11">11</xref>]。</p><p>线性预测分析的思路就是通过线性组合过去的语音样本，使得线性预测样本的误差和实际语音样本的误差平方和最小，进而可以确定线性预测系数。也就是说，通过线性预测分析，可以用模型或信号的输出描述语音信号。系统的传输函数可以表示为：</p><p>H ( z ) = G 1 − ∑ i = 1 r a i z − i (2)</p><p>这种系统称为全极点系统。其中分母多项式的系数称为线性预测系数。这样的信号模型在忽略一定精度情况下可以描述信号。线性预测分析就是在某个准则下，根据已知的语音信号对参数与进行参数估计。线性预测分析首先要解决的问题是如何通过语音信号确定参数集 { a i } ，使预测误差最小。一般采用最小均方误差准则进行参数估计。由于全极点模型易于计算，对其进行参数估计求解线性方程组，较易实现，因此线性预测模型采用全极点模型较为合理。</p><p>若该系统函数的分母多项式能够快速收敛，则仅仅需要前几项。因此，在实际应用中，可以用全极点模型近似表示零极点模型。</p><p>根据上述模型化思想，可对语音信号建立模型，将其中的声门激励、声道以及全部谱效应简化为一个时变数字滤波器来等效。其系统函数为：</p><p>H ( z ) = S ( z ) U ( z ) = G 1 − ∑ i = 1 r a i z − i (3)</p><p>2) 线性预测倒谱分析(Linear prediction cepstrumcoding, LPCC)</p><p>线性倒谱系数是LPC参数在倒数谱中的表示，能够很好地反应人的声道特征 [<xref ref-type="bibr" rid="hanspub.44267-ref10">10</xref>]。下面是由线性预测系数求解线性预测倒谱系数的递推公式，其中 a k 表示LPC系数。</p><p>c ( n ) = { 0                                                                       n &lt; 0 ln ( G )                                                       n = 0 a n + ∑ k = 1 n − 1 ( k n ) C ( k ) a n − k         0 &lt; n ≤ p ∑ k = n − p n − 1 ( k n ) C ( k ) a n − k                     n &gt; p (4)</p><p>3) 梅尔频率倒谱分析(Mel frequency cepstrumcoding, MFCC)</p><p>为了更好的处理语音信号，往往可以考虑将语音信号的时频谱转换为梅尔谱。转换的方法是把信号通过一组梅尔标度滤波器。声学理论可知，人耳对语音信号的感知不是线性关系。如果我们将声谱图转换为梅尔谱，这种非线性的感知关系将转换为线性关系。梅尔频率与普通频率的关系如下：</p><p>M e l ( f ) = 2565 &#215; log 10 ( 1 + f 700 ) (5)</p><p>MFCC特征参数的提取过程如下：</p><p>1、首先，对预处理后的信号进行快速傅里叶变换，得到各帧的频谱，平方以后可以得到语音信号的能量谱。</p><p>2、将得到的能量谱通过一组梅尔滤波器，其中滤波器的频率响应为：</p><p>H m ( k ) = { 0                                                                                                                       k &lt; f ( m − 1 )     or     k &gt; f ( m − 1 ) 2 ( k − f ( m − 1 ) ) ( f ( m + 1 ) − f ( m − 1 ) ) ( f ( m ) − f ( m − 1 ) )                     f ( m − 1 ) ≤ k ≤ f ( m ) 2 ( f ( m + 1 ) − k ) ( f ( m + 1 ) − f ( m − 1 ) ) ( f ( m ) − f ( m − 1 ) )                     f ( m ) ≤ k ≤ f ( m + 1 ) (6)</p><p>其中 f ( m ) 为中心频率， m = 1 , 2 , 3 , ⋯ , M . M通常取22~26。</p><p>3、计算每个滤波器输出的对数值：</p><p>S ( m ) = ln ( ∑ k = 0 N − 1 | X a ( k ) | 2 H m ( k ) ) , 0 ≤ m ≤ M (7)</p><p>由此可以得到L阶MFCC系数，一般L取12~16之间。</p><p>MFCC特征参数提取过程如图2所示。</p><p>图2. MFCC特征参数提取过程图</p></sec></sec></sec><sec id="s7"><title>3. 基于LSTM神经网络的说话人识别研究</title><sec id="s7_1"><title>3.1. 长短时记忆网络LSTM</title><p>近年来，循环神经网络越多地被应用到语音信号处理领域。随着应用地不断深入，循环神经网络的弊端开始显现。由于长时间学习而导致的梯度弥散或者梯度爆炸问题被越来越多的学者所揭示。为了解决循环神经网络出现的问题，有学者提出了长短时记忆网络 [<xref ref-type="bibr" rid="hanspub.44267-ref12">12</xref>] (Long short-term memory, LSTM)。在循环神经网络的基础上，长短时记忆网络改进了循环神经网络权值的计算规则。与普通的循环神经网络相比，LSTM能够学习长期依赖信息。</p><p>标准的循环神经网络具有一个单一的、完全相同的神经网络层。常见的是单tanh层，如图3所示。</p><p>图3. 一种简单的循环神经网络模式图</p><p>而对于LSTM来说，情况则大不相同。这里不再是单一的tanh层，而是有四个不同的结构，分别是1个tanh层和3个sigmoid层，如图4所示。</p><p>图4. 一种简单的长短时记忆网络模式图</p><p>通过引入一种控制结构，神经网络可以学会如何控制信息去除或者增加到细胞(cell)中。也就是说，神经网络可以让信息有选择地通过。这种结构包含了一个sigmoid神经网络层和一个乘法操作，如图5所示。</p><p>图5. LSTM中的控制结构</p><p>Sigmoid神经网络层会输出0到1之间的数字，0意味着禁止信息通过，1意味着允许所有信息通过。一个LSTM神经网络中有三个类似的控制结构，来控制神经网络中细胞的状态(state)。</p><p>对图4所示的神经网络计算过程如下：</p><p>第一步，从细胞状态中丢弃旧信息。此行为通过一个被称作是忘记门层的控制结构来完成。忘记门层会读取 h t − 1 和 x t ，计算公式为</p><p>f t = σ ( W f &#215; [ h t − 1 , x t ] + b f ) (8)</p><p>如图6所示，可以得到一个在0到1之间的数值，此时的神经网络状态记为 C t − 1 。</p><p>图6. LSTM神经网络第一步运算</p><p>第二步，存新信息到细胞状态中。控制结构的一部分是sigmoid层，该层决定什么值需要更新。另一部分的结构是tanh层。当信息通过tanh层时会得到一个新的向量，记为 C ˜ t ，被记入到细胞状态中。细胞会根据这两个值进行更新，如图7所示。这两部分的计算公式为：</p><p>i t = σ ( W f &#215; [ h t − 1 , x t ] + b f ) (9)</p><p>C ˜ t = tanh ( W c &#215; [ h t − 1 , x t ] + b c ) (10)</p><p>图7. LSTM神经网络第二步运算</p><p>第三步，更新细胞状态。将旧的细胞状态与 f t 相乘，丢弃掉决定要丢弃的信息，再加上 i t &#215; C ˜ t ，这就是新的细胞状态，如图8所示。计算公式为：</p><p>C t = f t &#215; C t − 1 + i t &#215; C ˜ t (11)</p><p>图8. LSTM神经网络第三步运算</p><p>第四步，计算细胞输出。输出来自于新的细胞状态，如图9所示。计算公式为：</p><p>o t = σ ( W o &#215; [ h t − 1 , x t ] + b o ) (12)</p><p>h t = o t &#215; tanh ( C t ) (13)</p><p>图9. LSTM神经网络第四步运算</p><p>至此，一层简单的LSTM神经网络创建完成。实验中通过Python语言的Pytorch工具包中的torch.nn.LSTM()函数即可快速构建LSTM神经网络，如图10所示。</p><p>图10. 使用nn.LSTM函数快速创建LSTM神经网络</p></sec><sec id="s7_2"><title>3.2. 基于三元组的端对端损失函数的声纹识别系统</title><p>参考文献 [<xref ref-type="bibr" rid="hanspub.44267-ref13">13</xref>] 中介绍了一种基于三元组的端到端损失函数(the end-to-end loss function based on triples, TE2E)声纹识别系统。</p><p>作者将评价话语 X j ~ 和M个注册话语 X k m ( m = 1 , 2 , ⋯ , M ) 作为一个元组送入LSTM神经网络。其中，X是从定长段语音信号中提取的梅尔频谱系数，j和k代表话语的说话者。两者相等的关系不确定。若两者相等，则说明来自同一个说话者，认为该元组是正元组(positive)；反之，认为该元组是负元组(negative)。</p><p>对于每个输入的元组，作者计算了LSTM神经网络的L2正则(the L2 normalized)： { e j ~ ， ( e k 1 , ⋯ , e k M ) } 。这里的每一个e是固定维度的嵌入矢量(d-vector)。它的维度由LSTM神经网络的投影层大小决定。元组的中心表示从M个话语构建的声纹(voiceprint)，计算过程如下：</p><p>c k = E m [ e k m ] = 1 M ∑ m = 1 M e k m (14)</p><p>使用余弦相似度函数定义两者之间的相似度：</p><p>s = w ⋅ cos ( e j ~ , c k ) + b (15)</p><p>其中w和b分别是神经网络可以学习的权值和偏置。最终损失函数被定义为：</p><p>L T ( e j ~ , c k ) = δ ( j , k ) σ ( s ) + ( 1 − δ ( j , k ) ) ( 1 − σ ( s ) ) (16)</p><p>其中 σ ( x ) = 1 / ( 1 + e − x ) 是标准的sigmoid函数。若 j = k ，则 δ ( j , k ) = 1 ，反之， δ ( j , k ) = 0 。当 j = k 时，TE2E损失函数会使相似度s越来越大；当 j ≠ k 时，TE2E损失函数会使相似度越来越小。这两种元组的更新方式和在FaceNet [<xref ref-type="bibr" rid="hanspub.44267-ref14">14</xref>] 中使用的正负元组非常类似。</p></sec><sec id="s7_3"><title>3.3. 改进后的端对端损失函数的声纹识别系统</title><p>在本章中，介绍了TE2E损失函数的一种改进后的形式(a generalization of our TE2E architecture)。我们称其为GE2E损失函数。这种改进后的算法以一种更加有效的方式生成嵌入矢量，这方式显著提高了与文本无关的说话人验证(TI-SV)的性能和训练速度。</p><p>此次提出的模型是基于批量处理的语音信号，此举大大节省了运算时间。模型训练时的每个批次(batch)包含N个说话者，平均每个说话者包含M个话语，如图11所示。</p><p>图11. 声纹识别系统概述</p><sec id="s7_3_1"><title>3.3.1. 广义的端到端模型</title><p>模型采取大小的话语来构造一个批次(batch)。每个说话者都有M个不同的话语，共计有N个不同的说话者。这些构成了 N &#215; M 的矩阵。记特征向量为 X j i ，其中j和i的范围分别是 1 ≤ j ≤ N 和 1 ≤ i ≤ M 。每一个特征向量说明了特征来自第j个说话者的第i个的话语。</p><p>将特征向量送入LSTM神经网络。LSTM神经网络的最后一层连接了一个线性层，用来降低特征向量的维度。将公式 f ( X j i ; W ) 定义为神经网络的输出，其中神经网络中所有可以学习的参数被记为W，包括LSTM层和线性层的参数。由网络输出的L2正则形式可以得到嵌入矢量(d-vector)，嵌入矢量的计算公式为：</p><p>e j i = f ( X j i ; W ) ‖ f ( X j i ; W ) ‖ 2 (17)</p><p>这里的 e j i 代表着第j个说话者的第i个话语的嵌入矢量。定于所有嵌入矢量的中心为该说话者的声纹。记 c j 为第j个说话者的声纹，计算公式为：</p><p>c j = c k = E m [ e k m ] = 1 M ∑ m = 1 M e k m (18)</p><p>定义每个嵌入矢量 e j i 和嵌入矢量的中心 c k 之间的余弦相似度为相似度矩阵 S j i , k ，计算公式为：</p><p>S j i , k = w ⋅ cos ( e j i , c k ) + b (19)</p><p>其中w和b分别是神经网络可以学习的权值和偏置。为了余弦相似度越大时，相似度矩阵也越大，我们限定 w &gt; 0 。与改进之前的损失函数相比，TE2E和GE2E损失函数的主要区别如下：</p><p>1、由公式17可知，TE2E的相似度是一个标量值。它定义了嵌入矢量和单个元组中心的相似度。</p><p>2、由公式19可知，GE2E的相似度是矩阵形式。它定义了嵌入矢量和所有元组中心之间的相似度。</p><p>图1展示了整个过程，包括来自不同说话人的特征、嵌入矢量以及相似度评分，并用不同的颜色表示。</p><p>训练过程中，我们希望每个话语的嵌入矢量与该说话者的所有话语的中心尽可能地靠近，同时尽可能地远离其他说话者的中心。图12的相似性矩阵说明了这个问题。我们希望主对角线上的相似值越大越好，而其他部分的相似值越小越好。图13以另一种形式说明了这个问题。我们希望蓝色嵌入矢量尽可能的靠近它自己说话者嵌入矢量的中心，即蓝色三角形；同时尽可能的远离其他说话者的中心，即紫色和红色三角形。尤其是需要远离距离它最近的红色三角形。如图12所示，给定一个嵌入矢量 e j i ，所有的说话者嵌入矢量的中心即说话者的声纹 c k ，以及所有的相似度矩阵 S j i , k ，有两种损失函数可以实现这种要求。</p><p>图12. 嵌入矢量与真假说话者声纹的示意图</p><p>1) Softmax.在相似度矩阵上设置一个交叉熵损失函数，其中 k = 1 , 2 , ⋯ , N 。若 j = k ，则输出等于1，否则输出等于0。因此，每个嵌入矢量 e j i 上的损失可定义为：</p><p>L ( e j i ) = − S j i , i + log ∑ k = 1 N exp ( S j i , k ) (20)</p><p>这个损失函数使得每个嵌入矢量距离说话人中心越来越近，并远离其他说话人的中心。</p><p>2) Contrast.在正元组和距离最远的负元组之间定义一个对比度损失函数：</p><p>L ( e j i ) = 1 − σ ( S j i , j ) + max 1 ≤ k ≤ N k ≠ j ( S j i , k ) (21)</p><p>其中 σ ( x ) = 1 / ( 1 + e − x ) 是sigmoid函数。对于每一个话语，有两部分组成了损失。这两部分分别是：</p><p>1、一个正项，它与嵌入矢量和它的真说话人声纹之间的正匹配呈现相关关系。</p><p>2、一个权重较大的负项，它与嵌入矢量和在所有假说话者中相似性最高的说话人声纹之间的负匹配呈现相关关系。</p><p>在图13中，正项对应于使嵌入矢量靠近真说话人的声纹。因为与相比更靠近嵌入矢量，所以负项对应于使嵌入矢量远离所有假说话者中相似性最高的假说话人的声纹。因此，对比度损失函数使得嵌入矢量和假说话人的声纹之间的相似度得到重点关注。</p><p>在此次实验中，我们发现使用softmax损失函数在与文本无关的说话人识别问题中表现更好。</p><p>此外，我们还观察到，在计算真正说话者的声纹时，一种简单的方案是去掉 e j i 。这样会使得模型更容易收敛。因此，我们在计算负相似性 ( k ≠ j ) 时仍然使用公式16；当 k = j 时，我们使用公式22：</p><p>c j ( − i ) = 1 M − 1 ∑ m = 1 m ≠ i M e j m (22)</p><p>S j i , k = { w ⋅ cos ( e j i , c j ( − i ) ) + b               k = j w ⋅ cos ( e j i , c k ) + b               otherwise (23)</p><p>结合公式17，20，21和22，最终的GE2E损失 L G 是相似矩阵上所有损失的总和：</p><p>L G ( x ; w ) = L G ( S ) = ∑ j , i L ( e j i ) (24)</p><p>其中 1 ≤ j ≤ N , 1 ≤ i ≤ M 。</p></sec><sec id="s7_3_2"><title>3.3.2. GE2E和TE2E的比较</title><p>考虑GE2E的单个批次的损失更新：每个批次有N个说话人，每个说话人有M个话语。每一步更新都会是所有的大小为 N &#215; M 的嵌入向量尽可能地靠近它们自己说话者的中心，并使其尽可能地远离其他说话者的中心。这反映了TE2E损失函数 [<xref ref-type="bibr" rid="hanspub.44267-ref13">13</xref>] 中每个 X j i 的所有可能元组的情况。假设我们随机选择P个话语：</p><p>1、正元组：记为 { X j i , ( X j , i 1 , ⋯ , X j , i P ) } ，其中 1 ≤ i p ≤ M ， p = 1 , ⋯ , P 。一共有 ( M P ) 个这样的元组。</p><p>2、负元组：记为 { X j i , ( X k , i 1 , ⋯ , X k , i P ) } ，其中 k ≠ j ， 1 ≤ i p ≤ M ， p = 1 , ⋯ , P 。对于每一个 X j i ，我们与其他所有的 N − 1 个中心进行比较。其中，每一组比较都包含 ( M P ) 个这样的元组。</p><p>3、一个正元组对应一个负元组，所以元组的总数是正元组和负元组的最大数目的2倍。因此TE2E损失的元组总数：</p><p>2 &#215; max ( ( M P ) , ( N − 1 ) ( M P ) ) ≥ 2 ( N − 1 ) (25)</p><p>当 P = M 时，公式25出现最小值。因此GE2E中每次更新 X j i 等价于TE2E中至少更新 2 ( N − 1 ) 次。</p><p>上述分析说明了为什么GE2E模型要优于TE2E模型。</p></sec></sec><sec id="s7_4"><title>3.4. 实验结果及分析</title><p>本实验使用的数据集为TIMIT语音数据集，它是由美国德州仪器公司、麻省理工学院和SRI国际公司构建的语音库。其中全部语音信号的采样频率都是16 kHz，一共包含6300个句子。实验采用了完整的TIMIT数据集。</p><p>实验中特征提取过程与 [<xref ref-type="bibr" rid="hanspub.44267-ref15">15</xref>] 一致。首先将音频信号转换为帧长为25，帧移为10的帧。然后提取40维度的梅尔频率倒谱系数作为每个帧的特征。对每个话语进行语音活动检测，然后选择每个话语的前180帧和后180帧，提取MPCC特征，然后送入神经网络。</p><p>使用带有投影层(projection) [<xref ref-type="bibr" rid="hanspub.44267-ref16">16</xref>] 的三层LSTM神经网络，嵌入矢量的维度和投影层的维度相同。对于此次文本无关的声纹识别，隐藏层大小被设置为768，投影层大小被设置为256。训练时每批次包含 N = 4 个说话者， M = 5 个话语。训练方法采用随机梯度下降法，训练的学习率被设置为0.01。同时，将梯度的L2范数裁剪 [<xref ref-type="bibr" rid="hanspub.44267-ref17">17</xref>] 为3。对于权值 ( W , b ) ，我们还观察到一个较小的初值是 ( W , b ) = ( 10 , − 5 ) 。较小的梯度有助于平滑收敛。如图13所示，可以看到，训练一共迭代到133,929次，损失下降到一个较低水平。</p><p>图13. 训练日志结果</p><p>测试时每批次包含 N = 4 个说话者， M = 6 个话语。如图14所示，从测试日志中可以看到等错误率(ERR)始终维持在一个较低水平，模型训练效果较好。</p><p>图14. 测试日志结果</p></sec></sec><sec id="s8"><title>4. 结束语</title><p>在本文中，我们提出了一种改进后的端对端损失函数，这种损失函数与前人提出的损失函数相比，可更有效地训练声纹识别模型。我们从理论上验证了这种损失函数的优越性，并用Python语言实现了它。通过使用这种改进后的端对端损失函数，我们得到了更精确的声纹识别模型。</p></sec><sec id="s9"><title>文章引用</title><p>王 飞,徐颖捷. 基于LSTM的端到端声纹识别算法实现LSTM-Based End-to-End Voiceprint Recognition Algorithm Implementation[J]. 软件工程与应用, 2021, 10(04): 467-479. https://doi.org/10.12677/SEA.2021.104052</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.44267-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">樊云云. 面向说话人识别的深度学习方法研究[D]: [硕士学位论文]. 南昌: 南昌航空大学, 2019.</mixed-citation></ref><ref id="hanspub.44267-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Luck, J.E. (1969) Auto-matic Speaker Verification Using Cepstral Measurements. Journal of the Acoustical Society of America, 46, 1026-1032. &lt;br&gt;https://doi.org/10.1121/1.1911795</mixed-citation></ref><ref id="hanspub.44267-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Atal, B.S. (1976) Automatic Recognition of Speakers from Their Voices. Proceedings of the IEEE, 64, 460-475.  
&lt;br&gt;https://doi.org/10.1109/PROC.1976.10155</mixed-citation></ref><ref id="hanspub.44267-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Davis, S. and Mermelstein, P. (1980) Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences. IEEE Transactions on Signal Processing, 28, 357-366.  
&lt;br&gt;https://doi.org/10.1109/TASSP.1980.1163420</mixed-citation></ref><ref id="hanspub.44267-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Sakoe, H. and Chiba, S. (1978) Dynamic Programming Algorithm Optimi-zation for Spoken Word Recognition. IEEE Transactions on Acoustics, Speech and Signal Processing, 26, 43-49. &lt;br&gt;https://doi.org/10.1109/TASSP.1978.1163055</mixed-citation></ref><ref id="hanspub.44267-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Matsui, T. and Furui, S. (1994) Comparison of Text-Independent Speaker Recognition Methods Using VQ-Distortion and Discrete/Continuous HMM’s. IEEE Transactions on Speech and Audio Processing, 2, 456-459.  
&lt;br&gt;https://doi.org/10.1109/89.294363</mixed-citation></ref><ref id="hanspub.44267-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Kenny, P. (2005) Joint Factor Analysis of Speaker and Session Variability: Theory and Algorithms.</mixed-citation></ref><ref id="hanspub.44267-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Lei, Y., Scheffer, N., Ferrer, L. and McLaren, M. (2014) A Novel Scheme for Speaker Recognition Using a Pho-netically-Aware Deep Neural Network. 2014 IEEE International Conference on Acoustics, Speech and Signal Processing, Florence, 4-9 May 2014, 1695-1699. &lt;br&gt;https://doi.org/10.1109/ICASSP.2014.6853887</mixed-citation></ref><ref id="hanspub.44267-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Deng, J., Dong, W., Socher, R., et al. (2009) ImageNet: A Large-Scale Hierarchical Image Database. IEEE Conference on Computer Vision and Pattern Recognition, Miami, 20-25 June 2009, 248-255.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2009.5206848</mixed-citation></ref><ref id="hanspub.44267-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">刘华平, 李昕, 徐柏龄, 姜宁. 语音信号端点检测方法综述及展望[J]. 计算机应用研究, 2008(8): 2278-2283.</mixed-citation></ref><ref id="hanspub.44267-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">胡航. 现代语音信号处理[M]. 北京: 电子工业出版社, 2014: 74.</mixed-citation></ref><ref id="hanspub.44267-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Hochreiter, S. and Schmidhuber, J. (1997) Long Short-Term Memory. Neural Computation, 9, 1735-1780.  
&lt;br&gt;https://doi.org/10.1162/neco.1997.9.8.1735</mixed-citation></ref><ref id="hanspub.44267-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Heigold, G., Moreno, I., Bengio, S. and Shazeer, N. (2016) End-to-End Text-Dependent Speaker Verification. 2016 IEEE International Conference on in Acoustics, Speech and Signal Processing, Shanghai, 20-25 March 2016, 5115-5119. &lt;br&gt;https://doi.org/10.1109/ICASSP.2016.7472652</mixed-citation></ref><ref id="hanspub.44267-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Schroff, F., Kalenichenko, D. and Philbin, J. (2015) Facenet: A Unified Embedding for Face Recognition and Clustering. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 815-823. &lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298682</mixed-citation></ref><ref id="hanspub.44267-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Prabhavalkar, R., Alvarez, R., Parada, C., Nakkiran, P. and Sainath, T.N. (2015) Automatic Gain Control and Multi-Style Training for Robust Small-Footprint Keyword Spotting with Deep Neural Networks. 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, South Brisbane, 19-24 April 2015, 4704-4708.  
&lt;br&gt;https://doi.org/10.1109/ICASSP.2015.7178863</mixed-citation></ref><ref id="hanspub.44267-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Sak, H., Senior, A. and Beaufays, F. (2014) Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition.&lt;br&gt; 
https://arxiv.org/abs/1402.1128</mixed-citation></ref><ref id="hanspub.44267-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Pascanu, R., Mikolov, T. and Bengio, Y. (2012) On the Difficulty of Training Recurrent Neural Networks.&lt;br&gt; 
https://arxiv.org/abs/1211.5063</mixed-citation></ref></ref-list></back></article>