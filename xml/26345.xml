<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2018.88128</article-id><article-id pub-id-type="publisher-id">CSA-26345</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20180800000_90867685.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  一种基于特征加权的K-Means算法研究
  A K-Means Algorithm Based on Feature Weighting
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>徐</surname><given-names>艳</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>付</surname><given-names>学良</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>宏慧</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>董</surname><given-names>改芳</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>晴</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>内蒙古农业大学计算机与信息工程学院，内蒙古 呼和浩特</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>31</day><month>07</month><year>2018</year></pub-date><volume>08</volume><issue>08</issue><fpage>1164</fpage><lpage>1171</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   聚类分析是将研究对象分为相对同质的群组的统计分析技术，聚类分析的核心就是发现有用的对象簇。K-means聚类算法由于具有出色的速度和良好的可扩展性，一直备受广大学者的关注。然而，传统的K-means算法，未考虑各个属性对于最终聚类结果的影响差异性，这使得聚类的精度有一定的影响。针对上述问题，本文提出一种改进的特征加权算法。改进算法通过采用信息熵和ReliefF特征选择算法对特征进行加权选择，修正聚类对象间的距离函数，使算法达到更准确更高效的聚类效果。仿真实验结果表明，与传统的K-means算法相比，改进后的算法聚类结果稳定，聚类的精度有明显提升。 Cluster analysis is a statistical analysis technique that divides the research objects into relatively homogeneous groups. The core of cluster analysis is to find useful clusters of objects. K-means clustering algorithm has been receiving much attention from scholars because of its excellent speed and good scalability. However, the traditional K-means algorithm does not consider the influence of each attribute on the final clustering result, which makes the accuracy of clustering have a certain impact. In response to the above problems, this thesis proposes an improved feature weighting algorithm. The improved algorithm uses the information entropy and ReliefF feature selection algorithm to weight the features and correct the distance function between clustering objects, so that the algorithm can achieve more accurate and efficient clustering effect. The simulation results show that compared with the traditional K-means algorithm, the improved algorithm clustering results are stable, and the accuracy of clustering is significantly improved.
    
  
 
</p></abstract><kwd-group><kwd>K-means聚类，信息熵，ReliefF算法，特征加权, K-Means Clustering</kwd><kwd> Information Entropy</kwd><kwd> ReliefF Algorithm</kwd><kwd> Feature Weighting</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>一种基于特征加权的K-Means算法研究<sup> </sup></title><p>徐艳，付学良，李宏慧，董改芳，王 晴</p><p>内蒙古农业大学计算机与信息工程学院，内蒙古 呼和浩特</p><p><img src="//html.hanspub.org/file/3-1541101x1_hanspub.png" /></p><p>收稿日期：2018年7月25日；录用日期：2018年8月5日；发布日期：2018年8月10日</p><disp-formula id="hanspub.26345-formula32"><graphic xlink:href="//html.hanspub.org/file/3-1541101x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>聚类分析是将研究对象分为相对同质的群组的统计分析技术，聚类分析的核心就是发现有用的对象簇。K-means聚类算法由于具有出色的速度和良好的可扩展性，一直备受广大学者的关注。然而，传统的K-means算法，未考虑各个属性对于最终聚类结果的影响差异性，这使得聚类的精度有一定的影响。针对上述问题，本文提出一种改进的特征加权算法。改进算法通过采用信息熵和ReliefF特征选择算法对特征进行加权选择，修正聚类对象间的距离函数，使算法达到更准确更高效的聚类效果。仿真实验结果表明，与传统的K-means算法相比，改进后的算法聚类结果稳定，聚类的精度有明显提升。</p><p>关键词 :K-means聚类，信息熵，ReliefF算法，特征加权</p><disp-formula id="hanspub.26345-formula33"><graphic xlink:href="//html.hanspub.org/file/3-1541101x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2018 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-1541101x7_hanspub.png" /> <img src="//html.hanspub.org/file/3-1541101x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>数据挖掘是目前人工智能和数据库领域研究的热点问题，指从大量的数据中提取隐含在其中的、人们事先不知道的、但又是潜在有用的信息和知识的过程。聚类分析现在已经成为数据挖掘领域中一个非常重要的研究方向。MacQueen提出 [<xref ref-type="bibr" rid="hanspub.26345-ref1">1</xref>] 的K-means算法是聚类分析中最常用的方法之一。它采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标 [<xref ref-type="bibr" rid="hanspub.26345-ref2">2</xref>] 。K-means算法假设样本的每个特征对最终聚类的贡献程度一样，但在实际情况中某些特征在聚类的过程中起到很大的作用，而某些特征的作用却很小，甚至对聚类过程没有影响。</p><p>针对传统K-means算法的这一问题，学者们进行了大量研究，研究表明：通过对特征赋予不同的特征权值，能够有效解决上述问题并提高聚类性能。目前，计算特征权重的算法有很多种：刘铭 [<xref ref-type="bibr" rid="hanspub.26345-ref3">3</xref>] 等人提出一种结合限制数据的特征权值量化函数，该函数通过用户指定的限制数据进行特征权值量化并对不同的限制数据赋予不同的置信度，解决了限制数据分布不均匀和限制数据中可能包含不一致性的问题；Li Jie [<xref ref-type="bibr" rid="hanspub.26345-ref4">4</xref>] 等人提出将针对分类问题的ReliefF算法应用于聚类问题，通过ReliefF算法计算特征权重值，并对各维特征进行加权，提高聚类的性能；Meng Qian [<xref ref-type="bibr" rid="hanspub.26345-ref5">5</xref>] 等人提出通过梯度下降技术最小化特征评估函数FLearning (w)为每个特征分配权重并进行加权，该算法采用遗传算法和模拟退火算法的优点，减弱冗余特征的影响，解决了容易陷入局部最优解的问题。Songtao Shang [<xref ref-type="bibr" rid="hanspub.26345-ref6">6</xref>] 等人提出一种改进的基尼指数算法计算特征权重，该算法克服了原始Gini的缺点，将条件概率与后验概率结合，抑制训练集不平衡时的影响。杨玉梅 [<xref ref-type="bibr" rid="hanspub.26345-ref7">7</xref>] 利用信息论中的信息熵计算特征权重并对各位特征加权，有效的解决了特征对聚类的影响。</p><p>综上所述，为了提高传统K-means算法的聚类精度，国内外学者对K-means算法进行了大量改进探索研究，并取得了一些阶段性的成果。本文拟研究传统K-means算法在聚类过程中聚类对象的每个特征对聚类结果的贡献度，使贡献程度大的特征优先利用，理论上讲可以有效提升K-means算法聚类的准确率和精度。因此，本文提出将熵值法和ReliefF特征选择算法有机融合，通过采用信息熵和ReliefF特征选择算法对特征进行加权选择，修正聚类对象间的距离函数，使算法达到更准确更高效的聚类效果。实验结果表明，改进后的算法聚类结果稳定，且具有较高的准确率，达到预期目的。</p></sec><sec id="s4"><title>2. K-means算法</title><p>K-means算法的核心思想是通过迭代把数据对象划分到不同的簇中，以求目标函数最小化，从而使生成的簇尽可能的紧凑和独立，算法具体流程如下。</p><p>输入：簇的数目k，包含n个对象的数据集D。</p><p>输出：k个簇。</p><p>步骤如下：</p><p>1) 从D中任意选择k个对象作为初始聚类中心；</p><p>2) 计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；</p><p>3) 重新计算每个聚类的均值；</p><p>4) 当满足一定条件，如没有对象再被重新分配给其他的类簇、聚类中心不再发生变化、误差平方和(SSE)最小，则算法终止；如果条件不满足则回到步骤(2)。</p><p>其中，每个对象与中心对象的距离为欧氏距离，距离公式如下：</p><p>d ( x , y ) = ∑ j = 1 m ( x j − y j ) 2 (1)</p><p>上式中：x，y分别表示样本和聚类中心；j表示第j维特征。</p></sec><sec id="s5"><title>3. 基于特征加权的改进算法</title><sec id="s5_1"><title>3.1. 信息熵</title><p>1948年，信息论之父克劳德•艾尔伍德•香农在他发表的论文“通信的数学理论(A Mathematical Theory of Communication)”中，提出了“信息熵”的概念。香农指出，任何信息都存在冗余，冗余大小与信息中每个符号(数字、字母或单词)的出现概率或者说不确定性有关。他用“信息熵”描述信息的不确定性。信息熵的数学表达式如下：</p><p>H ( x ) = − ∑ i = 1 m p i log 2 p i (2)</p><p>其中， p i 表示事件发生的概率。</p><p>在权重方面，对于任意两个特征 A i 、 A j ,若 H ( A i ) &gt; H ( A j ) ，则表示特征 A i 比特征 A j 好，在聚类过程中，特征 A i 比特征 A j 起到的作用大。</p></sec><sec id="s5_2"><title>3.2. RliefF算法</title><p>1994年，Knonenko提出了ReliefF算法，它是Relief算法的扩展，主要处理多分类问题 [<xref ref-type="bibr" rid="hanspub.26345-ref8">8</xref>] 。ReliefF算法的基本思想是从训练样本集中随机取出一个样本x<sub>i</sub>；然后从与x<sub>i</sub>同类的样本中取出k个近邻样本H<sub>i</sub>；再从其他与x<sub>i</sub>不同的类中分别取出k个样本M<sub>i</sub>；根据权重公式更新每个特征的权重；随机抽取m次，得到最终的特征权重。</p><p>权重表达式如下：</p><p>w ( j ) = w ( j ) + ∑ c ≠ c l a s s ( i ) p ( c ) 1 − p ( c l a s s ( x i ) ) ∑ j = 1 k d ( x i ( j ) , M i ( j ) ) m k − ∑ j = 1 k d ( x i ( j ) , H i ( j ) ) m k (3)</p><p>上式中： c l a s s ( i ) 表示样本x<sub>i</sub>所属的类别；c表示除样本x<sub>i</sub>所属的类别外的类别；p(c)表示类别c的先验概率。 x i ( j ) 表示样本x<sub>i</sub>关于第j个特征的值；m是随机抽取样本的次数； d ( x i ( j ) , M i ( j ) ) 表示距离函数，用于计算两个样本关于第j个特征的距离，计算公式如下：</p><p>d ( x i ( j ) , M i ( j ) ) = | x i ( j ) − M i ( j ) max ( j ) − min ( j ) | (4)</p><p>其中， max ( j ) , min ( j ) 表示第j个特征的所有取值中的最大值和最小值。</p><p>ReliefF算法用于处理多分类问题，每个样本都要有明确的类标记。但聚类分析中的样本没有类标记。所以，需要先对样本集进行一次初聚类，获得样本的类标记，再用ReliefF算法进行特征权重的计算。</p></sec><sec id="s5_3"><title>3.3. 基于特征加权的改进算法ER_Kmeans算法(EntropyReliefF_Kmeans)</title><p>传统的K-means算法在聚类过程中假设每个特征对于聚类的影响是相同的，忽略了特征对于聚类过程的影响，导致最终的聚类结果准确率不高。基于特征加权的改进算法则有效的解决了这一问题。</p><p>ER_Kmeans算法的基本思想是将聚类对象的特征权重与信息熵作为K-means算法的特征权重进行聚类。设信息熵权重为w<sub>1</sub>，特征权重为w<sub>2</sub>，则最终的特征权重为</p><p>w = w 1 + w 2 2 (5)</p><p>ER_Kmeans算法的步骤如下(图1)：</p><p>图1. ER_Kmeans算法流程图</p><p>输入：数据集D，簇的数目K</p><p>输出：K个簇</p><p>1) 随机选择K个初始聚类中心；</p><p>2) 计算信息熵特征权重ⱳ1；</p><p>3) ReliefF算法计算特征权重ⱳ2；</p><p>4) 计算特征权重ⱳ；</p><p>5) 计算每个样本与这些聚类中心的距离 d ( x , y ) = ∑ j = 1 m ω j ( x j − y j ) 2 ；并根据最小距离将样本划分到相应的聚类中心；</p><p>6) 重新计算每个聚类的均值；</p><p>7) 若聚类中心不再发生变化，则算法终止；如果聚类中心发生变化则回到步骤(5)。</p></sec></sec><sec id="s6"><title>4. 仿真实验</title><sec id="s6_1"><title>4.1. 实验环境与数据集</title><p>实验的硬件环境为Intel(R)Core(TM)i5-6500 3.20 GHz，8 G内存，软件环境为Matlab2016b，Windows7操作系统。实验选取的数据集为UCI数据库 [<xref ref-type="bibr" rid="hanspub.26345-ref9">9</xref>] 中的Iris、Balance-scale、Stalog数据集，数据集的主要信息如表1所示。</p></sec><sec id="s6_2"><title>4.2. 实验结果与分析</title><p>为了验证在聚类过程中聚类对象的每个特征对聚类结果的贡献度不同，在Iris、Balance-scale、Stalog 3个数据集各计算20次得到各特征对应的权重值如图2、图3、图4所示。图中一条线代表一次计算。</p><p>由图2、图3、图4可以看出，每个特征对于聚类结果具有不同的影响。以图4 Stalog数据集的特征权重值为例，从图中可以看出，特征7和特征12的权重值较高，说明其对聚类结果的影响较大；特征6和特征15的权重值较低，几乎趋近于0，说明对聚类结果的影响较小甚至可能没有影响。传统的K-means算法忽略了这一问题，导致最终的聚类结果在精度方面较低。</p><p>为了验证算法的有效性及稳定性，在相同的实验环境下，ER_Kmeans算法与传统K-means算法、文献 [<xref ref-type="bibr" rid="hanspub.26345-ref4">4</xref>] 的基于ReliefF算法的加权K-means算法ReliefF-Kmeans、文献 [<xref ref-type="bibr" rid="hanspub.26345-ref7">7</xref>] 的基于信息熵的加权K-means算法entropy-Kmeans在相同的数据集下，都进行了20次的单独实验，并取平均值，在准确率、误差平方和(SSE)、迭代次数和运行时间4个方面进行比较。结果如表2~表6所示。</p><p>从表2可以看出，在准确率方面，ER_Kmeans算法明显高于其他3种算法，由于传统K-means算法忽略了特征对于聚类结果的影响，聚类结果是不稳定的，所以准确率低于其他3种算法；从表3可以看出，ER_Kmeans算法的误差平方和低于其他3种算法，误差平方和越小，说明聚类中的对象越相似，所以ER_Kmeans算法每类的类内对象相似度高，聚类质量优于其他3种算法，达到了聚类分析的最终目的，</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Experimental data se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >数据个数</th><th align="center" valign="middle" >属性个数</th><th align="center" valign="middle" >类别个数</th></tr></thead><tr><td align="center" valign="middle" >Iris</td><td align="center" valign="middle" >150</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >3</td></tr><tr><td align="center" valign="middle" >Balance</td><td align="center" valign="middle" >625</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >3</td></tr><tr><td align="center" valign="middle" >Stalog</td><td align="center" valign="middle" >846</td><td align="center" valign="middle" >18</td><td align="center" valign="middle" >4</td></tr></tbody></table></table-wrap><p>表1. 实验数据集</p><p>图2. Iris数据集的特征权重</p><p>图3. Balance数据集的特征权重</p><p>图4. Stalog数据集的特征权重</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of accuracy of each algorithm in UCI dataset (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >传统K-means算法</th><th align="center" valign="middle" >entropy-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref7">7</xref>]</th><th align="center" valign="middle" >ReliefF-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref4">4</xref>]</th><th align="center" valign="middle" >本文算法(ER_Kmeans算法)</th></tr></thead><tr><td align="center" valign="middle" >Iris</td><td align="center" valign="middle" >83.40</td><td align="center" valign="middle" >85.10</td><td align="center" valign="middle" >87.07</td><td align="center" valign="middle" >89.50</td></tr><tr><td align="center" valign="middle" >Balance</td><td align="center" valign="middle" >63.03</td><td align="center" valign="middle" >64.95</td><td align="center" valign="middle" >64.65</td><td align="center" valign="middle" >66.09</td></tr><tr><td align="center" valign="middle" >Stalog</td><td align="center" valign="middle" >43.97</td><td align="center" valign="middle" >44.56</td><td align="center" valign="middle" >44.58</td><td align="center" valign="middle" >45.31</td></tr></tbody></table></table-wrap><p>表2. 各算法在UCI数据集的准确率比较(%)</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Comparison of error square sum (SSE) of each algorithm in UCI dataset (/ms2</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >传统K-means算法</th><th align="center" valign="middle" >entropy-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref7">7</xref>]</th><th align="center" valign="middle" >ReliefF-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref4">4</xref>]</th><th align="center" valign="middle" >本文算法(ER_Kmeans算法)</th></tr></thead><tr><td align="center" valign="middle" >Iris</td><td align="center" valign="middle" >96</td><td align="center" valign="middle" >89</td><td align="center" valign="middle" >93</td><td align="center" valign="middle" >79</td></tr><tr><td align="center" valign="middle" >Balance</td><td align="center" valign="middle" >3492</td><td align="center" valign="middle" >3489</td><td align="center" valign="middle" >3510</td><td align="center" valign="middle" >3478</td></tr><tr><td align="center" valign="middle" >Stalog</td><td align="center" valign="middle" >3,740,979</td><td align="center" valign="middle" >3,626,124</td><td align="center" valign="middle" >3,562,829</td><td align="center" valign="middle" >3,556,605</td></tr></tbody></table></table-wrap><p>表3. 各算法在UCI数据集的误差平方和(SSE)比较(/ms<sup>2</sup>)</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Comparison of the number of iterations of each algorithm in the UCI data set (/times</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >传统K-means算法</th><th align="center" valign="middle" >entropy-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref7">7</xref>]</th><th align="center" valign="middle" >ReliefF-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref4">4</xref>]</th><th align="center" valign="middle" >本文算法(ER_Kmeans算法)</th></tr></thead><tr><td align="center" valign="middle" >Iris</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >7</td></tr><tr><td align="center" valign="middle" >Balance</td><td align="center" valign="middle" >14</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >9</td></tr><tr><td align="center" valign="middle" >Stalog</td><td align="center" valign="middle" >17</td><td align="center" valign="middle" >17</td><td align="center" valign="middle" >13</td><td align="center" valign="middle" >13</td></tr></tbody></table></table-wrap><p>表4. 各算法在UCI数据集的迭代次数比较(次)</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Runtime comparison of each algorithm in UCI dataset (/ms</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >传统K-means算法</th><th align="center" valign="middle" >entropy-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref7">7</xref>]</th><th align="center" valign="middle" >ReliefF-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref4">4</xref>]</th><th align="center" valign="middle" >本文算法(ER_Kmeans算法)</th></tr></thead><tr><td align="center" valign="middle" >Iris</td><td align="center" valign="middle" >16.8</td><td align="center" valign="middle" >4.2</td><td align="center" valign="middle" >4.3</td><td align="center" valign="middle" >3.5</td></tr><tr><td align="center" valign="middle" >Balance</td><td align="center" valign="middle" >136.7</td><td align="center" valign="middle" >36.7</td><td align="center" valign="middle" >11.4</td><td align="center" valign="middle" >34.6</td></tr><tr><td align="center" valign="middle" >Stalog</td><td align="center" valign="middle" >172.2</td><td align="center" valign="middle" >63.7</td><td align="center" valign="middle" >48.4</td><td align="center" valign="middle" >40.6</td></tr></tbody></table></table-wrap><p>表5. 各算法在UCI数据集的运行时间比较(/ms)</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> The average run time of each algorithm in the UCI data set is compared with each iteration (/ms</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >传统K-means算法</th><th align="center" valign="middle" >entropy-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref7">7</xref>]</th><th align="center" valign="middle" >ReliefF-Kmeans [<xref ref-type="bibr" rid="hanspub.26345-ref4">4</xref>]</th><th align="center" valign="middle" >本文算法(ER_Kmeans算法)</th></tr></thead><tr><td align="center" valign="middle" >Iris</td><td align="center" valign="middle" >2.1</td><td align="center" valign="middle" >0.6</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >0.5</td></tr><tr><td align="center" valign="middle" >Balance</td><td align="center" valign="middle" >9.8</td><td align="center" valign="middle" >2.3</td><td align="center" valign="middle" >2.3</td><td align="center" valign="middle" >3.8</td></tr><tr><td align="center" valign="middle" >Stalog</td><td align="center" valign="middle" >10.1</td><td align="center" valign="middle" >3.7</td><td align="center" valign="middle" >3.7</td><td align="center" valign="middle" >3.1</td></tr></tbody></table></table-wrap><p>表6. 各算法在UCI数据集的平均每次迭代运行时间比较(/ms)</p><p>即类内相似度高，类间相似度低；从表4和表5可以看出，ER_Kmeans算法在迭代次数和运行时间上低于传统K-means算法和entropy-Kmeans算法，在Iris数据集和Stalog数据集上低于ReliefF-Kmeans算法，但在Balance数据集上高于ReliefF-Kmeans算法, 原因是算法的初始聚类中心是随机选择的，从而导致了算法迭代次数的不稳定、运行时间的长短不同。但在平均运行时间上二者差别不大，从表6可以看出，ER_Kmeans算法在平均每次迭代运行时间上低于传统K-means算法和entropy-Kmeans算法，在Balance数据集上高于ReliefF-Kmeans算法，但ER_Kmeans算法平均每次迭代的时间与ReliefF-Kmeans算法相差不大，说明迭代次数和运行时间较高是受到初始聚类中心的影响。</p></sec></sec><sec id="s7"><title>5. 结束语</title><p>本文提出一种利用信息熵和ReliefF算法对特征加权的K-means算法ER_Kmeans算法，有效的解决了不同的特征对于聚类具有不同的影响这一问题。实验结果表明：改进后的K-means算法在准确率和聚类误差上都优于传统K-means算法和其他两种特征加权方法，取得了较好的聚类结果。</p></sec><sec id="s8"><title>基金项目</title><p>本研究获得国家自然科学基金(61363016, 61063004)；内蒙古自然科学基金(NO.2015MS0605, NO.2015MS0626, NO.2015MS0627, NO.2017MS0605)；内蒙古教育厅高校研究项目(NJZC059)；内蒙古自治区高等学校科学研究重点项目(NJZZ14100)；教育部留学人员基金([<xref ref-type="bibr" rid="hanspub.26345-ref2014">2014</xref>]1685)；内蒙古自治区科技计划项目：穿透降水量GSM网络在线监测与数据传输系统的资助。</p></sec><sec id="s9"><title>文章引用</title><p>徐 艳,付学良,李宏慧,董改芳,王 晴. 一种基于特征加权的K-Means算法研究 A K-Means Algorithm Based on Feature Weighting[J]. 计算机科学与应用, 2018, 08(08): 1164-1171. https://doi.org/10.12677/CSA.2018.88128</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.26345-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Alexandropoulos, A., Plessas, F. and Birbas, M. (2010) A Dynamic DFI-Compatible Strobe Qualification System for Double Data Rate (DDR) Physical Interfaces. IEEE International Conference on Electronics, Athens, 12-15 December 2010, 277-280. https://doi.org/10.1109/ICECS.2010.5724507</mixed-citation></ref><ref id="hanspub.26345-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">卓金武, 周英. 量化投资: MATLAB数据挖掘技术与实践[M]. 北京: 电子工业出版社, 2017: 217-224.</mixed-citation></ref><ref id="hanspub.26345-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">刘铭, 吴冲. 基于特征权重量化的相似度计算方法[J]. 计算机学报, 2015, 38(7): 1420-1433.</mixed-citation></ref><ref id="hanspub.26345-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Li, J. and Gao, X.B. (2006) A New Feature Weighted Fuzzy Clustering Algorithm. Proceedings of SPIE, The Inter-national Society for Optical Engineering, 3641, 412-420.</mixed-citation></ref><ref id="hanspub.26345-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Meng, Q. (2015) An Improved Clustering Algorithm Based on Fea-ture-Weight Learning. Journal of Information and Computational Science, 12, 3519-3526. &lt;br&gt;https://doi.org/10.12733/jics20106074</mixed-citation></ref><ref id="hanspub.26345-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Shang, S.T. and Shi, M.Y. (2016) Improved Feature Weight Algorithm and Its Ap-plication to Text Classification. Mathematical Problems in Engineering, 2016, Article ID: 7819626. &lt;br&gt;https://doi.org/10.1155/2016/7819626</mixed-citation></ref><ref id="hanspub.26345-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">杨玉梅. 基于信息熵改进的K-means动态聚类算法[J]. 重庆邮电大学学报(自然科学版), 2016, 28(2): 254-259.</mixed-citation></ref><ref id="hanspub.26345-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">菅小艳, 韩素青. 不平衡数据集上的Relief特征选择算法[J]. 数据采集与处理, 2016, 31(4): 838-844.</mixed-citation></ref><ref id="hanspub.26345-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">UCI Machine Learning Repository: Data Sets [DB]. http://archive.ics.uci.edu/ml/datasets.html</mixed-citation></ref></ref-list></back></article>