<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.113052</article-id><article-id pub-id-type="publisher-id">CSA-40944</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210300000_38835132.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  一种用于遥感图像检索的双重注意力深度神经网络
  A Dual Attention Deep Neural Network for Remote Sensing Image Retrieval
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>光明</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>卓薇</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>立宜</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>邱</surname><given-names>俊豪</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>何</surname><given-names>俊霖</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>广东工业大学机电工程学院，广东 广州</addr-line></aff><aff id="aff2"><addr-line>广东工业大学计算机学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>03</month><year>2021</year></pub-date><volume>11</volume><issue>03</issue><fpage>515</fpage><lpage>524</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   因为遥感图像背景复杂，所以提取判别性强特征是遥感图像检索的一个核心技术。本文引入双重自注意力模块，利用空间和通道上的长距离上下文信息，编码局部特征，从而增强特征的表达能力。本文分别在3个典型的数据集上做了实验，在UC Merced Land Use、Satellite Remote Sensing Image Database、NWPU-RESISC45的平局检索精度分别为0.92、0.90和0.89。实验表明，双重自注意力深度学习网络对遥感图像检索性能的提升有显著的作用。 Extracting discriminative features is a core technology for remote sensing image retrieval due to the complex background of the remote sensing image. In order to enhance the expressive ability of the features, the paper introduces dual attention module to encode the long-distance length information on the spatial and the channel dimensions into local features. Experiments were carried out on three typical datasets. We have conducted experiments on three typical datasets to ascertain the effectiveness of our method. The retrieval precisions on UC Merced Land Use, Satellite Remote Sensing Image Database, and NWPU-RESISC45 are 0.92, 0.90 and 0.89. The experiment shows the self-attention deep learning network gets a significant effect on the improvement of remote sensing image retrieval performance. 
  
 
</p></abstract><kwd-group><kwd>遥感图像检索，注意力机制，CNN，深度学习, Remote Sensing Image Retrieval</kwd><kwd> Attention Mechanism</kwd><kwd> CNN</kwd><kwd> Deep Learning</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>因为遥感图像背景复杂，所以提取判别性强特征是遥感图像检索的一个核心技术。本文引入双重自注意力模块，利用空间和通道上的长距离上下文信息，编码局部特征，从而增强特征的表达能力。本文分别在3个典型的数据集上做了实验，在UC Merced Land Use、Satellite Remote Sensing Image Database、NWPU-RESISC45的平局检索精度分别为0.92、0.90和0.89。实验表明，双重自注意力深度学习网络对遥感图像检索性能的提升有显著的作用。</p></sec><sec id="s2"><title>关键词</title><p>遥感图像检索，注意力机制，CNN，深度学习</p></sec><sec id="s3"><title>A Dual Attention Deep Neural Network for Remote Sensing Image Retrieval<sup> </sup></title><p>Guangming Chen<sup>1</sup>, Zhuowei Wang<sup>1</sup>, Liyi Chen<sup>1</sup>, Junhao Qiu<sup>2</sup>, Junlin He<sup>1</sup></p><p><sup>1</sup>School of Computer Science, Guangdong University of Technology, Guangzhou Guangdong</p><p><sup>2</sup>School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/6-1542045x4_hanspub.png" /></p><p>Received: Feb. 15<sup>th</sup>, 2021; accepted: Mar. 9<sup>th</sup>, 2021; published: Mar. 16<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/6-1542045x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Extracting discriminative features is a core technology for remote sensing image retrieval due to the complex background of the remote sensing image. In order to enhance the expressive ability of the features, the paper introduces dual attention module to encode the long-distance length information on the spatial and the channel dimensions into local features. Experiments were carried out on three typical datasets. We have conducted experiments on three typical datasets to ascertain the effectiveness of our method. The retrieval precisions on UC Merced Land Use, Satellite Remote Sensing Image Database, and NWPU-RESISC45 are 0.92, 0.90 and 0.89. The experiment shows the self-attention deep learning network gets a significant effect on the improvement of remote sensing image retrieval performance.</p><p>Keywords:Remote Sensing Image Retrieval, Attention Mechanism, CNN, Deep Learning</p><disp-formula id="hanspub.40944-formula35"><graphic xlink:href="//html.hanspub.org/file/6-1542045x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/6-1542045x7_hanspub.png" /> <img src="//html.hanspub.org/file/6-1542045x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>随着遥感图像技术的高速发展，遥感图像的数量急剧增加。如何在大型遥感数据库中有效地组织，管理和检索遥感图像，已经成为遥感图像应用中的紧迫而迫切的问题。其中，基于内容的遥感图像检索(CBRSIR) [<xref ref-type="bibr" rid="hanspub.40944-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref2">2</xref>] 是遥感应用中最关键的技术。CBRSIR可以概括为两个步骤：特征提取和相似性度量。CBRSIR的性能通常取决于从遥感图像中提取的判别特征 [<xref ref-type="bibr" rid="hanspub.40944-ref3">3</xref>]。因此，作为CBRSIR的最关键步骤，特征提取是大多数CBRSIR研究的重点 [<xref ref-type="bibr" rid="hanspub.40944-ref4">4</xref>]。</p><p>特征提取主要有两种方法：基于手工特征的方法和基于学习特征的方法 [<xref ref-type="bibr" rid="hanspub.40944-ref5">5</xref>]。基于手工特征包括颜色、纹理、形状等全局特征和基于SIFT [<xref ref-type="bibr" rid="hanspub.40944-ref6">6</xref>] 和SURF [<xref ref-type="bibr" rid="hanspub.40944-ref7">7</xref>] 的局部特征。此外，词袋模型(BOW) [<xref ref-type="bibr" rid="hanspub.40944-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref9">9</xref>] 和局部聚集描述符的向量(VLAD) [<xref ref-type="bibr" rid="hanspub.40944-ref10">10</xref>] 用于编码局部特征，可以进一步增强特征的表达能力。无论是全局特征还是局部特征，它们都不能很精确地表达图像，所以在高级语义和低级语义之间存在“语义鸿沟”。随着深度学习的发展，卷积神经网络(CNN)，在计算机视觉领域，例如分类 [<xref ref-type="bibr" rid="hanspub.40944-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref13">13</xref>]、检测 [<xref ref-type="bibr" rid="hanspub.40944-ref14">14</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref15">15</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref16">16</xref>]、分割 [<xref ref-type="bibr" rid="hanspub.40944-ref17">17</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref18">18</xref>] 等方面，展现了优异的性能优势，CNN已经广泛应用于图像特征提取。CNN可以通过大量的卷积层堆叠来提取高级语义特征。GE等人 [<xref ref-type="bibr" rid="hanspub.40944-ref19">19</xref>] 将在ImageNet上训练得到预训练模型应用到遥感图像数据集上，表明CNN的特征明显优于传统的手工特征。</p><p>然而，在现有的CNN特征提取方法中存在一个问题。与其他图像相比，遥感图像具有几个特殊特征。例如，即使在同一类别中，不同图像中的目标也可能具有不同的大小，颜色和角度。更重要的是，目标区域周围的其他材料和背景可能会导致较高的组内差异和较低的组间差异。因此，在现有的CNN特征提取方法中存在一个问题，即提取到的特征空间中的图像表示可能无法准确反映其真实类别信息。准确检索具有相似视觉内容的图像需要提取足够描述性和鲁棒性的特征。</p><p>针对上述问题，在Fu的研究 [<xref ref-type="bibr" rid="hanspub.40944-ref20">20</xref>] 的启发下，本文提出了一种双重注意力模型。本文的主要贡献分为两个部分：</p><p>1) 本文设计了一种双重注意力深度学习网络，通过捕获空间和通道的特征依赖关系，提取具有复杂背景的遥感图像的显著性特征，以准确反映真实类别信息。</p><p>2) 本文引入的双重注意力模块包括空间注意力模块和通道注意力模块。对于空间注意力模块，使用自注意力机制来捕获特征图上任意两个位置上的依赖关系。对于通道注意力模块，我们引入自注意力机制来捕获特征图上任意两个通道的依赖关系。</p><p>本文的结构如下：第二章介绍遥感图像检索的相关工作，第三章介绍提出的方法，第四章展示实验，第五章总结本文。</p></sec><sec id="s6"><title>2. 相关工作</title><sec id="s6_1"><title>2.1. 基于学习的特征提取</title><p>在特征提取领域，CNN逐渐取代了传统的方法，变得越来越流行了。CNN通过深层网络结构中的非线性函数，从训练数据中学习参数权重。但是，遥感图像数据集数据量小，导致了不能从零开始训练CNN模型。即使大型基准数据集与遥感图像数据集有很大差异，但是使用大型基准数据集上训练得到的预训练模型可以在一定程度上解决因为遥感图像数据集数据量不足而带来的问题。一些研究 [<xref ref-type="bibr" rid="hanspub.40944-ref21">21</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref22">22</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref23">23</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref24">24</xref>] 已经比较了在不同网络和不同层之间提取的特征的性能。Mnih等人 [<xref ref-type="bibr" rid="hanspub.40944-ref21">21</xref>] 使用预训练模型的特征和简单的聚合特征，提高了检索性能。Wang等人 [<xref ref-type="bibr" rid="hanspub.40944-ref24">24</xref>] 提出了对遥感图像数据集上微调预训练模型的方法，同时提出了一种基于三层感知器的CNN网络结构。该感知器不仅参数较少，而且可以学习底层局部特征。Shao等人 [<xref ref-type="bibr" rid="hanspub.40944-ref25">25</xref>] 研究了在多标签遥感图像检索框架下研究了不同深度学习架构的有效性，并获得较好的检索效果。Roy等人 [<xref ref-type="bibr" rid="hanspub.40944-ref26">26</xref>] 提出三元组深度度量学习深度卷积神经网络，利用三元组损失函数，使得在语义空间中，来自同一类别的图像彼此接近，而来自不同类别的图像则彼此远离。</p></sec><sec id="s6_2"><title>2.2. 注意力机制</title><p>注意力机制通过学习不同区域的权重分布，来为不同区域分配不同的“关注度”。注意力机制的有效性已在许多任务中得到证明，包括机器翻译和文本等基于序列的任务以及分类和分割等计算机视觉任务。一些研究 [<xref ref-type="bibr" rid="hanspub.40944-ref27">27</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref28">28</xref>] [<xref ref-type="bibr" rid="hanspub.40944-ref29">29</xref>] 将学习到的权重应用于原始图像，Yuan等人 [<xref ref-type="bibr" rid="hanspub.40944-ref30">30</xref>] 将权重学习应用于特征图。Huang等人 [<xref ref-type="bibr" rid="hanspub.40944-ref31">31</xref>] 考虑了特征通道之间的关系，在特征通道上加入了注意力机制。Fu等人 [<xref ref-type="bibr" rid="hanspub.40944-ref20">20</xref>] 结合了特征通道和特征空间两个维度的注意力机制。Wang等 [<xref ref-type="bibr" rid="hanspub.40944-ref32">32</xref>] 提出多头注意机制，引入额外的特征映射和实现了自注意力机制。所有这些工作都被用于自然图像处理方面，其中在分类，检测等方面表现出了出色的性能。目前比较少应用于遥感图像处理的注意力模型。Du等人 [<xref ref-type="bibr" rid="hanspub.40944-ref33">33</xref>] 将结合了特征通道和特征空间两个维度的注意力机制应用于遥感图像处理。Maxim等人 [<xref ref-type="bibr" rid="hanspub.40944-ref34">34</xref>] 采用注意力机制提取遥感图像的深层局部特征，在图像背景内容复杂情况下，依然实现较好的检索性能。</p></sec></sec><sec id="s7"><title>3. 本文方法</title><p>本节介绍双重注意力深度学习网络的具体细节。在检索具有复杂背景的遥感图像时，关键是提取遥感图像关键特征。因此，我们引入双重注意力机制来，模拟跨越图像区域的长距离、多层的依赖关系，有效地对上下文进行建模，编码为局部特征，从而增强特征的表达能力。该模型的总体结构见图1。</p><p>图1. 模型的整体结构</p><sec id="s7_1"><title>3.1. 网络结构与池化</title><p>我们使用ResNet-50 [<xref ref-type="bibr" rid="hanspub.40944-ref13">13</xref>] 作为模型的骨架。ResNet-50包括五个卷积层，每个卷积层包括一个卷积操作、一个修正线性单元和最大池化操作。输入一个图像，我们只需要ResNet-50最后的一个卷积层的输出的特征图，不需要全连接层的输出。我们从最后一个卷积层得到一个张量 χ ∈ R W &#215; H &#215; N ，其中N表示通道数，W表示特征图的宽度，H表示特征图的高度。</p><p>SPoC [<xref ref-type="bibr" rid="hanspub.40944-ref35">35</xref>] 使用的是平均池化操作，，如公式(1)所示。MAC [<xref ref-type="bibr" rid="hanspub.40944-ref36">36</xref>] 使用的是最大池化操作，如公式(2)所示。这两种池化方法已经在标准数据库中已经取得较好的结果。</p><p>[ F 1 ( SPoC ) , ⋯ , F M ( SPoC ) ] T , F m ( SPoC ) = 1 | O m | ∑ ​ o ∈ O m   o (1)</p><p>[ F 1 ( MAC ) , ⋯ , F M ( MAC ) ] T , F m ( MAC ) = max o ∈ O m o (2)</p><p>相比这两种池化方法，GeM [<xref ref-type="bibr" rid="hanspub.40944-ref37">37</xref>] 使用的是一种广义平均池化操作，，如公式(3)所示。GeM性能更好，可以提升检索精度。所以，我们使用GeM来聚合特征，获得更紧凑的特征。假设 χ k 表示 χ 的第k个特征图。</p><p>[ F 1 ( GeM ) , ⋯ , F M ( GeM ) ] T , F m ( GeM ) = ( 1 | O m | ∑ ​ o ∈ O m     o p m ) 1 p m (3)</p><p>其中O是双重注意力模块输出的特征。SPoC和MAC是GeM特殊情况。在公式中，当 p m → ∞ 时，公式(3)会转化为公式(2)，即最大池化操作；当 p m → 1 时，公式(3)会转化为公式(1)，即平均池化操作。最后得到的特征图的维数等于M。在我们的模型中，M等于2048。最后，对特征图进行 l 2 归一化操作。</p></sec><sec id="s7_2"><title>3.2. 双重注意力模块</title><p>双重注意力模块的结构见图2。双重注意力模块包括空间注意力模块和通道注意力模块。我们将从ResNet-50最后一层卷积层得到的特征图，分别输入到空间注意力模块和通道注意力模块中。下一步，将两个注意力模块的结果通过加操作融合在一起。最后通过一个卷积层输出结果。</p><p>图2. 双重注意模块的整体结构</p><sec id="s7_2_1"><title>3.2.1. 空间注意力模块</title><p>遥感图像检索任务需要判别性强的特征。我们引入空间注意力模块，借助长距离上下文信息编码来增强特征的判别性。接下来，我们介绍空间注意力模块的处理过程。</p><p>我们将 χ 变换为 I ∈ R C &#215; H &#215; W ，然后将其输入到一个卷积层，生成两个特征J、K和L，其中 { J , K , L } ∈ R C &#215; H &#215; W 。我们将它们变换成二维矩阵 R C &#215; N ，其中N为像素个数， N = H &#215; W 。我们对J做多一个转置操作。如公式(4)，在将J和K进行乘操作之后，我们将得到的矩阵经过一个softmax层，得到空间注意力矩阵 S ∈ R N &#215; N ，如公式(4)所示。</p><p>s j i = exp ( J i ⋅ K j ) ∑ ​ i = 1 N exp ( J i ⋅ K j ) (4)</p><p>其中 s j i 表示第i个位置对第j个位置的影响。两个位置的特征越相似，它们之间的相关性就越高。同时，我们对变换过的L与S进行乘操作，并将结果转换为矩阵 T ∈ R C &#215; H &#215; W 。最后，我们将T乘以比例系数 α 后，与A进行加操作，得到最后的输出 P ∈ R C &#215; H &#215; W ，如公式(5)所示。</p><p>P j = α ∑ ​ i = 1 N ( s j i L i ) + I j (5)</p><p>其中 α 初始化为0，并随着学习，逐渐增大权重。P的每个位置的最终特征是所有位置的特征与原始特征的加权和。因此，它具有全局上下文信息，并根据空间注意力选择性地聚合上下文，从而提高了类内的紧凑性和语义一致性。</p></sec><sec id="s7_2_2"><title>3.2.2. 通道注意力模块</title><p>我们引入通道注意力机制来构建通道之间的依赖关系。接下来，我们介绍空间注意力模块的处理过程。</p><p>通道注意力模块和空间注意力模块是基本一样的操作，但有两点不同。在通道注意力模块中，我们直接没有使用卷积层来处理，而是直接将I转换，并和通道注意力特征矩阵 X ∈ R C &#215; C 。我们将I转换成矩阵 R C &#215; N ，然后将对I和I的转置矩阵进行乘操作。最后，我们将得到的矩阵经过一个softmax层，得到通道注意力矩阵 X ∈ R C &#215; C ，如公式(6)所示。</p><p>x j i = exp ( I i ⋅ I j ) ∑ ​ i = 1 N exp ( I i ⋅ I j ) (6)</p><p>其中 s j i 表示第i个通道对第j个通道的影响。同时，我们对变换过的X与A进行乘操作，并将结果转换为矩阵 T ∈ R C &#215; H &#215; W 。最后，我们将T乘以比例系数 β 后，与A进行加操作，得到最后的输出 Q ∈ R C &#215; H &#215; W ，如公式(7)所示。</p><p>Q j = β ∑ ​ i = 1 C ( x j i I i ) + I j (7)</p><p>其中 β 初始化为0，并随着学习，逐渐增大权重。Q是对通道间的长距离信息进行建模，从而提高了特征的可判别性。</p></sec></sec><sec id="s7_3"><title>3.3. 损失函数</title><p>Radenović等人 [<xref ref-type="bibr" rid="hanspub.40944-ref37">37</xref>] 发现使用对比损失函数的情况比使用三元组损失函数的情况，使用对比损失函数时的检索精度更高。所以，我们也使用对比损失函数，如公式(8)所示。</p><p>L O S S ( i , j ) = { 1 2 ‖ F ( i ) − F ( j ) ‖ 2 , Y ( i , j ) = 1 1 2 ( max { 0 , τ − ‖ F ( i ) − F ( j ) ‖ } ) 2 , Y ( i , j ) = 0 (8)</p><p>其中每个输入包括一组图片 ( i , j ) 和一组标签 Y ( i , j ) ∈ { 0 , 1 } 。当i和j匹配时， Y ( i , j ) = 1 ；否则， Y ( i , j ) = 0 。 τ 是边距超参数。</p></sec></sec><sec id="s8"><title>4. 实验与分析</title><sec id="s8_1"><title>4.1. 数据集</title><p>使用3个不同的数据集来评估所提出的方法在不检索性能。数据集分别为UCMerced Land Use (UCM) [<xref ref-type="bibr" rid="hanspub.40944-ref38">38</xref>]、Satellite Remote Sensing Image Database (SATREM) [<xref ref-type="bibr" rid="hanspub.40944-ref39">39</xref>] 和NWPU-RESISC4 (NWPU) [<xref ref-type="bibr" rid="hanspub.40944-ref40">40</xref>]。表1列出了每个数据集的详细信息(图像大小，图像数量等)。实验使用80%的数据集图像用于训练，20%图像用于测试。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Details of the dataset</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >图片数量</th><th align="center" valign="middle" >类别数</th><th align="center" valign="middle" >分辨率</th></tr></thead><tr><td align="center" valign="middle" >UCM</td><td align="center" valign="middle" >2100</td><td align="center" valign="middle" >21</td><td align="center" valign="middle" >256 &#215; 256 pix</td></tr><tr><td align="center" valign="middle" >SATREM</td><td align="center" valign="middle" >3000</td><td align="center" valign="middle" >20</td><td align="center" valign="middle" >256 &#215; 256 pix</td></tr><tr><td align="center" valign="middle" >NWPU</td><td align="center" valign="middle" >31500</td><td align="center" valign="middle" >45</td><td align="center" valign="middle" >256 &#215; 256 pix</td></tr></tbody></table></table-wrap><p>表1. 数据集的细节</p></sec><sec id="s8_2"><title>4.2. 评估标准</title><p>本文使用图像检索任务中的常用指标——平均检索精度(mAP)来评估检索性能。如果图像与查询图像属于同一类别，则认为该图像与查询图像非常匹配。平均检索精度定义见公式(9)、(10)、(11)、(12)。</p><p>m A P = 1 B ∑ ​ i = 1 B ( A P ) i (9)</p><p>A P = 1 T ∑ ​ i = 1 Q P i ( r e l ) i (10)</p><p>( r e l ) i = { 1         第 i 个 图 像 与 查 询 图 像 相 似 0                                   其 他 (11)</p><p>P i = N O i i (12)</p><p>其中，B为查询次数，Q为检索结果中最相似的Q幅图像，T为检索N幅图像时真正与须茶图像相似的图像个数， N O i 表示检索结果中真正与待查询图像相似的排序。</p></sec><sec id="s8_3"><title>4.3. 实验结果</title><p>将我们的方法，与ResNet-50、DBOW [<xref ref-type="bibr" rid="hanspub.40944-ref9">9</xref>]、D-CNN [<xref ref-type="bibr" rid="hanspub.40944-ref41">41</xref>]、V-DELF [<xref ref-type="bibr" rid="hanspub.40944-ref29">29</xref>] 这4个性能好的基于深度学习的方法作对比，以评估我们方法的检索性能。表2为每个方法在各个数据集上的平均检索精度。我们可以很明显观察到，除了在SATREM数据集上，我们的方法基本比其他方法的平均检索精度都要高。在NWPU数据集上，在其他方法的平均检索精度都有明显的精度下降的现象，但我们的方法的精度下降得不明显。图3是一个定性的检索结果，展示了在NWPU数据集上的一些检索示例。</p><p>图3. NWPU数据集的查询示例查询</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of average retrieval accuracy of different method</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >UCM</th><th align="center" valign="middle" >SATREM</th><th align="center" valign="middle" >NWPU</th></tr></thead><tr><td align="center" valign="middle" >ResNet-50</td><td align="center" valign="middle" >0.82</td><td align="center" valign="middle" >0.86</td><td align="center" valign="middle" >0.80</td></tr><tr><td align="center" valign="middle" >DBOW</td><td align="center" valign="middle" >0.83</td><td align="center" valign="middle" >0.93</td><td align="center" valign="middle" >0.82</td></tr><tr><td align="center" valign="middle" >D-CNN</td><td align="center" valign="middle" >0.87</td><td align="center" valign="middle" >0.85</td><td align="center" valign="middle" >0.74</td></tr><tr><td align="center" valign="middle" >V-DELF</td><td align="center" valign="middle" >0.92</td><td align="center" valign="middle" >0.89</td><td align="center" valign="middle" >0.86</td></tr><tr><td align="center" valign="middle" >our method</td><td align="center" valign="middle" >0.92</td><td align="center" valign="middle" >0.90</td><td align="center" valign="middle" >0.89</td></tr></tbody></table></table-wrap><p>表2. 不同方法的平均检索精度对比</p><p>同时，我们使用加权梯度类激活映射(Grad-CAM) [<xref ref-type="bibr" rid="hanspub.40944-ref42">42</xref>] 可视化了模型提取到的特征。图4中，可以观察到使用了双重注意力模块的方法提取的特征比原始ResNet-50提取的特征，更接近显著区域。这表明双重注意力模块可以充分利用显着区域中的信息并聚合特征。因此，实验结果表明了我们引入的双重注意力模块的有效性。</p><p>图4. 使用Grad-CAM分别可视化模型在UCM、SATREM和NWPU数据集中提取的特征</p><p>综上所述，我们的方法提高了遥感图像检索的平均检索精度。但是还有存在不足。在 NWPU数据集上表现出的检索性能不如在其他三个数据集上的检索性能。原因可能是，NWPU数据集的遥感图像数量多，是其他三个数据集的图片数量的10倍，类别也比其他三个数据集的类别多。NWPU数据集同类别的视觉差异比其他三个数据集的大，而且一些类别之间区分度较小，例如在池塘类中的两幅图像，两者之间的视觉差异性较大，有些与农田类的图像有较大视觉相似性。而其他三个数据集中，同类别的视觉差异较小，不同类别图像区分度较好。</p></sec></sec><sec id="s9"><title>5. 结论</title><p>本文引入双重自注意力模块，将空间和通道上的长距离上下文信息编码为局部特征，从而增强特征的表达能力。本文在3个典型的数据集上做了实验。实验表明，双重自注意力模块对遥感图像检索性能的提升有显著的作用。</p><p>尽管我们提出的方法有更好的性能，但是仍然存在一些不能忽略的缺点。例如，我们的双重自注意力模块只能连接到CNN的卷积层。但是，全连接层和卷积层也可以用作特征表示。在某些条件下，某些CNN的全连接层比卷积层可以获得更好的检索性能。因此，如何克服使用双重自注意力模块的局限性是我们未来的重点之一。</p></sec><sec id="s10"><title>基金项目</title><p>广东省信息物理融合重点实验室(2016B030301008)；国家自然科学基金(61701123)；国家高分地球观 测主要项目(83-Y40G33-9001-18/20)；广东省农业科学与技术创新团队项目(2019KJ147)；广东省科技计划 项目，水资源大数据项目(2016B010127005)；广东省自然科学基金项目(2018A030313195)；广州市科技 计划项目(201804010262)。</p></sec><sec id="s11"><title>文章引用</title><p>陈光明,王卓薇,陈立宜,邱俊豪,何俊霖. 一种用于遥感图像检索的双重注意力深度神经网络A Dual Attention Deep Neural Network for Remote Sensing Image Retrieval[J]. 计算机科学与应用, 2021, 11(03): 515-524. https://doi.org/10.12677/CSA.2021.113052</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.40944-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Du, P.J., Chen, Y.H., Tang, H. and Fang, T. (2005) Study on Content-Based Remote Sensing Image Retrieval. IEEE In-ternational Geoscience &amp; Remote Sensing Symposium, Seoul, 29 July 2005, 4.  
&lt;br&gt;https://doi.org/10.1109/IGARSS.2005.1525204</mixed-citation></ref><ref id="hanspub.40944-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Ning, X., Li, D. and Ye, W. (2005) Content-Based Remote Sensing Image Retrieval. Proceedings of SPIE—The International Society for Optical Engineering, 6044, 60440Q. &lt;br&gt;https://doi.org/10.1117/12.654549</mixed-citation></ref><ref id="hanspub.40944-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Sudha, S.K. and Aji, S. (2019) A Review on Recent Advances in Remote Sensing Image Retrieval Techniques. Journal of the Indian Society of Remote Sensing, 47, 2129-2139. &lt;br&gt;https://doi.org/10.1007/s12524-019-01049-8</mixed-citation></ref><ref id="hanspub.40944-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F. and Fraundorfer, F. (2017) Deep Learning in Remote Sensing: A Review. &lt;br&gt;https://arxiv.org/abs/1710.03959v1</mixed-citation></ref><ref id="hanspub.40944-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Wan, J., Wang, D., Hoi, S.C.H., Wu, P. and Li, J. (2014) Deep Learning for Content-Based Image Retrieval: A Comprehensive Study. Proceedings of the 22nd ACM International Conference on Multimedia, ACM, November 2014, 157-166. &lt;br&gt;https://doi.org/10.1145/2647868.2654948</mixed-citation></ref><ref id="hanspub.40944-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Lowe, D.G. (1999) Object Recognition from Local Scale-Invariant Features. Proceedings of the Seventh IEEE International Conference on Computer Vision, 2, 1150-1157. &lt;br&gt;https://doi.org/10.1109/ICCV.1999.790410</mixed-citation></ref><ref id="hanspub.40944-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Bay, H., Tuy-telaars, T. and Van Gool, L. (2006) SURF: Speeded up Robust Features. In: Leonardis, A., Bischof, H. and Pinz, A., Eds., European Conference on Computer Vision, Springer, Berlin, Heidelberg, 404-417.  
&lt;br&gt;https://doi.org/10.1007/11744023_32</mixed-citation></ref><ref id="hanspub.40944-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Yang, J., Liu, J. and Dai, Q. (2015) An Improved Bag-of-Words Framework for Remote Sensing Image Retrieval in Large-Scale Image Databases. International Journal of Digital Earth, 8, 273-292.  
&lt;br&gt;https://doi.org/10.1080/17538947.2014.882420</mixed-citation></ref><ref id="hanspub.40944-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Tang, X., Zhang, X., Liu, F. and Jiao, L. (2018) Unsupervised Deep Feature Learning for Remote Sensing Image Retrieval. Remote Sensing, 10, 1243. &lt;br&gt;https://doi.org/10.3390/rs10081243</mixed-citation></ref><ref id="hanspub.40944-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Jégou, H., Douze, M., Schmid, C. and Pérez, P. (2010) Aggregating Local Descriptors into a Compact Image Representation. 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, 13-18 June 2010, 3304-3311. &lt;br&gt;https://doi.org/10.1109/CVPR.2010.5540039</mixed-citation></ref><ref id="hanspub.40944-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky, A., Sutskever, I. and Hinton, G.E. (2012) Imagenet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems, 25, 1097-1105.</mixed-citation></ref><ref id="hanspub.40944-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al. (2015) Going Deeper with Convolutions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 1-9.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298594</mixed-citation></ref><ref id="hanspub.40944-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S. and Sun, J. (2016) Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.40944-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B. and Belongie, S. (2017) Feature Pyramid Networks for Object Detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 2117-2125. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.106</mixed-citation></ref><ref id="hanspub.40944-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y. and Berg, A.C. (2016) SSD: Single Shot Multibox Detector. In: Leibe, B., Matas, J., Sebe, N. and Welling, M., Eds., European Conference on Computer Vision, Springer, Cham, 21-37. &lt;br&gt;https://doi.org/10.1007/978-3-319-46448-0_2</mixed-citation></ref><ref id="hanspub.40944-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J. and Farhadi, A. (2017) YOLO9000: Better, Faster, Stronger. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 7263-7271.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.690</mixed-citation></ref><ref id="hanspub.40944-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Long, J., Shelhamer, E. and Darrell, T. (2015) Fully Convolutional Networks for Semantic Segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 3431-3440.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298965</mixed-citation></ref><ref id="hanspub.40944-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Ioffe, S., Vanhoucke, V. and Alemi, A. (2017) In-ception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 31, 4278-4284.</mixed-citation></ref><ref id="hanspub.40944-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Ge, Y., Jiang, S., Xu, Q., Jiang, C. and Ye, F. (2018) Exploiting Repre-sentations from Pre-Trained Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval. Mul-timedia Tools and Applications, 77, 17489-17515. &lt;br&gt;https://doi.org/10.1007/s11042-017-5314-5</mixed-citation></ref><ref id="hanspub.40944-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z. and Lu, H. (2019) Dual Attention Network for Scene Segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, 15-20 June 2019, 3146-3154. &lt;br&gt;https://doi.org/10.1109/CVPR.2019.00326</mixed-citation></ref><ref id="hanspub.40944-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Mnih, V., Heess, N., Graves, A. and Kavukcuoglu, K. (2014) Re-current Models of Visual Attention. arXiv preprint arXiv:1406.6247.</mixed-citation></ref><ref id="hanspub.40944-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Gregor, K., Danihelka, I., Graves, A., Rezende, D. and Wierstra, D. (2015) DRAW: A Recurrent Neural Network for Image Generation. Proceedings of the 32nd International Conference on Machine Learning, 37, 1462-1471.</mixed-citation></ref><ref id="hanspub.40944-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Ba, J., Mnih, V. and Kavukcuoglu, K. (2014) Multiple Object Recognition with Visual Attention. arXiv preprint arXiv:1412.7755.</mixed-citation></ref><ref id="hanspub.40944-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X. and Tang, X. (2017) Residual Attention Network for Image Classification. Pro-ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 3156-3164. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.683</mixed-citation></ref><ref id="hanspub.40944-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Shao, Z., Yang, K. and Zhou, W. (2018) Performance Evaluation of Single-Label and Multi-Label Remote Sensing Image Retrieval Using a Dense Labeling Dataset. Remote Sensing, 10, 964. &lt;br&gt;https://doi.org/10.3390/rs10060964</mixed-citation></ref><ref id="hanspub.40944-ref26"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Roy, S., Sangineto, E., Demir, B. and Sebe, N. (2020) Met-ric-Learning-Based Deep Hashing Network for Content-Based Retrieval of Remote Sensing Images. IEEE Geoscience and Remote Sensing Letters, 18, 226-230.  
&lt;br&gt;https://doi.org/10.1109/LGRS.2020.2974629</mixed-citation></ref><ref id="hanspub.40944-ref27"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Bello, I., Zoph, B., Vaswani, A., Shlens, J. and Le, Q.V. (2019) Attention Augmented Convolutional Networks. Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, 27 October-2 November 2019, 3286-3295. &lt;br&gt;https://doi.org/10.1109/ICCV.2019.00338</mixed-citation></ref><ref id="hanspub.40944-ref28"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">Xiong, W., Lv, Y., Cui, Y., Zhang, X. and Gu, X. (2019) A Discriminative Feature Learning Approach for Remote Sensing Image Retrieval. Remote Sensing, 11, 281. &lt;br&gt;https://doi.org/10.3390/rs11030281</mixed-citation></ref><ref id="hanspub.40944-ref29"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">Imbriaco, R., Sebastian, C., Bondarev, E. and de With, P.H.N. (2019) Aggregated Deep Local Features for Remote Sensing Image Retrieval. Remote Sensing, 11, 493. &lt;br&gt;https://doi.org/10.3390/rs11050493</mixed-citation></ref><ref id="hanspub.40944-ref30"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Yuan, Y. and Wang, J. (2018) Ocnet: Object Context Network for Scene Parsing. arXiv preprint arXiv:1809.00916.</mixed-citation></ref><ref id="hanspub.40944-ref31"><label>31</label><mixed-citation publication-type="other" xlink:type="simple">Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y. and Liu, W. (2019) CCNet: Criss-Cross Attention for Semantic Segmentation. Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, Seoul, 27 October-2 November 2019, 603-612. &lt;br&gt;https://doi.org/10.1109/ICCV.2019.00069</mixed-citation></ref><ref id="hanspub.40944-ref32"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Wang, X., Girshick, R., Gupta, A. and He, K. (2018) Non-Local Neural Networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 7794-7803.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00813</mixed-citation></ref><ref id="hanspub.40944-ref33"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">Du, Y., Yuan, C., Li, B., Zhao, L., Li, Y. and Hu, W. (2018) In-teraction-Aware Spatio-Temporal Pyramid Attention Networks for Action Classification. Proceedings of the European Conference on Computer Vision (ECCV), 373-389.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01270-0_23</mixed-citation></ref><ref id="hanspub.40944-ref34"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">Berman, M., Jégou, H., Vedaldi, A., Kokkinos, I. and Douze, M. (2019) Multigrain: A Unified Image Embedding for Classes and Instances. arXiv preprint arXiv:1902.05509.</mixed-citation></ref><ref id="hanspub.40944-ref35"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Babenko, A. and Lempitsky, V. (2015) Aggregating Deep Convolutional Features for Image Retrieval. arXiv preprint arXiv:1510.07493.</mixed-citation></ref><ref id="hanspub.40944-ref36"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">Tolias, G., Sicre, R. and Jégou, H. (2015) Particular Object Retrieval with Integral Max-Pooling of CNN Activations. arXiv preprint arXiv:1511.05879.</mixed-citation></ref><ref id="hanspub.40944-ref37"><label>37</label><mixed-citation publication-type="other" xlink:type="simple">Radenović, F., Tolias, G. and Chum, O. (2018) Fine-Tuning CNN Image Retrieval with No Human Annotation. IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 41, 1655-1668.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2018.2846566</mixed-citation></ref><ref id="hanspub.40944-ref38"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Yang, Y. and Newsam, S. (2010) Bag-of-Visual-Words and Spatial Extensions for Land-Use Classification. Proceedings of the 18th SIGSPATIAL International Conference on Ad-vances in Geographic Information Systems, November 2010, 270-279. &lt;br&gt;https://doi.org/10.1145/1869790.1869829</mixed-citation></ref><ref id="hanspub.40944-ref39"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">Tang, X., Jiao, L., Emery, W.J., Liu, F. and Zhang, D. (2017) Two-Stage Reranking for Remote Sensing Image Retrieval. IEEE Transactions on Geoscience and Remote Sensing, 55, 5798-5817.  
&lt;br&gt;https://doi.org/10.1109/TGRS.2017.2714676</mixed-citation></ref><ref id="hanspub.40944-ref40"><label>40</label><mixed-citation publication-type="other" xlink:type="simple">Zhao, B., Zhong, Y., Xia, G.S. and Zhang, L. (2015) Di-richlet-Derived Multiple Topic Scene Classification Model for High Spatial Resolution Remote Sensing Imagery. IEEE Transactions on Geoscience and Remote Sensing, 54, 2108-2123. &lt;br&gt;https://doi.org/10.1109/TGRS.2015.2496185</mixed-citation></ref><ref id="hanspub.40944-ref41"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">Cheng, G., Han, J. and Lu, X. (2017) Remote Sensing Image Scene Classification: Benchmark and State of the Art. Proceedings of the IEEE, 105, 1865-1883. &lt;br&gt;https://doi.org/10.1109/JPROC.2017.2675998</mixed-citation></ref><ref id="hanspub.40944-ref42"><label>42</label><mixed-citation publication-type="other" xlink:type="simple">Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D. and Batra, D. (2017) Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. Pro-ceedings of the IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 618-626. &lt;br&gt;https://doi.org/10.1109/ICCV.2017.74.</mixed-citation></ref></ref-list></back></article>