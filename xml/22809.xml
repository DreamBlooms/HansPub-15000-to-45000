<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2017.68118</article-id><article-id pub-id-type="publisher-id">AAM-22809</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20170800000_67856117.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  一种基于扰动分析的三支谱聚类算法
  A Three-Way Spectral Clustering Algorithm Based on Perturbation Analysis
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>晓磊</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>平心</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>江苏科技大学理学院，江苏 镇江</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>16</day><month>11</month><year>2017</year></pub-date><volume>06</volume><issue>08</issue><fpage>985</fpage><lpage>991</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    本文在多路谱聚类算法(NJW算法)的基础上进行三支聚类算法构造。主要思想是针对每个对象进行加权运算，利用多路谱聚类算法的稳定性，重复进行聚类运算从而获得加权结果。根据其加权后对全体对象的扰动影响，将其划入核心域或边界域。对S_Dbw(c)指标进行改造使其可以表示核心域与边界域的离散程度。最后分别在人工与UCI数据集上对算法进行测试，有较好效果。
    We structure a three-way Clustering Algorithm Based on NJW Algorithm. The main idea is that each object, making weighted arithmetic, repeats clustering arithmetic to obtain the weighted results, which benefits from the stability of NJW Algorithm. According to the perturbation for all objects, we can classify the object into core or fringe regions. The compact degree of core and fringe regions can be described by the modified S_Dbw(c) index. Testing the algorithm on the artificial and UCI data sets, the results meet expectation. 
  
 
</p></abstract><kwd-group><kwd>三支聚类，扰动影响，谱聚类, Three-Way Clustering</kwd><kwd> Disturbance Degree</kwd><kwd> NJW Algorithm</kwd></kwd-group></article-meta></front><body>
  <sec id="s1"><title>一种基于扰动分析的三支谱聚类算法<sup> </sup></title><p>王晓磊，王平心<sup>*</sup></p><p>江苏科技大学理学院，江苏 镇江</p><disp-formula id="hanspub.22809-formula54"><graphic xlink:href="//html.hanspub.org/file/9-2620515x5_hanspub.png"  xlink:type="simple"/></disp-formula><p>收稿日期：2017年11月8日；录用日期：2017年11月21日；发布日期：2017年11月27日</p><disp-formula id="hanspub.22809-formula55"><graphic xlink:href="//html.hanspub.org/file/9-2620515x6_hanspub.png"  xlink:type="simple"/></disp-formula>
  </sec>
<sec id="s2"><title>摘 要</title><p>本文在多路谱聚类算法(NJW算法)的基础上进行三支聚类算法构造。主要思想是针对每个对象进行加权运算，利用多路谱聚类算法的稳定性，重复进行聚类运算从而获得加权结果。根据其加权后对全体对象的扰动影响，将其划入核心域或边界域。对S_Dbw(c)指标进行改造使其可以表示核心域与边界域的离散程度。最后分别在人工与UCI数据集上对算法进行测试，有较好效果。</p><p>关键词 :三支聚类，扰动影响，谱聚类</p><disp-formula id="hanspub.22809-formula56"><graphic xlink:href="//html.hanspub.org/file/9-2620515x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2017 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/9-2620515x8_hanspub.png" /> <img src="//html.hanspub.org/file/9-2620515x9_hanspub.png" /></p>
</sec>
<sec id="s3"><title>1. 引言</title><p>聚类 [<xref ref-type="bibr" rid="hanspub.22809-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.22809-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.22809-ref3">3</xref>] 可以看作一种分类，是将具有一定相似度的对象按照其自身的特性进行分类，是当代分类学与多元分析的结合，并已用于多个领域 [<xref ref-type="bibr" rid="hanspub.22809-ref4">4</xref>] 。由于聚类所生成的簇是一组数据对象的集合，这些对象与同一个簇中的对象彼此相似，与其他簇中的对象相异。在聚类的研究和应用中存在两个主要问题 [<xref ref-type="bibr" rid="hanspub.22809-ref5">5</xref>] 。首先，如何有效的划分一个数据集。其次，如何确定划分出的类簇个数。使用不同的算法思想对同一个数据集进行操作往往会产生不同的聚类结果。产生分歧的原因往往是一些元素特征并不明显或过于明显，使得在分类过程中既可以被分入这个类也可以被分入那个类。因此会降低聚类的效果。</p><p>聚类结果的效果如何，取决于数据集中各个元素能否依照算法准确地被划分入相应类簇。无法被准确分类的元素除特征明显，与其余点明显相异的噪点(孤立点)，还有介于两类簇之间，既可以被划入此类簇也可以被划入彼类簇的边界点。因此引入三支聚类的概念，依照核心域，边界域，琐碎域对数据集进行划分，解决了由于特征不清导致分类不准的问题。</p>
</sec>
<sec id="s4"><title>2. 相关工作与概念</title><p>目前，大多二支聚类结果都是一种硬划分，即对某一个元素都有唯一确定的类簇与之匹配。而在某些数据集中，由于数据本身特征存在模糊与相异度较低的情况，无法对其进行准确的划分，此时被强制分配的元素会大大降低二支聚类结果的精度，破坏其结构。例如图1所示，位于中间的元素在两类簇的边界处出现交叉，其依照算法被划分为两类，但是从观察中可得这些元素并未明显差异。</p><p>以下介绍三支聚类的有关概念：</p><p>设给定一个数据集 U = { x 1 , x 2 , ⋯ , x n } ，依照二支聚类的定义，由一个集合代表一个类簇，即寻找一组集合 C 1 , C 2 , ⋯ , C k 满足 U = ∪ i = 1 k C i ，且满足 C i ∩ C j = ∅ ， ( i , j = 1 , 2 , ⋯ , k , i ≠ j ) ，其中k为聚类的个数。</p><p>图1. 数据集示意图</p><p>在三支聚类中，使用三个互不相交的集合表示一个类，即 C i P ， C i B 与 C i N ，分别称为类的核心域，边界域和琐碎域，其中</p><p>C i P ∪ C i B ∪ C i N = U (1)</p><p>核心域中的元素确定属于这个类，边界域中的元素可能属于也可能不属于这个类，而琐碎域中的元素则肯定不属于这个类。由关系式(1)，我们可以通过 C i P 和 C i B 来表示一个类。反过来，如果给定一组集合 C i P ， C i B ， i = 1 , 2 , ⋯ , k 满足</p><p>C i p ≠ ∅ , i = 1 , 2 , ⋯ , k (2)</p><p>∪ i = 1 k ( C i p ∪ C i B ) = U (3)</p><p>我们称其为数据集U三支聚类。其中(2)要求正域非空，即每个类中至少有一个对象，而(3)保证每个对象至少被分到一个类中。和传统的硬聚类的结果 C = { C 1 , C 2 , ⋯ , C k } 不同，三支聚类的结果应为如下形式，</p><p>T C = { ( C 1 P , C 1 B ) , ⋯ , ( C k P , C k B ) }</p><p>显然在三支聚类中如果有 C i B = ∅ ， ( i = 1 , 2 , ⋯ , k ) ，就变成了 传统二支决策的聚类形式。因此，三支决策聚类形式是传统二支聚类方法的推广，使用边界域去解决针对目前知识体系下难以聚类的对象，或是特征不明显难以准确归类的元素，即为三支聚类的主要思想。</p>
</sec>
<sec id="s5"><title>3. 基于扰动的三支聚类算法</title><p>本算法的主要思想是对每个对象进行加权处理，之后依照前后对比进行分类。首先使用NJW算法对数据集进行一次划分，之后在此次划分基础上对选定对象加权重新聚类，比较两次聚类的扰动情况。从图1中可以看出属于边界域的点往往会呈现一种边缘性，正是因为这种边缘性使得划分结果的不准确。边缘性反映在数据上即为元素距类簇中心的距离，但是单独判断显然不是一个可行方法。首先。作为一种无监督学习，在分析之前不知道判断距离的标准。其次，单纯的判断距离无法识别出所判断点所处区域的数据稠密情况，疏密情况对于域的划分有着显著的影响，最后，距离判断误差极大，一次性会将某一范围外的所有点归为边界域，失去参考价值。所以本文采用扰动的方法，设立标准扰动量，计算每个点加权后对全局的扰动，与标准扰动量进行比较判断所属域。</p><p>参数m为对某点添加新点的个数，通常为聚类总对象数量的特定倍数。通常这个倍数不易过大，一旦过大，新的类簇中心必然出现在选定点上，此时就转换为单纯的比较距离。失去了扰动算法的本质意义。</p><p>标准扰动距离并非一成不变，而是针对不同的数据集特意选定。并且标准扰动距离与参数m呈正相关(当添加的点愈多，偏移愈大)。两者需要考量数据集的形状，类型，才能较好的选定。这里给出一种通常的选定方式，通常把数据集分位数进行加权处理，以此作为标准扰动距离。</p><p>算法1： 基于全局扰动的三支谱聚类算法</p><p>Step1 通过NJW算法对数据集U进行一次聚类操作，得到 C = { C 1 , C 2 , ⋯ , C k } ；</p><p>Step 2 在数据集中任找一个尚未被判断的点q，在原本数据集中添加m个数据q得到新数据集 U 1 ，对 U 1 应用NJW算法重新聚类，得到 C * = { C 1 * , C 2 * , C 3 * , ⋯ } 。</p><p>Step3在 C * 中找到q所对应的聚类 C i * ，比较两聚类中心的欧氏距离，当其距离差值大于标准扰动聚类w时，将q划分至边界域，反之，将q划分至核心域。</p><p>Step4 若q以外所有点都已被判断则转入Step5，否则转入Step2。</p><p>Step5 输出核心域与边界域。</p><p>由于算法本身的不足，造成此算法的时间复杂度极大。这是由于在聚类过程中对于每个点都要进行一次聚类操作，并且每此参与聚类的数据量还大于原数据集。为了解决上述问题，以下考虑一种基于局部的聚类算法。仅在初始时进行一次聚类运算，之后在判断聚类中心时仅仅在一个类簇内进行考虑而非对全部数据进行操作。以此大大简化了运算量，当然精度不可避免的造成一定损失。但是经实验检测，此算法依然有较好的实验效果。</p><p>算法2：基于局部扰动的三支谱聚类算法</p><p>Step1 通过NJW算法对数据集U进行一次聚类操作，得到 C = { C 1 , C 2 , ⋯ , C k } ；</p><p>Step 2 在数据集中任找一个尚未被判断的点q， q ∈ C i ，在原本数据集U中添加m个数据得到新数据集 U 1 ，找到q原本所在类簇 C i * 。记 添 加 的 对 象 数 数 据 集 总 对 象 数 = 扰 动 系 数 。</p><p>Step3计算 C i * 的聚类中心，将其与原本C的中心进行比较，若欧式距离差大于标准扰动距离与扰动系数的乘积则将其划分至边界域，反之，将其划分至核心域。</p><p>Step4 若q以外所有点都已被判断则转入Step5，否则转入Step2</p><p>Step5 输出核心域与边界域。</p><p>基于局部的算法中，将原本需要重新聚类获得的聚类中心转变为直接在原本类簇中增加相应的点进行求中心处理。由于不需要考虑全局，可以取较小值，将添加的类簇数据量倍数记为扰动系数。这是因为单个聚类本身具有极好的扰动特性(完全的物理特性)。此时标准扰动距离与扰动系数的乘积在实验中有良好的表现。</p>
</sec><sec id="s6"><title>4. 实验结果</title></sec>
<sec id="s6_1"><title>4.1. 人工数据集</title><p>本节采用一组人工数据集U，U为从均值为 μ ，协方差为 σ 的正态分布中抽取 n * d 的矩阵r(n为样本数，d为样本维数，本实验d为2)。</p><p>其中， μ = [ a 1 a 2 ⋮ ⋮ ⋮ ⋮ a 7 a 8 ] ，R中的每一行以中对应的行为均值的正态分布中抽取的一个样本。同时</p><p>a i = ( ( r a n d ( 0 , 1 ) − 1 ) *11 ) +1, i = 1 , ⋯ , 8 ， r a n d ( 0 , 1 ) 表示在0~1中取随机数。</p><p>σ = [ b 1 b 2 ⋮ ⋮ ⋮ ⋮ b 7 b 8 ] ，对于 b j 有 b j = ( ( r a n d ( 0 , 1 ) *1 ) / 4 ) + 2 , j = 1 , ⋯ , 8 。</p><p>此为基于局部的NJW算法的结果，使用局部处理的结果在人工数据集上(图2)与全局处理并无明显差异。但是在局部算法中并不能彻底发挥NJW算法的稳定性优势。</p>
</sec>
<sec id="s6_2"><title>4.2. 评价标准</title><p>由于三支聚类的特殊性，需要将原本二支聚类评价指标进行修改，使其可以对三支聚类的结果进行正确评价，本指标利用S_Dbw(c) [<xref ref-type="bibr" rid="hanspub.22809-ref6">6</xref>] 指标的衡量方法，分别对核心域，边界域进行评价，将两者结果与原本的离散程度进行对比。</p><p>图2. 包含400个数据的人工数据集</p><p>S c a t ( c ) = 1 c ∑ i = 1 c ‖ σ ( v i ) ‖ ‖ σ ( U ) ‖ (4)</p><p>其中c为二支聚类结果中的类簇个数，S为数据集。 σ ( U ) 的方差，其p阶距定义如下：</p><p>σ x P = 1 n ∑ k = 1 n ( X k P − X &#175; P ) 2 (5)</p><p>X &#175; = 1 n ∑ k = 1 n X k , ∀ X k ∈ U (6)</p><p>σ ( v i ) 是类簇 C i 的方差：</p><p>σ v i P = 1 n i ∑ k = 1 n i ( X k P − v i P ) 2 (7)</p><p>对 ‖ x ‖ 的定义为：如果x是一个向量，则</p><p>‖ x ‖ = x * x ′ (8)</p><p>记 P = { c 1 P , c 2 P , ⋯ } ， B = { c 1 B , c 2 B , ⋯ } ， C = { c 1 , c 2 , ⋯ } ，其中P为每个已划分类簇核心域构成的集合，B为每个已划分类簇边界域构成的集合，C为原二支聚类的结果。根据Scat(c)指标，从理论上可以得出</p><p>S c a t ( c P ) = 1 c P ∑ i = 1 c P ‖ σ ( v i ) ‖ ‖ σ ( P ) ‖ ,</p><p>S c a t ( c ) = 1 c ∑ i = 1 c ‖ σ ( v i ) ‖ ‖ σ ( U ) ‖ ,</p><p>S c a t ( c B ) = 1 c B ∑ i = 1 c B ‖ σ ( v i ) ‖ ‖ σ ( B ) ‖ ,</p><p>S c a t ( c P ) &lt; S c a t ( c ) &lt; S c a t ( c B ) . (9)</p>
<table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> UCI data sets used in experiment</title></caption>
</table-wrap>
</sec>
</body>
<back><ref-list><title>References</title><ref id="hanspub.22809-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Jain, A.K., Murty, M.N. and Flynn, P.J. (1999) Data Clustering: A Review. ACM Computing Survey, 31, 264-323.  
&lt;br&gt;https://doi.org/10.1145/331499.331504</mixed-citation></ref><ref id="hanspub.22809-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Anderberg, M.R. (1973) Cluster Analysis for Application. Academic Press, New York.</mixed-citation></ref><ref id="hanspub.22809-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Omran, M.G.H., Engelbrecht, A.P. and Salman, A. (2007) An Overview of Clustering Methods. Intelligent Data Analysis, 11, 583-605.</mixed-citation></ref><ref id="hanspub.22809-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Xu, R. and Wunsch, D. (2005) Survey of Clustering Algorithms. IEEE Transactions on Neural Networks, 16, 645- 678. &lt;br&gt;https://doi.org/10.1109/TNN.2005.845141</mixed-citation></ref><ref id="hanspub.22809-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">孙吉贵, 刘杰, 赵连宇. 聚类算法研究[J]. 软件学报, 2008, 19(1): 48-61.</mixed-citation></ref><ref id="hanspub.22809-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Halkidi, M. and Vazirgiannis, M. (2002) Clustering Validity Assessment: Finding the Optimal Partitioning of a Data Set. Proceedings IEEE International Conference on Data Mining, 29 November-2 December 2001. 
http://ieeexplore.ieee.org/abstract/document/989517/</mixed-citation></ref></ref-list></back></article>