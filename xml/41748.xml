<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114099</article-id><article-id pub-id-type="publisher-id">CSA-41748</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_95897149.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于轻量级神经网络的食材识别方法研究
  Research on Food Ingredients Identification Method Based on Lightweight Neural Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>黄</surname><given-names>颖康</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>曾</surname><given-names>碧</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>962</fpage><lpage>974</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   随着计算机视觉技术的快速发展，基于深度学习的目标检测技术已广泛应用于诸多领域。由于目前目标检测算法模型复杂，计算量大，无法应用于嵌入式设备中，为了满足在嵌入式设备中使用食材识别功能的需求，提出了对目标识别模型YOLOv3的改进方法，将轻量化神经网络MobileNet应用于YOLOv3中，把YOLOv3的主干网络darknet53替换为MobileNet；然后采用Cluster-NMS算法，配合中心距离法和加权平均法提升网络的准确度。通过收集得来的食材数据集和VOC 2007数据集对网络进行对比实验。实验表明，改进后的网络模型，既能满足其迁移到嵌入式设备的轻量级需求，而且无论在识别速度和精度上都有提升，满足在嵌入式设备实现食材识别的功能。 With the rapid development of computer vision technology, object detection technology based on deep learning has been widely used in many fields. Due to the complexity of the current target detection algorithm model, the amount of calculation is large, model can not be applied to embedded devices. In order to meet the needs of using food ingredients identification function in embedded devices, an improved method of object recognition model YOLOv3 is proposed. The lightweight neural network MobileNet is applied in YOLOv3, and the main network darknet53 of YOLOv3 is replaced by MobileNet. And then Cluster Non-Maximum Suppression (Cluster-NMS), Distance NMS and Weighted NMS are used to improve the accuracy of the neural network. Through comparative experiments by testing neural network with food ingredients data set and VOC 2007 data set, the improved network model meets the needs of network migration to embedded devices, improves the accuracy of the neural network and the ability of food ingredients recognition, and realizes the function of food identification in embedded devices. 
  
 
</p></abstract><kwd-group><kwd>深度学习，轻量化，非极大抑制，食材识别，目标检测, Deep Learning</kwd><kwd> Lightweight</kwd><kwd> NMS</kwd><kwd> Food Ingredients Identification</kwd><kwd> Object Detection</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>随着计算机视觉技术的快速发展，基于深度学习的目标检测技术已广泛应用于诸多领域。由于目前目标检测算法模型复杂，计算量大，无法应用于嵌入式设备中，为了满足在嵌入式设备中使用食材识别功能的需求，提出了对目标识别模型YOLOv3的改进方法，将轻量化神经网络MobileNet应用于YOLOv3中，把YOLOv3的主干网络darknet53替换为MobileNet；然后采用Cluster-NMS算法，配合中心距离法和加权平均法提升网络的准确度。通过收集得来的食材数据集和VOC 2007数据集对网络进行对比实验。实验表明，改进后的网络模型，既能满足其迁移到嵌入式设备的轻量级需求，而且无论在识别速度和精度上都有提升，满足在嵌入式设备实现食材识别的功能。</p></sec><sec id="s2"><title>关键词</title><p>深度学习，轻量化，非极大抑制，食材识别，目标检测</p></sec><sec id="s3"><title>Research on Food Ingredients Identification Method Based on Lightweight Neural Network</title><p>Yingkang Huang, Bi Zeng</p><p>Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/20-1542113x4_hanspub.png" /></p><p>Received: Mar. 21<sup>st</sup>, 2021; accepted: Apr. 15<sup>th</sup>, 2021; published: Apr. 22<sup>nd</sup>, 2021</p><p><img src="//html.hanspub.org/file/20-1542113x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>With the rapid development of computer vision technology, object detection technology based on deep learning has been widely used in many fields. Due to the complexity of the current target detection algorithm model, the amount of calculation is large, model can not be applied to embedded devices. In order to meet the needs of using food ingredients identification function in embedded devices, an improved method of object recognition model YOLOv3 is proposed. The lightweight neural network MobileNet is applied in YOLOv3, and the main network darknet53 of YOLOv3 is replaced by MobileNet. And then Cluster Non-Maximum Suppression (Cluster-NMS), Distance NMS and Weighted NMS are used to improve the accuracy of the neural network. Through comparative experiments by testing neural network with food ingredients data set and VOC 2007 data set, the improved network model meets the needs of network migration to embedded devices, improves the accuracy of the neural network and the ability of food ingredients recognition, and realizes the function of food identification in embedded devices.</p><p>Keywords:Deep Learning, Lightweight, NMS, Food Ingredients Identification, Object Detection</p><disp-formula id="hanspub.41748-formula17"><graphic xlink:href="//html.hanspub.org/file/20-1542113x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/20-1542113x7_hanspub.png" /> <img src="//html.hanspub.org/file/20-1542113x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>随着社会的发展使得生活水平不断提高，人们对衣食住行的要求逐渐增高。其中吃是体现生活质量的一个重要方面。随着信息领域和物流领域的迅速发展，人们能够通过网络等渠道采购到各种丰富的食材。以往人们对食材种类的认知以及对应烹饪菜谱是依靠经验得来，如何使用嵌入式移动设备(比如：手机)自动识别采购得来的食材，配合移动端的菜谱应用软件，就能生成各种美味食品的烹饪方法，对满足人们对美食的追求具有现实意义。但这也对食材识别模型和识别速度提出了更高的要求。</p><p>近年来，随着人工智能技术的发展，深度学习得到了很大的关注。深度学习与传统手工设计目标特征的方法不同，深度学习以端到端的方式训练深度神经网络，实现自动提取目标的深层特征，避免人为设计的干扰，同时，深层特征与传统特征相比，深层特征可以多层次表示目标，从浅层的局部特征到深层的全局特征，具有更强的鲁棒性和表达能力。随着深度神经网络发展，出现了许多经典的用于目标识别的神经网络模型，包括R-CNN [<xref ref-type="bibr" rid="hanspub.41748-ref1">1</xref>]、Fast RCNN [<xref ref-type="bibr" rid="hanspub.41748-ref2">2</xref>]、SSD [<xref ref-type="bibr" rid="hanspub.41748-ref3">3</xref>]、YOLO系列 [<xref ref-type="bibr" rid="hanspub.41748-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref6">6</xref>] 等，但是为了让模型达到更高的精度，大多数神经网络模型制造得更深，复杂度更高高，难以应用在真实场景中，所以必须要对模型进行轻量化处理，以达到移植到各种嵌入式设备的要求 [<xref ref-type="bibr" rid="hanspub.41748-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref10">10</xref>]。</p><p>为了提高嵌入式设备对食材图片识别的效率和能力，同时需要满足存储空间和功耗的限制，设计适用于嵌入式设备的轻量化深度神经网络架构是解决该问题的关键。在最近的研究中，人们对建立轻量级和高精度的神经网络越来越感兴趣 [<xref ref-type="bibr" rid="hanspub.41748-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref12">12</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref14">14</xref>]，因此，轻量级神经网络架构的设计得到了学术界和工业界的广泛关注，也提出了一些典型的方法 [<xref ref-type="bibr" rid="hanspub.41748-ref15">15</xref>] [<xref ref-type="bibr" rid="hanspub.41748-ref16">16</xref>]，主要包括三个方向，分别是：(1) 人工设计轻量化神经网络模型；(2) 基于神经网络架构搜索(Neural Architecture Search，NAS)的自动化神经网络架构设计；(3) 神经网络模型的压缩。</p><p>对比目标检测的其他神经网络模型，YOLOv3在速度与效果方面表现不错，因此针对食材识别任务，本文提出一种基于聚类加权中心非极大值抑制的轻量级YOLOv3方法(Cluster-Weighted-Distance NMSlightweight YOLOv3, CWDNMS-lightweightYOLOv3)。采用YOLOv3作为主要的网络结构，使用轻量化神经网络MobileNet [<xref ref-type="bibr" rid="hanspub.41748-ref17">17</xref>] 对YOLOv3进行轻量化处理，能够有效减少整个网络的参数量，提升整个模型的运行效率，使模型能够迁移到移动端或其他嵌入式设备中。除此之外为了进一步提升模型的精度和效率，对传统有缺陷的非极大值抑制(Non-Maximum Suppression, NMS)算法作出改进，使用交并比(Intersection-over-Union, IoU)矩阵运行方式，并且加入了中心距离惩罚项改变了仅使用IoU作为判断检测框是否抑制的依据，采用加权平均法得到了更准确的最高得分检测框，进一步增强食材识别的效果。</p></sec><sec id="s6"><title>2. 轻量级神经网络的构建</title><sec id="s6_1"><title>2.1. YOLOv3</title><p>YOLOv3网络结构图如图1所示，把图片输进YOLOv3，网络会对图片分别进行32倍降采样、16倍降采样、8倍降采样，输出三种尺寸的特征图。但是在进行16倍降采样检测时会先对32倍降采样的特征图进行上采样后再与16倍降采样的特征图结合输出最终的16倍降采样特征图，8倍降采样的操作与16倍降采样的操作相同，这样既可以提高非线性处理能力，增加泛化性能以提高网络精度，又能减少参数提高实时性。例如输入416 * 416的图片，会输出13 * 13、26 * 26、52 * 52的特征图。YOLOv3的降采样操作不使用池化层，而是将卷积步骤中的步长设置为2以达到降采样的目的，这样能够降低池化操作所带来的梯度负面效果。</p><p>图片经过32倍降采样后，特征图的感受野最大，适合检测较大的目标；图片经过16倍降采样后，特征图的感受野适中，适合检测一般大小的目标；图片经过8倍降采样后，特征图的感受野最小，适合检测较小的目标。YOLOv3的这种多尺度预测的特点，能够适应各种大小的识别目标。</p><p>图1. YOLOv3网络结构图</p><p>YOLOv3最后输出的特征图里，每一个网络单元预测3个不同尺寸的检测框，检测框的尺寸通过对输入图片里的目标进行聚类得到，因为YOLOv3输出3种征图，每种特征图有三种尺寸的检测框，所以YOLOv3一共有9种尺寸的检测框检测不同大小的检测目标，能够提升网络的泛化能力。最终特征图里，每一个网络单元含有检测框的坐标信息，检测框的高度和宽度以及目标预测的置信度。</p><p>YOLOv3的主干网络是darknet53，上图左边部分。darknet53网络有52层卷积操作，采用3 * 3大小的卷积核，分别在第2、5、10、27、44层结构处进行降采样的操作。</p></sec><sec id="s6_2"><title>2.2. MobileNet</title><p>MobileNet是一种轻量及高效的神经网络，相比于darknet53，它的网络模型较小，准确度高以及网络结构图有一定的相似之处。</p><p>MobileNet使用深度可分离卷积代替传统卷积，深度可分离卷积区别于传统卷积，在于卷积的方式和构成不一样。深度可分离卷积操作时一个通道的卷积核对一个通道的特征图进行卷积后，直接作为该通道的输出特征图，而传统卷积操作时是对每一通道特征图卷积的结果进行叠加后作为该通道的输出特征图。</p><p>YOLOv3的基本单位由3 * 3普通卷积层，批量归一化(Batch Normalization, BN)层和Relu激活函数层构成，如图2所示。其中，模型通过3 * 3普通卷积层对输入图片进行特征提取；BN层通过对数据进行归一化的操作，不仅提升模型训练速度，加快收敛过程，还能防止模型训练时容易出现的过拟合现象。Relu激活函数是非线性函数，引入模型能够增加模型的非线性因素，增加神经网络各层之间的非线性关系，增强模型的表达能力；Relu激活函数相对于其他激活函数的优势是：计算量少；不容易出现梯度消失的问题；缓解过拟合问题的发生。</p><p>MobileNet的基本单位是深度可分离卷积 [<xref ref-type="bibr" rid="hanspub.41748-ref18">18</xref>]，如图2所示，深度可分离卷积由3 * 3和1 * 1卷积层，BN层和Relu激活函数层构成，通过3 * 3深度卷积提取特征图的特征，通过1 * 1卷积操作控制输出特征图的通道数。</p><p>图2 深度可分离卷积与普通卷积比较图</p><p>假设输入和输出的特征图大小为 D F ，其中输入特征图通道数为E，输出特征图通道数为N，卷积核大小为 D K ，则深度可分离卷积和普通卷积的计算量之比公式为：</p><p>D K &#215; D K &#215; E &#215; D F &#215; D F + E &#215; N &#215; D F &#215; D F D K &#215; D K &#215; E &#215; N &#215; D F &#215; D F = 1 N + 1 D K 2 (1)</p><p>通道数N一般较大，卷积核大小一般为3*3，所以深度可分离卷积的计算量大约只有普通卷积的计算量的九分之一 [<xref ref-type="bibr" rid="hanspub.41748-ref19">19</xref>]。</p><p>MobileNet网络结构图如图3所示，整个网络有27层卷积操作，并且在1，4，8，12，24层进行降采样操作。整个网络与darknet53网络相似，但是MobileNet除了第一层卷积，其余卷积使用了深度可分离卷积操作，减少了网络计算量，使MobileNet移植到YOLOv3提供了可能性。</p><p>图3. MobileNet网络结构图</p><p>把MobileNet代替darknet53移植到YOLOv3网络中，得到YOLOv3-MobileNet网络结构，如图4所示。</p><p>图4中CBR为普通卷积，如图2左侧部分所示，DBR为深度可分离卷积，如图2右侧部分所示，Conv为普通卷积操作，不包括BN算法和激活函数。</p><p>从图4中可得在MobileNet部分第6层后输出8倍降采样的特征图，第12层后输出16倍降采样的特征图，最后输出32倍降采样的特征图。各个倍数的特征图经过卷积或上采样等操作后，输出最终的特征图。</p><p>YOLOv3-MobileNet网络结构通过将MobileNet代替darknet53移植到YOLOv3的主干网络中减少整个网络的冗余程度，参数量从61 M降到了24 M。YOLOv3-MobileNet整体结构与YOLOv3相似，并且在降低网络复杂度的同时，保留了许多YOLOv3的特性，例如多尺度预测等。</p></sec></sec><sec id="s7"><title>3. NMS算法设计</title><p>NMS，是目标检测的重要部分，大部分神经网络，包括YOLOv3，在最后输出特征图中保存着大量的信息，其中含有大量的检测框信息，每个检测框有各自的置信度，用来表达检测框属于某个种类的可信程度。大量的检测框互相重叠，需要NMS算法抑制大量的检测框，确保检测框的正确预测某个种类。NMS的作用就是选取置信度高的检测框，抑制重叠度高的检测框。</p><p>图4. YOLOv3-MobileNet网络结构图</p><p>其算法流程如下:</p><p>1) 对检测框按置信度从高到低进行排序，得到集合M；</p><p>2) 选取集合M中置信度最高的检测框b，遍历其余检测框；使与b的IoU大于阈值的其余检测框置信度设为0，因为它们判断为属于同一种类；</p><p>3) 从集合M中剔除b重复步骤2和3直到遍历完集合M。</p><p>这种算法的缺点是：IoU阈值设置高了，会导致抑制错误检测框的效果较差，设置低了，容易把重叠的不同种类的检测框给抑制了。</p><sec id="s7_1"><title>3.1. Cluster-NMS</title><p>NMS算法在计算IoU方面的公式为：</p><p>( n − 1 ) + ( n − 2 ) + ⋯ + 1 = 1 2 n 2 − 2 n (2)</p><p>想要加速NMS，就要将IoU计算并行化。</p><p>IoU矩阵为：</p><p>X = IoU ( B , B ) = [ x 11 x 12 ⋯ x 1 n x 21 x 22 ⋯ x 2 n ⋮ ⋮ ⋱ ⋮ x n 1 x n 2 ⋯ x n n ] , x i j = IoU ( B i , B j ) (3)</p><p>式(3)中， X 为IoU矩阵，B为检测框集合，其中 B = { B i } i = 1   to   n ，集合B按照置信度降序排列，也就是 B 1 是最高得分框， B n 为最低得分框。</p><p>因为 IoU ( B i , B j ) = IoU ( B j , B i ) ,所以 X 为对称矩阵，且 B i i 没有意义，所以 X 可以简化为：</p><p>X = [ 0 x 12 x 13 ⋯ x 1 n 0 0 x 23 ⋯ x 2 n 0 0 0 ⋯ x 3 n ⋮ ⋮ ⋮ ⋱ ⋮ 0 0 0 ⋯ 0 ] (4)</p><p>对 X 执行按列取最大值操作，得到一维张量 b = [ b 1 , b 2 , ⋯ , b n ] ，对 b 中元素进行二值化，大于IoU阈值的元素取0，小于IoU阈值的元素取1。最后张量 b 中 b i 为0代表第i个检测框抑制，为1代表第i个检测框保留。</p><p>但是这种方法会导致检测框被过度抑制 [<xref ref-type="bibr" rid="hanspub.41748-ref20">20</xref>]。例如，矩阵 X 第二行 B 2 是与比它低分的检测框之间的IoU数值，若 b 2 为0，代表第二高分的检测框被抑制了，也就是说矩阵 X 第二行应该为0，但矩阵 X 第二行依然有数值并且该数值有可能是某列的最大值，有可能在二值化时被设为0，造成该列的检测框被抑制。</p><p>为了解决这种缺点采取Cluster-NMS算法：</p><p>1) 使矩阵 C 1 = E &#215; X ，第一次迭代用单位矩阵 E 左乘矩阵 X 得到矩阵 C 1 ，按照前面方法得到 C 1 的一维张量 b 1 ；</p><p>2) 把得到的一维张量 b 1 展开成对角矩阵 A 1 ，也就是使 A 1 的对角线元素与一维张量 b 1 的元素相同，其余元素为0，例如 b 1 = [ 1 0 1 ⋯ 1 ] ，得到以下矩阵：</p><p>A 1 = [ 1 0 0 ⋯ 0 0 0 0 ⋯ 0 0 0 1 ⋯ 0 ⋮ ⋮ ⋮ ⋱ ⋮ 0 0 0 ⋯ 1 ] (5)</p><p>3) 使 C 2 = A 1 &#215; X ，得到 C 2 的一维张量 b 2 和 A 2 ，重复步骤(2)(3)直到 b n − 1 = b n 。</p><p>如果对角矩阵 A 中某行为0，则与矩阵 X 相乘后该行元素全为0，代表该行被抑制了。Cluster-NMS算法最后的结果和传统NMS算法的结果一样，但是速度比传统的NMS快 [<xref ref-type="bibr" rid="hanspub.41748-ref21">21</xref>]。</p></sec><sec id="s7_2"><title>3.2. 中心距离惩罚法(DIoU-NMS)</title><p>中心距离惩罚法的核心思想是相邻检测框的中心约接近得分最大的检测框，越有可能是冗余的检测框 [<xref ref-type="bibr" rid="hanspub.41748-ref22">22</xref>]。检测框示意图如图5所示。</p><p>图5. 两个相邻检测框示意图</p><p>DIoU = IoU − ( d 2 c 2 ) β (6)</p><p>式(6)中d为两检测框中心的距离，c为两检测框最远的距离，参数 β 控制惩罚项对IoU的影响程度。</p><p>用DIoU代替IoU作为NMS的评判准则，当 β → ∞ ，惩罚项趋向0，此时DIoU=IoU，DIoU-NMS与传统NMS效果相当；当 β → 0 ，大部分检测框都没有被抑制。</p><p>使用DIoU-NMS能有在一定程度缓解传统NMS算法中抑制遮蔽物体检测框的问题，并且DIoU最差的效果，即惩罚项趋向于0，效果也不会比传统NMS效果差，对NMS只有改进的效果。在阈值不变的情况下，会有更多的检测框保留下来，提升检测的召回率。</p></sec><sec id="s7_3"><title>3.3. 加权平均法(Weighted NMS)</title><p>传统NMS得到的置信度最高的检测框定位不一定是最精确的，也就是说冗余的检测框有可能比置信度最高的检测框定位更好，所以采取加权平均法是对坐标进行加权平均 [<xref ref-type="bibr" rid="hanspub.41748-ref23">23</xref>]，公式：</p><p>M = ∑ i w i B i ∑ i w i , B i ∈ { B | IoU ( M , B ) ≥ 阈 值 } ∪ { M } (7)</p><p>式(7)中权重 w i = s i IoU ( M , B i ) 。</p><p>通过Weighted NMS，对检测框进行加权平均，能够得到定位效果较准确的最高置信度检测框，能够较为稳定地提升检测的召回率和精度。</p><p>在Cluster-NMS的基础上使用中心距离惩罚法和加权平均法对NMS进行进一步的改进，结合以上三种NMS改进算法得到聚类加权中心非极大值抑制算法(Cluster-Weighted-Distance NMS, CWDNMS)，该算法可以使神经网络性能在效率和精度方面有所提升。</p></sec></sec><sec id="s8"><title>4. 实验结论与分析</title><sec id="s8_1"><title>4.1. 实验步骤与方法</title><p>实验的硬件平台是：</p><p>操作系统：Windows 10；</p><p>CPU：英特尔 i7-6700 3.4 GHz；</p><p>内存：16 GB；</p><p>显卡：英伟达 GTX 1070。</p><p>软件平台：Pycharm、Anaconda3。</p><p>实验步骤：收集食材数据集，对食材数据集进行筛选，对筛选后的食材数据集进行预处理，预处理包括划分食材数据集为训练集，验证集，测试集，对数据集进行标定。在软件平台上构建YOLOv3-MobileNet网络结构，编写Cluster-Weighted-Distance NMS算法程序。把训练集送进YOLOv3-MobileNet网络结构进行训练，得到相关的参数，把测试集送进训练后网络中，得到网络的精度和速度，评估网络的泛化能力。使用VOC 2007数据集代替食材数据集进行上述实验步骤，得出结果与食材数据集进行结果对比；把两种数据集分别送进YOLOv3和是否使用Cluster-Weighted-Distance NMS算法的网络中，进行实验结果的对比和分析。主要分析对YOLOv3进行轻量化操作后，模型的检测速度是否提升了，改进后的NMS算法，模型的检测精度是否提升了。</p><p>VOC 2007数据集是为图像识别和分类提供了一整套标准化的优秀的数据集。VOC 2007数据集共包含：训练集5011张图片，测试集4952张图片，共9963张图片，共包含20个种类，其中包括人类、单车、汽车等常见的种类。</p><p>自制食材数据集包括2300张图片，共包含10个种类，每个种类200到300张图片，种类分别是茄子、鸡蛋、肉、青菜、西红柿、西兰花、玉米、洋葱、萝卜、胡萝卜。标注工具为LabelImg，方式是人工标注，图片来源于实景拍摄。</p></sec><sec id="s8_2"><title>4.2. 实验结果比较</title><p>表1是以VOC2007作为数据集对各种网络评估的结果。可以从表中得知YOLOv3-MobileNet模型的速度为36 ms，YOLOv3模型的速度为61 ms，YOLOv3-MobileNet模型比YOLOv3模型快了近一倍。在精度方面，使用IoU为0.5作为判断基准，使用NMS的YOLOv3模型的准确度为65.84%，使用NMS的YOLOv3-MobileNet的模型准确度为62.66%，YOLOv3使用了MobileNet代替darknet53作为主干网络，在精度方面有略微的下降，但是在速度方面快了近一倍。模型采用CWD-NMS算法后，检测准确度提升3%左右。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison of VOC 2007 operation result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >参数量/M</th><th align="center" valign="middle" >速度/ms</th><th align="center" valign="middle" >mAP@IoU = 0.5</th></tr></thead><tr><td align="center" valign="middle" >YOLOv3 + NMS</td><td align="center" valign="middle" >61</td><td align="center" valign="middle" >61</td><td align="center" valign="middle" >65.84%</td></tr><tr><td align="center" valign="middle" >YOLOv3 + CWDNMS</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >68.12%</td></tr><tr><td align="center" valign="middle" >YOLOv3-MobileNet + NMS</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >36</td><td align="center" valign="middle" >62.66%</td></tr><tr><td align="center" valign="middle" >YOLOv3-MobileNet + CWDNMS</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >65.96%</td></tr></tbody></table></table-wrap><p>表1. VOC 2007运行结果比较</p><p>表2是自制的食材数据集对各种网络评估的结果。可以从表中得知YOLOv3-MobileNet模型的速度为28 ms，YOLOv3模型的速度为62 ms，YOLOv3-MobileNet模型比YOLOv3模型快了一倍多。在精度方面，使用IoU为0.5作为判断基准，使用NMS的YOLOv3模型的准确度为69.29%，使用NMS的YOLOv3-MobileNet的模型准确度为68.44%，YOLOv3使用了MobileNet代替darknet53作为主干网络，在精度方面有略微的下降，但是在速度方面快了一倍多。模型采用CWD-NMS算法后，检测准确度提升7%左右。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of VOC 2007 operation result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >参数量/M</th><th align="center" valign="middle" >速度/ms</th><th align="center" valign="middle" >mAP@IoU = 0.5</th></tr></thead><tr><td align="center" valign="middle" >YOLOv3 + NMS</td><td align="center" valign="middle" >61</td><td align="center" valign="middle" >61</td><td align="center" valign="middle" >69.29%</td></tr><tr><td align="center" valign="middle" >YOLOv3 + CWDNMS</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >76.05%</td></tr><tr><td align="center" valign="middle" >YOLOv3-MobileNet + NMS</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >28</td><td align="center" valign="middle" >68.44%</td></tr><tr><td align="center" valign="middle" >YOLOv3-MobileNet + CWDNMS</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >\</td><td align="center" valign="middle" >75.21%</td></tr></tbody></table></table-wrap><p>表2. VOC 2007运行结果比较</p><p>结合表和表中数据分析可得出结论是，YOLOv3-MobileNet对比与YOLOv3，使用MobileNet代替YOLOv3原有的darknet作为主干网络，参数减少了一大半，速度从61 ms左右上升到30 ms左右，快了一倍，精度只是略微下降；模型采用CWD-NMS算法后，精度提升了7%。使用MobileNet作为主干网络的YOLOv3模型达到轻量化的效果，精度只略微下降，采用CWD-NMS算法后，模型有不错的提升。</p></sec><sec id="s8_3"><title>4.3. 实验结果分析</title><p>从表2可知，使用自制食材数据集对YOLOv3-MobileNet模型进行训练，并使用CWD-NMS算法提升精度，模型的精度达到75.21%，使用VOC 2007数据集对YOLOv3-MobileNet模型进行训练，并使用CWD-NMS算法提升精度，得到的模型精度达到65.96%。自制食材数据集在该模型中表现比VOC 2007数据集好。</p><p>由图6可得在VOC 2007的全部种类中，火车种类准确度达到87%，盆栽种类准确度只达到30%；超一半种类的准确度达到60%以上，有五个种类准确度达到80%及以上。</p><p>图6. VOC 2007数据集的各种类准确度图</p><p>由图7可得食材数据集中洋葱和青菜的检测准确度很好，鸡蛋和玉米的检测准确度很低，尤其是玉米的准确度只有36%。</p><p>最终检测效果由图8所示，图片中含有青菜、鸡蛋、胡萝卜食材，全部品种都检测出来，青菜的检测置信度有100%，鸡蛋的检测置信度只有41%。</p><p>图7. 食材数据集的各种类准确度图</p><p>图8. 食材检测图片</p><p>通过分析准确率及食材数据集可得出以下特点及其有待改进的地方：</p><p>1) 食材数据集中的食材图片较为简单，背景较为单一，模型容易过拟合，泛化性不足，可能会在复杂背景的食材检测中表现不佳，其中有部分食材种类的检测准确度较低，应该收集更多复杂的食材数据，提升模型的泛化性。</p><p>2) 食材数据集种类只有10种，每一种类的数据数量只有200多张，样本数量较少，人工数据收集难度大，实用性不高，可以通过数据增强增加样本数量提升实用性。</p><p>YOLOv3-MobileNet模型比YOLOv3模型在运行速度上有很大的提升，并且使用CWD-NMS算法后模型的准确度有一定的提升；CWDNMS-lightweight YOLOv3模型在VOC 2007数据集的准确度表现不错，并且在食材数据集的准确度表现方面有一定的提升，主要提升空间在于食材数据集的收集以及制作方面，更好的数据集能够给模型带来更高的准确度。</p></sec></sec><sec id="s9"><title>5. 结论</title><p>本文介绍了一种基于轻量级神经网络的食材识别方法，通过比较各个神经网络的表现选择了YOLOv3作为特征提取网络，然后为了满足移植到嵌入式设备的需求，对YOLOv3结构进行轻量化处理，使用轻量化神经网络MobileNet代替原有的darknet53，构建YOLOv3-MobileNet神经网络模型，使模型达到轻量化的目的并且在准确度方面保持优势，最后使用CWD-NMS算法提升模型的准确度，最终的实验结果证明YOLOv3-MobileNet比YOLOv3在不明显降低准确度的前提下快了一倍，使用CWD-NMS算法能够提升模型7%的准确度，并且因为食材的数据集收集及处理方面相对于VOC2007表现更好，所以模型在食材识别方面有一定的准确度提升空间。CWDNMS-lightweight YOLOv3模型已经达到移植其他嵌入式设备对需求，能够应用在嵌入式的菜谱应用程序中，为菜谱应用程序提供食材识别的应用场景。</p></sec><sec id="s10"><title>基金项目</title><p>国家自然科学基金项目(项目编号：61702110)；广东省重大科技专项项目(项目编号：2016B010108004)。</p></sec><sec id="s11"><title>文章引用</title><p>黄颖康,曾 碧. 基于轻量级神经网络的食材识别方法研究Research on Food Ingredients Identification Method Based on Lightweight Neural Network[J]. 计算机科学与应用, 2021, 11(04): 962-974. https://doi.org/10.12677/CSA.2021.114099</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41748-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Girshick, R., Donahue, J., Darrell, T. and Malik, J. (2013) Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, Columbus, 23-28 June 2014, 580-587. &lt;br&gt;https://doi.org/10.1109/CVPR.2014.81</mixed-citation></ref><ref id="hanspub.41748-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Girshick, R. (2015) Fast R-CNN. 2015 IEEE International Conference on Computer Vision, Santiago, 7-13 December 2015, 1440-1448. &lt;br&gt;https://doi.org/10.1109/ICCV.2015.169</mixed-citation></ref><ref id="hanspub.41748-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., et al. (2016) SSD：Single Shot MultiBox Detector. Proceedings of European Conference on Computer Vision, Amster-dam, 8-16 October 2016, 21-37.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46448-0_2</mixed-citation></ref><ref id="hanspub.41748-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J., Divvala, S., Girshick, R. and Farhadi, A. (2016) You Only Look Once: Unified, Real-Time Object Detectiono. 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 779-788.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.91</mixed-citation></ref><ref id="hanspub.41748-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J. and Farhadi, A. (2017) YOLO9000: Better, Faster, Stronger. Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 6517-6525.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.690</mixed-citation></ref><ref id="hanspub.41748-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J. and Farhadi, A. (2018) YOLOv3: An Incremental Im-provement. arXiv: 1804.02767.</mixed-citation></ref><ref id="hanspub.41748-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S. and Sun, J. (2016) Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.41748-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan, K. and Zisserman, A. (2014) Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv: 1409.1556.</mixed-citation></ref><ref id="hanspub.41748-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Ioffe, S., Vanhoucke, V. and Alemi, A.A. (2017) Inception-V4, Inception-Resnet and the Impact of Residual Connections on Learning. Proceedings of the 31th AAAI Conference on Artificial Intelligence, San Francisco, 4-9 February 2017, 4278–4284.</mixed-citation></ref><ref id="hanspub.41748-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. and Wojna, Z. (2016) Rethinking the Inception Architecture for Computer Vision. Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 2818-2826. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.308</mixed-citation></ref><ref id="hanspub.41748-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Wang, M., Liu, B. and Foroosh, H. (2017) Factorized Convolutional Neural Networks. Proceedings of the 2017 IEEE International Conference on Computer Vision Work-shops, Venice, 22-29 October 2017, 545-553.  
&lt;br&gt;https://doi.org/10.1109/ICCVW.2017.71</mixed-citation></ref><ref id="hanspub.41748-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J. and Keutzer, K. (2016) SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and&lt; 0.5 MB Model Size. arXiv: 1602.07360.</mixed-citation></ref><ref id="hanspub.41748-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Wu, J., Leng, C., Wang, Y., Hu, Q. and Cheng, J. (2016) Quantized Convolutional Neural Networks for Mobile Devices. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 4820-4828. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.521</mixed-citation></ref><ref id="hanspub.41748-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Rastegari, M., Ordonez, V., Redmon, J. and Farhadi, A. (2016) XNOR-Net: Imagenet Classification Using Binary convolutional Neural Networks. European Conference on Computer Vision, Amsterdam, 8-16 October, 525-542.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46493-0_32</mixed-citation></ref><ref id="hanspub.41748-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Han, S., Mao, H. and Dally, W.J. (2015) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. arXiv: 1510.00149.</mixed-citation></ref><ref id="hanspub.41748-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J. and Han, S. (2018) AMC: AutoML for Model Compres-sion and Acceleration on Mobile Devices. Proceedings of the European Conference on Computer Vision, Munich, 8-14 September, 815-532.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01234-2_48</mixed-citation></ref><ref id="hanspub.41748-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., et al. (2017) MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv: 1704.04861,.</mixed-citation></ref><ref id="hanspub.41748-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Sifre, L. and Mallat, S. (2014) Rigid-Motion Scattering for Texture Classification. arXiv: 1403.1687.</mixed-citation></ref><ref id="hanspub.41748-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Jin, J., Dundar, A. and Culurciello, E. (2014) Flattened Convolutional Neural Networks for Feedfor-ward Acceleration.  arXiv: 1412.5474,.</mixed-citation></ref><ref id="hanspub.41748-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Bolya, D., Zhou, C., Xiao, F. and Lee, Y.J. (2019) YOLACT: Real-Time Instance Segmentation. Proceedings of the 2019 IEEE International Conference on Computer Vision, Seoul, 27 Octo-ber-2 November 2019, 9157-9166.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2019.00925</mixed-citation></ref><ref id="hanspub.41748-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Zheng, Z., Wang, P., Ren, D., Liu, W., Ye, R., Hu, Q., et al. (2020) Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation. arXiv: 2005.03572.</mixed-citation></ref><ref id="hanspub.41748-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R. and Ren, D. (2020) Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression. Proceedings of the AAAI Conference on Artificial Intelligence, 34, 12993-13000.  
&lt;br&gt;https://doi.org/10.1609/aaai.v34i07.6999</mixed-citation></ref><ref id="hanspub.41748-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Ning, C., Zhou, H., Song, Y. and Tang, J. (2017) Inception Single Shot Multibox Detector for Object Detection. 2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW), Hong Kong, 10-14 July 2017, 549-554.  
&lt;br&gt;https://doi.org/10.1109/ICMEW.2017.8026312</mixed-citation></ref></ref-list></back></article>