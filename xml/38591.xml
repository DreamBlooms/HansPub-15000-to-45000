<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.1011205</article-id><article-id pub-id-type="publisher-id">CSA-38591</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20201100000_89531919.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于双重注意力特征增强网络的语义分割方法
  Dual Attention Based Feature Enhanced Networks for Semantic Segmentation
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>赵</surname><given-names>芮</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>于</surname><given-names>晓艳</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>荣</surname><given-names>宪伟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>哈尔滨师范大学，黑龙江 哈尔滨</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>11</month><year>2020</year></pub-date><volume>10</volume><issue>11</issue><fpage>1944</fpage><lpage>1951</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   语意分割作为计算机视觉领域的研究热点之一，在地理信息系统、医疗影像分析和机器人等领域有广泛应用。然而现有的语义分割方法主要面临两个挑战，即类内不一致和类间难区分问题。为此，我们提出了一种基于双重注意力特征增强网络的方法来实现语义分割。该方法采用位置注意力模块与通道注意力模块来获取丰富的空间信息与上下文信息，并且在网络末端添加金字塔池化模块来聚合不同区域的上下文信息，提高网络捕获全局信息的能力。最终在标准数据集上的实验结果验证了本文方法的有效性。 As one of the research hotspots in the field of computer vision, semantic segmentation has been widely applied in various fields such as geographic information systems, medical image analysis and robotics. However, contemporary semantic segmentation tasks generally face two challenges, namely intra-class inconsistency problem and inter-class indistinction problem. To this end, we solve the semantic segmentation by proposing Dual Attention based Feature Enhanced Networks. In this method, the position attention module and channel attention module are used to obtain rich spatial and context information, and the pyramid pooling module is added at the end of the network to aggregate the context information of different regions, which could improve the capability of the networks to capture global information. Finally, the experimental results on the standard dataset demonstrate the effectiveness of the proposed method. 
  
 
</p></abstract><kwd-group><kwd>语义分割，双重注意力特征增强网络，位置注意力模块，通道注意力模块, Semantic Segmentation</kwd><kwd> Dual Attention Based Feature Enhanced Networks</kwd><kwd> Position Attention Module</kwd><kwd> Channel Attention Module</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>语意分割作为计算机视觉领域的研究热点之一，在地理信息系统、医疗影像分析和机器人等领域有广泛应用。然而现有的语义分割方法主要面临两个挑战，即类内不一致和类间难区分问题。为此，我们提出了一种基于双重注意力特征增强网络的方法来实现语义分割。该方法采用位置注意力模块与通道注意力模块来获取丰富的空间信息与上下文信息，并且在网络末端添加金字塔池化模块来聚合不同区域的上下文信息，提高网络捕获全局信息的能力。最终在标准数据集上的实验结果验证了本文方法的有效性。</p></sec><sec id="s2"><title>关键词</title><p>语义分割，双重注意力特征增强网络，位置注意力模块，通道注意力模块</p></sec><sec id="s3"><title>Dual Attention Based Feature Enhanced Networks for Semantic Segmentation</title><p>Rui Zhao, Xiaoyan Yu, Xianwei Rong</p><p>Harbin Normal University, Harbin Heilongjiang</p><p><img src="//html.hanspub.org/file/3-1541890x4_hanspub.png" /></p><p>Received: Oct. 22<sup>nd</sup>, 2020; accepted: Nov. 6<sup>th</sup>, 2020; published: Nov. 13<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/3-1541890x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>As one of the research hotspots in the field of computer vision, semantic segmentation has been widely applied in various fields such as geographic information systems, medical image analysis and robotics. However, contemporary semantic segmentation tasks generally face two challenges, namely intra-class inconsistency problem and inter-class indistinction problem. To this end, we solve the semantic segmentation by proposing Dual Attention based Feature Enhanced Networks. In this method, the position attention module and channel attention module are used to obtain rich spatial and context information, and the pyramid pooling module is added at the end of the network to aggregate the context information of different regions, which could improve the capability of the networks to capture global information. Finally, the experimental results on the standard dataset demonstrate the effectiveness of the proposed method.</p><p>Keywords:Semantic Segmentation, Dual Attention Based Feature Enhanced Networks, Position Attention Module, Channel Attention Module</p><disp-formula id="hanspub.38591-formula8"><graphic xlink:href="//html.hanspub.org/file/3-1541890x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-1541890x7_hanspub.png" /> <img src="//html.hanspub.org/file/3-1541890x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>语意分割作为像素级别的分类任务，在自动驾驶，人机交互，增强现实等领域有广泛应用 [<xref ref-type="bibr" rid="hanspub.38591-ref1">1</xref>]。由于深度学习具有拟合能力及表征能力强、灵活性高、应用范围广等优点 [<xref ref-type="bibr" rid="hanspub.38591-ref2">2</xref>]，因此近年来许多图像语义分割问题使用深度学习来解决。美国伯克利大学研究团队提出了全卷积网络 [<xref ref-type="bibr" rid="hanspub.38591-ref3">3</xref>]，该网络由编码器和解码器组成，编码器是利用卷积层及池化层对图像进行下采样，解码器是利用反卷积层实现上采样，恢复图像分辨率，实现端到端训练的全卷积网络。为了保证输入的空间分辨率同时增加感受野，Fisher Yu等人提出了扩张卷积 [<xref ref-type="bibr" rid="hanspub.38591-ref4">4</xref>]，相对于传统卷积核各个像素是紧密相连的，扩张卷积依据扩张率来控制卷积核中各个像素之间的间隔，将最后几层池化层替换成了扩张率逐渐升高的扩张卷积层来进一步提高算法的精度。华中科技大学王兴刚等人通过改进了非局部神经网络 [<xref ref-type="bibr" rid="hanspub.38591-ref5">5</xref>] 提出了交叉关注语意分割算法 [<xref ref-type="bibr" rid="hanspub.38591-ref6">6</xref>]，该算法能在更好地捕获上下文语意的同时减少GPU运行内存，提高计算效率。然而上述算法并不能在获取足够上下文信息的同时保证精准的空间信息。为了在不损失空间信息的前提下获取充分的上下文信息，一些研究 [<xref ref-type="bibr" rid="hanspub.38591-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.38591-ref8">8</xref>] 在U-Net [<xref ref-type="bibr" rid="hanspub.38591-ref9">9</xref>] 结构的基础上连接来自高阶和低阶的特征来捕获空间信息和上下文信息。此外，BiSeNet [<xref ref-type="bibr" rid="hanspub.38591-ref10">10</xref>] 设计了包含小步幅卷积层的空间路径来获取空间信息，同时提出了包含下采样策略的上下文路径来获得较大的感受野及上下文信息，最后，引入特征融合模块来融合由上面两路径生成的特征。尽管这些方法可以确保空间及上下文特征的获得，但是它们难以分辩外观不同但是具有相同语义标签的对象。</p><p>为此，通过借鉴特征鉴别网络 [<xref ref-type="bibr" rid="hanspub.38591-ref11">11</xref>]，本文提出了双重注意力特征增强网络用于实现语义分割。如图1所示，该网络包括平滑网络与边界网络，前者用于处理类内不一致问题，后者用于解决类间难区分问题。为获取更精准的图像细节信息，本文提出在平滑网络中加入位置注意力模块用于捕获由网络低阶产生的有效的空间特征，依据图像处理中的非局部均值原理，该模块在计算某一特定像素点的特征时，采用对图像中所有点的特征值进行加权平均，其中各个特征分配的权值取决于各个像素之间的依赖关系，同时，利用通道注意力模块来获取由网络高阶产生的精准的上下文信息，通道注意力模块与位置注意力模块构成双重注意力机制来捕获网络不同阶段所产生的有效语义特征。另一方面，我们采用修正残差模块统一实验过程中平滑网络的内部通道数及进一步细化各个阶段产生的语义信息。最后，在残差网络的末端加入金字塔池化模块 [<xref ref-type="bibr" rid="hanspub.38591-ref12">12</xref>]，以此获取局部及全局有效的语义信息。</p><p>图1. 双重注意力特征增强网络</p></sec><sec id="s6"><title>2. 位置注意力模块</title><p>在本文中，我们引入位置注意力模块来筛选更加有用的特征，通过学习长距离特征之间的依赖关系，位置注意力模块可以弥补由于连续卷积和池化所带来的图像细节信息的损失。受非局部神经网络 [<xref ref-type="bibr" rid="hanspub.38591-ref5">5</xref>] 的启发，我们提出了位置注意力模块，其实现功能如下：(1) 连接高阶特征和低阶特征，实现从高阶到低阶特征的指导功能。(2) 学习特征之间的依赖关系来丰富空间信息，并且改善网络的辨别能力。接下来，我们进一步介绍聚合特征的过程。</p><p>非局部模块 [<xref ref-type="bibr" rid="hanspub.38591-ref5">5</xref>] 的思想是位置i的特征 x i 取决于 x i 和 x j 之间的依赖性，其中 x j 代表特征图中所有点的特征。在本文中，采用嵌入式高斯函数来获得依赖关系，如公式(1)所示，并且包含特征图中任意两点之间依赖关系的特征图可以由公式(2)得到。</p><p>f ( x i , x j ) = e θ ( x i ) T φ ( x j ) (1)</p><p>f ( x i , x j ) 代表位置i和j位置的特征之间的依赖关系，θ和φ是卷积操作，其中 { i , j } ∈ R C &#215; W &#215; H 。</p><p>y i j = 1 C ( x ) ∑ ∀ j f ( x i , x j ) (2)</p><p>y i j 代表特征 x j 对 x i 的影响，且 C ( X ) = ∑ ∀ j f ( x i , x j ) ，根据softmax函数的定义，公式(2)可以进一步转化为公式(3)。</p><p>y i j = softmax ( θ ( x i ) T φ ( x j ) ) (3)</p><p>如图2所示，通过上述softmax函数获得包含各个位置特征之间依赖关系的注意力图，通过公式(4)获得位置注意力模块的最终输出结果。</p><p>Z = CBR ( A z L ) + H (4)</p><p>其中，Z代表位置注意力模块最终输出结果，A<sub>z</sub>代表注意力图，L代表经过卷积处理的低阶特征，H代表来自高阶的特征，CBR为由卷积、批归一化、Relu激活函数组成的处理模块。</p><p>图2. 位置注意力模块</p></sec><sec id="s7"><title>3. 双重注意力特征增强网络</title><p>对于给定分割图像，语义分割效果通常会受到光照及图像尺寸大小的影响，因此捕获更加有效的特征对于实现精准分割至关重要。同时，网络的低级阶段虽然可以获取更多的空间信息，但是因其感受野小而导致上下文信息不足；然而网络的高级阶段因其感受野较大，所以可以获取准确的上下文信息，但是连续的卷积会导致空间信息的损失 [<xref ref-type="bibr" rid="hanspub.38591-ref13">13</xref>]。因此，为了获得更加有效的信息，我们需要不同的注意力模块来处理不同网络阶段产生的特征。</p><p>为解决上述问题，我们提出了双重注意力特征增强网络，该网络主要包括平滑网络与边界网络，其中，平滑网络用于解决将具有相同标签但外观不同的目标分为不同类别的情况，即类内不一致问题，而边界网络主要改善将外观相似不同类别的目标划分为同一类别的现象，即类间难区分问题。同时，在网络中加入双重注意力机制，即位置注意力模块与通道注意力模块，来获取充分的语义信息与上下文信息以便得到精准的分割结果。</p><p>平滑网络部分利用位置注意力模块与通道注意力模块来获取充分的语义信息，如图1所示，在平滑网络的前两层加入位置注意力模块来捕获更加有效的图像细节及边界等空间信息，在后两层加入通道注意力模块通过对贡献更大的通道特征赋予更大的权值来捕获更加精确的上下文信息。另一方面，如图2及图3所示，位置注意力模块与通道注意力模块连接相邻阶段的特征，以此实现利用高级别特征来指导低级别特征。与特征鉴别网络相比，我们对于低阶特征不使用通道注意力模块，因为低阶特征的语义在不同的通道上几乎没有差别，本方法使用位置注意力模块替代网络低阶的通道注意力模块以便网络可以更好的捕获有效的空间特征，同时，为了获取不同感受野下的全局信息，本算法在残差网络末端加入金字塔池化模块从而进一步增强网络获取语义信息的能力。最后，如图4所示，引入了残差修正模块用于统一平滑网络内部通道数量及进一步整合语义信息。</p><p>本文采用特征鉴别网络中的边界网络，该部分的功能是放大类与类之间的特征区别，从而解决类别之间模型不清晰问题。边界网络采用语义边界来指导特征学习，增强网络对不同类别的区别能力，鉴于网络在不同的阶段具有不同的识别能力，边缘网络利用在低级阶段获得边缘信息进一步细化在高级阶段捕获语义信息，同时利用局部损失函数 [<xref ref-type="bibr" rid="hanspub.38591-ref14">14</xref>] 来监督该部分的输出，以达到更好的提取语义边界的效果。</p><p>图3. 通道注意力模块</p><p>图4. 修正残差模块</p><p>在本文中，我们采用深度监督来获得更好的实验结果。交叉熵损失函数可以更好的衡量真实分布和预测的分布的差异情况，其具体计算公式如公式(6)所示。因此对于平滑网络，采用交叉熵损失函数来计算除了全局池化层以外每个阶段的损失，如公式(7)所示。对于边界网络，如公式(8)所示，采用局部损失函数来监督每一阶段的输出。此外，如公式(9)所示，我们引入参数σ来平衡平滑网络和边界网络的损失，通过实验表明，当σ取值为0.5时，可以达到最好的语义分割效果。</p><p>L o s s = − ∑ i = 1 m y i log ( p i ) (6)</p><p>其中 p i 是该图第i类的预测概率值， y i 是该图第i类的标注， y i 可取值为0或1，其中标注0代表该像素点属于第i类，反之，标注1则代表该像素点不属于第i类。</p><p>S N L O S S i = C r o s s E n t r o p y L o s s ( y s i ; w ) (7)</p><p>B N L O S S i = F o c a l L o s s ( y b i ; w ) (8)</p><p>L = ∑ i = 0 3 S N L O S S i + σ ∑ i = 0 3 B N L O S S i (9)</p><p>其中，i代表平滑网络与边界网络的层数，其中 i ∈ { 0 , 1 , 2 , 3 } ， S N L O S S i 和 B N L O S S i 是平滑网络和边界网络第i层的损失。</p></sec><sec id="s8"><title>4. 性能评价</title><sec id="s8_1"><title>4.1. 数据集及参数设置</title><p>我们使用交并比均值(Mean IOU)作为性能指标来评价语义分割的性能，并使用了标准数据集PASCAL VOC 2012验证本文所提出方法的性能。</p><p>PASCAL VOC 2012：作为语义分割标准数据库，PASCAL VOC 2012包括20个类别以及一个背景，其中包含1464张训练图像和1449张验证图像。通过使用语义边界数据集 [<xref ref-type="bibr" rid="hanspub.38591-ref15">15</xref>] 对PASCAL VOC 2012进行扩充，扩充后的PASCAL VOC 2012数据集包含10582张训练数据集。</p><p>为了防止由于实验数据过少而导致训练过程中出现过拟合的情况，对于输入的训练图像，我们采用均值减法及水平翻转对图像进行预处理。同时，实验中对训练图像进行均值减法和随机水平翻转，同时，为了实现实验数据的扩充，在我们的实验中训练图像被随机按比例缩放，缩放比例设置为{0.5, 0.75, 1, 1.5, 1.75}，最后统一将图像调整为512 &#215; 512大小作为最终训练数据输入到网络中。本文采用随机梯度下降 [<xref ref-type="bibr" rid="hanspub.38591-ref16">16</xref>] 作为梯度下降算法来进行网络训练，其中批量大小为4，动量参数为0.9，学习率衰减值为0.0001。实验中采用poly学习速度策略，其中动量值设为0.9，初始学习率为1e<sup>−4</sup>并且在每次迭代后将它乘以 ( 1 − 当 前 迭 代 次 数 总 迭 代 次 数 ) 0.9 。</p></sec><sec id="s8_2"><title>4.2. 实验结果</title><p>借鉴文献 [<xref ref-type="bibr" rid="hanspub.38591-ref17">17</xref>] 的方法，本实验的训练过程使用PASCAL VOC 2012的扩充数据集在ImageNet [<xref ref-type="bibr" rid="hanspub.38591-ref18">18</xref>] 预训练的ResNet-101模型上进行训练，在训练时，采用扩充PASCAL VOC 2012的trainval数据集作为我们的训练集，同时输入图像的大小裁剪为512 &#215; 512。在验证过程中，我们使用原始PASCAL VOC 2012的trainval数据集来验证我们的方法，采用多尺度输入以及水平翻转进一步优化验证数据集。最后，我们将基准网络与本论文提出方法的实验结果进行可视化，如图5所示，通过与基准网络ResNet-101的输出结果进行对比，本实验在物体边界及图像细节部分的分割结果更加清晰与准确，这是由于注意力模块及金字塔池化模块帮助网络获取更加有效的语义信息，细化分割结果的边界及细节部分。本文方法在PASCAL VOC 2012数据集上取得了在比较算法中最佳的性能，如表1所示，即最高的88.4%的交并比均值，这说明本文方法在比较算法中取得了最好的分割效果。</p><p>图5. 双重注意力特征增强网络及基准网络分割结果对比</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The comparison of the performance in Mean IOU between different algorithm</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Method</th><th align="center" valign="middle" >Backbone</th><th align="center" valign="middle" >Mean IOU (%)</th></tr></thead><tr><td align="center" valign="middle" >FCN [<xref ref-type="bibr" rid="hanspub.38591-ref3">3</xref>]</td><td align="center" valign="middle" >VGG-16</td><td align="center" valign="middle" >62.2</td></tr><tr><td align="center" valign="middle" >ParseNet [<xref ref-type="bibr" rid="hanspub.38591-ref19">19</xref>]</td><td align="center" valign="middle" >VGG-16</td><td align="center" valign="middle" >69.6</td></tr><tr><td align="center" valign="middle" >PSPNet [<xref ref-type="bibr" rid="hanspub.38591-ref12">12</xref>]</td><td align="center" valign="middle" >ResNet-101</td><td align="center" valign="middle" >85.4</td></tr><tr><td align="center" valign="middle" >Deeplab v3 [<xref ref-type="bibr" rid="hanspub.38591-ref20">20</xref>]</td><td align="center" valign="middle" >ResNet-101</td><td align="center" valign="middle" >85.7</td></tr><tr><td align="center" valign="middle" >EncNet [<xref ref-type="bibr" rid="hanspub.38591-ref21">21</xref>]</td><td align="center" valign="middle" >ResNet-101</td><td align="center" valign="middle" >85.9</td></tr><tr><td align="center" valign="middle" >DFN [<xref ref-type="bibr" rid="hanspub.38591-ref11">11</xref>]</td><td align="center" valign="middle" >ResNet-101</td><td align="center" valign="middle" >86.2</td></tr><tr><td align="center" valign="middle" >Exfuse [<xref ref-type="bibr" rid="hanspub.38591-ref22">22</xref>]</td><td align="center" valign="middle" >ResNet-101</td><td align="center" valign="middle" >86.2</td></tr><tr><td align="center" valign="middle" >SDN [<xref ref-type="bibr" rid="hanspub.38591-ref23">23</xref>]</td><td align="center" valign="middle" >Dense-Net-161</td><td align="center" valign="middle" >86.6</td></tr><tr><td align="center" valign="middle" >DIS [<xref ref-type="bibr" rid="hanspub.38591-ref24">24</xref>]</td><td align="center" valign="middle" >ResNet-101</td><td align="center" valign="middle" >86.8</td></tr><tr><td align="center" valign="middle" >EMANet [<xref ref-type="bibr" rid="hanspub.38591-ref25">25</xref>]</td><td align="center" valign="middle" >ResNet-101</td><td align="center" valign="middle" >87.7</td></tr><tr><td align="center" valign="middle" >Ours</td><td align="center" valign="middle" >ResResNet-101</td><td align="center" valign="middle" >88.4</td></tr></tbody></table></table-wrap><p>表1. 不同算法的交并比均值性能比较</p></sec></sec><sec id="s9"><title>5. 结论</title><p>本文提出了一种双重注意力特征增强网络来解决当前语义分割面临的挑战任务。该网络引入了位置注意力模块来增强长距离特征之间的依赖关系并且进一步细化了空间特征，利用位置注意力模块与通道注意力模块来增强特征的表征能力，以便获得更加有效的语义特征，同时在网络末端添加金字塔池化模块来提高网络捕获全局场景信息的能力。为了验证提出方法的有效性，我们在PASCAL VOC 2012标准数据集上进行了比较实验，实验结果表明位置注意力模块及金字塔池化模块的添加可以明显提高语义分割的效果。为了扩展本文方法的应用领域，未来的工作将着重研究降低双重注意力特征增强网络的计算复杂度方法。</p></sec><sec id="s10"><title>基金项目</title><p>本课题是在国家自然科学基金资助项目61401127及黑龙江省自然科学基金资助项目F2018022的支持下完成的。</p></sec><sec id="s11"><title>文章引用</title><p>赵 芮,于晓艳,荣宪伟. 基于双重注意力特征增强网络的语义分割方法Dual Attention Based Feature Enhanced Networks for Semantic Segmentation[J]. 计算机科学与应用, 2020, 10(11): 1944-1951. https://doi.org/10.12677/CSA.2020.1011205</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.38591-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Garcia-Garcia, A., et al. (2017) A Review on Deep Learning Techniques Applied to Semantic Segmentation. Internation-al Conference on Computational Linguistics, Spain, 22 April 2017, 2132-2144.</mixed-citation></ref><ref id="hanspub.38591-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">陈一鸣, 彭艳兵, 高剑飞. 基于深度学习的遥感图像新增建筑物语义分割[J]. 计算机与数字工程, 2019, 47(12): 3182-3186.</mixed-citation></ref><ref id="hanspub.38591-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Long, J., Shel-hamer, E. and Darrell, T. (2014) Fully Convolutional Networks for Semantic Segmentation. IEEE Transactions on Pat-tern Analysis &amp; Machine Intelligence, 39, 640-651. &lt;br&gt;https://doi.org/10.1109/TPAMI.2016.2572683</mixed-citation></ref><ref id="hanspub.38591-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Yu, F. and Koltun, V. (2016) Multi-Scale Context Aggregation by Dilated Convolutions.</mixed-citation></ref><ref id="hanspub.38591-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Wang, X., Girshick, R., Gupta, A., et al. (2018) Non-Local Neural Networks. IEEE Computer Society Conference on Computer Vision and Pattern Recogni-tion, Salt Lake City, 18-22 June 2018, 7794-7803.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00813</mixed-citation></ref><ref id="hanspub.38591-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Huang, Z., Wang, X., Huang, L., et al. (2019) CCNet: Criss-Cross Attention for Semantic Segmentation. IEEE International Conference on Computer Vision, Seoul, 27-28 October 2019, 603-612.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2019.00069</mixed-citation></ref><ref id="hanspub.38591-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Chao, P., et al. (2017) Large Kernel Matters—Improve Semantic Segmentation by Global Convolutional Network. IEEE Conference on Computer Vision and Pattern Recognition, Hono-lulu, 21-26 July 2017, 1743-1751.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.189</mixed-citation></ref><ref id="hanspub.38591-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Chen, L., et al. (2020) ANU-Net: Attention-Based Nested U-Net to Exploit Full Resolution Features for Medical Image Segmentation. Computers &amp; Graphics, 90, 11-20. &lt;br&gt;https://doi.org/10.1016/j.cag.2020.05.003</mixed-citation></ref><ref id="hanspub.38591-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Ronneberger, O., Fischer, P. and Brox, T. (2015) U-Net: Convolu-tional Networks for Biomedical Image Segmentation. International Conference on Medical Image Computing and Com-puter-Assisted Intervention, Munich, 5-9 October 2015, 234-241. &lt;br&gt;https://doi.org/10.1007/978-3-319-24574-4_28</mixed-citation></ref><ref id="hanspub.38591-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Yu, C., et al. (2018) BiSeNet: Bilateral Segmentation Network for Real-Time Semantic Segmentation. European Conference on Computer Vision, Munich, 8-14 September 2018, 334-349. &lt;br&gt;https://doi.org/10.1007/978-3-030-01261-8_20</mixed-citation></ref><ref id="hanspub.38591-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Yu, C., Wang, J., Peng, C., et al. (2018) Learning a Discriminative Feature Network for Semantic Segmentation. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-22 June 2018, 1857-1866.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00199</mixed-citation></ref><ref id="hanspub.38591-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Zhao, H., Shi, J., Qi, X., et al. (2016) Pyramid Scene Parsing Network. IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 6230-6239. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.660</mixed-citation></ref><ref id="hanspub.38591-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">翟鹏博, 杨浩, 宋婷婷. 结合注意力机制的双路径语义分割[J]. 中国图象图形学报, 2020, 25(8): 1627-1636.</mixed-citation></ref><ref id="hanspub.38591-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Lin, T.Y., Goyal, P., Girshick, R., et al. (2017) Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 99, 2999-3007.</mixed-citation></ref><ref id="hanspub.38591-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Hari-haran, B., Arbelaez, P., Bourdev, L.D., et al. (2011) Semantic Contours from Inverse Detectors. IEEE International Conference on Computer Vision, Barcelona, 6-13 November 2011, 991-998.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2011.6126343</mixed-citation></ref><ref id="hanspub.38591-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky, A., Sutskever, I. and Hinton, G.E. (2012) Imagenet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems, Vol. 2, 1097-1105.</mixed-citation></ref><ref id="hanspub.38591-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">熊炜, 童磊, 金靖熠. 基于卷积神经网络的语义分割算法研究[J/OL]. 计算机应用研究, 2020, 38(3): 1-5.</mixed-citation></ref><ref id="hanspub.38591-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Russakovsky, O., Deng, J., Su, H., et al. (2015) ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115, 211-252. &lt;br&gt;https://doi.org/10.1007/s11263-015-0816-y</mixed-citation></ref><ref id="hanspub.38591-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Liu, W., Rabinovich, A. and Berg, A.C. (2015) ParseNet: Looking Wider to See Better. arXiv preprint  
arXiv: 1506.04579</mixed-citation></ref><ref id="hanspub.38591-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Chen, L.C., Papandreou, G., Schroff, F., et al. (2017) Rethinking Atrous Convolution for Se-mantic Image Segmentation.</mixed-citation></ref><ref id="hanspub.38591-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, H., Dana, K., Shi, J., et al. (2018) Context Encoding for Semantic Segmenta-tion. IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 7151-7160.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00747</mixed-citation></ref><ref id="hanspub.38591-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Z., Zhang, X., Peng, C., et al. (2018) ExFuse: Enhancing Feature Fusion for Semantic Segmentation. European Conference on Computer Vision, Munich, 8-14 September 2018, 269-284.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01249-6_17</mixed-citation></ref><ref id="hanspub.38591-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Jun, F., et al. (2019) Stacked Deconvolutional Network for Semantic Segmentation. International Conference on Image Processing, Taipei, 22-25 September 2019, 3085-3089. &lt;br&gt;https://doi.org/10.1109/TIP.2019.2895460</mixed-citation></ref><ref id="hanspub.38591-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Luo, P., Wang, G., Lin, L., et al. (2017) Deep Dual Learning for Semantic Image Segmentation. IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 2737-2745. &lt;br&gt;https://doi.org/10.1109/ICCV.2017.296</mixed-citation></ref><ref id="hanspub.38591-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Xia, L., Zhong, Z.S., Wu, J.L., et al. (2019) Expecta-tion-Maximization Attention Networks for Semantic Segmentation. IEEE International Conference on Computer Vision, Seoul, 27-28 October 2019, 9166-9175.</mixed-citation></ref></ref-list></back></article>