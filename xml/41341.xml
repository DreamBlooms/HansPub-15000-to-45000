<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2021.103087</article-id><article-id pub-id-type="publisher-id">AAM-41341</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20210300000_72667633.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  二元逻辑回归模型中的一阶近似刀切Liu估计
  A First-Order Approximated Jackknifed Liu Estimator in Binary Logistic Regression Model
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>邹</surname><given-names>媛</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>贵州民族大学，数据科学与信息工程学院，贵州 贵阳</addr-line></aff><pub-date pub-type="epub"><day>10</day><month>03</month><year>2021</year></pub-date><volume>10</volume><issue>03</issue><fpage>790</fpage><lpage>800</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    为了解决二元逻辑回归模型中的复共线性问题，我们结合一阶近似Liu估计和刀切法的优点提出了一个新的估计即一阶近似刀切Liu估计。研究得出了新估计偏差的优良性以及在均方误差矩阵、均方误差准则下优于一阶近似极大似然估计、一阶近似Liu估计和一阶近似刀切岭估计的充要或充分条件。更进一步使用了蒙特卡罗模拟和实证分析来探讨一阶近似刀切Liu估计偏差和在均方误差意义下的优良性。
    In order to solve the problem of multicollinearity in the binary logistic regression model, we combine the advantages of the first-order approximated Liu estimator and the jackknife procedure, and propose a new estimator, namely the first-order approximated jackknifed Liu estimator. The research obtained the sufficient and necessary or sufficient conditions for the new estimator to be superior to the first-order approximated maximum likelihood estimator, the first-order approximated Liu estimator and the first-order approximated jackknifed ridge estimatior under the bias, mean square error matrix or mean square error criterion. Furthermore, Monte Carlo simulation and empirical analysis are used to explore the first-order approximated jackknifed Liu estimator’s performance in the sense of bias and mean square error. 
  
 
</p></abstract><kwd-group><kwd>二元逻辑回归模型，复共线性，一阶近似刀切Liu估计，偏差, Binary Logistic Regression Model</kwd><kwd> Multicollinearity</kwd><kwd> First-Order Approximated Jackknifed Liu 
Estimator</kwd><kwd> Bias</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>为了解决二元逻辑回归模型中的复共线性问题，我们结合一阶近似Liu估计和刀切法的优点提出了一个新的估计即一阶近似刀切Liu估计。研究得出了新估计偏差的优良性以及在均方误差矩阵、均方误差准则下优于一阶近似极大似然估计、一阶近似Liu估计和一阶近似刀切岭估计的充要或充分条件。更进一步使用了蒙特卡罗模拟和实证分析来探讨一阶近似刀切Liu估计偏差和在均方误差意义下的优良性。</p></sec><sec id="s2"><title>关键词</title><p>二元逻辑回归模型，复共线性，一阶近似刀切Liu估计，偏差</p></sec><sec id="s3"><title>A First-Order Approximated Jackknifed Liu Estimator in Binary Logistic Regression Model</title><p>Yuan Zou</p><p>School of Data Science and Information Engineering, Guizhou Minzu University, Guiyang Guizhou</p><p><img src="//html.hanspub.org/file/18-2621523x4_hanspub.png" /></p><p>Received: Feb. 25<sup>th</sup>, 2021; accepted: Mar. 23<sup>rd</sup>, 2021; published: Mar. 30<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/18-2621523x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In order to solve the problem of multicollinearity in the binary logistic regression model, we combine the advantages of the first-order approximated Liu estimator and the jackknife procedure, and propose a new estimator, namely the first-order approximated jackknifed Liu estimator. The research obtained the sufficient and necessary or sufficient conditions for the new estimator to be superior to the first-order approximated maximum likelihood estimator, the first-order approximated Liu estimator and the first-order approximated jackknifed ridge estimatior under the bias, mean square error matrix or mean square error criterion. Furthermore, Monte Carlo simulation and empirical analysis are used to explore the first-order approximated jackknifed Liu estimator’s performance in the sense of bias and mean square error.</p><p>Keywords:Binary Logistic Regression Model, Multicollinearity, First-Order Approximated Jackknifed Liu Estimator, Bias</p><disp-formula id="hanspub.41341-formula21"><graphic xlink:href="//html.hanspub.org/file/18-2621523x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/18-2621523x7_hanspub.png" /> <img src="//html.hanspub.org/file/18-2621523x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>逻辑回归模型是生物统计学和健康科学中常用的二元数据建模方法。这种模型有时也称为概率模型，因为给定一组协变量，事件发生的概率可以估计。二元逻辑回归模型的基本假设有：模型的响应变量y的分量 y i 是相互独立且服从Bernoulli ( π i )分布，其中：</p><p>π i = exp ( x ′ i β ) 1 + exp ( x ′ i β ) , i = 1 , ⋯ , n (1)</p><p>x ′ i 是n &#215; p样本资料矩阵X的第i行元素组成的向量。 β   = ( β 1 , ⋯ , β p ) ′ 为p &#215; 1的系数向量。 π i   = P ( y i = 1 | x i ) 是在 x i 的条件下 y i = 1 的概率。</p><p>在逻辑回归模型中，一般使用极大似然(ML)方法来估计回归参数 β ，模型(1)的对数似然函数为：</p><p>L ( β ) = ∑ i = 1 n [ y i x ′ i β − ln ( 1 + exp ( x ′ i β ) ) ] (2)</p><p>对等式(2)进行求导并令其等于0求 L ( β ) 的极大值：</p><p>∂ L ( β ) ∂ β = X ′ ( y − π ) = 0 (3)</p><p>由于等式(3)不是线性的，因此ML估计是通过Newton-Raphson方法求解方程组(3)而得。用Newton-Raphson方法给出了 β 一个数值解：</p><p>β ^ ( m ) = β ^ ( m − 1 ) + ( X ′ V ^ ( m − 1 ) X ) − 1 X ′ ( y − π ^ i ( m − 1 ) ) (4)</p><p>在上述迭代运算中， β ^ ( m − 1 ) 是 β 的第 m − 1 次迭代估计的向量， π ^ i ( m − 1 ) 是 π ^ ( m − 1 ) 的第i个元素， V ^ ( m − 1 ) = d i a g ( π ^ i ( m − 1 ) ) ( 1 − π ^ i ( m − 1 ) ) 是权重矩阵。当 | β ^ ( m ) − β ^ ( m − 1 ) | &lt; δ 时收敛，运算终止，其中 δ 为事先给定的计算精度，求得 β ^ ( m ) 为极大似然估计 β ^ 的近似解 β ^ M L E ：</p><p>β ^ M L E = ( X ′ V ^ X ) − 1 X ′ V ^ z ^ (5)</p><p>其中， z ^ = X β ^ + V ^ − 1 ( y − π ^ ) ， V ^ 是一个对角矩阵且第i个对角元素为 π ^ i ( 1 − π ^ i ) ， π ^ 为收敛后的取值。</p><p>在逻辑回归模型中，当 X ′ V ^ X 的一些特征值很小时，极大似然估计(MLE)的方差会膨胀，可能导致符号与现实情况不符，统计推断可能出现错误。为了克服这个问题，学者们提出了很多有偏估计来改进MLE。例如，Schaefer等 [<xref ref-type="bibr" rid="hanspub.41341-ref1">1</xref>] 提出了岭估计(RE)：</p><p>β ^ ( k ) = ( X V ^ X + k I ) − 1 X V ^ X β ^ M L E , k &gt; 0 (6)</p><p>k为岭参数。</p><p>M&#229;nsson等 [<xref ref-type="bibr" rid="hanspub.41341-ref2">2</xref>] 提出的Liu估计(LE)，表达式如下：</p><p>β ^ ( d ) = ( X ′ V ^ X + I ) − 1 ( X ′ V ^ X + d I ) β ^ M L E , 0 &lt; d &lt; 1 (7)</p><p>为了减小复共线性的影响，基于Newton-Raphson方法，LeCessie和Van Houwelingen [<xref ref-type="bibr" rid="hanspub.41341-ref3">3</xref>] 提出了与Schaefer等人 [<xref ref-type="bibr" rid="hanspub.41341-ref1">1</xref>] 提出的岭估计渐近等价的一阶近似岭估计(FAR)，表达式如下：</p><p>β ^ ( 1 ) ( k ) = ( X V ^ ( 0 ) X + k I ) − 1 X V ^ ( 0 ) X β ^ ( 1 ) ( M L ) , k &gt; 0 (8)</p><p>其中 V ^ ( 0 ) 是真实参数值 β 0 估计的权重矩阵。 β ^ ( 1 ) ( M L ) 是由等式(4)所得的一阶近似极大似然估计(FAE)，表达式为：</p><p>β ^ ( 1 ) ( M L ) = ( X V ^ ( 0 ) X ) − 1 X V ^ ( 0 ) z ^ ( 0 )</p><p>&#214;zkale [<xref ref-type="bibr" rid="hanspub.41341-ref4">4</xref>] 提出了一阶近似Liu估计(FAL)，表达式如下：</p><p>β ^ ( 1 ) ( d ) = ( X ′ V ^ ( 0 ) X + I ) − 1 ( X ′ V ^ ( 0 ) X + d I ) β ^ ( 1 ) ( M L ) , 0 &lt; d &lt; 1 (9)</p><p>为了减小估计的偏差，Quenouille [<xref ref-type="bibr" rid="hanspub.41341-ref5">5</xref>] 和Tukey [<xref ref-type="bibr" rid="hanspub.41341-ref6">6</xref>] 提出了刀切法。它的基本思想是利用一种特殊的方法来处理实验数据，从而得到一个未知参数的统计估计量。即系统地从数据中去除每个观测值后重新计算估计量，再将这些估计量取平均值。在二元逻辑回归模型中，结合一阶近似岭估计和刀切法，&#214;zkale和Arıcan [<xref ref-type="bibr" rid="hanspub.41341-ref7">7</xref>] 提出了一阶近似刀切岭估计(FAJR)。表达式如下：</p><p>β ˜ ( 1 ) ( k ) = ( I − k 2 ( X V ^ ( 0 ) X + k I ) − 2 ) β ^ ( 1 ) ( M L ) , k &gt; 0 (10)</p></sec><sec id="s6"><title>2. 提出的估计</title><p>在本文中，我们结合一阶近似Liu估计和刀切法，提出了一个新的估计即一阶近似刀切Liu估计。接下来我们应用刀切法来定义一阶近似刀切Liu估计。</p><p>当X和y的第i个观测值删除时一阶近似Liu估计的表达式为：</p><p>β ^ − i ( 1 ) ( d ) = ( X ′ − i V ^ − i ( 0 ) X − i + I ) − 1 ( X ′ − i V ^ − i ( 0 ) z ^ − i ( 0 ) + d β ^ − i ( 1 ) ( M L ) ) (11)</p><p>其中， X ′ − i V ^ − i ( 0 ) X − i = X ′ V ^ ( 0 ) X − x i v ^ i ( 0 ) x ′ i ， X ′ − i V ^ − i ( 0 ) z ^ − i ( 0 ) = X ′ V ^ ( 0 ) z ^ ( 0 ) − x i v ^ i ( 0 ) z ^ i ( 0 ) 。化简可得：</p><p>β ^ − i ( 1 ) ( d ) = β ^ ( 1 ) ( d ) − 1 1 − h i i ( X ′ V ^ ( 0 ) X + I ) − 1 x i v ^ i ( 0 ) ( z ^ i ( 0 ) − x ′ i β ^ ( 1 ) ( d ) )     − d 1 ( 1 − h i i ) ( 1 − h i ) ( X ′ V ^ ( 0 ) X + I ) − 1 x i v ^ i ( 0 ) x ′ i ( X ′ V ^ ( 0 ) X + I ) − 1     ⋅ ( X ′ V ^ ( 0 ) X ) − 1 x i v ^ i ( 0 ) ( z ^ i ( 0 ) − x ′ i β ^ ( 1 ) ( M L ) ) (12)</p><p>其中 h i i = v ^ i ( 0 ) x ′ i ( X ′ V ^ ( 0 ) X + I ) − 1 x i ， h i = v ^ i ( 0 ) x ′ i ( X ′ V ^ ( 0 ) X ) − 1 x i 。</p><p>根据Hinkley [<xref ref-type="bibr" rid="hanspub.41341-ref8">8</xref>] 我们可以得出加权伪值：</p><p>Q i = β ^ ( 1 ) ( d ) + n ( 1 − h i i ) ( β ^ ( 1 ) ( d ) − β ^ − i ( 1 ) ( d ) ) (13)</p><p>和加权伪值相对应的加权刀切估计：</p><p>β ˜ ( 1 ) ( d ) = n − 1 ∑ Q i (14)</p><p>根据等式(12)，(13)，(14)， ∑ i = 1 n x i v ^ i ( 0 ) z ^ i ( 0 ) = X ′ V ^ ( 0 ) z ^ ( 0 ) 和 ∑ i = 1 n x i v ^ i ( 0 ) x ′ i = X ′ V ^ ( 0 ) X 我们在逻辑回归模型中定义了一个新的估计即一阶近似刀切Liu估计(FAJL)，表达式为：</p><p>β ˜ ( 1 ) ( d ) = { ( I − ( X ′ V ^ ( 0 ) X + I ) − 1 X ′ V ^ ( 0 ) X ) ( X ′ V ^ ( 0 ) X + I ) − 1 ( X ′ V ^ ( 0 ) X + d I )     + ( X ′ V ^ ( 0 ) X + I ) − 1 X ′ V ^ ( 0 ) X } β ^ ( 1 ) ( M L ) (15)</p></sec><sec id="s7"><title>3. 一阶近似刀切Liu估计的性质</title><p>为了方便讨论一阶近似刀切Liu估计的性质，我们对矩阵 Φ = X ′ V ( 0 ) X 进行特征分解，可以表示为 Φ = X ′ V ^ ( 0 ) X = T Λ T ′ ，这里 Λ = T ′ Φ T = Z ′ V ^ ( 0 ) Z = d i a g ( λ j ) 是由矩阵 X ′ V ^ ( 0 ) X 的特征值组成的对角矩阵，且 λ 1 ≥ λ 2 ≥ ⋯ ≥ λ p 。 T = ( T 1 ⋯ T p ) 是由矩阵 X ′ V ^ ( 0 ) X 的特征值所对应的标准化特征向量组成的 p &#215; p 阶正交矩阵。 Z = X T ， α = T ′ β 。为了方便对所提出的新估计与其他估计进行比较，我们首先定义参数 β 的估计 β ^ 的偏差和偏差的平方和分别为：</p><p>b i a s ( β ^ ) = E ( β ^ ) − β (16)</p><p>‖ B i a s ( β ^ ) ‖ 2 = b i a s ( β ^ ) ′ b i a s ( β ^ ) (17)</p><p>均方误差矩阵：</p><p>M S E M ( β ^ ) = E ( β ^ − β ) ( β ^ − β ) ′ (18)</p><p>均方误差：</p><p>M S E ( β ^ ) = E ( β ^ − β ′ ) ( β ^ − β ) (19)</p><p>为了在均方误差矩阵准则下对一阶近似极大似然估计、一阶近似刀切岭估计、一阶近似Liu估计和一阶近似刀切Liu估计进行比较，我们使用到了如下引理：</p><p>引理1 (Farebrother [<xref ref-type="bibr" rid="hanspub.41341-ref9">9</xref>] )设A是一个 n &#215; n 阶的正定矩阵，c是一个 n &#215; 1 阶非零列向量，u是正的标量。如果 c ′ A − 1 c &lt; u ，则 u A − c c ′ 是正定的。</p><p>定理1. ‖ B i a s ( β ˜ ( 1 ) ( d ) ) ‖ 2 &lt; ‖ B i a s ( β ^ ( 1 ) ( d ) ) ‖ 2 。</p><p>证明：</p><p>b i a s ( β ^ ( 1 ) ( d ) ) = ( d − 1 ) ( X ′ V ^ ( 0 ) X + I ) − 1 a 0 (20)</p><p>b i a s ( β ˜ ( 1 ) ( d ) ) = ( d − 1 ) ( X ′ V ^ ( 0 ) X + I ) − 2 a 0 (21)</p><p>则</p><p>‖ B i a s ( β ^ ( 1 ) ( d ) ) ‖ 2 − ‖ B i a s ( β ˜ ( 1 ) ( d ) ) ‖ 2 = ( a 0 ) ′ G 1 a 0</p><p>其中 G 1 = ( d − 1 ) 2 ( X ′ V ^ ( 0 ) X + I ) − 2 − ( d − 1 ) 2 ( X ′ V ^ ( 0 ) X + I ) − 4 ， G 1 是对角元素为 ( d − 1 ) 2 ( ( λ i + 1 ) 2 − 1 ) ( λ i + 1 ) 4 的对角矩阵， λ i 是矩阵 X ′ V ^ ( 0 ) X 的第i个特征值，所以 ( d − 1 ) 2 ( ( λ i + 1 ) 2 − 1 ) ( λ i + 1 ) 4 ≥ 0 ，即 ( a 0 ) ′ G 1 a 0 是正定的。定理得证。</p><p>定理2. 若 k 4 ( λ i + 1 ) 4 − ( d − 1 ) 2 ( λ i + k ) 4 &gt; 0 则 ‖ B i a s ( β ˜ ( 1 ) ( d ) ) ‖ 2 &lt; ‖ B i a s ( β ˜ ( 1 ) ( k ) ) ‖ 2 。</p><p>证明：由等式(16)得，一阶近似刀切岭估计的偏差为：</p><p>b i a s ( β ˜ ( 1 ) ( k ) ) = − k 2 ( X ′ V ^ ( 0 ) X + k I ) − 2 a 0 (22)</p><p>则</p><p>‖ B i a s ( β ˜ ( 1 ) ( k ) ) ‖ 2 − ‖ B i a s ( β ˜ ( 1 ) ( d ) ) ‖ 2 = ( a 0 ) ′ G 2 a 0</p><p>其中 G 2 = k 4 ( X ′ V ^ ( 0 ) X + k I ) − 4 − ( d − 1 ) 2 ( X ′ V ^ ( 0 ) X + I ) − 4 ， G 2 是对角元素为 k 4 ( λ i + 1 ) 4 − ( d − 1 ) 2 ( λ i + k ) 4 ( λ i + 1 ) 4 ( λ i + k ) 4 的对角矩阵，所以当 k 4 ( λ i + 1 ) 4 − ( d − 1 ) 2 ( λ i + k ) 4 &gt; 0 时 k 4 ( λ i + 1 ) 4 − ( d − 1 ) 2 ( λ i + k ) 4 ( λ i + 1 ) 4 ( λ i + k ) 4 &gt; 0 ，即 ( a 0 ) ′ G 2 a 0 是正定的。定理得证。</p><p>定理3. 当 0 &lt; d &lt; 1 时，一阶近似刀切Liu估计在MSEM准则下优于一阶近似极大似然估计当且仅当</p><p>( 1 − d ) ( a 0 ) ′ { 2 Λ + 4 I + ( d + 1 ) Λ − 1 } − 1 a 0 &lt; 1 。</p><p>证明：令 M 1 ( d ) = M S E M ( β ^ ( 1 ) ( M L ) ) − M S E M ( β ˜ ( 1 ) ( d ) ) ，由公式(18)可得：</p><p>M 1 ( d ) = M 1 − ( d − 1 ) 2 ( Λ + I ) − 2 a 0 ( a 0 ) ′ ( Λ + I ) − 2</p><p>其中</p><p>M 1 = Λ − 1 − { ( I − ( Λ + I ) − 1 Λ ) ( Λ + I ) − 1 ( Λ + d I ) + ( Λ + I ) − 1 Λ } Λ − 1     ⋅ { ( Λ + d I ) ( Λ + I ) − 1 ( I − ( Λ + I ) − 1 Λ ) + Λ ( Λ + I ) − 1 } = ( 1 − d ) ( Λ + I ) − 2 { 2 Λ + 4 I + ( d + 1 ) Λ − 1 } ( Λ + I ) − 2</p><p>因此 M 1 ( d ) 是正定的当且仅当 M 1 − ( d − 1 ) 2 ( Λ + I ) − 2 a 0 ( a 0 ) ′ ( Λ + I ) − 2 正定。由引理1可得，当 0 &lt; d &lt; 1 ， ( 1 − d ) ( a 0 ) ′ { 2 Λ + 4 I + ( d + 1 ) Λ − 1 } − 1 a 0 &lt; 1 时， M 1 − ( d − 1 ) 2 ( Λ + I ) − 2 a 0 ( a 0 ) ′ ( Λ + I ) − 2 是正定的。定理得证。</p><p>定理4. 一阶近似刀切Liu估计在MSEM准则下优于一阶近似Liu估计当且仅当</p><p>( d − 1 ) ( a 0 ) ′ { 2 Λ 2 + ( d + 3 ) Λ + 2 d I } − 1 a 0 &lt; 1 。</p><p>证明：令 M 2 ( d ) = M S E M ( β ^ ( 1 ) ( d ) ) − M S E M ( β ˜ ( 1 ) ( d ) ) ，由公式(18)可得：</p><p>M 2 ( d ) = M 2 + ( d − 1 ) 2 ( Λ + I ) − 1 a 0 ( a 0 ) ′ ( Λ + I ) − 1 − ( d − 1 ) 2 ( Λ + I ) − 2 a 0 ( a 0 ) ′ ( Λ + I ) − 2</p><p>其中</p><p>M 2 = { ( Λ + I ) − 1 ( Λ + d I ) } Λ − 1 { ( Λ + d I ) ( Λ + I ) − 1 } − { ( I − ( Λ + I ) − 1 Λ ) ( Λ + I ) − 1 ( Λ + d I )     + ( Λ + I ) − 1 Λ } Λ − 1 { ( Λ + d I ) ( Λ + I ) − 1 ( I − ( Λ + I ) − 1 Λ ) + Λ ( Λ + I ) − 1 } = ( d − 1 ) ( Λ + I ) − 2 { 2 Λ 2 + ( d + 3 ) Λ + 2 d I } ( Λ + I ) − 2</p><p>因为 ( d − 1 ) 2 ( Λ + I ) − 1 a 0 ( a 0 ) ′ ( Λ + I ) − 1 是正定的，因此 M 2 ( d ) 是正定的当且仅当 M 2 − ( d − 1 ) 2 ( Λ + I ) − 2 a 0 ( a 0 ) ′ ( Λ + I ) − 2 正定。由引理1可得，当 0 &lt; d &lt; 1 ， ( d − 1 ) ( a 0 ) ′ { 2 Λ 2 + ( d + 3 ) Λ + 2 d I } − 1 a 0 &lt; 1 时， M 2 − ( d − 1 ) 2 ( Λ + I ) − 2 a 0 ( a 0 ) ′ ( Λ + I ) − 2 是正定的。定理得证。</p><p>定理5. 对于任意的i，如果 0 &lt; d ≤ ( a i 0 ) 2 − ( 2 λ i + 3 ) / ( λ i + 2 ) ( a i 0 ) 2 + 1 / λ i &lt; 1 ，则 M S E ( β ˜ ( 1 ) ( d ) ) ≤ M S E ( β ^ ( 1 ) ( d ) ) 。</p><p>证明：根据等式(19)可得出：</p><p>M S E ( β ˜ ( 1 ) ( d ) ) = ∑ i = 1 p ( λ i 2 + 2 λ i + d ) 2 + ( d − 1 ) 2 ( a i 0 ) 2 λ i λ i ( λ i + 1 ) 4 (23)</p><p>M S E ( β ^ ( 1 ) ( d ) ) = ∑ i = 1 p ( λ i + d ) 2 + ( d − 1 ) 2 ( a i 0 ) 2 λ i λ i ( λ i + 1 ) 2 (24)</p><p>它们的差：</p><p>M S E ( β ˜ ( 1 ) ( d ) ) − M S E ( β ^ ( 1 ) ( d ) ) = ∑ i = 1 p ( λ i 2 + 2 λ i + d ) 2 + ( d − 1 ) 2 ( a i 0 ) 2 λ i λ i ( λ i + 1 ) 4 − ∑ i = 1 p ( λ i + d ) 2 + ( d − 1 ) 2 ( a i 0 ) 2 λ i λ i ( λ i + 1 ) 2 = ∑ i = 1 p { ( λ i 2 + 2 λ i + d ) 2 − ( λ i + 1 ) 2 + ( d − 1 ) 2 ( λ i + d ) 2 λ i ( λ i + 1 ) 4 + ( d − 1 ) 2 ( a i 0 ) 2 ( λ i + 1 ) 4 − ( d − 1 ) 2 ( a i 0 ) 2 ( λ i + 1 ) 2 } = ( 1 − d ) ∑ i = 1 p 2 λ i 2 + ( d + 3 ) λ i + 2 d ( λ i + 1 ) 4 + ( d − 1 ) 2 ∑ i = 1 p ( a i 0 ) 2 ( λ i + 1 ) 4 − ( d − 1 ) 2 ∑ i = 1 p ( a i 0 ) 2 ( λ i + 1 ) 2 = ( 1 − d ) ∑ i = 1 p 1 ( λ i + 1 ) 4 f i ( d )</p><p>其中 f i ( d ) = λ i ( 2 λ i + 3 ) − α i 0 λ i ( λ i + 2 ) + d ( λ i + 2 ) ( 1 + λ i ( a i 0 ) 2 ) 。</p><p>当 1 &lt; ( a i 0 ) 2 ( λ i + 2 ) / ( 2 λ i + 3 ) ， d ≤ ( a i 0 ) 2 − ( 2 λ i + 3 ) / ( λ i + 2 ) ( a i 0 ) 2 + 1 / λ i 时 f i ( d ) ≤ 0 。因为 0 &lt; d &lt; 1 故 M S E ( β ˜ ( 1 ) ( d ) ) − M S E ( β ^ ( 1 ) ( d ) ) ≤ 0 。定理得证。</p><p>由定理5，我们可以得出如下两个推论：</p><p>推论1. 假设</p><p>0 &lt; d ≤ min { ( a i 0 ) 2 − ( 2 λ i + 3 ) / ( λ i + 2 ) ( a i 0 ) 2 + 1 / λ i } &lt; 1</p><p>则 M S E ( β ˜ ( d ) ) ≤ M S E ( β ^ ( d ) ) 。</p><p>推论2. 假设</p><p>max { 0 , ( a i 0 ) 2 − ( 2 λ i + 3 ) / ( λ i + 2 ) ( a i 0 ) 2 + 1 / λ i } &lt; d &lt; 1</p><p>则 M S E ( β ˜ ( 1 ) ( d ) ) &gt; M S E ( β ^ ( 1 ) ( d ) ) 。</p></sec><sec id="s8"><title>4. 蒙特卡罗模拟</title><p>为了进一步对理论成果进行说明，针对不同的复共线性程度及不同的自相关程度，本节我们用Monte Carlo模拟方法探讨上述各类估计在偏差和均方误差准则下的优良性。解释变量的数据产生采用与McDonald和Galarneau [<xref ref-type="bibr" rid="hanspub.41341-ref10">10</xref>] 和Kibria [<xref ref-type="bibr" rid="hanspub.41341-ref11">11</xref>] 相同的方法，即由以下方程生成：</p><p>x i j = ( 1 − ρ 2 ) 1 / 2 z i j + ρ z i p + 1 , i = 1 , ⋯ , n ; j = 1 , ⋯ , p (25)</p><p>其中， z i j 是标准正态随机变量产生的随机数； ρ 是给定的常数； ρ 2 表示两个不同解释变量之间的相关性，因而 ρ 2 某种程度上体现了模型复共线性的程度。在模拟实验中，我们取协变量的数目 p = 4 和 p = 6 ，样本数n考虑100、150和200三种情况， ρ 考虑0.85、0.9、0.95和0.99四种不同的情况。偏参数d我们考虑取0.1、0.3、0.5、0.7和0.9五种不同的取值。</p><p>响应变量对应的随机数来自伯努利分布 B e ( π i ) ，其中 π i = exp ( x ′ i β ) 1 + exp ( x ′ i β ) 。对于系数向量 β ，采用与Kibria [<xref ref-type="bibr" rid="hanspub.41341-ref11">11</xref>] 相同的方法，对其做一定的限制，使其满足 ∑ j = 1 p β j 2 = 1 。本次模拟重复2000次。估计的MSE可以通过以下式子得到：</p><p>M S E ( β ^ ) = 1 2000 ∑ m = 1 2000 t r ( M S E M ( β ^ ( m ) ) ) (26)</p><p>其中 β ^ ( m ) 是估计 β ^ 的第m次所得的估计值。模拟结果见表1~表4。</p><p>观察表1和表2可以看到，在不同复共线性程度、样本量、协变量的数目和偏参数d的情况下，一阶近似刀切Liu估计的均方误差值小于极大似然估计和一阶近似极大似然估计的均方误差值，即一阶近似刀切Liu估计在均方误差准则下优于极大似然估计和一阶近似极大似然估计。同时由表1和表2可以看出，当偏参数d取0.1时一阶近似刀切Liu估计的均方误差值小于d取0.3、0.5、0.7和0.9时一阶近似刀切Liu估计的均方误差值。当固定给定的d、n和p值时，各估计的均方误差值随着复共线性程度 ρ 的增大而增大。当固定给定的d、n和 ρ 值时，各估计的均方误差值随着协变量的数目p的增大而增大。当固定给定的d、p和 ρ 值时，各估计的均方误差值随着样本量n的增大而减小。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Estimated MSE values of the MLE, FAE and FAJL when p = </title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >d</th><th align="center" valign="middle"  rowspan="2"  ></th><th align="center" valign="middle"  rowspan="2"  >MLE</th><th align="center" valign="middle"  rowspan="2"  >FAE</th><th align="center" valign="middle"  colspan="5"  >FAJL</th></tr></thead><tr><td align="center" valign="middle" >0.1</td><td align="center" valign="middle" >0.3</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >0.7</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.85</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >1.3927</td><td align="center" valign="middle" >1.2171</td><td align="center" valign="middle" >1.0357</td><td align="center" valign="middle" >1.0733</td><td align="center" valign="middle" >1.1125</td><td align="center" valign="middle" >1.1532</td><td align="center" valign="middle" >1.1954</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >0.8523</td><td align="center" valign="middle" >0.7907</td><td align="center" valign="middle" >0.7255</td><td align="center" valign="middle" >0.7395</td><td align="center" valign="middle" >0.7538</td><td align="center" valign="middle" >0.7684</td><td align="center" valign="middle" >0.7832</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.6223</td><td align="center" valign="middle" >0.5851</td><td align="center" valign="middle" >0.5551</td><td align="center" valign="middle" >0.5616</td><td align="center" valign="middle" >0.5683</td><td align="center" valign="middle" >0.5751</td><td align="center" valign="middle" >0.5819</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >2.0610</td><td align="center" valign="middle" >1.8175</td><td align="center" valign="middle" >1.3578</td><td align="center" valign="middle" >1.4478</td><td align="center" valign="middle" >1.5448</td><td align="center" valign="middle" >1.6487</td><td align="center" valign="middle" >1.7595</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >1.2784</td><td align="center" valign="middle" >1.1843</td><td align="center" valign="middle" >0.9980</td><td align="center" valign="middle" >1.0366</td><td align="center" valign="middle" >1.0768</td><td align="center" valign="middle" >1.1186</td><td align="center" valign="middle" >1.1620</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.9237</td><td align="center" valign="middle" >0.8726</td><td align="center" valign="middle" >0.7810</td><td align="center" valign="middle" >0.8005</td><td align="center" valign="middle" >0.8205</td><td align="center" valign="middle" >0.8410</td><td align="center" valign="middle" >0.8620</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.95</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >4.1806</td><td align="center" valign="middle" >3.6570</td><td align="center" valign="middle" >1.9873</td><td align="center" valign="middle" >2.2635</td><td align="center" valign="middle" >2.5939</td><td align="center" valign="middle" >2.9735</td><td align="center" valign="middle" >3.4173</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >2.5668</td><td align="center" valign="middle" >2.3768</td><td align="center" valign="middle" >1.5618</td><td align="center" valign="middle" >1.7121</td><td align="center" valign="middle" >1.8800</td><td align="center" valign="middle" >2.0655</td><td align="center" valign="middle" >2.2687</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >1.8695</td><td align="center" valign="middle" >1.7673</td><td align="center" valign="middle" >1.2976</td><td align="center" valign="middle" >1.3890</td><td align="center" valign="middle" >1.4878</td><td align="center" valign="middle" >1.5940</td><td align="center" valign="middle" >1.7077</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.99</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >22.4064</td><td align="center" valign="middle" >19.5469</td><td align="center" valign="middle" >5.6987</td><td align="center" valign="middle" >6.5321</td><td align="center" valign="middle" >8.6477</td><td align="center" valign="middle" >12.0457</td><td align="center" valign="middle" >16.7260</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >13.6913</td><td align="center" valign="middle" >12.6788</td><td align="center" valign="middle" >4.0059</td><td align="center" valign="middle" >4.7671</td><td align="center" valign="middle" >6.1946</td><td align="center" valign="middle" >8.2885</td><td align="center" valign="middle" >11.0488</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >9.9752</td><td align="center" valign="middle" >9.4003</td><td align="center" valign="middle" >3.2597</td><td align="center" valign="middle" >3.9126</td><td align="center" valign="middle" >4.9722</td><td align="center" valign="middle" >6.4384</td><td align="center" valign="middle" >8.3113</td></tr></tbody></table></table-wrap><p>表1. 当p = 4时，估计MLE、FAE和FAJL的MSE</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Estimated MSE values of the MLE, FAE and FAJL when p = </title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >d</th><th align="center" valign="middle"  rowspan="2"  ></th><th align="center" valign="middle"  rowspan="2"  >MLE</th><th align="center" valign="middle"  rowspan="2"  >FAE</th><th align="center" valign="middle"  colspan="5"  >FAJL</th></tr></thead><tr><td align="center" valign="middle" >0.1</td><td align="center" valign="middle" >0.3</td><td align="center" valign="middle" >0.5</td><td align="center" valign="middle" >0.7</td><td align="center" valign="middle" >0.9</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.85</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >2.6568</td><td align="center" valign="middle" >2.2046</td><td align="center" valign="middle" >1.7519</td><td align="center" valign="middle" >1.8419</td><td align="center" valign="middle" >1.9381</td><td align="center" valign="middle" >2.0403</td><td align="center" valign="middle" >2.1484</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >1.5989</td><td align="center" valign="middle" >1.4216</td><td align="center" valign="middle" >1.2474</td><td align="center" valign="middle" >1.2838</td><td align="center" valign="middle" >1.3215</td><td align="center" valign="middle" >1.3606</td><td align="center" valign="middle" >1.4009</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >1.1273</td><td align="center" valign="middle" >1.0448</td><td align="center" valign="middle" >0.9611</td><td align="center" valign="middle" >0.9790</td><td align="center" valign="middle" >0.9973</td><td align="center" valign="middle" >1.0160</td><td align="center" valign="middle" >1.0351</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >4.1114</td><td align="center" valign="middle" >3.3823</td><td align="center" valign="middle" >2.2859</td><td align="center" valign="middle" >2.4872</td><td align="center" valign="middle" >2.7127</td><td align="center" valign="middle" >2.9624</td><td align="center" valign="middle" >3.2363</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >2.4261</td><td align="center" valign="middle" >2.1629</td><td align="center" valign="middle" >1.6946</td><td align="center" valign="middle" >1.7871</td><td align="center" valign="middle" >1.8862</td><td align="center" valign="middle" >1.9919</td><td align="center" valign="middle" >2.1042</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >1.7291</td><td align="center" valign="middle" >1.5959</td><td align="center" valign="middle" >1.3494</td><td align="center" valign="middle" >1.4000</td><td align="center" valign="middle" >1.4530</td><td align="center" valign="middle" >1.5084</td><td align="center" valign="middle" >1.5661</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.95</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >8.5367</td><td align="center" valign="middle" >7.0108</td><td align="center" valign="middle" >3.3480</td><td align="center" valign="middle" >3.8997</td><td align="center" valign="middle" >4.6013</td><td align="center" valign="middle" >5.4527</td><td align="center" valign="middle" >6.4540</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >5.0352</td><td align="center" valign="middle" >4.5013</td><td align="center" valign="middle" >2.6422</td><td align="center" valign="middle" >2.9568</td><td align="center" valign="middle" >3.3277</td><td align="center" valign="middle" >3.7549</td><td align="center" valign="middle" >4.2384</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >3.5709</td><td align="center" valign="middle" >3.3047</td><td align="center" valign="middle" >2.1915</td><td align="center" valign="middle" >2.3932</td><td align="center" valign="middle" >2.6210</td><td align="center" valign="middle" >2.8749</td><td align="center" valign="middle" >3.1549</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.99</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >46.5859</td><td align="center" valign="middle" >38.2250</td><td align="center" valign="middle" >9.5522</td><td align="center" valign="middle" >11.3917</td><td align="center" valign="middle" >15.8210</td><td align="center" valign="middle" >22.8402</td><td align="center" valign="middle" >32.4493</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >27.3406</td><td align="center" valign="middle" >24.4286</td><td align="center" valign="middle" >6.6708</td><td align="center" valign="middle" >8.2228</td><td align="center" valign="middle" >11.1429</td><td align="center" valign="middle" >15.4311</td><td align="center" valign="middle" >21.0874</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >19.5496</td><td align="center" valign="middle" >18.0425</td><td align="center" valign="middle" >5.4978</td><td align="center" valign="middle" >6.7696</td><td align="center" valign="middle" >8.9077</td><td align="center" valign="middle" >11.9120</td><td align="center" valign="middle" >15.7824</td></tr></tbody></table></table-wrap><p>表2. 当p = 6时，估计MLE、FAE和FAJL的MSE</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> The sum of squares of the bias values of the MLE, FAE and FAJL when p = </title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >d</th><th align="center" valign="middle"  rowspan="2"  ></th><th align="center" valign="middle"  colspan="2"  >0.1</th><th align="center" valign="middle"  colspan="2"  >0.3</th><th align="center" valign="middle"  colspan="2"  >0.5</th><th align="center" valign="middle"  colspan="2"  >0.7</th><th align="center" valign="middle"  colspan="2"  >0.9</th></tr></thead><tr><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.85</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.0315</td><td align="center" valign="middle" >0.0040</td><td align="center" valign="middle" >0.0190</td><td align="center" valign="middle" >0.0024</td><td align="center" valign="middle" >0.0097</td><td align="center" valign="middle" >0.0012</td><td align="center" valign="middle" >0.0035</td><td align="center" valign="middle" >0.0004</td><td align="center" valign="middle" >0.0003</td><td align="center" valign="middle" >0.0000</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >0.0110</td><td align="center" valign="middle" >0.0007</td><td align="center" valign="middle" >0.0066</td><td align="center" valign="middle" >0.0004</td><td align="center" valign="middle" >0.0033</td><td align="center" valign="middle" >0.0002</td><td align="center" valign="middle" >0.0012</td><td align="center" valign="middle" >0.0000</td><td align="center" valign="middle" >0.0001</td><td align="center" valign="middle" >0.0000</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.0049</td><td align="center" valign="middle" >0.0002</td><td align="center" valign="middle" >0.0030</td><td align="center" valign="middle" >0.0001</td><td align="center" valign="middle" >0.0015</td><td align="center" valign="middle" >0.0000</td><td align="center" valign="middle" >0.0005</td><td align="center" valign="middle" >0.0000</td><td align="center" valign="middle" >0.0000</td><td align="center" valign="middle" >0.0000</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.0849</td><td align="center" valign="middle" >0.0810</td><td align="center" valign="middle" >0.0513</td><td align="center" valign="middle" >0.0109</td><td align="center" valign="middle" >0.0262</td><td align="center" valign="middle" >0.0055</td><td align="center" valign="middle" >0.0094</td><td align="center" valign="middle" >0.0020</td><td align="center" valign="middle" >0.0010</td><td align="center" valign="middle" >0.0002</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >0.0313</td><td align="center" valign="middle" >0.0040</td><td align="center" valign="middle" >0.0189</td><td align="center" valign="middle" >0.0024</td><td align="center" valign="middle" >0.0096</td><td align="center" valign="middle" >0.0012</td><td align="center" valign="middle" >0.0034</td><td align="center" valign="middle" >0.0004</td><td align="center" valign="middle" >0.0038</td><td align="center" valign="middle" >0.0000</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.0152</td><td align="center" valign="middle" >0.0013</td><td align="center" valign="middle" >0.0092</td><td align="center" valign="middle" >0.0007</td><td align="center" valign="middle" >0.0046</td><td align="center" valign="middle" >0.0004</td><td align="center" valign="middle" >0.0016</td><td align="center" valign="middle" >0.0001</td><td align="center" valign="middle" >0.0001</td><td align="center" valign="middle" >0.0000</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.95</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.3589</td><td align="center" valign="middle" >0.1426</td><td align="center" valign="middle" >0.2171</td><td align="center" valign="middle" >0.0863</td><td align="center" valign="middle" >0.1107</td><td align="center" valign="middle" >0.0440</td><td align="center" valign="middle" >0.0398</td><td align="center" valign="middle" >0.0158</td><td align="center" valign="middle" >0.0044</td><td align="center" valign="middle" >0.0017</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >0.1593</td><td align="center" valign="middle" >0.0464</td><td align="center" valign="middle" >0.0964</td><td align="center" valign="middle" >0.0281</td><td align="center" valign="middle" >0.0491</td><td align="center" valign="middle" >0.0143</td><td align="center" valign="middle" >0.0177</td><td align="center" valign="middle" >0.0051</td><td align="center" valign="middle" >0.0019</td><td align="center" valign="middle" >0.0005</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.0889</td><td align="center" valign="middle" >0.0199</td><td align="center" valign="middle" >0.0538</td><td align="center" valign="middle" >0.0120</td><td align="center" valign="middle" >0.0274</td><td align="center" valign="middle" >0.0061</td><td align="center" valign="middle" >0.0098</td><td align="center" valign="middle" >0.0022</td><td align="center" valign="middle" >0.0010</td><td align="center" valign="middle" >0.0002</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.99</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >4.4535</td><td align="center" valign="middle" >3.4889</td><td align="center" valign="middle" >2.6941</td><td align="center" valign="middle" >2.1106</td><td align="center" valign="middle" >1.3745</td><td align="center" valign="middle" >1.0768</td><td align="center" valign="middle" >0.4948</td><td align="center" valign="middle" >0.3876</td><td align="center" valign="middle" >0.0549</td><td align="center" valign="middle" >0.0430</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >2.5537</td><td align="center" valign="middle" >1.8134</td><td align="center" valign="middle" >1.5448</td><td align="center" valign="middle" >1.0969</td><td align="center" valign="middle" >0.7881</td><td align="center" valign="middle" >0.5596</td><td align="center" valign="middle" >0.2837</td><td align="center" valign="middle" >0.2014</td><td align="center" valign="middle" >0.0315</td><td align="center" valign="middle" >0.0223</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >1.7017</td><td align="center" valign="middle" >1.1097</td><td align="center" valign="middle" >1.0294</td><td align="center" valign="middle" >0.6713</td><td align="center" valign="middle" >0.5252</td><td align="center" valign="middle" >0.3425</td><td align="center" valign="middle" >0.1890</td><td align="center" valign="middle" >0.1233</td><td align="center" valign="middle" >0.0210</td><td align="center" valign="middle" >0.0137</td></tr></tbody></table></table-wrap><p>表3. 当p = 4时，估计FAL和 FAJL的偏差的平方和</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> The sum of squares of the bias values of the MLE, FAE and FAJL when p = </title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >d</th><th align="center" valign="middle"  rowspan="2"  ></th><th align="center" valign="middle"  colspan="2"  >0.1</th><th align="center" valign="middle"  colspan="2"  >0.3</th><th align="center" valign="middle"  colspan="2"  >0.5</th><th align="center" valign="middle"  colspan="2"  >0.7</th><th align="center" valign="middle"  colspan="2"  >0.9</th></tr></thead><tr><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >FAJL</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.85</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.0683</td><td align="center" valign="middle" >0.0134</td><td align="center" valign="middle" >0.0413</td><td align="center" valign="middle" >0.0081</td><td align="center" valign="middle" >0.0210</td><td align="center" valign="middle" >0.0041</td><td align="center" valign="middle" >0.0075</td><td align="center" valign="middle" >0.0014</td><td align="center" valign="middle" >0.0008</td><td align="center" valign="middle" >0.0001</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >0.0246</td><td align="center" valign="middle" >0.0029</td><td align="center" valign="middle" >0.0148</td><td align="center" valign="middle" >0.0017</td><td align="center" valign="middle" >0.0075</td><td align="center" valign="middle" >0.0008</td><td align="center" valign="middle" >0.0027</td><td align="center" valign="middle" >0.0003</td><td align="center" valign="middle" >0.0003</td><td align="center" valign="middle" >0.0000</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.0118</td><td align="center" valign="middle" >0.0009</td><td align="center" valign="middle" >0.0071</td><td align="center" valign="middle" >0.0005</td><td align="center" valign="middle" >0.0036</td><td align="center" valign="middle" >0.0002</td><td align="center" valign="middle" >0.0013</td><td align="center" valign="middle" >0.0001</td><td align="center" valign="middle" >0.0001</td><td align="center" valign="middle" >0.0000</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.9</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.1748</td><td align="center" valign="middle" >0.0541</td><td align="center" valign="middle" >0.1057</td><td align="center" valign="middle" >0.0327</td><td align="center" valign="middle" >0.0539</td><td align="center" valign="middle" >0.0167</td><td align="center" valign="middle" >0.0194</td><td align="center" valign="middle" >0.0060</td><td align="center" valign="middle" >0.0021</td><td align="center" valign="middle" >0.0006</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >0.0717</td><td align="center" valign="middle" >0.0152</td><td align="center" valign="middle" >0.0434</td><td align="center" valign="middle" >0.0092</td><td align="center" valign="middle" >0.0221</td><td align="center" valign="middle" >0.0047</td><td align="center" valign="middle" >0.0079</td><td align="center" valign="middle" >0.0016</td><td align="center" valign="middle" >0.0008</td><td align="center" valign="middle" >0.0001</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.0343</td><td align="center" valign="middle" >0.0051</td><td align="center" valign="middle" >0.0208</td><td align="center" valign="middle" >0.0031</td><td align="center" valign="middle" >0.0106</td><td align="center" valign="middle" >0.0015</td><td align="center" valign="middle" >0.0038</td><td align="center" valign="middle" >0.0005</td><td align="center" valign="middle" >0.0004</td><td align="center" valign="middle" >0.0000</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.95</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >0.6484</td><td align="center" valign="middle" >0.3266</td><td align="center" valign="middle" >0.3922</td><td align="center" valign="middle" >0.1976</td><td align="center" valign="middle" >0.2001</td><td align="center" valign="middle" >0.1008</td><td align="center" valign="middle" >0.0720</td><td align="center" valign="middle" >0.0362</td><td align="center" valign="middle" >0.0080</td><td align="center" valign="middle" >0.0040</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >0.3162</td><td align="center" valign="middle" >0.1267</td><td align="center" valign="middle" >0.1913</td><td align="center" valign="middle" >0.0766</td><td align="center" valign="middle" >0.0976</td><td align="center" valign="middle" >0.0391</td><td align="center" valign="middle" >0.0351</td><td align="center" valign="middle" >0.0140</td><td align="center" valign="middle" >0.0039</td><td align="center" valign="middle" >0.0015</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >0.1779</td><td align="center" valign="middle" >0.0582</td><td align="center" valign="middle" >0.1076</td><td align="center" valign="middle" >0.0352</td><td align="center" valign="middle" >0.0549</td><td align="center" valign="middle" >0.0179</td><td align="center" valign="middle" >0.0197</td><td align="center" valign="middle" >0.0064</td><td align="center" valign="middle" >0.0021</td><td align="center" valign="middle" >0.0007</td></tr><tr><td align="center" valign="middle" >ρ</td><td align="center" valign="middle" >0.99</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle"  rowspan="3"  >n</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >7.2163</td><td align="center" valign="middle" >5.8941</td><td align="center" valign="middle" >4.3654</td><td align="center" valign="middle" >3.5656</td><td align="center" valign="middle" >2.2272</td><td align="center" valign="middle" >1.8191</td><td align="center" valign="middle" >0.8018</td><td align="center" valign="middle" >0.6549</td><td align="center" valign="middle" >0.0890</td><td align="center" valign="middle" >0.0727</td></tr><tr><td align="center" valign="middle" >150</td><td align="center" valign="middle" >4.0267</td><td align="center" valign="middle" >3.0332</td><td align="center" valign="middle" >2.4359</td><td align="center" valign="middle" >1.8349</td><td align="center" valign="middle" >1.2428</td><td align="center" valign="middle" >0.9361</td><td align="center" valign="middle" >0.4474</td><td align="center" valign="middle" >0.3370</td><td align="center" valign="middle" >0.0497</td><td align="center" valign="middle" >0.0374</td></tr><tr><td align="center" valign="middle" >200</td><td align="center" valign="middle" >2.7304</td><td align="center" valign="middle" >1.9266</td><td align="center" valign="middle" >1.6517</td><td align="center" valign="middle" >1.1655</td><td align="center" valign="middle" >0.8427</td><td align="center" valign="middle" >0.5946</td><td align="center" valign="middle" >0.3033</td><td align="center" valign="middle" >0.2140</td><td align="center" valign="middle" >0.0337</td><td align="center" valign="middle" >0.0237</td></tr></tbody></table></table-wrap><p>表4. 当p = 6时，估计FAL和 FAJL的偏差的平方和</p><p>根据表3和表4可知，在不同复共线性程度、样本量、协变量的数目和偏参数的情况下，一阶近似刀切Liu估计的偏差的平方和始终小于一阶近似Liu估计的偏差的平方和。且当偏参数d取0.9时一阶近似Liu估计和一阶近似刀切Liu估计的偏差的平方和小于d取0.1、0.3、0.5和0.7时一阶近似Liu估计和一阶近似刀切Liu估计的偏差的平方和。当固定给定的d、n和p值时，各估计的偏差的平方和随着复共线性程度 ρ 的增大而增大。当固定给定的d、n和 ρ 值时，各估计的偏差的平方和随着协变量的数目p的增大而增大。当固定给定的d、p和 ρ 值时，各估计的偏差的平方和随着样本量n的增大而减小。</p></sec><sec id="s9"><title>5. 实证分析</title><p>为了验证我们的理论结果，这部分我们考虑实例来分析所提出估计的优良性。我们所使用的数据来自Agresti Alan [<xref ref-type="bibr" rid="hanspub.41341-ref12">12</xref>]。数据涉及Heinze和Schemper [<xref ref-type="bibr" rid="hanspub.41341-ref13">13</xref>] 所描述的一种关于子宫内膜癌的研究。分析了79个案例，因变量y为组织学分级(低分级y取0，高分级时 y = 1 )，其中低分级的病人有30个，高分级患者有49个。涉及的三个风险因素： x 1 为新血管生成 (有： x 1 = 1 ，无： x 1 = 0 )， x 2 为子宫动脉搏动指数(取值范围在0到49之间)， x 3 为子宫内膜高度(取值范围在0.27到3.61之间)。</p><p>迭代的计算精度δ我们取10<sup>−6</sup>，得到矩阵 X ′ V ^ X 的特征值 λ 1 = 3068.0790 ， λ 2 = 7.1753 ， λ 3 = 0.3069 ， λ 4 = 1.2496 &#215; 10 − 7 。条件数 κ = λ max / λ min = 156687.8 ，因此可以判断数据集存在严重的复共线性问题。</p><p>为了对我们所提的新估计一阶近似刀切Liu估计的优良性进行研究。我们得到极大似然估计、一阶近似极大似然估计和一阶近似刀切Liu估计的均方误差值，一阶近似Liu估计和一阶近似刀切Liu估计偏差的平方值和，如表5和表6：</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Estimated MSE values of the MLE, FAE and FAJ</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >d</th><th align="center" valign="middle" >0.1</th><th align="center" valign="middle" >0.3</th><th align="center" valign="middle" >0.5</th><th align="center" valign="middle" >0.7</th><th align="center" valign="middle" >0.9</th></tr></thead><tr><td align="center" valign="middle" >MLE</td><td align="center" valign="middle" >8002097</td><td align="center" valign="middle" >8002097</td><td align="center" valign="middle" >8002097</td><td align="center" valign="middle" >8002097</td><td align="center" valign="middle" >8002097</td></tr><tr><td align="center" valign="middle" >FAE</td><td align="center" valign="middle" >84.2875</td><td align="center" valign="middle" >84.2875</td><td align="center" valign="middle" >84.2875</td><td align="center" valign="middle" >84.2875</td><td align="center" valign="middle" >84.2875</td></tr><tr><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >7.4009</td><td align="center" valign="middle" >13.1352</td><td align="center" valign="middle" >25.3562</td><td align="center" valign="middle" >44.0637</td><td align="center" valign="middle" >69.2580</td></tr></tbody></table></table-wrap><p>表5. 估计MLE、FAE和FAJL的MSE</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> The sum of squares of the bias values of the FAL and FAJL when p = </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >d</th><th align="center" valign="middle" >0.1</th><th align="center" valign="middle" >0.3</th><th align="center" valign="middle" >0.5</th><th align="center" valign="middle" >0.7</th><th align="center" valign="middle" >0.9</th></tr></thead><tr><td align="center" valign="middle" >FAL</td><td align="center" valign="middle" >5.4893</td><td align="center" valign="middle" >3.3207</td><td align="center" valign="middle" >1.6942</td><td align="center" valign="middle" >0.6099</td><td align="center" valign="middle" >0.0677</td></tr><tr><td align="center" valign="middle" >FAJL</td><td align="center" valign="middle" >4.6307</td><td align="center" valign="middle" >2.8013</td><td align="center" valign="middle" >1.4292</td><td align="center" valign="middle" >0.5145</td><td align="center" valign="middle" >0.0571</td></tr></tbody></table></table-wrap><p>表6. 估计FAL和FAJL的偏差的平方和</p><p>通过表5我们可以看出，对于给定的d值，新估计一阶近似刀切Liu估计的均方误差值小于极大似然估计和一阶近似极大似然估计的均方误差值，且当 d = 0.1 时一阶近似刀切Liu估计的均方误差值 M S E ( β ˜ ( 1 ) ( d ) ) = 7.4009 最小。再由表6我们可以看到一阶近似刀切Liu估计偏差的平方和小于一阶近似Liu估计偏差的平方和，且当 d = 0.9 时一阶近似刀切Liu估计偏差的平方和取值最小 ‖ B i a s ( β ˜ ( 1 ) ( d ) ) ‖ 2 = 0.0571 ，同时对我们所得理论结果定理2进行了验证。</p></sec><sec id="s10"><title>6. 结论</title><p>本文中，针对二元逻辑回归模型中的复共线性问题，我们在一阶近似Liu估计的基础上使用刀切法的思想提出了一个新估计，即一阶近似刀切Liu估计。研究了一阶近似刀切Liu估计的偏差以及在均方误差矩阵和均方误差准则下的优良性。证明并得出了新估计的偏差平方和总是优于一阶近似Liu估计以及新估计优于一阶近似刀切岭估计的充分条件，得出了一阶近似刀切Liu估计在均方误差矩阵、均方误差准则下优于一阶近似极大似然估计、一阶近似Liu估计和一阶近似刀切岭估计的充要或者充分条件。此外，我们使用蒙特卡罗模拟，得到了一阶近似刀切Liu估计在均方误差准则下优于极大似然估计和一阶近似极大似然估计，各估计的均方误差值随着复共线性程度 ρ 的增大而增大，各估计的均方误差值随着协变量的数目p的增大而增大，各估计的均方误差值随着样本量n的增大而减小。然后利用实证分析探讨了一阶近似刀切Liu估计在实际应用中的实现问题，证明一阶近似刀切Liu估计能够有效地解决复共线性问题。</p></sec><sec id="s11"><title>文章引用</title><p>邹 媛. 二元逻辑回归模型中的一阶近似刀切Liu估计A First-Order Approximated Jackknifed Liu Estimator in Binary Logistic Regression Model[J]. 应用数学进展, 2021, 10(03): 790-800. https://doi.org/10.12677/AAM.2021.103087</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41341-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Schaefer, R.L., Roi, L.D. and Wolfe, R.A. (1984) A Ridge Logistic Estimator. Communications in Statistics-Theory and Methods, 13, 99-113. &lt;br&gt;https://doi.org/10.1080/03610928408828664</mixed-citation></ref><ref id="hanspub.41341-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Månsson, K., Golam Kibria, B.M. and Shukur, G. (2012) On Liu Estimators for the Logit Regression Model. Economic Modelling, 29, 1483-1488. &lt;br&gt;https://doi.org/10.1016/j.econmod.2011.11.015</mixed-citation></ref><ref id="hanspub.41341-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">LeCessie, S. and VanHouwelingen, J.C. (1992) Ridge Estimators in Logistic Regression. Journal of Applied Statistics, 41, 191-201. &lt;br&gt;https://doi.org/10.2307/2347628</mixed-citation></ref><ref id="hanspub.41341-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Revan Özkale, M. (2016) Iterative Algorithms of Biased Estimation Methods in Binary Logistic Regression. Statistical Papers, 57, 991-1016. &lt;br&gt;https://doi.org/10.1007/s00362-016-0780-9</mixed-citation></ref><ref id="hanspub.41341-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Quenouille, M.H. (1956) Notes on Bias in Estimation. Biometrika, 43, 353-360.  
&lt;br&gt;https://doi.org/10.1093/biomet/43.3-4.353</mixed-citation></ref><ref id="hanspub.41341-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Tukey, J.W. (1958) Bias and Confidence in Not Quite Large Samples (Abstract). Annals of Mathematical Statistics, 29, 614. &lt;br&gt;https://doi.org/10.1214/aoms/1177706635</mixed-citation></ref><ref id="hanspub.41341-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Revan Özkale, M. and Arıcan, E. (2019) A First-Order Approximated Jackknifed Ridge Estimator in Binary Logistic Regression. Computational Statistics, 34, 683-712. &lt;br&gt;https://doi.org/10.1007/s00180-018-0851-6</mixed-citation></ref><ref id="hanspub.41341-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Hinkley, V. (1977) Jackknifing in Unbalanced Situations. Technometrics, 19, 285-292.  
&lt;br&gt;https://doi.org/10.1080/00401706.1977.10489550</mixed-citation></ref><ref id="hanspub.41341-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Farebrother, R.W. (1976) Further Results on the Mean Square Error of Ridge Regression. Journal of the Royal Statistical Society B, 28, 248-250. &lt;br&gt;https://doi.org/10.1111/j.2517-6161.1976.tb01588.x</mixed-citation></ref><ref id="hanspub.41341-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Gary, C., McDonald, D. and, Galarneau, I. (1975) A Monte Carlo Evaluation of Some Ridge-Type Estimators. Journal of the American Statistical Association, 70, 407-416. &lt;br&gt;https://doi.org/10.1080/01621459.1975.10479882</mixed-citation></ref><ref id="hanspub.41341-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Kibria, B.M.G. (2003) Performance of Some New Ridge Regression Estimators. Communications in Statistics Simulation and Computation, 32, 419-435. &lt;br&gt;https://doi.org/10.1081/SAC-120017499</mixed-citation></ref><ref id="hanspub.41341-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Agresti, A. (2015) Foundations of Linear and Generalized Linear Models. Wiley, Hoboken.</mixed-citation></ref><ref id="hanspub.41341-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Heinze, G. and Schemper, M. (2002) A Solution to the Problem of Separation in Logistic Regression. Statistics in Medicine, 21, 2409-2419. &lt;br&gt;https://doi.org/10.1002/sim.1047</mixed-citation></ref></ref-list></back></article>