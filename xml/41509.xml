<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114080</article-id><article-id pub-id-type="publisher-id">CSA-41509</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_20081557.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于轻量级人体姿态估计和图卷积的摔倒实时检测方法
  Real-Time Fall Detection Based on Light-weight Human Pose Estimation and Graph Convolution Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>何</surname><given-names>炜婷</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>曾</surname><given-names>碧</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>文轩</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学计算机学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>783</fpage><lpage>794</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   基于人体姿态估计的摔倒检测方法，因其人体姿态估计模型涉及十几个关节点的识别与处理，导致整体模型的检测速度较慢。为了摔倒检测达到实时性，提出了一种基于轻量级人体姿态估计模型和图卷积的摔倒实时检测方法。该方法首先采用优化后的基于目标检测的两阶段轻量级人体姿态估计模型进行关节点检测，使整体模型达到轻量级；然后使用只有6个特征提取模块的时空图卷积网络对人体关节点序列进行摔倒检测，提高整体模型摔倒检测的准确率。本文通过NTU-D-RGB-120和UR Fall Detection Dataset两个数据集进行实验，摔倒检测的正确率达到96.1%，整体模型在GTX1060Ti显卡中达到约33FPS。 The fall detection based on human pose estimation, because the human pose estimation involves the recognition and processing of more than a dozen joint points, the detection speed of the overall model is slow. In order to achieve real-time fall detection, a real-time fall detection method based on a lightweight human pose estimation and graph convolution network is proposed. The method first uses an optimized two-stage lightweight human pose estimation based on object detection to detect joint points, so that the overall model is lightweight; then uses the spatio-temporal graph convolutional network with only 6 feature extraction modules to perform fall detection on the human joint point sequence to improve the accuracy of the overall model fall detection. This article conducts experiments on two data sets, NTU-D-RGB-120 and UR Fall Detection Dataset, and the accuracy rate of fall detection reaches 96.1%, and the overall model reaches about 33FPS in the GTX1060Ti. 
  
 
</p></abstract><kwd-group><kwd>人体姿态估计，图卷积网络，轻量级，摔倒检测，动作识别, Human Pose Estimation</kwd><kwd> Graph Convolutional Network</kwd><kwd> Lightweight</kwd><kwd> Fall-Down Detection</kwd><kwd> Action Recognition</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>基于人体姿态估计的摔倒检测方法，因其人体姿态估计模型涉及十几个关节点的识别与处理，导致整体模型的检测速度较慢。为了摔倒检测达到实时性，提出了一种基于轻量级人体姿态估计模型和图卷积的摔倒实时检测方法。该方法首先采用优化后的基于目标检测的两阶段轻量级人体姿态估计模型进行关节点检测，使整体模型达到轻量级；然后使用只有6个特征提取模块的时空图卷积网络对人体关节点序列进行摔倒检测，提高整体模型摔倒检测的准确率。本文通过NTU-D-RGB-120和UR Fall Detection Dataset两个数据集进行实验，摔倒检测的正确率达到96.1%，整体模型在GTX1060Ti显卡中达到约33FPS。</p></sec><sec id="s2"><title>关键词</title><p>人体姿态估计，图卷积网络，轻量级，摔倒检测，动作识别</p></sec><sec id="s3"><title>Real-Time Fall Detection Based on Lightweight Human Pose Estimation and Graph Convolution Network</title><p>Weiting He, Bi Zeng, Wenxuan Chen</p><p>School of Computers, Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/1-1542090x4_hanspub.png" /></p><p>Received: Mar. 9<sup>th</sup>, 2021; accepted: Apr. 6<sup>th</sup>, 2021; published: Apr. 13<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/1-1542090x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>The fall detection based on human pose estimation, because the human pose estimation involves the recognition and processing of more than a dozen joint points, the detection speed of the overall model is slow. In order to achieve real-time fall detection, a real-time fall detection method based on a lightweight human pose estimation and graph convolution network is proposed. The method first uses an optimized two-stage lightweight human pose estimation based on object detection to detect joint points, so that the overall model is lightweight; then uses the spatio-temporal graph convolutional network with only 6 feature extraction modules to perform fall detection on the human joint point sequence to improve the accuracy of the overall model fall detection. This article conducts experiments on two data sets, NTU-D-RGB-120 and UR Fall Detection Dataset, and the accuracy rate of fall detection reaches 96.1%, and the overall model reaches about 33FPS in the GTX1060Ti.</p><p>Keywords:Human Pose Estimation, Graph Convolutional Network, Lightweight, Fall-Down Detection, Action Recognition</p><disp-formula id="hanspub.41509-formula2"><graphic xlink:href="//html.hanspub.org/file/1-1542090x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/1-1542090x7_hanspub.png" /> <img src="//html.hanspub.org/file/1-1542090x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>我国开始步入老龄化社会，老年人数量众多。老人因为身体机能下降而容易发生摔倒的情况，如果不及时发现并进行救护，可能会导致一些严重的后果。</p><p>目前，检测摔倒的技术主要有两种方式：通过随身穿戴设备的传感器检测摔倒和通过摄像头的视频检测摔倒。通过穿戴设备检测的方法，需要人随身带着设备，例如手机、手环、腰带等。通过这些设备上的加速度计、陀螺仪等传感器采集的重心变化数据运用机器学习算法进行摔倒检测 [<xref ref-type="bibr" rid="hanspub.41509-ref1">1</xref>]。该检测方法虽然简单有效，但对于一些老人来说，他们会忘记或者不喜欢佩戴这些设备。通过摄像头的视频图像检测的方法则不需要人随身携带设备，但因图像数据本身就相对较大，处理起来较慢，所以检测算法的计算量较大，检测速度也会较慢。如何优化关于视频图像摔倒检测方法的速度是非常值得研究的一个方向。</p><p>通过视频图像检测摔倒的方法有很多种，例如基于边缘轮廓 [<xref ref-type="bibr" rid="hanspub.41509-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.41509-ref3">3</xref>]、基于密集光流 [<xref ref-type="bibr" rid="hanspub.41509-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.41509-ref5">5</xref>]、基于人体姿态估计 [<xref ref-type="bibr" rid="hanspub.41509-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.41509-ref7">7</xref>]、基于三维时空卷积网络(C3D) [<xref ref-type="bibr" rid="hanspub.41509-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.41509-ref9">9</xref>] 等。其中，基于人体姿态估计的方法不止可以用于实现摔倒检测，还可以用于实现其他动作识别、动作分析、人机交互等算法，相对于其他方法来说扩展性更强。此外，该方法不需要考虑环境背景干扰的问题，检测准确率会比其他方法相对较高。但该方法因人体姿态估计算法本身的计算量较大，检测速度相比其他方法较慢，难以达到实时检测的效果，所以本文提出一种基于轻量级人体姿态估计的摔倒实时检测方法。</p><p>本文的主要贡献如下：</p><p>(1) 提出了一种基于目标检测的两阶段轻量级人体姿态估计模型来优化整体摔倒检测的速度；</p><p>(2) 通过消融实验对比分析该人体姿态估计模型各阶段的Loss与精度，验证该模型优化方法的有效性；</p><p>(3) 采用规模缩小后的图卷积网络来提高模型速度和摔倒识别的准确率；</p><p>(4) 在NTU-D-RGB-120和UR Fall Detection Dataset这两个数据集上进行实验，实验结果表明本文方法的优化有效。</p></sec><sec id="s6"><title>2. 相关工作</title><p>基于人体姿态估计的摔倒检测方法主要分两个检测阶段。第一个阶段是人体关节点检测，即人体姿态估计。第二个阶段就是根据检测出来的关节点进行摔倒检测，即分类问题。无论第二阶段使用哪种方法，因为人体姿态估计涉及十几个关节点的识别与处理，整个模型的计算量大部分在人体姿态估计算法这块，所以为了达到实时效果，检测方法的优化重点在于人体姿态估计模型的性能。</p><sec id="s6_1"><title>2.1. 人体姿态估计</title><p>人体姿态估计算法的设计思路主要有两种：自顶向下和自底向上。自顶向下的方法分为两阶段，首先通过目标检测算法检测出人体检测框，然后再从人体检测框内检测关节点，代表算法有G-RMI [<xref ref-type="bibr" rid="hanspub.41509-ref10">10</xref>]、AlphaPose [<xref ref-type="bibr" rid="hanspub.41509-ref11">11</xref>]、CPN [<xref ref-type="bibr" rid="hanspub.41509-ref12">12</xref>] 等。自底向上的方法也是分为两阶段，首先检测出图像中所有的关节点，然后再把全部关节点按一定的策略组合成每个人的姿态，代表算法有Openpose [<xref ref-type="bibr" rid="hanspub.41509-ref13">13</xref>]、PifPaf [<xref ref-type="bibr" rid="hanspub.41509-ref14">14</xref>]、DeeperCut [<xref ref-type="bibr" rid="hanspub.41509-ref15">15</xref>] 等。一般自底向上的方法会比自顶向下的方法快。因为自底向上的方法只需要一次性识别出图像中所有的关节点，而自顶向下的方法检测关节点的次数随着图像中人数的增加而增加，所以一般轻量级的人体姿态估计会采用自底向上的方法。例如，Intel公司的Osokin在OpenPose的基础上，提出OpenPose的轻量级版本Lightweight OpenPose [<xref ref-type="bibr" rid="hanspub.41509-ref16">16</xref>]。Osokin把Openpose的特征提取网络VGG-19换成MobileNet，并把5个修正预测的精炼阶段减少到1个。虽然该模型的精度下降了，但模型的速度却提高了不少。除此之外，Sekii [<xref ref-type="bibr" rid="hanspub.41509-ref17">17</xref>] 也是使用自底向上的方法思路提出一种基于YOLO [<xref ref-type="bibr" rid="hanspub.41509-ref18">18</xref>] 目标检测网格级别的轻量级人体姿态估计模型，把像素级别的热力图预测换成网格级别的目标检测来预测关节点，从而大幅度地提升关节点检测的速度。使用轻量级的人体姿态估计模型，因其关节点的检测精度降低，从而导致后续整个摔倒检测的判断结果会有一定的影响。</p></sec><sec id="s6_2"><title>2.2. 摔倒检测</title><p>摔倒检测一般会使用长短期记忆网络(Long Short-Term Memory, LSTM) [<xref ref-type="bibr" rid="hanspub.41509-ref19">19</xref>] [<xref ref-type="bibr" rid="hanspub.41509-ref20">20</xref>]、支持向量机(Support Vector Machine, SVM) [<xref ref-type="bibr" rid="hanspub.41509-ref2">2</xref>]、随机森林(Random Forest, RF)等方法，但这些方法不一定能学到区分关节点的一些运动特征，误判率较高，例如躺在床上、坐在地上、蹲下、摔倒等行为的区分。所以Yan等人 [<xref ref-type="bibr" rid="hanspub.41509-ref21">21</xref>] 提出一种基于人体关节点的时空图卷积网络ST-GCN进行动作识别，该模型能更好地学习到一些隐藏的人体关节点运动的特征，泛化能力更强。所以本文方法中第二阶段的摔倒检测会采用比传统判别算法泛化能力更强的图卷积网络来进行摔倒判断，从而提高摔倒检测的准确率。</p></sec></sec><sec id="s7"><title>3. 方法设计</title><p>基于轻量级的人体姿态估计和图卷积的摔倒实时检测方法的整体算法流程如图1所示。首先把实时的视频提取关键帧，然后把提取的关键帧图片输入到人体姿态估计模型中进行关节点检测。同时，根据检测出来的关节点生成目标跟踪框来进行目标追踪。然后，把同个跟踪ID的人的带有时序的关节点坐标序列输入到时空图卷积网络中。时空图卷积网络利用每个动作带有时序的坐标序列的前后变化特征进行动作分类，从而进行摔倒检测警告。</p><p>图1. 摔倒检测整体算法流程图</p><sec id="s7_1"><title>3.1. 轻量级人体姿态估计</title><p>为了整体算法模型能达到实时效果，需要将计算量较大的人体姿态估计模型进行优化，以达到实时检测的效果。本文参考文献 [<xref ref-type="bibr" rid="hanspub.41509-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.41509-ref16">16</xref>] [<xref ref-type="bibr" rid="hanspub.41509-ref17">17</xref>]，提出一种基于目标检测的两阶段轻量级人体姿态估计模型Lightweight Pose Detection Network。如图2所示，该模型基于目标检测的思想，先把统一尺寸后的图像分为 H &#215; W 个网格，然后用CNN网络来预测每个网格的候选的关节点和其连接的肢体，接着对候选框进行非极大值抑制(NMS)操作，最后利用Hungarian Algorithm [<xref ref-type="bibr" rid="hanspub.41509-ref22">22</xref>] 算法生成每个人的姿态。</p><p>图2. 人姿态估计算法流程图</p><p>其中，CNN网络有两个预测阶段，第一阶段网络主要预测细粒度的关节点，第二阶段网络是为了修正预测粗粒度的肢体连接，如图3所示。网络采用MobilenetV2 [<xref ref-type="bibr" rid="hanspub.41509-ref23">23</xref>] 作为特征提取网络，然后经过两个3 &#215; 3和一个1 &#215; 1的卷积操作之后输出第一阶段局部特征的关节点预测结果与粗略的肢体连接预测结果。第二阶段为了修正属于全局特征的肢体连接预测，在特征网络输出的后面加入一层空洞卷积来增强感受野，并融合上一个阶段的关节点与肢体预测结果。特征融合之后，再经过两个3 &#215; 3和一个1 &#215; 1的卷积操作输出第二阶段修正的肢体预测结果。</p><p>图3. Lightweight Pose Detection Network的网络结构</p><p>CNN网络第一阶段的输出为 6 ( K + 1 ) + H ′ W ′ | L | 个通道。其中， 6 ( K + 1 ) 为关节点预测信息，K为要预测的关节点种类，6为每个预测关节点所包含的信息数量。关节点预测包含的信息由公式(1)所示：</p><p>K k i = { p ( R | k , i ) , p ( I | R , k , i ) , w k i , h k i , x k i , y k i } (1)</p><p>其中， p ( R | k , i ) 为第i个网格预测关节点k的概率； p ( I | R , k , i ) 为第i个网格预测关节点k的预测框与真实值的IoU； x k i , y k i 为第i个网格预测关节点k的中心坐标相对于网格边界的距离； w k i , h k i 为第i个网格预测关节点k的预测框的宽和高，如图4所示。</p><p>图4. 关节点与肢体预测示意图</p><p>其次， H ′ W ′ | L | 为肢体预测的信息，其中， | L | 为肢体的种类， H ′ W ′ 为两个关节点连接的肢体范围，如图4所示。预测肢体连接的概率由公式(2)所示：</p><p>L k 1 k 2 = { p ( L | k 1 , k 2 , l , l + Δ l ) } (2)</p><p>其中， Δ l 的范围为 H ′ W ′ 。</p><p>CNN网络的第二阶段主要是为了修正粗粒度的肢体连接预测结果，所以网络输出的只有肢体预测的信息，输出通道为 H ′ W ′ | L | 。</p><p>整体网络的损失函数为：</p><p>L total = L 1 + L 2 (3)</p><p>L 1 = ∂ resp ∑ i ∈ ϑ ∑ k ∈ κ { δ k i − p ^ ( R | k , i ) } 2   + ∂ IoU ∑ i ∈ ϑ ∑ k ∈ κ δ k i { p ( I | R , k , i ) − p ^ ( I | R , k , i ) } 2   + ∂ coor ∑ i ∈ ϑ ∑ k ∈ κ δ k i { ( x k i − x ^ k i ) 2 + ( y k i − y ^ k i ) 2 }   + ∂ size ∑ i ∈ ϑ ∑ k ∈ κ δ k i { ( w k i − w ^ k i ) 2 + ( h k i − h ^ k i ) 2 }   + ∂ limb1 ∑ i ∈ ϑ ∑ Δ l ∈ ι ∑ ( k 1 , k 2 ) ∈ χ max ( δ k 1 i , δ k 2 j ) { δ k 1 i δ k 2 j − p ( L | k 1 , k 2 , l , l + Δ l ) } 2 (4)</p><p>L 2 = ∂ limb2 ∑ i ∈ ϑ ∑ Δ l ∈ ι ∑ ( k 1 , k 2 ) ∈ χ max ( δ k 1 i , δ k 2 j ) { δ k 1 i δ k 2 j − p ( L | k 1 , k 2 , l , l + Δ l ) } 2 (5)</p><p>其中， δ k i ∈ { 1 , 0 } 表示该网格是否存在k关节点； ( ∂ resp , ∂ IoU , ∂ coor , ∂ size , ∂ limb1 , ∂ limb2 ) 为每个损失的权重。</p><p>最后CNN网络最终输出的预测结果只使用第一阶段的关节点预测信息和第二阶段的肢体预测信息。</p><p>CNN网络输出关节点与肢体预测结果后，根据关节点的置信度 p ( R | k , i ) p ( I | R , k , i ) 和肢体连接的置信度 p ( L | k 1 , k 2 , l , l + Δ l ) ，通过匈牙利算法(Hungarian Algorithm) [<xref ref-type="bibr" rid="hanspub.41509-ref22">22</xref>] 用最大权重二分匹配来进行关节点连接，从而生成每个人的姿态，如图5所示。</p><p>图5. 关节点连接示意图</p></sec><sec id="s7_2"><title>3.2. 目标跟踪</title><p>虽然人体姿态估计采用的是自底向上的方法，没有生成人体检测框，但可以根据检测出来的关节点来生成目标追踪的检测框。遍历每个人的关节点坐标，找出最左、最右、最上和最下的坐标生成检测框。得出检测框后，用轻量级的Sort目标跟踪算法进行跟踪。Sort算法 [<xref ref-type="bibr" rid="hanspub.41509-ref24">24</xref>] 利用上下帧中的框重合度IoU和框内关节点的距离作为评判是否为同一个ID的标准，进而存储不同ID的连续关节点序列。因本文的人体姿态估计模型速度约35 FPS，为了整体模型达到实时检测效果，设定暂存的阈值为30帧。然后使用卡尔曼滤波(Kalman filtering)对关节点形成的检测框以及运动规则的预测进行位置的评估优化，为生成下一帧追踪的特征提供更好的条件，从而形成稳定的追踪。最后，利用匈牙利算法(Hungarian Algorithm) [<xref ref-type="bibr" rid="hanspub.41509-ref22">22</xref>] 将追踪到的ID对图像中的关节点进行分配，分别存储帧中提取的不同的关节点数据。</p></sec><sec id="s7_3"><title>3.3. 图神经网络摔倒识别</title><p>时空图卷积网络通过学习摔倒动作的带有时序的关节点坐标序列的前后变化特征来对摔倒进行分类识别。参考ST-GCN [<xref ref-type="bibr" rid="hanspub.41509-ref21">21</xref>] 模型，把每帧全部关节点作为数据流，输入到模拟关节点沿空间和时间维度的结构化信息的时空图神经网络中，图6为所构建的时空关节点图数据流示意图，橙色线连接为空间维度，蓝色线连接为时间维度。</p><p>图6. 时空关节点图数据流</p><p>网络结构如图7所示。首先，摔倒动作的特征对比其他动作会相对较简单，所以仅采用30帧的连续关节点数据作为网络的输入并对其位置特征进行归一化。然后，相应地把模型规模缩小，特征提取模块只使用6层：前2层为低维特征64通道数；中间2层为128通道数；后2层为高维特征256通道数。在特征模块中，ATT是注意力模块，负责针对不同层的GCN提取的语义特征的作用，GCN负责学习空间中相邻关节的特征，TCN负责学习时间维度中关节点变化的特征。最后，输出部分进行池化后使用FC全连接层输出结果。</p><p>图7. 时空图卷积网络结构</p></sec></sec><sec id="s8"><title>4. 实验</title><p>本文实验环境使用Intel Core i7-6700 3.4 GHz处理器与GTX1060Ti 6 GB独立显卡的笔记本电脑作为测试设备，使用GTX1080Ti 11G独立显卡作为训练设备。</p><sec id="s8_1"><title>4.1. 实验数据与处理</title><p>人体姿态估计模型因为较轻量级，所以数据集采用MPII数据集。MPII数据集一共包含4 &#215; 10<sup>4</sup>个人，25000张图片。使用官方划分训练与验证数据集。</p><p>摔倒识别的图神经网络使用NTU-D-RGB-120和UR Fall Detection Dataset数据集。NTU-D-RGB-120数据集中截取A9 standing up、A8 sitting down、A43 falling、A59 walking towards each other和A60 walking apart from each other这5个动作分类样本作为数据集，在训练时需要手动筛选并分成多个30帧的可用数据集，去除Z坐标并换成1置信度。UR Fall Detection Dataset数据集中截取standing up 、sitdown、 falling、waliking 、standing、sitting、lying这7个动作分类样本作为数据集。其中，UR Fall Detection Dataset数据集是没有关节点的标签，所以使用精度较高的AlphaPose人体姿态估计模型来输出制作视频数据集里的关节点标签。另外，由于使用的是二维的人体关节点，头部、肩部、臀部和腿部的运动特征对比其他关节点更能作为摔倒的判断依据，因此在数据预处理中需要筛选这些关节点训练的置信度，直接置为1。过滤掉一些无效的视频数据后，整体图神经网络的数据集一共有2300个视频，其中训练集有1600个视频，验证集700个视频，每个类别平均分布。</p></sec><sec id="s8_2"><title>4.2. 人体姿态估计分析</title><p>人体姿态估计模型训练输入的图片尺寸为384 &#215; 384，批次大小为32，初始学习率设置为5 &#215; 10<sup>−4</sup>，训练150轮，使用SDG随机梯度下降，冲量为0.9，权重衰减5 &#215; 10<sup>−4</sup>。损失函数的loss权重设置为。采用迭代训练的方式进行训练，先用COCO数据集训练特征提取网络，然后加上第一阶段网络用MPII数据集继续训练，最后加上第二阶段网络用MPII数据集一起训练。训练完成后，模型与其他模型的精度与速度的对比，如图8和表1所示。</p><p>为验证第一个阶段输出关节点和肢体连接预测与第二个阶段输出肢体连接预测的模型优化方法是有效的，设计消融实验进行验证。消融实验为使用第一阶段直接输出关节点与肢体预测结果与第二阶段直接输出关节点与肢体预测作对比，使用训练了150轮之后损失函数中输出的 L resp 、 L IoU 、 L coor 、 L size 、 L limb 与mAP作为评价指标，如表2所示。</p><p>图8. 不同算法精度与速度对比</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison of accuracy and speed of different algorithms on the MPII data se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >头部</th><th align="center" valign="middle" >肩部</th><th align="center" valign="middle" >手肘</th><th align="center" valign="middle" >手腕</th><th align="center" valign="middle" >臀部</th><th align="center" valign="middle" >膝盖</th><th align="center" valign="middle" >脚踝</th><th align="center" valign="middle" >mAP</th><th align="center" valign="middle" >FPS</th></tr></thead><tr><td align="center" valign="middle" >AlphaPose [<xref ref-type="bibr" rid="hanspub.41509-ref11">11</xref>]</td><td align="center" valign="middle" >87.2</td><td align="center" valign="middle" >85.8</td><td align="center" valign="middle" >77.4</td><td align="center" valign="middle" >69.7</td><td align="center" valign="middle" >74.0</td><td align="center" valign="middle" >72.2</td><td align="center" valign="middle" >64.4</td><td align="center" valign="middle" >76.3</td><td align="center" valign="middle" >22</td></tr><tr><td align="center" valign="middle" >OpenPose [<xref ref-type="bibr" rid="hanspub.41509-ref13">13</xref>]</td><td align="center" valign="middle" >90.8</td><td align="center" valign="middle" >87.2</td><td align="center" valign="middle" >77.3</td><td align="center" valign="middle" >66.5</td><td align="center" valign="middle" >74.8</td><td align="center" valign="middle" >67.6</td><td align="center" valign="middle" >61.2</td><td align="center" valign="middle" >75.2</td><td align="center" valign="middle" >7</td></tr><tr><td align="center" valign="middle" >Pose Proposal Networks-ResNet18 [<xref ref-type="bibr" rid="hanspub.41509-ref17">17</xref>]</td><td align="center" valign="middle" >92.5</td><td align="center" valign="middle" >88.5</td><td align="center" valign="middle" >73.8</td><td align="center" valign="middle" >62.1</td><td align="center" valign="middle" >71.7</td><td align="center" valign="middle" >61.9</td><td align="center" valign="middle" >54.6</td><td align="center" valign="middle" >72.1</td><td align="center" valign="middle" >42</td></tr><tr><td align="center" valign="middle" >Pose Proposal Networks-ResNet50 [<xref ref-type="bibr" rid="hanspub.41509-ref17">17</xref>]</td><td align="center" valign="middle" >92.9</td><td align="center" valign="middle" >89.6</td><td align="center" valign="middle" >77.3</td><td align="center" valign="middle" >67.2</td><td align="center" valign="middle" >73.5</td><td align="center" valign="middle" >66.8</td><td align="center" valign="middle" >58.4</td><td align="center" valign="middle" >74.7</td><td align="center" valign="middle" >26</td></tr><tr><td align="center" valign="middle" >本文模型</td><td align="center" valign="middle" >93.1</td><td align="center" valign="middle" >89.5</td><td align="center" valign="middle" >77.3</td><td align="center" valign="middle" >67.4</td><td align="center" valign="middle" >73.4</td><td align="center" valign="middle" >66.9</td><td align="center" valign="middle" >58.7</td><td align="center" valign="middle" >74.8</td><td align="center" valign="middle" >35</td></tr></tbody></table></table-wrap><p>表1. MPII 数据集上不同算法精度与速度对比</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Analysis of ablation experiment</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >L resp</th><th align="center" valign="middle" >L IoU</th><th align="center" valign="middle" >L coor</th><th align="center" valign="middle" >L size</th><th align="center" valign="middle" >L limb</th><th align="center" valign="middle" >mAP</th><th align="center" valign="middle" >FPS</th></tr></thead><tr><td align="center" valign="middle" >第一阶段</td><td align="center" valign="middle" >12.647</td><td align="center" valign="middle" >0.062</td><td align="center" valign="middle" >1.675</td><td align="center" valign="middle" >0.071</td><td align="center" valign="middle" >13.287</td><td align="center" valign="middle" >71.7</td><td align="center" valign="middle" >48</td></tr><tr><td align="center" valign="middle" >第二阶段</td><td align="center" valign="middle" >13.012</td><td align="center" valign="middle" >0.063</td><td align="center" valign="middle" >1.769</td><td align="center" valign="middle" >0.137</td><td align="center" valign="middle" >12.979</td><td align="center" valign="middle" >74.2</td><td align="center" valign="middle" >30</td></tr><tr><td align="center" valign="middle" >本文模型</td><td align="center" valign="middle" >12.654</td><td align="center" valign="middle" >0.062</td><td align="center" valign="middle" >1.676</td><td align="center" valign="middle" >0.073</td><td align="center" valign="middle" >12.982</td><td align="center" valign="middle" >74.8</td><td align="center" valign="middle" >35</td></tr></tbody></table></table-wrap><p>表2. 消融实验分析</p><p>图9. 消融实验的Loss对比分析</p><p>从图9中，通过Loss损失函数的分析可以看出，第一阶段关节点相关的Loss比第二阶段小，细粒度的关节点预测更为准确；第二阶段肢体预测的Loss比第一阶段小，粗粒度的肢体预测更为准确。其次，从图10中，从两阶段网络模型输出的检测效果可以看出来，第一阶段的关节点预测会比第二阶段的更准确，第二阶段对肢体的预测会比第一阶段更加准确，所以验证了本文模型的优化方法有效。图11为优化后的人体姿态估计模型的检测效果。</p><p>图10. 第一和第二阶段检测效果</p><p>图11. 本文人体姿态估计模型的检测效果</p></sec><sec id="s8_3"><title>4.3. 摔倒识别分析</title><p>摔倒识别的时空图卷积网络先使用NTU数据集数据进行预训练，然后使用UR数据集进行进一步训练，训练使用30帧连续的16个关节点数据，批大小为128，初始学习率为1 &#215; 10<sup>−3</sup>，训练50轮，每10个 epoch衰减0.1，使用交叉熵损失函数以及Adam优化器。</p><p>本文采用4个评价指标，分别为精确度(Precision)、召回率(Recall)、准确率(Accuracy)和F1-measure综合性评价指标。首先，将实验样本分为四类：真正例TP、假正例FP、假反例FN和真反例TN。真正例 TP(True Position)表示正样本被正确地分类为正样本；假正例FP (False Position)表示负样本被错误地分类为正样本；假反例FN (False Negative)表示正样本被错误地分类为负样本；真反例TN (TrueNegative)表示负样本被正确地分类为负样本。</p><p>所以，精确度(Precision)表示正确地分类的摔倒样本占分类为摔倒样本的比例，如公式(6)所示。</p><p>Precision = TP TP + FP (6)</p><p>召回率(Recall)表示模型正确地分类的摔倒样本占实际摔倒样本的比例，如公式(7)所示。</p><p>Recall = TP TP + FN (7)</p><p>准确率(Accuracy)表示模型分类地正确(包含摔倒和非摔倒)的样本占所有样本的比例，如公式(8)所示。</p><p>Accuracy = TP + TN P + N (8)</p><p>F1-measure为分类常用的一个综合性评价指标，如公式(9)所示。</p><p>F1-measure = 2 ∗ Recall ∗ Precision Recall + Precision (9)</p><p>动作识别算法一般会使用300帧连续的关节点数据，但本文的人体姿态估计模型速度约35 FPS，为了整体模型达到实时检测效果，设置使用30帧的数据。实验结果如表3所示，设置使用30帧，整体模型的摔倒检测准确率并没有下降多少。其次，把网络的特征提模块改成6层，摔倒检测的准确率也没有下降多少。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Analysis of the influence of the number of key frames and the number of feature extraction layers on detectio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >帧数</th><th align="center" valign="middle" >特征提取层数</th><th align="center" valign="middle" >Precision</th><th align="center" valign="middle" >Recall</th><th align="center" valign="middle" >Accuracy</th><th align="center" valign="middle" >F1-measure</th></tr></thead><tr><td align="center" valign="middle" >300</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >0.973</td><td align="center" valign="middle" >0.947</td><td align="center" valign="middle" >0.981</td><td align="center" valign="middle" >0.961</td></tr><tr><td align="center" valign="middle" >30</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >0.970</td><td align="center" valign="middle" >0.942</td><td align="center" valign="middle" >0.975</td><td align="center" valign="middle" >0.956</td></tr><tr><td align="center" valign="middle" >30</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >0.957</td><td align="center" valign="middle" >0.925</td><td align="center" valign="middle" >0.961</td><td align="center" valign="middle" >0.941</td></tr></tbody></table></table-wrap><p>表3. 关键帧数和特征提取层数的设置对检测的影响分析</p><p>虽然使用了轻量级的人体姿态估计模型，但使用时空图卷积网络进行摔倒识别之后，整体模型的准确率并没有比使用精度较高的Alphapose人体姿态估计模型的准确率低多少。此外，对比使用SVM动作分类的方法，准确率却要高很多，如表4所示。本文方法的摔倒检测效果如图12所示。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Comparison of accuracy and speed of fall detectio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Precision</th><th align="center" valign="middle" >Recall</th><th align="center" valign="middle" >Accuracy</th><th align="center" valign="middle" >F1-measure</th><th align="center" valign="middle" >FPS</th></tr></thead><tr><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >0.797</td><td align="center" valign="middle" >0.237</td><td align="center" valign="middle" >0.801</td><td align="center" valign="middle" >0.343</td><td align="center" valign="middle" >36</td></tr><tr><td align="center" valign="middle" >Alphapose + GCN</td><td align="center" valign="middle" >0.963</td><td align="center" valign="middle" >0.932</td><td align="center" valign="middle" >0.969</td><td align="center" valign="middle" >0.949</td><td align="center" valign="middle" >23</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >0.957</td><td align="center" valign="middle" >0.925</td><td align="center" valign="middle" >0.961</td><td align="center" valign="middle" >0.941</td><td align="center" valign="middle" >33</td></tr></tbody></table></table-wrap><p>表4. 摔倒检测的准确率与速度对比</p><p>图12. 摔倒检测效果</p></sec></sec><sec id="s9"><title>5. 总结</title><p>为了使基于人体姿态估计的摔倒检测方法达到实时检测的效果，本文先把计算量较大的人体姿态估计模型进行优化，提出一种基于目标检测的两阶段轻量级人体姿态估计模型。同时，为了验证两阶段网络的优化方法有效，设计实验分析各阶段网络预测关节点和肢体的效果。然后，为了提高整体检测方法的准确率与检测速度，使用规模缩小后的时空图卷积网络来对关节点序列进行摔倒检测。经过实验，本文方法对比使用高精度的人体姿态估计模型，整体算法的摔倒检测准确率并没有下降很多，但速度却提升了不少。</p></sec><sec id="s10"><title>文章引用</title><p>何炜婷,曾 碧,陈文轩. 基于轻量级人体姿态估计和图卷积的摔倒实时检测方法Real-Time Fall Detection Based on Light-weight Human Pose Estimation and Graph Convolution Network[J]. 计算机科学与应用, 2021, 11(04): 783-794. https://doi.org/10.12677/CSA.2021.114080</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41509-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Abbate, S., Avvenuti, M., Bonatesta, F., Cola, G., Corsini, P. and Vecchio, A. (2012) A Smartphone-Based Fall Detec-tion System. Pervasive &amp; Mobile Computing, 8, 883-899. &lt;br&gt;https://doi.org/10.1016/j.pmcj.2012.08.003</mixed-citation></ref><ref id="hanspub.41509-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Feng, W., Liu, R. and Zhu, M. (2014) Fall Detection for Elderly Person Care in a Vision-Based Home Surveillance Environ-ment Using a Monocular Camera. Signal Image &amp; Video Processing, 8, 1129-1138.  
&lt;br&gt;https://doi.org/10.1007/s11760-014-0645-4</mixed-citation></ref><ref id="hanspub.41509-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Alhimale, L., Zedan, H. and Al-Bayatti, A. (2014) The Implemen-tation of an Intelligent and Video-Based Fall Detection System Using a Neural Network. Applied Soft Computing, 18, 59-69. &lt;br&gt;https://doi.org/10.1016/j.asoc.2014.01.024</mixed-citation></ref><ref id="hanspub.41509-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Nunez-Marcos, A., Azkune, G. and Arganda-Carreras, I. (2018) Vision-Based Fall Detection with Convolutional Neural Networks. Wireless Communications &amp; Mobile Compu-ting, 2017, Article ID: 9474806.  
&lt;br&gt;https://doi.org/10.1155/2017/9474806</mixed-citation></ref><ref id="hanspub.41509-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Wang, H. and Schmid, C. (2013) Action Recognition with Improved Trajectories. Proceedings of the IEEE International Conference on Computer Vision, Sydney, 1-8 December 2013, 3551-3558.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2013.441</mixed-citation></ref><ref id="hanspub.41509-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Chen, W., Jiang, Z., Guo, H. and Ni, X. (2020) Fall Detection Based on Key Points of Human-Skeleton Using Open Pose. Symmetry, 12, 744. &lt;br&gt;https://doi.org/10.3390/sym12050744</mixed-citation></ref><ref id="hanspub.41509-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Song, S., Lan, C., Xing, J., Zeng, W. and Liu, J. (2017) An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data. Proceedings of the AAAI Confer-ence on Artificial Intelligence, 31, 501.</mixed-citation></ref><ref id="hanspub.41509-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Tran, D., Bourdev, L., Fergus, R., Torresani, L. and Paluri, M. (2015) Learning Spatiotemporal Features with 3d Convolutional Networks. Proceedings of the IEEE International Conference on Computer Vision, Santiago, 7-13 December 2015, 4489-4497. &lt;br&gt;https://doi.org/10.1109/ICCV.2015.510</mixed-citation></ref><ref id="hanspub.41509-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Xu, H., Das, A. and Saenko, K. (2017) R-c3d: Region Convolutional 3d Network for Temporal Activity Detection. Proceed-ings of the IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 5783-5792.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.617</mixed-citation></ref><ref id="hanspub.41509-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Papandreou, G., et al. (2017) Towards Accurate Multi-Person Pose Estimation in the Wild. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 4903-4911.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.395</mixed-citation></ref><ref id="hanspub.41509-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Fang, H.-S., Xie, S., Tai, Y.-W. and Lu, C. (2017) Rmpe: Regional Multi-Person Pose Estimation. Proceedings of the IEEE International Conference on Computer Vision, Venice, 22-29 October 2017, 2334-2343.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.256</mixed-citation></ref><ref id="hanspub.41509-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G. and Sun, J. (2018) Cascaded Pyramid Network for Multi-Person Pose Estimation. Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 7103-7112.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00742</mixed-citation></ref><ref id="hanspub.41509-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Cao, Z., Simon, T., Wei, S. and Sheikh, Y. (2017) Realtime Mul-ti-Person 2D Pose Estimation Using Part Affinity Fields. Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 1302-1310.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.143</mixed-citation></ref><ref id="hanspub.41509-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Kreiss, S., Bertoni, L. and Alahi, A. (2019) PifPaf: Composite Fields for Human Pose Estimation. Computer Vision and Pattern Recognition, Long Beach, 16-20 June 2019, 11977-11986. &lt;br&gt;https://doi.org/10.1109/CVPR.2019.01225</mixed-citation></ref><ref id="hanspub.41509-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M. and Schiele, B. (2016) DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model. European Confer-ence on Computer Vision, Amsterdam, 11-14 October 2016, 34-50. &lt;br&gt;https://doi.org/10.1007/978-3-319-46466-4_3</mixed-citation></ref><ref id="hanspub.41509-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Osokin, D. (2019) Real-Time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose. International Conference on Pattern Recognition Applications and Methods, Prague, 19-21 February 2019, 744-748.  
&lt;br&gt;https://doi.org/10.5220/0007555407440748</mixed-citation></ref><ref id="hanspub.41509-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Sekii, T. (2018) Pose Proposal Networks. European Conference on Computer Vision, Munich, 8-14 September 2018, 350-366. &lt;br&gt;https://doi.org/10.1007/978-3-030-01261-8_21</mixed-citation></ref><ref id="hanspub.41509-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J., Divvala, S.K., Girshick, R. and Farhadi, A. (2016) You Only Look Once: Unified, Real-Time Object Detection. Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 779-788.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.91</mixed-citation></ref><ref id="hanspub.41509-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Lin, H.Y., Hsueh, Y.L. and Lie, W.N. (2017) Abnormal Event Detec-tion Using Microsoft Kinect in a Smart Home. 2016 International Computer Symposium (ICS), Chiayi, 15-17 December 2016, 285-289.  
&lt;br&gt;https://doi.org/10.1109/ICS.2016.0064</mixed-citation></ref><ref id="hanspub.41509-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Lie, W.N., Le, A.T. and Lin, G.H. (2018) Human Fall-Down Event Detection Based on 2D Skeletons and Deep Learning Approach. 2018 International Workshop on Advanced Image Technology (IWAIT), Chiang Mai, 7-10 January 2018, 1-4. &lt;br&gt;https://doi.org/10.1109/IWAIT.2018.8369778</mixed-citation></ref><ref id="hanspub.41509-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Yan, S., Xiong, Y. and Lin, D. (2018) Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recogni-tion.</mixed-citation></ref><ref id="hanspub.41509-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Kuhn, H.W. (1955) The Hungarian Method for the Assignment Problem. Naval Research Logistics Quarterly, 2, 83-97.  
&lt;br&gt;https://doi.org/10.1002/nav.3800020109</mixed-citation></ref><ref id="hanspub.41509-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. and Chen, L. (2018) MobileNetV2: Inverted Residuals and Linear Bottlenecks. Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 4510-4520.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00474</mixed-citation></ref><ref id="hanspub.41509-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Bewley, A., Ge, Z., Ott, L., Ramos, F. and Upcroft, B. (2016) Simple Online and Real-Time Tracking. 2016 IEEE International Conference on Image Processing (ICIP), Phoenix, 25-28 September 2016, 3464-3468.  
&lt;br&gt;https://doi.org/10.1109/ICIP.2016.7533003</mixed-citation></ref></ref-list></back></article>