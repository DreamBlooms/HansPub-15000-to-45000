<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">PM</journal-id><journal-title-group><journal-title>Pure  Mathematics</journal-title></journal-title-group><issn pub-type="epub">2160-7583</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/PM.2017.72013</article-id><article-id pub-id-type="publisher-id">PM-20059</article-id><article-categories><subj-group subj-group-type="heading"><subject>PM20170200000_19361378.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  求解抛物方程的MPI并行方法
  Parallel Methods for Parabolic Equations Based on MPI Implementation
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>高</surname><given-names>玉羊</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>顾</surname><given-names>海明</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>青岛科技大学，山东 青岛</addr-line></aff><aff id="aff2"><addr-line>null</addr-line></aff><author-notes><corresp id="cor1">* E-mail:<email>crazy3435@163.com(高玉)</email>;</corresp></author-notes><pub-date pub-type="epub"><day>06</day><month>03</month><year>2017</year></pub-date><volume>07</volume><issue>02</issue><fpage>89</fpage><lpage>98</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   数值求解偏微分方程广泛应用于数学与工程领域。大规模数值计算在当今科学技术运用中得到飞速发展，其中可并行的有限差分格式受到越来越多的重视。在本文中，主要阐述了经典的分组显示方法求解抛物方程，并简单扼要的分析了该格式的建立以及稳定性。随后本文着重介绍了如何在MPI并行环境下对该格式进行数值计算，构建了两种不同的并行计算模型，即阻塞通信(等待模式)和非阻塞通信(即非等待模式)模式。并与非并行状态下的差分格式做出比较，结果表明，相对于一个进程求解偏微分方程，两种模式都表现出较好的效果，而且非阻塞通信相较于阻塞通信模式亦表现出较好的并行效率。 Many applications in mathematics and engineering involve numerical solutions of partial diffe-rential equations (PDEs). The demands of large-scale computing are quickly increasing in modern science and technology, and parallel computing has received more and more attention. In this paper, the main idea is that classical Group Explicit method (GEM) for parabolic equations, the group explicit method is established briefly and the stability analysis of the method is indicated simply. Then we focus on how to calculate the format in MPI parallel environment. Two parallel MPI algorithms are established and compared with non-parallel algorithm based on GEM. They are MPI block communication (wait communication) and non-blocking communication (no-wait communication). These two MPI schemas both better than one single process to calculate numerical solutions use group explicit method. Also, the non-blocking communication program has higher computational efficiency than blocking communication program.
    
  
 
</p></abstract><kwd-group><kwd>有限差分法，分组显式格式，抛物方程，MPI (Message Passing Interface), Finite Difference Method</kwd><kwd> Group Explicit Method</kwd><kwd> Parabolic Equations</kwd><kwd> 
MPI (Message Passing Interface)</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>求解抛物方程的MPI并行方法<sup> </sup></title><p>高玉羊，顾海明</p><p>青岛科技大学，山东 青岛</p><p>收稿日期：2017年3月11日；录用日期：2017年3月28日；发布日期：2017年3月31日</p><disp-formula id="hanspub.20059-formula62"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>数值求解偏微分方程广泛应用于数学与工程领域。大规模数值计算在当今科学技术运用中得到飞速发展，其中可并行的有限差分格式受到越来越多的重视。在本文中，主要阐述了经典的分组显示方法求解抛物方程，并简单扼要的分析了该格式的建立以及稳定性。随后本文着重介绍了如何在MPI并行环境下对该格式进行数值计算，构建了两种不同的并行计算模型，即阻塞通信(等待模式)和非阻塞通信(即非等待模式)模式。并与非并行状态下的差分格式做出比较，结果表明，相对于一个进程求解偏微分方程，两种模式都表现出较好的效果，而且非阻塞通信相较于阻塞通信模式亦表现出较好的并行效率。</p><p>关键词 :有限差分法，分组显式格式，抛物方程，MPI (Message Passing Interface)</p><disp-formula id="hanspub.20059-formula63"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2017 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="http://image.hanspub.org:8080\Html/htmlimages\1-2890033x\e70a10f1-7c93-45ea-9603-062237856e4b.png" /><img src="http://image.hanspub.org:8080\Html\htmlimages\1-2890033x\e898c85e-ffc4-45c9-b817-14224a4d6960.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>现代科学技术的发展很大程度上依赖于物理学、化学、生物学的成就与进展，而这些学科自身的精确化又是他们取得进展的重要保证。学科的精确化往往是通过建立数学模型来实现的，而数学模型的大都可以归纳为偏微分方程的形式。现代科学技术对大规模科学与工程计算的需求日益增长，在科学与工程计算领域提出了许多大规模计算和超大规模计算问题，解决这些问题需要在高性能并行计算机( [<xref ref-type="bibr" rid="hanspub.20059-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.20059-ref2">2</xref>] )上进行，而并行计算机系统的研制和发展推动了偏微分方程并行算法的研究与发展。</p><p>分组显式方法(文献 [<xref ref-type="bibr" rid="hanspub.20059-ref3">3</xref>] )是Evans等人上世纪80年代设计的可以并行计算的经典有限差分算法。分组显式格式是使用不同类型的Saul'yev非对称格式( [<xref ref-type="bibr" rid="hanspub.20059-ref4">4</xref>] )进行恰当的组合，在不同的时间层上连续交替的使用不同的非对称格式，这样，可带来截断误差的明显改善，从而使算法的精度得以提高。基于分组显式的思想，有限并行差分格式得到广泛的发展并应用于求解其他偏微分方程( [<xref ref-type="bibr" rid="hanspub.20059-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.20059-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.20059-ref7">7</xref>] )中。</p><p>MPI是目前最重要的并行编程工具( [<xref ref-type="bibr" rid="hanspub.20059-ref8">8</xref>] )，它具有移植性好、功能强大、效率高等多种优点，MPI其实是一个“库”，共有上百个函数调用接口，在FORTRAN和C语言中可以直接对这些函数进行调用。MPI为并行算法提供了多种通信模式，一般的阻塞通讯模式即可满足大多数并行算法，但非阻塞通讯模式由于相对减少了通讯时间，从而使并行模式的效率得到较好的提升。将MPI并行技术与有限并行差分格式结合需要构造相匹配的消息传递模型( [<xref ref-type="bibr" rid="hanspub.20059-ref9">9</xref>] )，因此，构造合适的并行消息传递模型是并行数值计算偏微分方程的重点。</p></sec><sec id="s4"><title>2. 有限并行差分格式</title><sec id="s4_1"><title>2.1. 分组显式方法</title><p>在区域<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x9_hanspub.png" xlink:type="simple"/></inline-formula>上，考虑如下附有初始条件和边界条件的抛物方程。</p><disp-formula id="hanspub.20059-formula64"><label>(1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x10_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula65"><label>(2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x11_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula66"><label>(3)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x12_hanspub.png"  xlink:type="simple"/></disp-formula><p>将区域<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x13_hanspub.png" xlink:type="simple"/></inline-formula>进行剖分，空间步长<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x14_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x15_hanspub.png" xlink:type="simple"/></inline-formula>为正整数，时间步长为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x16_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x17_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x18_hanspub.png" xlink:type="simple"/></inline-formula>。为简单计，将节点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x19_hanspub.png" xlink:type="simple"/></inline-formula>记为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x20_hanspub.png" xlink:type="simple"/></inline-formula>。构造分组显式格式需要用到Saul’yev非对称格式。首先，可以得到以下很明显的关系式，</p><disp-formula id="hanspub.20059-formula67"><label>(4)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x21_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula68"><label>(5)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x22_hanspub.png"  xlink:type="simple"/></disp-formula><p>应用如下等式</p><disp-formula id="hanspub.20059-formula69"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x23_hanspub.png"  xlink:type="simple"/></disp-formula><p>将(4)，(5)代入方程(1)得到</p><disp-formula id="hanspub.20059-formula70"><label>(7)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x24_hanspub.png"  xlink:type="simple"/></disp-formula><p>最后，应用如下显然的表达式</p><disp-formula id="hanspub.20059-formula71"><label>(8)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x25_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula72"><label>(9)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x26_hanspub.png"  xlink:type="simple"/></disp-formula><p>便可以将(7)写成如下形式，</p><disp-formula id="hanspub.20059-formula73"><label>(10)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x27_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，</p><disp-formula id="hanspub.20059-formula74"><label>(11)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x28_hanspub.png"  xlink:type="simple"/></disp-formula><p>令<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x29_hanspub.png" xlink:type="simple"/></inline-formula>，并舍弃无穷小项<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x30_hanspub.png" xlink:type="simple"/></inline-formula>，可以得到如下网格方程，</p><disp-formula id="hanspub.20059-formula75"><label>(12)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x31_hanspub.png"  xlink:type="simple"/></disp-formula><p>完全类似的可以推导出公式</p><disp-formula id="hanspub.20059-formula76"><label>(13)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x32_hanspub.png"  xlink:type="simple"/></disp-formula><p>将方程(12)、(13)写成矩阵的形式，</p><disp-formula id="hanspub.20059-formula77"><label>(14)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x33_hanspub.png"  xlink:type="simple"/></disp-formula><p>将矩阵移向右侧，(14)可以写成</p><disp-formula id="hanspub.20059-formula78"><label>(15)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x34_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x35_hanspub.png" xlink:type="simple"/></inline-formula>，因此方程(15)可以写成如下矩阵的形式，</p><disp-formula id="hanspub.20059-formula79"><label>(16)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x36_hanspub.png"  xlink:type="simple"/></disp-formula><p>在靠近求解区域左边界和右边界的点可以分别使用方程(12)、(13)进行求解。</p><disp-formula id="hanspub.20059-formula80"><label>(17)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x37_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula81"><label>(18)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x38_hanspub.png"  xlink:type="simple"/></disp-formula><p>以上即为分组显式格式的基本思想，为了适配下文中的MPI消息传递模型，将会选择适当的内部节点数进行网格剖分。考虑区域<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x39_hanspub.png" xlink:type="simple"/></inline-formula>上的划分间隔为偶数，既<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x40_hanspub.png" xlink:type="simple"/></inline-formula>为偶数，则在每一时间层上，内部节点数为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x41_hanspub.png" xlink:type="simple"/></inline-formula>，为奇数。这样，就会存在一个单独的点无法使用方程(14)求解。为了更好的处理边界节点的值，在下文中给出了具体的方法。</p></sec><sec id="s4_2"><title>2.2. 交替分组方法</title><p>在每一时间层，除初始条件外，所有内部节点是未知的。内部节点的最后一个点使用格式(18)，其余</p><p>诸点，从内部节点的第一个点开始到第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x42_hanspub.png" xlink:type="simple"/></inline-formula>个内部节点使用方程(16)，共进行<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x43_hanspub.png" xlink:type="simple"/></inline-formula>次计算，将该</p><p>格式称为右端分组显式(GER)格式。写成矩阵的形式如下，</p><disp-formula id="hanspub.20059-formula82"><label>(19)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x44_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula83"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x45_hanspub.png"  xlink:type="simple"/></disp-formula><p>方程(19)写成矩阵的形式为</p><disp-formula id="hanspub.20059-formula84"><label>(20)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x46_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中</p><disp-formula id="hanspub.20059-formula85"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x47_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula86"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x48_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula87"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x49_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula88"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x50_hanspub.png"  xlink:type="simple"/></disp-formula><p>运用相似的方法，在每一时间层，内部节点的第一个节点使用方程(18)，其余内部节点使用方程(16)，可以得到左端分组显式(GEL)格式。</p><disp-formula id="hanspub.20059-formula89"><label>(21)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x51_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula90"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x52_hanspub.png"  xlink:type="simple"/></disp-formula><p>在相邻的时间层上交替使用方程(20)、(21)，即可得到交替分组方法(AGE)。其矩阵形式如下，</p><disp-formula id="hanspub.20059-formula91"><label>(22)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/6-1250507x53_hanspub.png"  xlink:type="simple"/></disp-formula><p>方程(16)的稳定分析在文献 [<xref ref-type="bibr" rid="hanspub.20059-ref3">3</xref>] 中已经得到充分的说明，稳定性条件为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x54_hanspub.png" xlink:type="simple"/></inline-formula>时误差的增长得到有效的控制，同样，使用文献 [<xref ref-type="bibr" rid="hanspub.20059-ref10">10</xref>] 中的Kellogg引理，可以得到方程(20)，(21)，(22)的稳定性条件为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x55_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec></sec><sec id="s5"><title>3. MPI并行算法</title><p>在本文中，所有并行算法都是针对上文中给出的有限差分格式进行并行化计算。将求解区域<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x56_hanspub.png" xlink:type="simple"/></inline-formula>划分为若干大致相等的区域，比如说，若将方程(1)进行4进程并行计算，就将求解区域化为4条带状的区域，每个进程负责一条带状计算区域，在时间层上逐次计算，为了实现高效计算，比较适宜的做法是为每一条进程分配大约相等的任务量，即保证在空间方向上分割的带状区域大小几乎是一致的。每个相邻进程进行数据通信，当通信与计算完毕，继续进行下一时间层的计算。下文中，将会给出两种不同的通讯模式，即阻塞通信与非阻塞通信，对交替分组格式进行并行计算。表1和表2分别展示了MPI阻塞通信与非阻塞通信的步骤流。</p><p>考虑划分区域内部间隔为偶数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x57_hanspub.png" xlink:type="simple"/></inline-formula>，内部节点数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x58_hanspub.png" xlink:type="simple"/></inline-formula> (同上文，划分区域第一个节点标号为0，最后一个节点标号为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x59_hanspub.png" xlink:type="simple"/></inline-formula>)，内点数为奇数。不失一般性，考虑以上差分格式在4条进程下完成并行计算，不妨假设已知时间层为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x60_hanspub.png" xlink:type="simple"/></inline-formula>上的数据，计算时间层为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x61_hanspub.png" xlink:type="simple"/></inline-formula>上的值时，将内部节点大约的分为4个部分，由于节点</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The steps of blocking communication mod</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >进程0</th><th align="center" valign="middle" >其它进程</th></tr></thead><tr><td align="center" valign="middle" >步骤1</td><td align="center" valign="middle" >计算本进程的所负责的计算区间</td><td align="center" valign="middle" >同0进程</td></tr><tr><td align="center" valign="middle" >步骤2</td><td align="center" valign="middle" >读取本进程的计算数据</td><td align="center" valign="middle" >同0进程</td></tr><tr><td align="center" valign="middle" >步骤3</td><td align="center" valign="middle" >发送数据到其他进程或从其他进程接收数据</td><td align="center" valign="middle" >同0进程</td></tr><tr><td align="center" valign="middle" >步骤4</td><td align="center" valign="middle" >将数据带入差分格式并进行计算</td><td align="center" valign="middle" >同0进程</td></tr><tr><td align="center" valign="middle" >步骤5</td><td align="center" valign="middle" >进入下一时间层的计算，转到步骤3</td><td align="center" valign="middle" >同0进程</td></tr></tbody></table></table-wrap><p>表1. 阻塞通信模式下并行详细步骤</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> The steps of non-blocking communication mod</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >进程0</th><th align="center" valign="middle" >其他进程</th></tr></thead><tr><td align="center" valign="middle" >步骤1</td><td align="center" valign="middle" >计算本进程的所负责的计算区间，并读取本进程的计算数据</td><td align="center" valign="middle" >同0进程</td></tr><tr><td align="center" valign="middle" >步骤2</td><td align="center" valign="middle" >发送数据到其他进程或从其他进程接收数据</td><td align="center" valign="middle" >同0进程</td></tr><tr><td align="center" valign="middle" >步骤3</td><td align="center" valign="middle" >非通信区域的数据计算</td><td align="center" valign="middle" >同0进程</td></tr><tr><td align="center" valign="middle" >步骤4</td><td align="center" valign="middle" >等待通信完成，计算通信区域的数据</td><td align="center" valign="middle" >同0进程</td></tr><tr><td align="center" valign="middle" >步骤5</td><td align="center" valign="middle" >进入下一时间层的计算，转到步骤2</td><td align="center" valign="middle" >同0进程</td></tr></tbody></table></table-wrap><p>表2. 非阻塞通信模式下并行详细步骤</p><p>数是<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x62_hanspub.png" xlink:type="simple"/></inline-formula>为奇数，则必然有一个点无法使用方程(16)分组计算。在使用GEL格式计算的情况下，在0号进程中，划分区域内部节点的第1个点由方程(17)计算，进程0的其他节点和其他进程的点使用方程(16)计算。由于与0号进程相邻的1号进程之间需要进行数据通信，既计算第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x63_hanspub.png" xlink:type="simple"/></inline-formula>层中0号进程的最右端的两个点需要第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x64_hanspub.png" xlink:type="simple"/></inline-formula>层中的1号进程的第一个点的数据，计算第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x65_hanspub.png" xlink:type="simple"/></inline-formula>层中1号进程最左端两个点需要第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x66_hanspub.png" xlink:type="simple"/></inline-formula>层中0号进程最右端一个点的数据，其他进程如1号进程与2号进程、2号进程与3号进程之间的通讯也是如此。在下文中，以0，1号进程之间的通讯传递为例，给出MPI通信模型，GEL通讯简图见图1。</p><p>在使用GER格式计算的情况下，与GEL格式类似，单独的内部节点的计算由最后一个进程执行，既<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x67_hanspub.png" xlink:type="simple"/></inline-formula>时间层中3号进程的最右端内部节点由方程(18)计算，其他节点两两成组由方程(16)计算，与GEL格式一致，相邻进程的节点进行数据通信，GER通讯简图见图2。</p><p>在使用交替分组格式计算的情况下，与GER，GEL格式有些不同之处，首先考虑在第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x68_hanspub.png" xlink:type="simple"/></inline-formula>层上使用GEL格式，在第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x69_hanspub.png" xlink:type="simple"/></inline-formula>层上使用GER格式，第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x70_hanspub.png" xlink:type="simple"/></inline-formula>层上的通讯与计算与普通的GEL格式一样，在进行<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x71_hanspub.png" xlink:type="simple"/></inline-formula>层上的计算时，相邻两个进程的最后两个数据与最初的两个数据分别向后一个进程和前一个进程发送。通讯简图如图3。</p><p>在MPI并行环境下，阻塞通信模式下用到的计算与通讯过程中的命令，如下，</p><p>MPI_Send(void* buf,int count,MPI_Datatype,int destination,int tag,MPI_Comm comm)</p><p>MPI_Recv(void* buf,int count,MPI_Datatype,int source,int tag,MPI_Comm comm, MPI_Status*status)</p><p>非阻塞通信模式下的通信和计算与阻塞通信模式下的基本一致，差异之处在于各个进程之间的通讯可能尚未结束时，就已经在进行数据的计算，这样往往可以使CPU在各个核心在通讯的过程中，仍然满载运行，从而减少程序运行的时间，达到并行计算效率的提升。</p><p>图1. GEL格式通讯简图</p><p>图2. GER格式通讯简图</p><p>图3. AGE格式通讯简图</p><p>在MPI并行环境下，非阻塞通信模式下用到的计算与通讯过程中的命令，如下</p><p>MPI_Isend(void* buf,int count,MPI_Datatype,int destination,int tag,MPI_Comm comm, MPI_Request*request)</p><p>MPI_Irecv(void* buf,int count,MPI_Datatype,int source,int tag,MPI_Comm comm, MPI_Request*request)</p><p>MPI_Waitall(int count,MPI_Request*request,MPI_Status*status)</p></sec><sec id="s6"><title>4. 数值并行计算</title><p>为了验证MPI并行环境下交替分组格式的并行效率的提升情况。作如下数值运算。对方程(1)，考虑如下初始条件和边界条件，</p><disp-formula id="hanspub.20059-formula92"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x75_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.20059-formula93"><graphic xlink:href="http://html.hanspub.org/file/6-1250507x76_hanspub.png"  xlink:type="simple"/></disp-formula><p>下文给出的数值模拟计算的结果是在双核计算机上进行4进程并行计算得到的运算结果。在上述三种并行有限差分格式下，其中，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x77_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x78_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/6-1250507x79_hanspub.png" xlink:type="simple"/></inline-formula>。由于MPI并行消息传递模型并未改变交替分组差分格式的逻辑结构，此处不再赘述方程(1)的数值解的误差分析，而且在文献 [<xref ref-type="bibr" rid="hanspub.20059-ref3">3</xref>] 中，亦给出了该类型抛物方程的误差分析状况，在下文中，针对不同的通信模式，得到了两种MPI并行模型在不同进程下的加速比及计算效率。表3~5分别展示了GEL，GER，AGE格式下的MPI阻塞与非阻塞通信模型的程序运算时间，图4显示了运用交替分组格式求解抛物方程，在非并行环境与MPI并行环境下的程序运算时间柱状图。图5给出了4进程并行计算下阻塞通信与非阻塞通信的时间柱状图。表6给出了4进程通信下的并行效率及加速比。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> The executive time of 4 processes in blocking and non-blocking communication mode for GEL (unit/sec.</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >进程0</th><th align="center" valign="middle" >进程1</th><th align="center" valign="middle" >进程2</th><th align="center" valign="middle" >进程3</th><th align="center" valign="middle" >单个进程</th></tr></thead><tr><td align="center" valign="middle" >GEL格式阻塞通信计算时间</td><td align="center" valign="middle" >5.504456</td><td align="center" valign="middle" >5.541956</td><td align="center" valign="middle" >5.534419</td><td align="center" valign="middle" >5.27101</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >gel格式阻塞通信总时间</td><td align="center" valign="middle" >6.941848</td><td align="center" valign="middle" >6.941303</td><td align="center" valign="middle" >6.941578</td><td align="center" valign="middle" >6.941439</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >GEL单进程运算执行时间</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >25.39734</td></tr><tr><td align="center" valign="middle" >GEL格式非阻塞通信计算时间</td><td align="center" valign="middle" >5.189739</td><td align="center" valign="middle" >5.414192</td><td align="center" valign="middle" >5.513848</td><td align="center" valign="middle" >5.287586</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >GEL格式非阻塞通信总时间</td><td align="center" valign="middle" >6.307096</td><td align="center" valign="middle" >6.308636</td><td align="center" valign="middle" >6.309482</td><td align="center" valign="middle" >6.308666</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >GEL格式非阻塞通信等待时间</td><td align="center" valign="middle" >0.580585</td><td align="center" valign="middle" >0.201888</td><td align="center" valign="middle" >0.063008</td><td align="center" valign="middle" >0.493103</td><td align="center" valign="middle" ></td></tr></tbody></table></table-wrap><p>表3. GEL格式下4进程阻塞通信与非阻塞通信模式的程序运算时间比较(单位/秒)</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> The executive time of 4 processes in blocking and non-blocking communication mode for GER (unit/sec.</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >进程0</th><th align="center" valign="middle" >进程1</th><th align="center" valign="middle" >进程2</th><th align="center" valign="middle" >进程3</th><th align="center" valign="middle" >单个进程</th></tr></thead><tr><td align="center" valign="middle" >GER格式阻塞通信计算时间</td><td align="center" valign="middle" >5.56917</td><td align="center" valign="middle" >5.632037</td><td align="center" valign="middle" >5.605388</td><td align="center" valign="middle" >5.385093</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >geR格式阻塞通信总时间</td><td align="center" valign="middle" >6.804605</td><td align="center" valign="middle" >6.805075</td><td align="center" valign="middle" >6.805131</td><td align="center" valign="middle" >6.805165</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >GER单进程运算执行时间</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >25.372994</td></tr><tr><td align="center" valign="middle" >GER格式非阻塞通信计算时间</td><td align="center" valign="middle" >5.214437</td><td align="center" valign="middle" >5.44246</td><td align="center" valign="middle" >5.518787</td><td align="center" valign="middle" >5.217341</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >GER格式非阻塞通信总时间</td><td align="center" valign="middle" >6.332524</td><td align="center" valign="middle" >6.333553</td><td align="center" valign="middle" >6.335751</td><td align="center" valign="middle" >6.332815</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >GER格式非阻塞通信等待时间</td><td align="center" valign="middle" >0.576401</td><td align="center" valign="middle" >0.207211</td><td align="center" valign="middle" >0.089865</td><td align="center" valign="middle" >0.577639</td><td align="center" valign="middle" ></td></tr></tbody></table></table-wrap><p>表4. GER格式下4进程阻塞通信与非阻塞通信模式的程序运算时间比较(单位/秒)</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> The executive time of 4 processes in blocking and non-blocking communication mode for AGE (unit/sec.</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >进程0</th><th align="center" valign="middle" >进程1</th><th align="center" valign="middle" >进程2</th><th align="center" valign="middle" >进程3</th><th align="center" valign="middle" >单个进程</th></tr></thead><tr><td align="center" valign="middle" >AGE格式阻塞通信计算时间</td><td align="center" valign="middle" >5.58503</td><td align="center" valign="middle" >5.613905</td><td align="center" valign="middle" >5.533567</td><td align="center" valign="middle" >5.387905</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >AGE格式阻塞通信总时间</td><td align="center" valign="middle" >6.708155</td><td align="center" valign="middle" >6.708058</td><td align="center" valign="middle" >6.707898</td><td align="center" valign="middle" >6.70738</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >AGE单进程运算执行时间</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >25.099077</td></tr><tr><td align="center" valign="middle" >AGE格式非阻塞通信计算时间</td><td align="center" valign="middle" >5.449957</td><td align="center" valign="middle" >5.648745</td><td align="center" valign="middle" >5.650483</td><td align="center" valign="middle" >5.468564</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >AGE格式非阻塞通信总时间</td><td align="center" valign="middle" >6.367587</td><td align="center" valign="middle" >6.368233</td><td align="center" valign="middle" >6.367122</td><td align="center" valign="middle" >6.364805</td><td align="center" valign="middle" ></td></tr><tr><td align="center" valign="middle" >AGE格式非阻塞通信等待时间</td><td align="center" valign="middle" >0.559427</td><td align="center" valign="middle" >0.155265</td><td align="center" valign="middle" >0.188809</td><td align="center" valign="middle" >0.513443</td><td align="center" valign="middle" ></td></tr></tbody></table></table-wrap><p>表5. 交替分组格式下4进程阻塞通信与非阻塞通信模式的程序运算时间比较(单位/秒)</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Comparison of the parallel efficiency in 4 processe</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >总时间(单位/秒)</th><th align="center" valign="middle" >加速比</th><th align="center" valign="middle" >并行效率</th></tr></thead><tr><td align="center" valign="middle" >非并行时</td><td align="center" valign="middle" >25.099</td><td align="center" valign="middle" >-----</td><td align="center" valign="middle" >-----</td></tr><tr><td align="center" valign="middle" >GEL格式4进程阻塞通信</td><td align="center" valign="middle" >6.9418</td><td align="center" valign="middle" >3.62</td><td align="center" valign="middle" >0.905</td></tr><tr><td align="center" valign="middle" >GEL格式4进程非阻塞通信</td><td align="center" valign="middle" >6.3070</td><td align="center" valign="middle" >3.97</td><td align="center" valign="middle" >0.993</td></tr><tr><td align="center" valign="middle" >GER格式4进程阻塞通信</td><td align="center" valign="middle" >6.8052</td><td align="center" valign="middle" >3.68</td><td align="center" valign="middle" >0.92</td></tr><tr><td align="center" valign="middle" >GER格式4进程非阻塞通信</td><td align="center" valign="middle" >6.3357</td><td align="center" valign="middle" >3.96</td><td align="center" valign="middle" >0.99</td></tr><tr><td align="center" valign="middle" >AGE格式4进程阻塞通信</td><td align="center" valign="middle" >6.7081</td><td align="center" valign="middle" >3.74</td><td align="center" valign="middle" >0.93</td></tr><tr><td align="center" valign="middle" >AGE格式4进程非阻塞通信</td><td align="center" valign="middle" >6.3675</td><td align="center" valign="middle" >3.94</td><td align="center" valign="middle" >0.985</td></tr></tbody></table></table-wrap><p>表6. 不同格式下的4进程并行效率比较</p><p>图4. AGE阻塞通信模式下四进程与单进程计算的程序运行时间</p><p>图5. 四进程下AGE阻塞通信模式与非阻塞通信模式下的程序运行时间</p></sec><sec id="s7"><title>5. 总结</title><p>在上述数值模拟计算的数据表格中，在图4中可以看到，在MPI并行环境中，多条进程运用上述差分格式数值计算抛物方程要比单个进程计算的效率更高。多次试验数据表明，4进程并行计算的时间约是单进程计算的1/4，而且三种差分格式并未出现较大的差别，因为可以在上文中知道，三种差分格式的计算任务量是大致相等的，而MPI并行化处理并未改变计算逻辑，而只是将计算任务分摊，从而达到计算效率的提升。在表3~5与图5中，可以看到MPI阻塞通信对于非阻塞通信的优势，其中，对某一进程来说，非阻塞与阻塞通信的程序计算时间是大致相等的，而非阻塞通信程序的总运算时间是相对较小的，因为非阻塞通信减少了各进程之间的消息传递时间，即通信等待时间，在表3~5中并未给出阻塞通信的通信等待时间，是因为在阻塞通信模型中该时间是无法显示的，不过在实际运算中，阻塞通信的通信等待时间是要高于非阻塞通信的。因此非阻塞通信相对于阻塞通信来说表现出更大的优势。但是，非阻塞通信模型的建立要比阻塞通信更加复杂，这在程序的编码实现上有较多的问题需要解决。在表6还可以看到不同差分格式下的4进程并行效率的比较，可以进一步表明非阻塞通信相对于阻塞通信的优势。即在相同的进程数目下，无论是加速比还是并行效率，非阻塞通信都要高于阻塞通信。</p></sec><sec id="s8"><title>文章引用</title><p>高玉羊,顾海明. 求解抛物方程的MPI并行方法 Parallel Methods for Parabolic Equations Based on MPI Implementation[J]. 理论数学, 2017, 07(02): 89-98. http://dx.doi.org/10.12677/PM.2017.72013</p></sec><sec id="s9"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.20059-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">都志辉, 李三立, 陈渔, 刘鹏. 高性能计算之并行编程技术——MPI并行程序设计[M]. 北京: 清华大学出版社, 2001.</mixed-citation></ref><ref id="hanspub.20059-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Quinn, M.J. (2004) Parallel Programming in C with MPI and Open MP. McGraw-Hill, New York.</mixed-citation></ref><ref id="hanspub.20059-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Evans, D.J. and Abdullah, A.R. (1983) Group Explicit Methods for Parabolic Equations. International Journal Computer Mathematics, 14, 73-105. &lt;br&gt;https://doi.org /10.1080/00207168308803377</mixed-citation></ref><ref id="hanspub.20059-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Saul’yev, V.K. (1965) Integration of Equations of Parabolic Type by the Method of Nets. Proceedings of the Edinburgh Mathematical Society, 14, 247-248. &lt;br&gt;https://doi.org /10.1017/S0013091500008890</mixed-citation></ref><ref id="hanspub.20059-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Mohd, A., Norhashidah, H. and Khoo, K.T. (2010) Numerical Performance of Parallel Group Explicit Solvers for the Solution of Fourth Order Elliptic Equations. Applied Mathematics and Computation, 217, 2737-2749.</mixed-citation></ref><ref id="hanspub.20059-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Vabishchevich, P.N. (2015) Explicit Schemes for Parabolic and Hyperbolic Equations. Applied Mathematics and Computation, 250, 424-431.</mixed-citation></ref><ref id="hanspub.20059-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">张宝琳. 求解扩散方程的交替分段显-隐式方法[J]. 数值计算与计算机应用, 1991, 4: 245-253.</mixed-citation></ref><ref id="hanspub.20059-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">张武生, 薛巍, 李建江, 郑纬民. MPI并行程序设计实例教程[M]. 北京: 清华大学出版社, 2009.</mixed-citation></ref><ref id="hanspub.20059-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Gorobets, A.V., Trias, F.X. and Oliva, A. (2013) A Parallel MPI + Open MP + Open CL Algorithm for Hybridsuper Computations of Incompressible Flows. Computers &amp; Fluids, 88, 764-772.</mixed-citation></ref><ref id="hanspub.20059-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Kellogg, R.B. (1964) An Alternating Direction Method for Operator Equations. Journal of the Society of Industrial and Applied Mathematics, 12, 7. &lt;br&gt;https://doi.org /10.1137/0112072</mixed-citation></ref></ref-list></back></article>