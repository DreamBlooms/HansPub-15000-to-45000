<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">SEA</journal-id><journal-title-group><journal-title>Software Engineering and Applications</journal-title></journal-title-group><issn pub-type="epub">2325-2286</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/SEA.2019.85027</article-id><article-id pub-id-type="publisher-id">SEA-32511</article-id><article-categories><subj-group subj-group-type="heading"><subject>SEA20190500000_55402311.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>工程技术</subject></subj-group></article-categories><title-group><article-title>
 
 
  用户群组发现及兴趣用户推荐的改进的K-Means聚类算法
  Improved K-Means Clustering Algorithm for User Group Discovery and Interest User Recommendation
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>曾</surname><given-names>东香</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>曹</surname><given-names>彩凤</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>黎</surname><given-names>冬园</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>五邑大学智能制造学部，广东 江门</addr-line></aff><aff id="aff2"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>10</day><month>10</month><year>2019</year></pub-date><volume>08</volume><issue>05</issue><fpage>223</fpage><lpage>231</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  当前，电子商务网站、学习资源平台网站以及社交网站普遍都具备对评价事物的推荐功能，而不具备给用户发现和推荐感兴趣用户的功能。针对此问题，本文在K-means聚类算法的基础上加入包含抽样、降维、层次聚类等过程的预聚类阶段，设计出为用户推荐兴趣用户的FPSHK-means群组发现算法，并通过其与经典K-means聚类算法的对照实验，验证了FPSHK-means群组发现算法能比经典K-means算法发现更多的群组，且聚类结果更贴近数据对象的实际分布情况。
   At present, e-commerce websites, learning resource platforms websites and social networking sites generally have the recommendation function for comment things, but not the function of users discovering and recommending users who are interested. In this paper, the FPSHK-means group discovery algorithm was designed by blending a pre-clustering stage on the basis of the K-means clustering algorithm. The pre-clustering stage includes sampling, dimensionality reduction and hierarchical clustering. The FPSHK-means group discovery algorithm is designed to recommend interested users for the users. Through the comparison experiment with the classical K-means clustering algorithm, it is verified that the FPSHK-means group discovery algorithm can find more groups than the classical K-means algorithm. And the result of clustering is closer to the actual distribution of the data object.
 
</p></abstract><kwd-group><kwd>K-Means聚类算法，群组发现，用户推荐, K-Means Clustering Algorithm</kwd><kwd> Group Discovery</kwd><kwd> Users Recommendation</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>用户群组发现及兴趣用户推荐的改进的K-Means聚类算法<sup> </sup></title><p>曾东香，曹彩凤，黎冬园</p><p>五邑大学智能制造学部，广东 江门</p><p><img src="//html.hanspub.org/file/3-2690385x1_hanspub.png" /></p><p>收稿日期：2019年9月25日；录用日期：2019年10月7日；发布日期：2019年10月14日</p><disp-formula id="hanspub.32511-formula30"><graphic xlink:href="//html.hanspub.org/file/3-2690385x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>当前，电子商务网站、学习资源平台网站以及社交网站普遍都具备对评价事物的推荐功能，而不具备给用户发现和推荐感兴趣用户的功能。针对此问题，本文在K-means聚类算法的基础上加入包含抽样、降维、层次聚类等过程的预聚类阶段，设计出为用户推荐兴趣用户的FPSHK-means群组发现算法，并通过其与经典K-means聚类算法的对照实验，验证了FPSHK-means群组发现算法能比经典K-means算法发现更多的群组，且聚类结果更贴近数据对象的实际分布情况。</p><p>关键词 :K-Means聚类算法，群组发现，用户推荐</p><disp-formula id="hanspub.32511-formula31"><graphic xlink:href="//html.hanspub.org/file/3-2690385x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-2690385x7_hanspub.png" /> <img src="//html.hanspub.org/file/3-2690385x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>随着大数据的到来，我们需要一种方式来处理各种类型的海量数据以支持正确的决策和行动。聚类就是获得那些有意义、有实际价值的数据之间的内在分布结构，进而简化海量数据的描述。聚类的目标是把具有相似特性的实物放到一起，即“类内的相似性与类间的排他性” [<xref ref-type="bibr" rid="hanspub.32511-ref1">1</xref>] 。到目前为止，聚类研究及其应用领域已经非常广泛，如语音识别、字符识别、图像分割、数据挖掘、时空数据库应用、序列和异类数据分析等领域 [<xref ref-type="bibr" rid="hanspub.32511-ref2">2</xref>] 。</p><p>本文首先简要介绍了K-means聚类算法的主要过程，然后分析并总结了课题组已发表的SPHK-means聚类算法，提出了一种改进的为用户推荐兴趣用户的FPSHK-means群组发现算法，并与经典的K-means算法进行对比实验。实验结果表明，FPSHK-means群组发现算法的聚类的效果更好、质量更优。</p></sec><sec id="s4"><title>2. 相关知识和工作</title><sec id="s4_1"><title>2.1. K-Means聚类算法概述</title><p>K-means是一种较典型的基于样本间相似性度量的逐点修改迭代的动态聚类算法，属于机器学习中方法中的非监督学习。此算法依据设定的参数k，把n个对象划分到k个簇中，每个簇中心为簇中对象的平均值，使得每一个对象与所属簇的中心具有较高的相似度，而与不同簇的中心相似度较低。K-means算法简单易实现，并且广泛使用，其算法流程图如图1所示。</p><p>K-means聚类算法步骤描述如下：</p><p>步骤1：输入参数k (类数目)、参数t (准则函数阈值)、距离度量(通常是欧氏距离)和准则函数(通常是误差平方和)，并在总体中随机选取k个数据对象作为初始聚类中心 [<xref ref-type="bibr" rid="hanspub.32511-ref3">3</xref>] 。</p><p>步骤2：分别所有数据对象到k个中心的距离，将总体中的每个数据对象划分到与之距离最小的聚类中心所属的类中。</p><p>步骤3：重新计算k个类的中心，每个类的新中心为该类中所有数据对象的均值。</p><p>步骤4：计算准则函数，若其值与上一轮迭代相同或少于t，则进行步骤5；否则转至步骤2。</p><p>图1. K-means聚类算法流程</p><p>步骤5：输出各个簇的中心及其成员对象。</p><p>K-means聚类算法 [<xref ref-type="bibr" rid="hanspub.32511-ref4">4</xref>] - [<xref ref-type="bibr" rid="hanspub.32511-ref12">12</xref>] 存在以下缺点。其一，经典的K-means聚类算法对大规模数据集的处理效率低。其二，类数目k难以事先确定，以及初始聚类中心的随机性，这两点使得K-means算法的聚类结果很多时候会陷入局部最优而难以得到全局最优。其三，因为K-means是基于样本间相似度的聚类算法，所以它只能发现球形簇。这些局限都使得经典K-means算法的聚类结果很多时候跟真实的群组分布情况相差较大。</p></sec><sec id="s4_2"><title>2.2. 群组发现算法SPHK-Means概述</title><p>在课题组已发表的论文 [<xref ref-type="bibr" rid="hanspub.32511-ref13">13</xref>] 中，提出了一种改进的K-means聚类算法——SPHK-means聚类算法。该算法以弥补本文上一节所述的经典K-means聚类算法的缺点为目标，在进行K-means聚类之前加入两次预聚类。预聚类阶段的步骤包括抽样(Sampling)、主成分分析(Principal Component Analysis)降维和层次聚类(Agglomerative Hierarchical)，目的是确定适宜的簇数目k和k个初始聚类中心。在课题组已发表的另一篇论文 [<xref ref-type="bibr" rid="hanspub.32511-ref14">14</xref>] 中，使用Movie Lens数据集设计SPHK-means聚类算法与经典K-means聚类算法的对照实验，分别用它们来发现电影类别。在该数据集中，电影实际有19个类别，SPHK-means算法能发现12至15个类别，而经典的K-means算法只能发现2至3个类别，而且SPHK-means算法的F分值比经典K-means算法的F分值高约5%，这说明SPHK-means算法能比经典K-means算法发现更多群组且聚类效果更优。</p><p>现对F分值加以说明。假定聚类结果如图2 [<xref ref-type="bibr" rid="hanspub.32511-ref15">15</xref>] 所示，Precision和Recall定义如公式(1)和(2)，F的定义如公式(3)所示。</p><p>Precision = | true positives | | true positives | + | false positives | (1)</p><p>Recall = | true positives | | true positives | + | false negatives | (2)</p><p>F = 2 ⋅ Precision ⋅ Recall Precision + Recall (3)</p><p>图2. 描述precision和recall</p></sec></sec><sec id="s5"><title>3. FPSHK-Means聚类算法设计</title><p>在社交网站中，用户不仅能对具体事物给出评分和书写评论，还能通过浏览其他用户的评分、评论动态，发现感兴趣的其他用户。针对这一应用情景，将设计一种改进的K-means聚类算法，命名为“FPSHK-means”，用于帮助用户发现兴趣群组，并为用户推荐同属一个群组的其他用户。</p><sec id="s5_1"><title>3.1. 设计思路</title><p>SPHK-means聚类算法仍有以下不足之处：</p><p>1) SPHK-means算法是在预聚类阶段的每次抽样之后对样本进行降维处理，在真聚类阶段再对数据总体降维。这样的操作顺序可能会因数据的不同而导致样本和总体的主成分分析的系数存在差异，造成样本和总体在降维时的线性变换可能略有不同，进而影响最终聚类结果。</p><p>2) SPHK-means算法是直接使用原始用户–物品稀疏评分矩阵R做聚类，对缺失值的处理方法是直接赋0值，这样会忽略用户对未知物品的潜在信息，而这些信息却能够直接影响用户聚类的结果。</p><p>针对以上两点做出改进，FPSHK-means聚类算法以奇异值分解(Singular Value Decomposition, SVD)算法计算评分预测值的过程为其数据预处理阶段，用户–物品预测评分矩阵<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-2690385x14_hanspub.png" xlink:type="simple"/></inline-formula>(而非原始稀疏评分矩阵R)做聚类，并将主成分分析降维安排在其他所有步骤之前。</p></sec><sec id="s5_2"><title>3.2. 模型设计</title><p>设用户总数为U，物品总数为M，则经BC-SVD算法计算填充的用户–物品预测评分矩阵 R ^ U &#215; M 可表示为：</p><p>R ^ = [ r ^ 1 , 1 r ^ 1 , 2 ⋯ r ^ 1 , M r ^ 2 , 1 ⋱ ⋮ ⋮ ⋱ ⋮ r ^ U , 1 ⋯ ⋯ r ^ U , M ] = [ X 1 , X 2 , ⋯ , X M ] (4)</p><p>其中， X i = [ r ^ 1 , i , r ^ 2 , i − 1 , r ^ U , i ] T 为代表第i个物品的列向量。</p><p>FPSHK-means聚类算法的操作流程图如图3所示。</p><p>图3. FPSHK-means聚类算法流程</p><p>FPSHK-means聚类算法步骤如下：</p><p>步骤1：为了将代表每位用户u的向量 ( r ^ u , 1 , r ^ u , 2 , ⋯ , r ^ u , M ) 降至 ρ 维( ρ ≪ M )，对 R ^ 整体做主成分分析 [<xref ref-type="bibr" rid="hanspub.32511-ref16">16</xref>] ：</p><p>{ F 1 = a 11 X 1 + a 21 X 2 + ⋯ + a M 1 X M F 2 = a 12 X 1 + a 22 X 2 + ⋯ + a M 2 X M                                         ⋮ F p = a 1 p X 1 + a 2 p X 2 + ⋯ + a M p X M (5)</p><p>并使其满足以下三个条件限制 [<xref ref-type="bibr" rid="hanspub.32511-ref16">16</xref>] ：</p><p>1) 对于每个主成分 F i ( i = 1 , 2 , ⋯ , ρ ) 系数的平方和为1：</p><p>a 1 i 2 + a 2 i 2 + ⋯ + a M i 2 = 1 (6)</p><p>2) 不同的主成分 F i 和 F i ( i ≠ j ; i , j = 1 , 2 , ⋯ , ρ ) 相互独立，即协方差为零：</p><p>C o v ( F i , F j ) = 0 (7)</p><p>3) 所有主成分依其重要程度(方差)呈递减排列，即：</p><p>V a r ( F 1 ) ≥ V a r ( F 2 ) ≥ ⋯ ≥ V a r ( F p ) (8)</p><p>预测评分矩阵 R ^ 降维后得到以下矩阵：</p><p>R r e d = [ f 1 , 1 f 1 , 2 ⋯ f 1 , p f 2 , 1 ⋱ ⋮ ⋮ ⋱ ⋮ f U , 1 ⋯ ⋯ f U , p ] (9)</p><p>其中， f u i 为预测评分矩阵 R ^ 中代表用户u的行向量的第 i ( i = 1 , 2 , ⋯ , ρ ) 个主成分：</p><p>f u i = a 1 i r ^ u , 1 + a 2 i r ^ u , 2 + ⋯ + a M i r ^ u , M (10)</p><p>步骤2：进行第一阶段的预聚类。将所有用户随机分成s组(s由系统预设)，每一组成为一份样本，每份样本含有U/s位用户。对s个样本，分别做层次聚类，第i个样本生成的簇的数目为 k ( i ) ， ( i = 1 , 2 , 3 , ⋯ , s ) ，计算k的均值 k &#175; 和标准差 σ ( k ) ：</p><p>k &#175; = 1 n ∑ i = 1 n k ( i ) (11)</p><p>σ ( k ) = 1 n ∑ i = 1 n ( k ( i ) − k &#175; ) 2 (12)</p><p>令用户群组数的取值范围为 ( k &#175; − σ ( k ) / 2 , k &#175; + σ ( k ) / 2 ) 。</p><p>步骤3：进行第二阶段的预聚类。逐次抽取一个样本量为s的样本并做层次聚类，直到出现第一个簇数目在 ( k &#175; − σ ( k ) / 2 , k &#175; + σ ( k ) / 2 ) 内的样本为止。设该样本的簇数目为 k 0 ，簇中心为 { v 1 , v 2 , ⋯ , v k 0 } 。</p><p>步骤4：进入真聚类阶段，以 { v 1 , v 2 , ⋯ , v k 0 } 为初始中心，对数据对象总体做K-means聚类，最终得到 k 0 个用户群组 { C 1 , C 2 , ⋯ , C k 0 } ，则为用户u推荐的用户集合为： C ( u ) − { u } ， C ( u ) 为用户u所属的簇。</p></sec></sec><sec id="s6"><title>4. 实验结果与分析</title><sec id="s6_1"><title>4.1. 实验数据集与实验环境</title><p>算法实验同样使用Movie Lens的Movie Lens Latest Datasets (Small)数据集，其中表movies.csv包含Movie Lens网站对9125部电影的分类标签信息，共19个分类标签，其中前三条记录如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The top five records of movie</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >movieId</th><th align="center" valign="middle" >title</th><th align="center" valign="middle" >genres</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >Toy Story (1995)</td><td align="center" valign="middle" >Adventure/Animation/Children/Comedy/Fantasy</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Jumanji (1995)</td><td align="center" valign="middle" >Adventure/Children/Fantasy</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >Grumpier Old Men (1995)</td><td align="center" valign="middle" >Comedy/Romance</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >Waiting to Exhale (1995)</td><td align="center" valign="middle" >Comedy/Drama/Romance</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >Father of the Bride Part II (1995)</td><td align="center" valign="middle" >Comedy</td></tr></tbody></table></table-wrap><p>表1. Movies的前五条记录</p><p>上表中，“movieId”是Movie Lens数据集对其所包含的电影的编号，“title”是电影的名称，“genres”是电影的分类标签。</p><p>由于Movie Lens数据集并没有对671位用户的预定分组，所以对于用户真实的兴趣群组数目只能用电影的预定分类数目估计。</p><p>算法实验同样在Ubuntu 16.04操作系统中使用Python 3.6完成。</p></sec><sec id="s6_2"><title>4.2. 评价指标</title><p>聚类属于机器学习中的无监督学习，实验数据集中并没有用户分类的标签数据，无法使用上一章实验的系列评价指标对聚类的结果进行评价。</p><p>FPSHK-means算法与经典的K-means算法都属于基于相似度的聚类算法。评价这类算法通常使用误差平方和(Sum of Squared Errors, SSE)作为评价指标。</p><p>SSE = ∑ i = 1 k ∑ x ∈ C i ( x − v i ) 2 (13)</p><p>其中， C i 为第i个类簇，i为 C i 的聚类中心。SSE越小，说明聚类结果中各簇的聚合程度越高，聚类的效果越好。</p><p>当聚类算法用于群组发现时，希望其能发现贴近数据对象真实分类情况的群组数目和群组结构。若发现的群组数目比数据对象的真实分类数目还多，则该算法对数据对象有比真是分类情况更进一步的细分。</p></sec><sec id="s6_3"><title>4.3. 实验结果分析</title><p>分别以FPSHK-means算法和经典的K-means算法对实验数据集中所有用户做聚类，均用误差平方和SSE作为准则函数且迭代至准则函数收敛为止(相继两次迭代的SSE相同)。重复三次实验，记录每次实验发现的群组数目和算法结束时的SSE，如表2所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Experimental results comparison of FPSHK-means algorithm and K-means algorith</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >实验次数</th><th align="center" valign="middle" >聚类算法</th><th align="center" valign="middle" >发现群组数</th><th align="center" valign="middle" >SSE</th></tr></thead><tr><td align="center" valign="middle"  rowspan="2"  >1</td><td align="center" valign="middle" >FPSHK-means</td><td align="center" valign="middle" >13</td><td align="center" valign="middle" >1575163</td></tr><tr><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1811250</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >2</td><td align="center" valign="middle" >FPSHK-means</td><td align="center" valign="middle" >14</td><td align="center" valign="middle" >1576007</td></tr><tr><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1810974</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >3</td><td align="center" valign="middle" >FPSHK-means</td><td align="center" valign="middle" >12</td><td align="center" valign="middle" >1575784</td></tr><tr><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >1811042</td></tr></tbody></table></table-wrap><p>表2. FPSHK-means算法与K-means算法实验结果对比</p><p>Segaran在他的著作中提到了用于数据可视化的多维缩放技术 [<xref ref-type="bibr" rid="hanspub.32511-ref17">17</xref>] ，这个是一种将多维数据集展现在二维平面上的技术。在多维缩放效果图中，数据成员两两间的距离越大，它们在原多维空间中的相似度越小。</p><p>应用多维缩放技术，展示FPSHK-means算法和经典K-means算法对实验数据集中671位用户的聚类结果，分别如图4和图5所示，每一位用户用一个点表示。</p><p>需要特别说明的是，在此实验中，由于经典的K-means算法需要输入类数目参数k，所以先进行FPSHK-means聚类算法的实验，将FPSHK-means算法发现的群组数目赋给经典K-means算法的参数k。但是随着经典K-means聚类算法的逐次迭代，会陆续出现空组(一次迭代结束后，没有数据对象被归入其中的组称为“空组”)。在此实验中，每出现一个空组就其消除后再进入下一轮迭代(因为就算不将其消除，再往后的迭代过程中也不会有数据对象被划入其中了)。</p><p>即便给予经典K-means算法这种事先获得合适k值的“优待”，但由表2可知，FPSHK-means聚类算法依然总是比经典K-means聚类算法发现更多的用户群组(平均多发现约10个群组)。数据集中的电影被预定分19类，但没有用户的预定分组信息，如果以每类电影拥有一个兴趣用户群来估计真实用户群组的话，应该有19个兴趣用户群，希望群组发现算法能发现接近19个或更多的群组。FPSHK-means算法能发现其中的12至14个群组，比经典K-means算法发现的2至3个群组更接近19个兴趣用户群。</p><p>而且，FPSHK-means算法结束时的误差平方和SSE总是比经典K-means算法结束时的误差平方和SSE更少，即FPSHK-means算法发现的群组比经典K-means算法发现的群组的组内用户相似性更高，同一群组内的用户的聚拢程度更高。再对比图4和图5可知，FPSHK-means算法不仅比经典K-means算法能发现更多的群组，而且聚类的最终结果也更贴近数据成员在数据集中的自然分布情况，聚类效果更好。</p><p>图4. FPSHK-means聚类算法的多维缩放效果</p><p>图5. K-means聚类算法的多维缩放效果</p><p>综上所述，本文设计的FPSHK-means群组发现算法能比经典K-means算法发现更多的群组，而且聚类结果更贴近数据对象的实际分布情况。相较于经典K-means算法，FPSHK-means算法的聚类效果更好、聚类结果更优质。</p></sec></sec><sec id="s7"><title>5. 结语</title><p>在对K-means算法深入研究的基础上，根据商务网站和社交网站应用场景的需要，设计出为用户推荐兴趣用户的FPSHK-means群组发现算法。阐述了算法的设计思路、模型构建、操作流程和对照实验。下一步工作是将FPSHK-means算法应用到实际应用系统中，已着手设计并实现学习资源推荐系统。并以此为基础，进一步推广与应用。</p></sec><sec id="s8"><title>文章引用</title><p>曾东香,曹彩凤,黎冬园. 用户群组发现及兴趣用户推荐的改进的K-Means聚类算法Improved K-Means Clustering Algorithm for User Group Discovery and Interest User Recommendation[J]. 软件工程与应用, 2019, 08(05): 223-231. https://doi.org/10.12677/SEA.2019.85027</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.32511-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">章永来, 周耀鉴. 聚类算法综述[J]. 计算机应用, 2019, 39(7): 1869-1882.</mixed-citation></ref><ref id="hanspub.32511-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">甘月松, 陈秀宏, 陈晓晖. 一种AP算法的改进: M-AP聚类算法[J]. 计算机科学, 2015, 42(1): 232-235+267.</mixed-citation></ref><ref id="hanspub.32511-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">王娟. 一种基于遗传算法的K-means聚类算法[J]. 微型机与应用, 2011, 30(20): 71-76.</mixed-citation></ref><ref id="hanspub.32511-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Arthur, D. and Vassilvitskii, S. (2007) K-Means++: The Advantages of Careful Seeding. In: Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, Society for Industrial and Applied Mathematics, Philadelphia, PA, 1027-1035.</mixed-citation></ref><ref id="hanspub.32511-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Goel, L., Jain, N. and Srivastava, S. (2016) A Novel PSO Based Algorithm to Find Initial Seeds for the K-Means Clustering Algorithm. Communication and Computing Systems Clustering Algorithm. 2016 The International Conference on Communication and Computing Systems, Gurgaon, 9-11 September 2016, 159-163.</mixed-citation></ref><ref id="hanspub.32511-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Gu, L. (2017) A Novel Locality Sensitive K-Means Clustering Algorithm Based on Subtractive Clustering. 2016 7th IEEE International Conference on Software Engineering and Service Science, Beijing, 26-28 August 2016, 836-839. &lt;br&gt;https://doi.org/10.1109/ICSESS.2016.7883196</mixed-citation></ref><ref id="hanspub.32511-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">谢娟英, 王艳娥. 最小方差优化初始聚类中心的K-Means算法[J]. 计算机工程, 2014, 40(8): 205-211+223.</mixed-citation></ref><ref id="hanspub.32511-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">贾瑞玉, 李玉功. 类簇数目和初始中心点自确定的K-Means算法[J]. 计算机工程与应用, 2018, 54(7): 152-158.</mixed-citation></ref><ref id="hanspub.32511-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">马福民, 逯瑞强, 张腾飞. 基于局部密度自适应度量的粗糙K-Means聚类算法[J]. 计算机工程与科学, 2018, 40(1): 184-190.</mixed-citation></ref><ref id="hanspub.32511-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">丛思安, 王星星. K-Means算法研究综述[J]. 电子技术与软件工程, 2018(17): 155-156.</mixed-citation></ref><ref id="hanspub.32511-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">明小红. 基于用户聚类的协同过滤推荐算法研究[D]: [硕士学位论文]. 北京: 北京交通大学, 2017.</mixed-citation></ref><ref id="hanspub.32511-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">王千, 王成, 冯振元, 叶金凤. K-Means聚类算法研究综述[J]. 电子设计工程, 2012, 20(7): 21-24.</mixed-citation></ref><ref id="hanspub.32511-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Li, D.-Y. and Cao, C.-F. (2017) An Improved K-Means Clustering Algorithm Applicable to Massive High-Dimensional Matrix Datasets. Proceedings of the 2017 International Conference on Information Science and Technology, EDP Sciences, Wuhan, 24-26 March 2017, 269-275.</mixed-citation></ref><ref id="hanspub.32511-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Li, D.-Y. and Cao, C.-F. (2017) Discovering Movie Categories Based on SPHK-Means Clustering Algorithm. Proceedings of the 2017 International Conference on Information Science and Technology, EDP Sciences, Wuhan, 24-26 March 2017, 276-283.</mixed-citation></ref><ref id="hanspub.32511-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">https://en.wikipedia.org/wiki/Precision_and_recall</mixed-citation></ref><ref id="hanspub.32511-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Shlens, J. (2014) A Tutorial on Principal Component Analysis.&lt;br&gt; https://arxiv.org/pdf/1404.1100.pdf</mixed-citation></ref><ref id="hanspub.32511-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Segaran, T. (2015) Programming Collective Intelligence. Publishing House of Electronics Industry, Beijing, 49-52.</mixed-citation></ref></ref-list></back></article>