<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">SA</journal-id><journal-title-group><journal-title>Statistics and Application</journal-title></journal-title-group><issn pub-type="epub">2325-2251</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/SA.2021.102018</article-id><article-id pub-id-type="publisher-id">SA-41417</article-id><article-categories><subj-group subj-group-type="heading"><subject>SA20210200000_93670165.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  高维数据在Cox回归模型中的自变量选择
  Independent Variable Selection of High-Dimensional Data in Cox Regression Model
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>锋</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>胡</surname><given-names>天英</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>俊霖</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>但</surname><given-names>晨</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff2"><addr-line>重庆理工大学理学院，重庆</addr-line></aff><pub-date pub-type="epub"><day>31</day><month>03</month><year>2021</year></pub-date><volume>10</volume><issue>02</issue><fpage>183</fpage><lpage>192</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    在高维数据分析中，LASSO维数约简方法占有很重要的位置。针对数据愈发繁杂，LASSO回归不再适应一些具有较高相关性的高维数据分析，由此产生了Elastic Net和其他相关的一些高维数据拓展分析方法。Elastic Net是在LASSO的思想方法基础上结合非凸罚函数和岭回归方法得到的，Adaptive Elastic Net等是在Elastic Net的思想方法上，通过数据特征的不同改进惩罚函数，缓和稀疏性和过拟合的问题。文章对Elastic Net、Adaptive Elastic Net、Weight Elastic Net进行了介绍并且通过实际例子做了简单的比较，最终得到了较好的维数约简方法。
    In high-dimensional data analysis, LASSO occupies a very important position. For data becoming more and more complicated, LASSO regression is not very suitable for some relevant high-dimen- sional data analysis. Some of the extended analysis methods are produced, such as adaptive Elastic Net and other high-dimensional data analysis methods. Elastic Net is obtained by combining non- convex penalty and ridge regression methods on the basis of LASSO’s method of thinking. Adaptive Elastic Net, etc., based on Elastic Net’s method of thinking, uses different data characteristics to improve the penalty function, and continuously corrects the sparsity and overfitting problem. The article mainly introduces Elastic Net, Adaptive Elastic Net, Weight Elastic Net and makes a simple comparison of several methods through practical examples. 
  
 
</p></abstract><kwd-group><kwd>高维共线性，变量选择，Elastic Net, High-Dimensional Collinearity</kwd><kwd> Variable Selection</kwd><kwd> Elastic Net</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>在高维数据分析中，LASSO维数约简方法占有很重要的位置。针对数据愈发繁杂，LASSO回归不再适应一些具有较高相关性的高维数据分析，由此产生了Elastic Net和其他相关的一些高维数据拓展分析方法。Elastic Net是在LASSO的思想方法基础上结合非凸罚函数和岭回归方法得到的，Adaptive Elastic Net等是在Elastic Net的思想方法上，通过数据特征的不同改进惩罚函数，缓和稀疏性和过拟合的问题。文章对Elastic Net、Adaptive Elastic Net、Weight Elastic Net进行了介绍并且通过实际例子做了简单的比较，最终得到了较好的维数约简方法。</p></sec><sec id="s2"><title>关键词</title><p>高维共线性，变量选择，Elastic Net</p></sec><sec id="s3"><title>Independent Variable Selection of High-Dimensional Data in Cox Regression Model</title><p>—Dimension Reduction Based on Elastic Net<sup> </sup></p><p>Feng Liu<sup>*</sup>, Tianying Hu, Junlin Chen, Chen Dan</p><p>School of Science, Chongqing University of Technology, Chongqing</p><p><img src="//html.hanspub.org/file/1-2580720x4_hanspub.png" /></p><p>Received: Mar. 5<sup>th</sup>, 2021; accepted: Mar. 13<sup>th</sup>, 2021; published: Mar. 31<sup>st</sup>, 2021</p><p><img src="//html.hanspub.org/file/1-2580720x6_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>In high-dimensional data analysis, LASSO occupies a very important position. For data becoming more and more complicated, LASSO regression is not very suitable for some relevant high-dimen- sional data analysis. Some of the extended analysis methods are produced, such as adaptive Elastic Net and other high-dimensional data analysis methods. Elastic Net is obtained by combining non- convex penalty and ridge regression methods on the basis of LASSO’s method of thinking. Adaptive Elastic Net, etc., based on Elastic Net’s method of thinking, uses different data characteristics to improve the penalty function, and continuously corrects the sparsity and overfitting problem. The article mainly introduces Elastic Net, Adaptive Elastic Net, Weight Elastic Net and makes a simple comparison of several methods through practical examples.</p><p>Keywords:High-Dimensional Collinearity, Variable Selection, Elastic Net</p><disp-formula id="hanspub.41417-formula7"><graphic xlink:href="//html.hanspub.org/file/1-2580720x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/1-2580720x8_hanspub.png" /> <img src="//html.hanspub.org/file/1-2580720x9_hanspub.png" /></p></sec><sec id="s5"><title>1. 几种EN的讨论</title><sec id="s5_1"><title>1.1. 经典LASSO</title><p>针对高维数据的特殊性质，Tibshirani (1996)受到Frank (1993)提出的bridge regression和Breiman (1995)提出的non-negative garrote思想启发，提出了LASSO (least absolute shrinkage and selection operator)变量选择方法。Non-negative garrote比其他子集选择方法的预测误差小，而且当回归系数中有很多较小非零系数时，non-negative garrote方法也比ridge regression的预测误差小。Non-negative garrote方法在变量选择方面有很好的性质，但是它在进行参数估计时会过度依赖OLS估计值的符号和大小，LASSO方法避免了由于OLS估计带来的负面影响，所以LASSO估计在高维数据变量选择方面得到了广泛的应用，之后也有ALASSO、RLASSO、GLASSO等“优化”后的LASSO方法提出，LASSO估计定义为：</p><p>β ^ l a s s o = a r g m i n ( ∑ i = 1 n ( y i − ∑ j = 1 p β j x i j ) 2 ) , s t ∑ j = 1 p | β j | ≤ λ (1)</p><p>其中 λ ≥ 0 为惩罚参数，可以用来控制参数的压缩量。选择适当的λ可以使得一些回归系数缩小并趋向于0，且可依据高维数据的特殊结构，不要求设计矩阵为满秩。</p><p>LASSO继承了non-negative garrote 的在变量选择方面的优良性质，同时改进了其缺点，之后Bradley Efron等提出的最小角回归(LARS)算法又解决了LASSO的计算问题，使得LASOO在高维数据变量选择方面得到广泛的应用 [<xref ref-type="bibr" rid="hanspub.41417-ref1">1</xref>]。当变量间存在共线性的情况时，LASSO的效果不如岭回归，所以结合岭回归的优势，提出了和LASSO相似，能够进行变量选择(可解释的预测规则)和收缩(防止过度拟合)的EN (Elastic Net)，而且EN还能以组的形式选入或者选出模型，这一思想也可以应用于图像识别处理技术上 [<xref ref-type="bibr" rid="hanspub.41417-ref2">2</xref>]。</p></sec><sec id="s5_2"><title>1.2. EN估计</title><p>Zou等提出了Elastic Net方法 [<xref ref-type="bibr" rid="hanspub.41417-ref3">3</xref>]，且说明了在具有共线性变量的时候EN效果会优于LASSO，高维数据中的协变量通常具有复共线性 [<xref ref-type="bibr" rid="hanspub.41417-ref4">4</xref>]。LASSO分析在面对有多个强相关变量的时候，会选择只保留一个变量，而EN会全部保留。EN和LASSO一样，借鉴了NEN (Naive Elastic Net)的思想，折中了岭回归和LASSO回归 [<xref ref-type="bibr" rid="hanspub.41417-ref5">5</xref>]，通过混合比控制，其表达式可表现为：</p><p>J ( θ ) = M S E ( θ ) + r α ∑ i = 1 n | θ i | + 1 − r 2 α ∑ i = 1 n   θ i 2 (2)</p><p>当r = 0时，EN方法与岭回归方法同表达式；当r = 1时，EN方法与LASSO方法同表达式。NEN原本对具有共线性变量的数据就有很好的建模性质，克服了LASSO在针对共线性变量选择的不足后，也综合了岭回归在处理共线性变量问题上的优势，EN估计定义为：</p><p>β ^ E N = a r g m i n ( ∑ i = 1 n ( y i − ∑ j = 1 p   β j x i j ) 2 + λ 1 ‖ β ‖ 1 + λ 2 ‖ β ‖ 2 ) (3)</p><p>其中， ‖ β ‖ 2 = ∑ k = 1 p   β k 2 , ‖ β ‖ 1 = ∑ k = 1 p | β k | 。</p><p>λ 1 和 λ 2 为调和参数且均非负，当 λ 2 = 0 时就是LASSO回归， λ 1 = 0 时就是岭回归，岭回归以增大模型的偏差作为代价，通过压缩模型的系数来减少模型的预测方差，但不会将系数压缩为0，因此达不到变量选择的理想效果。惩罚项 λ 1 ‖ β ‖ 1 能得到稀疏模型， 也就能控制模型的稀疏度； λ 2 ‖ β ‖ 2 允许变量具有共线性，虽然会增大模型的偏差但是会使得预测方差变小，后两项就可以达到通过变量影响力降维的目的。</p></sec><sec id="s5_3"><title>1.3. AEN估计</title><p>针对LASSO估计在某些情况下不相合的问题，Zou于2006年提出了具有Oracle性质的Adaptive LASSO方法 [<xref ref-type="bibr" rid="hanspub.41417-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.41417-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.41417-ref8">8</xref>]，很好的缓和了LASSO的不足。AEN针对变量共线性的性质在Cox模型上有很好的体现，即强相关变量得到的系数估计大致相同，这是呈递了EN的优点，但Elastic Net估计不具有Oracle性质，而Zou和Zhang在Elastic Net的基础上，结合Adaptive LASSO的Oracle性质，对一阶范数惩罚部分加权，提出了具有Oracle性质的Adaptive Elastic Net (Adaptive LASSO &amp; Elastic Net)方法 [<xref ref-type="bibr" rid="hanspub.41417-ref9">9</xref>]，对应的，当建造的模型具有Oracle性质时说明选取的权重是合适的，AEN估计定义为：</p><p>β ^ A E N = a r g m i n ( ∑ i = 1 n ( y i − ∑ k = 1 p   β i T X i j ) 2 + λ 1 ∑ k = 1 p   ω ^ k | β k | + λ 2 ∑ k = 1 p   β k 2 ) (4)</p><p>其中 ω ^ k = ( | β ^ ( E N ) k | ) − γ 。</p><p>EN和AEN都能处理共线性问题，但是在遇到数据具有组效应时，AEN的表现会明显优于EN，相应的算法也会更加复杂一些。</p></sec><sec id="s5_4"><title>1.4. WEN估计</title><p>高维数据会涉及很多的变量，且这些变量间不可避免的存在相关关系，有些变量甚至来自于同一个组类，然而对于决策者而言，需要知道的是最重要的哪几个变量。通常我们只想将重要的组，或者组中重要的变量选择出来(双重变量选择)，在一定程度上可以解决LASSO中惩罚过度的情况 [<xref ref-type="bibr" rid="hanspub.41417-ref10">10</xref>]，此时对存在的重要变量需要赋予更高的权重，那么WEN (weight Elastic Net)就是很好的维数约简方法。当相关性较高时Fan和Li提出非凸罚函数SCAD (smoothly clipped absolute deviation)和MCP (minimax concave penalty)就有很好的表现，SCAD估计器具有许多理想的属性(比如oracle性质)，而WEN中主要包含的就是SCAD与EN的结合Snet以及MCP与EN的结合Mnet。</p><sec id="s5_4_1"><title>1.4.1. Snet</title><p>SCAD结合岭回归和LASSO回归的思想， Zeng和Xie提出当变量相互相关且都与响应变量或者残差高度相关的时候，可以将变量归为同一组获取关于分组的信息，且通过以下方法在保证模型稀疏性质的同时减少预测误差 [<xref ref-type="bibr" rid="hanspub.41417-ref11">11</xref>]，SCAD可以解决LASSO情况下惩罚过度问题，其估计的定义为：</p><p>β ^ S n e t = a r g m i n ( ∑ i = 1 n ( y i − ∑ j = 1 p   β j x i j ) 2 + ∑ j = 1 p   f λ 1 , α S C A D ( β j ) + λ 2 ∑ j = 1 p   β j 2 ) (5)</p><p>其中 α &gt; 2 ，对应的SCAD函数为</p><p>f λ , α S C A D ( β ) = { λ | β | , 0 ≤ | β | &lt; λ − β 2 − 2 a λ | β | + λ 2 2 ( a − 1 ) , λ ≤ | β | &lt; a λ ( a + 1 ) λ 2 / 2 , o t h e r (6)</p></sec><sec id="s5_4_2"><title>1.4.2. Mnet</title><p>Mnet和Snet的约简思想相似，以提供一种惩罚函数用来处理高维数据中具有高度相关数据，Mnet估计定义为：</p><p>β ^ m n e t = a r g m i n ( ∑ i = 1 n ( y i − ∑ j = 1 p   β j x i j ) 2 + ∑ j = 1 p   f λ 1 , α M C P ( | β j | ) + 1 2 λ 2 ∑ j = 1 p   β j 2 ) (7)</p><p>其中MCP函数为</p><p>f λ , α M C P ( θ ) = { λ θ − θ 2 2 α , θ ≤ α λ α λ 2 2 , θ &gt; α λ (8)</p><p>从函数表达式可知，选取合适的惩罚参数后 | β | ≤ α λ 时， | β | 越大， L M C P 增大越缓慢； | β | &gt; α λ 时， L M C P 不变化，也就不惩罚对应的系数，这将改善模型中对较大系数的过度惩罚。</p></sec></sec></sec><sec id="s6"><title>2. 数据分析试验</title><p>在实际的高维数据分析下，变量间有相关性的情况很多，变量间呈现成组出现也是常遇到的情况。Cox模型作为一种半参数模型在生存模型上的应用比较广泛，半参数模型比参数模型灵活，又比非参数模型更好解释。因为模型本身具有的优点，Cox模型在生存分析中一直得到研究者的关注，所以这方面的研究越来越多，本文研究中就用R自带的“smart”数据进行Cox建模分析，通过建立几种不同的高维数据模型得到的结果进行比较，并讨论出这几种处理方法的异同。</p><sec id="s6_1"><title>2.1. 模型结构</title><p>Smart数据集是有3873个观测值、29个变量的数据集，包含了27个变量，1个时间变量，1个结果变量的数字矩阵。SEX (男 = 1，女 = 2)、SMOKING(从不 = never，以前 = former，现在 = current)、ALCHOHL (从不 = never，以前 = former，现在 = current)、BMI (糖尿病)、DIABETES (血压)等变量，关于数据的详细信息可见R软件的官方网站 [<xref ref-type="bibr" rid="hanspub.41417-ref12">12</xref>]。</p><p>在对数据进行生存分析时，Cox比例风险模型中因为参数估计稳健而成为目前最常用的分析方法，它会同时考虑生存时间与生存结局，但是该模型要求各自变量间相互独立，自变量个数要小于样本量,对于高维度、强相关、小样本的生存资料，Cox比例风险模型便不再适用。Tibshirani (1997)将LASSO方法应用于生存分析的Cox模型中进行变量选择与降维，闫丽娜等(2012)比较Cox模型的岭估计、基于LASSO的Cox模型和Elastic Net的原理，并通过模拟比较了三种方法对于小样本、高维度、强相关下生存数据分析的优劣，展示了惩罚Cox模型和Elastic Net技术在生存分析中的应用，在研究生存时间对协变量的依赖性时，Cox的比例危险模型包括一个受试者 h 0 ( t ) e β T z , h ( t | z ) 是受试者在t时的危险函数， h 0 是完全未指定的基线危险函数， ( β 1 , ⋯ , β d ) T 是回归系数的未知向量。</p><disp-formula id="hanspub.41417-formula8"><label>(9)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/1-2580720x36_hanspub.png"  xlink:type="simple"/></disp-formula><p>模型解的稀疏性是变量选择的核心，稀疏性也能大致表现“降维”的结果。变量系数真实值为0并且估计值都为0的概率为1，则称函数解具有稀疏性。也就是使原本系数值就比较小的系数的估计值被设置为0，这样就将无效变量踢出了模型，目前解决稀疏问题比较有效的方法是惩罚方法。</p></sec><sec id="s6_2"><title>2.2. EN和AEN方法</title><sec id="s6_2_1"><title>2.2.1. 诺曼图</title><p>诺莫图(Nomo gram)主要是以变量的系数大小来制定评分标准，给每个自变量的每种取值水平一个评分，由观测值对应的变量就可以得到一个综合评分，转换后和结果发生概率进行对比就能计算出观测值的结果发生概率。在试验中，检验方法我们都是选择的3-fold CV检验，在有关k-fold CV 的学习中，可以知道实验数据集选择3折交叉验证是不错的选择，满足试验要求而且计算量相对较简便一些。</p><p>图1. EN约简的诺曼图</p><p>图2. AEN约简的诺曼图</p><p>计算得到的EN和AEN约简的诺曼图(见图1、图2)的结果可以知道，EN方法将之前的27个变量减少到了5个(AGE, AAA, CREAT, IMT, ALBUMIN)，AEN方法则将变量减少到了6个变量，两者的降维效果都很显著。根据诺曼图可以“测度”一个变量的得分，比如AGE = 45，AAA = 15，CREAT = 400，IMT = 2.5，ALBUMIN = 2，那么综合得分就大约为117分，2年后的生存概率就约为87%，AEN诺曼图也是同理。诺曼图像一张“预测表”，将观测值对应的指标进行评分，再对应到综合评分对应的生存概率，这样就能预测到观测值的生存概率。</p><p>图3. EN方法的生存分析图</p></sec><sec id="s6_2_2"><title>2.2.2. 建立Cox模型</title><p>直接运用诺曼图中选出的变量建立Cox生存模型，展示为图3和图4：</p><p>在两种生存分析图中，蓝色曲线(SEX = 2)都略高于黄色曲线(SEX = 1)，说明女性在相同的情况下存活的概率更大，生存函数得到的系数估计值如表1所示。</p><p>图4. AEN方法的生存分析图</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Model coefficients corresponding to EN and AE</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >EN</th><th align="center" valign="middle" >exp (coef)</th><th align="center" valign="middle" >se (coef)</th><th align="center" valign="middle" >AEN</th><th align="center" valign="middle" >Exp (coef)</th><th align="center" valign="middle" >se (coef)</th></tr></thead><tr><td align="center" valign="middle" >AGE</td><td align="center" valign="middle" >1.0345341</td><td align="center" valign="middle" >0.0054041</td><td align="center" valign="middle" >AGE</td><td align="center" valign="middle" >1.033197</td><td align="center" valign="middle" >0.005492</td></tr><tr><td align="center" valign="middle" >AAA</td><td align="center" valign="middle" >1.7992594</td><td align="center" valign="middle" >0.1130432</td><td align="center" valign="middle" >AAA</td><td align="center" valign="middle" >1.865128</td><td align="center" valign="middle" >0.114153</td></tr><tr><td align="center" valign="middle" >CREAT</td><td align="center" valign="middle" >1.0019975</td><td align="center" valign="middle" >0.0003894</td><td align="center" valign="middle" >STENOSIS</td><td align="center" valign="middle" >1.339386</td><td align="center" valign="middle" >0.104096</td></tr><tr><td align="center" valign="middle" >IMT</td><td align="center" valign="middle" >1.7948385</td><td align="center" valign="middle" >0.1230567</td><td align="center" valign="middle" >HDL</td><td align="center" valign="middle" >0.615617</td><td align="center" valign="middle" >0.14583</td></tr><tr><td align="center" valign="middle" >ALBUMIN</td><td align="center" valign="middle" >1.4853492</td><td align="center" valign="middle" >0.082036</td><td align="center" valign="middle" >IMT</td><td align="center" valign="middle" >1.558151</td><td align="center" valign="middle" >0.13202</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" >ALBUMIN</td><td align="center" valign="middle" >1.678384</td><td align="center" valign="middle" >0.074117</td></tr></tbody></table></table-wrap><p>表1. EN和AEN对应的模型系数</p><p>系数表中有各变量对应的系数，可以看出变量AAA的系数在五个变量中最大，即变量AAA有较大权重影响模型结果。</p></sec></sec><sec id="s6_3"><title>2.3. Snet和Mnet方法</title>诺曼图<p>与EN、AEN同理，Snet、Mnet得到所示的诺曼图(见图5、图6)：Snet和Mnet得到的结果相似，Snet方法维数约简后得到的变量较Mnet方法多，这也可以更加贴切的得到综合评分，预测也会更加贴合实际情况。Snet最终得到19个变量，Mnet最终得到的18个变量，此时影响较大的是变量依然是AAA，Snet和Mnet这两种方法的一致性大小是一样的，参考资料和实验数据可以知道在实际运用中Snet会更加优质一点，且Snet适应的情况更多，适应性更加广泛。</p><p>建模得到的系数结果如表2所示，可以看出变量AAA的影响力最大其次是变量IMT，这和EN和AEN方法的结果相似。Snet方法最终得到的变量比EN、AEN方法多很多，且一致性检验值比前两者都大，又达到了降维的目的，所以Snet方法即能降低模型的稀疏度，又可以防止过拟合的情况出现，是很好的一种建模方法。</p><p>图5. Snet方法的诺曼图</p><p>图6. Mnet方法的诺曼图</p><table-wrap-group id="2"><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Model coefficients corresponding to Snet and Mne</title></caption><table-wrap id="2_1"><table><tbody><thead><tr><th align="center" valign="middle" >Snet</th><th align="center" valign="middle" >Exp (coef)</th><th align="center" valign="middle" >se (coef)</th><th align="center" valign="middle" >Mnet</th><th align="center" valign="middle" >exp (coef)</th><th align="center" valign="middle" >se (coef)</th></tr></thead><tr><td align="center" valign="middle" >SEX</td><td align="center" valign="middle" >0.836104</td><td align="center" valign="middle" >0.139971</td><td align="center" valign="middle" >SEX</td><td align="center" valign="middle" >0.829289</td><td align="center" valign="middle" >0.138925</td></tr><tr><td align="center" valign="middle" >AGE</td><td align="center" valign="middle" >1.03044</td><td align="center" valign="middle" >0.00579</td><td align="center" valign="middle" >AGE</td><td align="center" valign="middle" >1.030199</td><td align="center" valign="middle" >0.005769</td></tr><tr><td align="center" valign="middle" >CEREBRAL</td><td align="center" valign="middle" >1.313872</td><td align="center" valign="middle" >0.125713</td><td align="center" valign="middle" >CEREBRAL</td><td align="center" valign="middle" >1.313396</td><td align="center" valign="middle" >0.125684</td></tr><tr><td align="center" valign="middle" >CARDIAC</td><td align="center" valign="middle" >1.432419</td><td align="center" valign="middle" >0.109455</td><td align="center" valign="middle" >CARDIAC</td><td align="center" valign="middle" >1.422217</td><td align="center" valign="middle" >0.108412</td></tr><tr><td align="center" valign="middle" >AAA</td><td align="center" valign="middle" >2.011532</td><td align="center" valign="middle" >0.121689</td><td align="center" valign="middle" >AAA</td><td align="center" valign="middle" >2.018208</td><td align="center" valign="middle" >0.121479</td></tr><tr><td align="center" valign="middle" >PERIPH</td><td align="center" valign="middle" >1.309684</td><td align="center" valign="middle" >0.115444</td><td align="center" valign="middle" >PERIPH</td><td align="center" valign="middle" >1.299256</td><td align="center" valign="middle" >0.11422</td></tr><tr><td align="center" valign="middle" >STENOSIS</td><td align="center" valign="middle" >1.212051</td><td align="center" valign="middle" >0.117457</td><td align="center" valign="middle" >STENOSIS</td><td align="center" valign="middle" >1.205186</td><td align="center" valign="middle" >0.116837</td></tr></tbody></table></table-wrap><table-wrap id="2_2"><table><tbody><thead><tr><th align="center" valign="middle" >DIASTBP</th><th align="center" valign="middle" >1.002423</th><th align="center" valign="middle" >0.005078</th><th align="center" valign="middle" >SYSTH</th><th align="center" valign="middle" >1.002981</th><th align="center" valign="middle" >0.002232</th></tr></thead><tr><td align="center" valign="middle" >SYSTH</td><td align="center" valign="middle" >1.002625</td><td align="center" valign="middle" >0.002354</td><td align="center" valign="middle" >HDL</td><td align="center" valign="middle" >0.680314</td><td align="center" valign="middle" >0.15758</td></tr><tr><td align="center" valign="middle" >HDL</td><td align="center" valign="middle" >0.676136</td><td align="center" valign="middle" >0.158176</td><td align="center" valign="middle" >LDL</td><td align="center" valign="middle" >1.056917</td><td align="center" valign="middle" >0.04819</td></tr><tr><td align="center" valign="middle" >LDL</td><td align="center" valign="middle" >1.054776</td><td align="center" valign="middle" >0.048391</td><td align="center" valign="middle" >WEIGHT</td><td align="center" valign="middle" >0.990172</td><td align="center" valign="middle" >0.00419</td></tr><tr><td align="center" valign="middle" >WEIGHT</td><td align="center" valign="middle" >0.98991</td><td align="center" valign="middle" >0.004226</td><td align="center" valign="middle" >HOMOC</td><td align="center" valign="middle" >0.997983</td><td align="center" valign="middle" >0.003428</td></tr><tr><td align="center" valign="middle" >HOMOC</td><td align="center" valign="middle" >0.997986</td><td align="center" valign="middle" >0.003427</td><td align="center" valign="middle" >GLUT</td><td align="center" valign="middle" >1.050039</td><td align="center" valign="middle" >0.020727</td></tr><tr><td align="center" valign="middle" >GLUT</td><td align="center" valign="middle" >1.049795</td><td align="center" valign="middle" >0.020738</td><td align="center" valign="middle" >CREAT</td><td align="center" valign="middle" >1.002003</td><td align="center" valign="middle" >0.000406</td></tr><tr><td align="center" valign="middle" >CREAT</td><td align="center" valign="middle" >1.002012</td><td align="center" valign="middle" >0.000406</td><td align="center" valign="middle" >IMT</td><td align="center" valign="middle" >1.501722</td><td align="center" valign="middle" >0.137137</td></tr><tr><td align="center" valign="middle" >IMT</td><td align="center" valign="middle" >1.508739</td><td align="center" valign="middle" >0.137637</td><td align="center" valign="middle" >ALBUMIN</td><td align="center" valign="middle" >1.399052</td><td align="center" valign="middle" >0.085163</td></tr><tr><td align="center" valign="middle" >ALBUMIN</td><td align="center" valign="middle" >1.392327</td><td align="center" valign="middle" >0.085744</td><td align="center" valign="middle" >ALCOHOL</td><td align="center" valign="middle" >0.935767</td><td align="center" valign="middle" >0.05928</td></tr><tr><td align="center" valign="middle" >ALCOHOL</td><td align="center" valign="middle" >0.936229</td><td align="center" valign="middle" >0.059276</td><td align="center" valign="middle" >PACKYRS</td><td align="center" valign="middle" >1.004874</td><td align="center" valign="middle" >0.00228</td></tr><tr><td align="center" valign="middle" >PACKYRS</td><td align="center" valign="middle" >1.004885</td><td align="center" valign="middle" >0.00228</td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr></tbody></table></table-wrap></table-wrap-group><p>表2. Snet和Mnet对应的模型系数</p></sec></sec><sec id="s7"><title>3. 总结与讨论</title><p>随着大数据行业的不断发展，如何在海量数据中快速且有效的找到能展现总体特征的变量，对我们来说是很大的挑战，而针对高维数据的独特数据格式，LASSO系列算法又通过不同的考量得到了多种维数约简的解决方法。本文结合理论与实践，主要介绍了以EN为核心的多种维数约简方法，如何平衡变量的共线性和稀疏性成为几种方法的核心知识点，用不同的检验方法对这四种维数约简方法的结果做比较汇总得到表3的汇总表：</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Three test results of four dimensionality reduction method</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >EN</th><th align="center" valign="middle" >AEN</th><th align="center" valign="middle" >Snet</th><th align="center" valign="middle" >Mnet</th></tr></thead><tr><td align="center" valign="middle" >Likelihood ratio test</td><td align="center" valign="middle" >222.8</td><td align="center" valign="middle" >223.2</td><td align="center" valign="middle" >273.2</td><td align="center" valign="middle" >273</td></tr><tr><td align="center" valign="middle" >Wald test</td><td align="center" valign="middle" >253.3</td><td align="center" valign="middle" >237.2</td><td align="center" valign="middle" >304.1</td><td align="center" valign="middle" >304.8</td></tr><tr><td align="center" valign="middle" >Score (logrank) test</td><td align="center" valign="middle" >290.8</td><td align="center" valign="middle" >257.4</td><td align="center" valign="middle" >347.3</td><td align="center" valign="middle" >347.3</td></tr><tr><td align="center" valign="middle" >Concordance</td><td align="center" valign="middle" >0.675</td><td align="center" valign="middle" >0.677</td><td align="center" valign="middle" >0.691</td><td align="center" valign="middle" >0.691</td></tr></tbody></table></table-wrap><p>表3. 四种维数约简方法的三种检验结果</p><p>三种模型检验的结果说明Snet和Mnet的拟合效果比较好，这和之前的结果推测一致，次之是AEN方法，最后是EN方法。一致性检验是评价模型的预测性能的，仅适用于比较同一模型的“预测性能”。一致性检验会在0.5~1之间，然而在实际应用中较高的一致性是很难达到的，所以这里的0.691预测性能值可以是预测性能较好的表现。本文对几种维数约简的方法比较，是建立在Cox生存模型的基础上的，如何将LASSO系列的变量选择方法推广到更多统计模型的研究是必要的，且目前评估降维的方法较单一，如何寻找到合适的高维模型评估方法也应与维数约简方法同步发展，这也可以有利于进一步的研究更多高维数据分析方法的优劣性。</p></sec><sec id="s8"><title>文章引用</title><p>刘 锋,胡天英,陈俊霖,但 晨. 高维数据在Cox回归模型中的自变量选择Independent Variable Selection of High-Dimensional Data in Cox Regression Model[J]. 统计学与应用, 2021, 10(02): 183-192. https://doi.org/10.12677/SA.2021.102018</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41417-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Noah, S., Jerome, F., Trevor, H. and Rob, T. (2011) Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent. Journal of Statistical Software, 39, 1-13. https://doi.org/10.18637/jss.v039.i05</mixed-citation></ref><ref id="hanspub.41417-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Zou, H. and Hastie, T. (2005) Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67, 301-320. https://doi.org/10.1111/j.1467-9868.2005.00503.x</mixed-citation></ref><ref id="hanspub.41417-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">宋瑞琪, 朱永忠, 王新军. 高维数据中变量选择研究[J]. 统计与决策, 2019, 35(2): 13-16.</mixed-citation></ref><ref id="hanspub.41417-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, H.H. and Lu, W.B. (2007) Adaptive Lasso for Cox’s Proportional Hazards Model. Biometrika, 94, 691-703.  
https://doi.org/10.1093/biomet/asm037</mixed-citation></ref><ref id="hanspub.41417-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Zou, H. (2006) The Adaptive Lasso and Its Oracle Properties. Journal of the American Statistical Association, 101, 1418-1429. https://doi.org/10.1198/016214506000000735</mixed-citation></ref><ref id="hanspub.41417-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Huang, J., Ma, S.G. and Zhang, C.H. (2006) Adaptive LASSO for Sparse High-Dimensional Regression. Statistica Sinica, 18, 1603-1618.</mixed-citation></ref><ref id="hanspub.41417-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Simon, N., Friedman, J.H., Hastie, T. and Tibshirani, R. (2011) Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent. Journal of Statistical Software, 39, 1-13. https://doi.org/10.18637/jss.v039.i05</mixed-citation></ref><ref id="hanspub.41417-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">董英, 黄品贤. Cox模型及预测列线图在R软件中的实现[J]. 数理医药学杂志, 2012, 25(6): 711-713.</mixed-citation></ref><ref id="hanspub.41417-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">韦新星, 李春红, 戴洪帅. Adaptive Elastic Net方法在Cox模型变量选择中的研究[J]. 西南大学学报(自然科学版), 2017, 39(9): 88-94.</mixed-citation></ref><ref id="hanspub.41417-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">闫丽娜, 覃婷, 王彤. LASSO方法在Cox回归模型中的应用[J]. 中国卫生统计, 2012, 29(1): 58-60+64.</mixed-citation></ref><ref id="hanspub.41417-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Fan, J.Q. and Li, R.Z. (2001) Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties. Journal of the American Statistical Association, 96, 1348-1360. https://doi.org/10.1198/016214501753382273</mixed-citation></ref><ref id="hanspub.41417-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">The R Project for Statistical Computing. https://www.r-project.org/</mixed-citation></ref></ref-list></back></article>