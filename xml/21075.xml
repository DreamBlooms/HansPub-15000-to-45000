<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2017.76064</article-id><article-id pub-id-type="publisher-id">CSA-21075</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20170600000_18511421.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  过程纹理的语义描述与预测
  Semantic Descriptions and Prediction for Procedural Texture
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>丽娜</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>君</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>孙</surname><given-names>鑫</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>董</surname><given-names>军宇</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>杨</surname><given-names>占宾</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff4"><addr-line>青岛高校软控股份有限公司，山东 青岛</addr-line></aff><aff id="aff1"><addr-line>中国海洋大学计算机科学与技术系，山东 青岛</addr-line></aff><aff id="aff3"><addr-line>青岛农业大学理学与信息科学学院，山东 青岛</addr-line></aff><aff id="aff2"><addr-line>null</addr-line></aff><author-notes><corresp id="cor1">* E-mail:<email>wanglina@stu.ouc.edu.cn (王丽)</email>;</corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>06</month><year>2017</year></pub-date><volume>07</volume><issue>06</issue><fpage>537</fpage><lpage>545</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   不同模式的过程纹理，往往是由带有不同参数的数学模型产生，这些参数又是经过有经验的研究人员的精心挑选而得到。而大多数人在日常生活与工作中，常常会用一些语言描述例如“规则的”，“蕾丝状的”和“重复的”等来定义或者寻找想要得到的纹理，希望能够借此被推荐合适的生成模型和参数来产生符合条件的纹理图像。然而这就造成了人的思维描述和纹理图像的生成模型和参数之间巨大的鸿沟。因此，对纹理图像添加语义描述，可以建立起人的视觉感知与图像之间沟通的桥梁。通过对人们所定义的语义进行分析，可以帮助人们找到合适的生成模型和参数来产生符合自己描述的纹理图像。本文以纹理图像语义描述为切入点，通过收集人们对纹理图像的语义描述，借助于多标签学习算法构建预测模型，为人们和过程纹理图像之间的沟通奠定基础。 Procedural textures with different patterns are normally generated from mathematical models with parameters carefully selected by experienced users. However, for naive users, the intuitive way to obtain a desired texture is to provide semantic descriptions such as “regular”, “lacelike” and “repetitive” and then a procedural model with proper parameters will be automatically suggested to generate the corresponding textures. By contrast, it is less practical for users to learn mathematical models and tune parameters based on multiple examinations of large numbers of generated textures. Taken the semantic description of textures as the breakthrough point, this study explores the way to automatically generate human desired textures by collecting and analyzing people’s descriptions, so that it can lay the foundation for the communication between human descriptions and procedural textures.
    
  
 
</p></abstract><kwd-group><kwd>过程纹理，语义描述，预测, Procedural Texture</kwd><kwd> Semantic Description</kwd><kwd> Prediction</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>过程纹理的语义描述与预测<sup> </sup></title><p>王丽娜<sup>1</sup>，刘君<sup>2</sup>，孙鑫<sup>1</sup>，董军宇<sup>1</sup>，杨占宾<sup>3</sup></p><p><sup>1</sup>中国海洋大学计算机科学与技术系，山东 青岛</p><p><sup>2</sup>青岛农业大学理学与信息科学学院，山东 青岛</p><p><sup>3</sup>青岛高校软控股份有限公司，山东 青岛</p><p>收稿日期：2017年6月3日；录用日期：2017年6月19日；发布日期：2017年6月22日</p><disp-formula id="hanspub.21075-formula164"><graphic xlink:href="http://html.hanspub.org/file/5-1540782x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>不同模式的过程纹理，往往是由带有不同参数的数学模型产生，这些参数又是经过有经验的研究人员的精心挑选而得到。而大多数人在日常生活与工作中，常常会用一些语言描述例如“规则的”，“蕾丝状的”和“重复的”等来定义或者寻找想要得到的纹理，希望能够借此被推荐合适的生成模型和参数来产生符合条件的纹理图像。然而这就造成了人的思维描述和纹理图像的生成模型和参数之间巨大的鸿沟。因此，对纹理图像添加语义描述，可以建立起人的视觉感知与图像之间沟通的桥梁。通过对人们所定义的语义进行分析，可以帮助人们找到合适的生成模型和参数来产生符合自己描述的纹理图像。本文以纹理图像语义描述为切入点，通过收集人们对纹理图像的语义描述，借助于多标签学习算法构建预测模型，为人们和过程纹理图像之间的沟通奠定基础。</p><p>关键词 :过程纹理，语义描述，预测</p><disp-formula id="hanspub.21075-formula165"><graphic xlink:href="http://html.hanspub.org/file/5-1540782x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2017 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="http://image.hanspub.org:8080\Html/htmlimages\1-2890033x\e70a10f1-7c93-45ea-9603-062237856e4b.png" /><img src="http://image.hanspub.org:8080\Html\htmlimages\1-2890033x\e898c85e-ffc4-45c9-b817-14224a4d6960.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>自然界中的纹理无处不在，一幅纹理图像往往包含描述物体表面特征的重要信息，因此纹理常常被用来进行材质识别等相关研究 [<xref ref-type="bibr" rid="hanspub.21075-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.21075-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.21075-ref3">3</xref>] 。例如，与材质相关的数据集CuRET [<xref ref-type="bibr" rid="hanspub.21075-ref4">4</xref>] 、Outex [<xref ref-type="bibr" rid="hanspub.21075-ref5">5</xref>] ，和Drexel Texture Database [<xref ref-type="bibr" rid="hanspub.21075-ref6">6</xref>] 都体现了纹理图像对材质识别的重要作用。在Gabriel [<xref ref-type="bibr" rid="hanspub.21075-ref2">2</xref>] 等人的工作中，首次提出用“visual material traits”(例如shiny, fuzzy, soft等)描述不同材质纹理的特性。论文通过学习不同的卷积核来为图像中每个像素添加这样的“material traits”，最终达到了在没有物体信息的前提下，准确进行材质识别的效果。而此处所提出的“material traits”恰恰是对不同材质纹理的语义描述的体现，通过对材质表面的纹理进行语义描述，将其与材质类别关联起来，大大增强了机器对材质识别的效果。</p><p>为了更好的理解纹理图像所含有的信息，非常多的学者在纹理感知 [<xref ref-type="bibr" rid="hanspub.21075-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.21075-ref8">8</xref>] 与纹理描述 [<xref ref-type="bibr" rid="hanspub.21075-ref9">9</xref>] 上投入了巨大的精力。Rao等人在 [<xref ref-type="bibr" rid="hanspub.21075-ref10">10</xref>] 中通过心理物理学实验搜集了大量能够描述纹理的视觉特性的词汇，然后通过分析人们所给出的这些描述性的属性词，得出三个非常重要的、用来描述纹理视觉感知特性的词汇，分别为线性、重复性和复杂性。虽然这个工作并没有分析和设计实验来验证人们如何才能根据自己的意愿找到想要的纹理，但是却在很大程度上指导着相关研究开展视觉感知实验，自由分组实验等等。本文亦参考了该研究工作，通过设计心理物理学实验搜集和整理人们对纹理的语义描述。</p><p>除了通过上述以心理物理学实验方式搜集对纹理视觉特性的描述以外，还有一些工作通过网络下载的方式收集纹理图像的语义标注，例如在论文 [<xref ref-type="bibr" rid="hanspub.21075-ref11">11</xref>] 中，所收集的数据集DTD (Describing Textures Dataset)便是来源于网络，该数据集中有47个描述纹理的常见词汇，都是来源于文献 [<xref ref-type="bibr" rid="hanspub.21075-ref10">10</xref>] 中所整理的词汇。在DTD中，每一幅自然纹理图像都代表了不同的物体表面，其表现的信息非常多样化，并且没有固定的变化模式，可以认为是非常随机的。这个工作中主要探讨数据的收集和如何为具有单一属性的自然纹理标注语义，论文借助IFV [<xref ref-type="bibr" rid="hanspub.21075-ref12">12</xref>] 、DeCAF [<xref ref-type="bibr" rid="hanspub.21075-ref13">13</xref>] 等算法，提取图像不同的特征表示，通过分类的途径为图像添加语义描述。</p><p>为了给过程纹理添加合适的语义描述，本文构建出了具有语义描述的过程纹理数据集。通过开展心理物理学实验的方式，收集人对过程纹理图像的语义描述，并通过多标签学习算法建立了语义预测模型。</p></sec><sec id="s4"><title>2. 过程纹理语义数据库的构建</title><sec id="s4_1"><title>2.1. 数据库与样本</title><p>本研究工作所采用的过程纹理图像数据主要是从刘君等人发布的过程纹理数据库 [<xref ref-type="bibr" rid="hanspub.21075-ref14">14</xref>] 及用同样的生成模型变换参数所产生的过程纹理图像。网站上发布的过程纹理包含450张512 &#215; 512大小的图像(示例请见图1，分别由23种不同的生成模型通过变换其对应的生成参数产生。</p><p>在上述450张过程纹理图像的基础上，本文通过调用同样的23个生成模型中的22个方法，为其赋予不同的参数，又另外产生了2200张512 &#215; 512大小的过程纹理图像，然后通过四分法将每幅图像裁割为256 &#215; 256大小，额外获得了8800幅过程纹理图像，作为后续实验的样本集。</p></sec><sec id="s4_2"><title>2.2. 心理物理学实验设计及开展</title><p>本文选择了20位计算机专业学生参加了自由分组实验，其中包括8位女生和12位男生，年龄从23岁到35岁。所有的观察者具有正常的视力或者矫正到正常的视力。参加实验的人员并不具有过程纹理模型的专业知识，而且不了解心理物理学实验的目的。本文让每一位实验者，按照如下过程进行实验：</p><p>1) 自由分组</p><p>自由分组大致分为两个步骤：第一，参与者首先浏览一遍所有的纹理图像，然后根据自己对这些纹理图像的视觉认知，从自己的语言描述角度出发，将全部的纹理图像分成不同的组，要求被分到同一个组的多幅纹理可以用同一个描述词来形容，例如假设纹理A和纹理B都被分到了第一组，是因为这两张图像都可以用“复杂的”来描述。在这一步，分组的描述词都由参与者自己完全按照自身的语义认知给出，事前不予以任何约定；分组数目也不予以固定，每位参与者可以根据自己的描述划分为不同的分组。此外，在这一步，希望参与者可以将分组划分的越细致越好。当参与者确定自己已经完成该分组步骤，需要其记下为每一组赋予的描述词。</p><p>2) 自由组合</p><p>在上述自由分组后，当前参与者需要继续进行第二步实验，即根据相似的语义描述进行不同小组合</p><p>图1. 过程纹理图像示例</p><p>并的实验。该实验的目的是为了给纹理图像多添加一些描述词。小组合并实验的要求是参与者需要将上一步中自己划分好的不同组进行小组与小组之间的合并工作，在合并的过程中，被合并的多个小组要求共同体现某一个语义属性。假设自由分组后，小组A与小组B各自包含不同数量的纹理图像，也各自被实验人员标注了不同的语义属性，然而，组A与组B又同时可以体现出某个共同的语义属性，例如A组具有属性“线性的”，B组具有的属性是“球状的”，当A中含有多个线条，B中含有多个小球的时候，就可以把A、B合并为一组，并添加新的属性“重复的”。多次进行这样的分组合并，直到观察者认为没有小组可以继续合并为止。</p></sec><sec id="s4_3"><title>2.3. 数据处理</title><p>经过上述心理物理学实验，本文已经获得了来自20位观察者的语义描述数据，每个人都按照自己的语言表述为这450幅纹理添加了不同的语义描述。</p><p>词汇的选择：参考着Rao等人的工作，本文最终确定了本文43个语义描述词，分别是Irregular (不规则的)，grid (网格状的)，granular (颗粒状)，complex (复杂的)，uniform (均匀的)，spiraled (涡旋的)，marbled (大理石的)，mottled (斑驳的)，fuzzy (模糊的)，crinkled (皱的)，well-ordered (有序的)，speckled (斑点状)，polka-dotted (圆点的)，repetitive (重复的)，ridged (脊状的)，uneven (凹凸不平的)，smooth (平坦的)，cellular (细胞状的)，globular (球状的)，porous (多孔的)，regular (规则的)，veined (叶脉状的)，cyclical (循环的)，freckled (斑点的)，simple (简单的)，dense (密集的)，stained (被污染的)，honeycombed (蜂巢状的)，coarse (粗糙的)，rough (粗略的)，gouged (沟凿的)，rocky (岩石的)，woven (织物状的)，lined (线性的)，fine (细致的)，non-uniform (不均匀的)，disordered (无序的)，fibrous (纤维状的)，random (随机的)，lacelike (蕾丝花边的)，messy (混乱的)，scaly (丑陋的)，netlike (网状的)。</p><p>统计量的确定：在确定了43个语义描述词后，本文通过一定的机制来确定过程纹理在该语义描述上的表现强度。这个机制是按照如下方法进行的：对于每一幅过程纹理，本文统计出其在每个语义词上所表现出的强度，该强度是指：20个人中有几个人为其标注了该语义描述。例如，当20个参与者中有5位成员认为纹理样本x具有“规则的”(regular)这个属性的时候，那样本x就在属性“规则的”(regular)上具有强度为5的数值。当所有属性上所有纹理样本的强弱程度统计结束后，本文采取了取平均的归一化方法，即所有统计得到的强度均除以20。经过这一步基本操作，本文已经得到了含有450幅过程纹理及其43维语义的纹理语义数据集。为了增大样本的数量，本文将每一幅纹理一分为四，每一小幅被裁剪出来的问题均是256 &#215; 256大小，最终获得1800幅纹理图像。每4幅来自同一幅纹理的图像，其语义都是对应的纹理的语义。这样本文最终获得了含有1800幅过程纹理及其语义描述的数据集，本文将之称为基本的语义纹理数据集(the Basic Semantic Texture Dataset)，简称STD-basic。STD-basic中的图像样例如图2所示(图中仅列出语义强度值较大的描述词汇)。</p></sec></sec><sec id="s5"><title>3. 语义分析与预测</title><p>上述部分提到，1800幅有着语义属性的纹理图像的确不足以构建鲁棒的纹理生成框架，因此，通过有效的算法建立语义预测模型，让更多的纹理获得语义描述，从而进一步扩充语义数据集，是非常有必要的。本文在前述的8,800幅纹理图像的基础上展开纹理语义预测工作，并将通过语义预测模型得到语义描述后的这8800幅过程纹理数据库，称为额外的语义描述数据集(The Additional Semantic Texture Dataset)，简称STD-add。</p><p>从43维语义属性的角度进行分析，一幅纹理图像可以被看作同时具有43个语义标签(属性)。由此，对43个语义同时进行预测的工作可以看作图像的多标签学习。正是这个思路引导本文查阅和研究了相关的工作 [<xref ref-type="bibr" rid="hanspub.21075-ref15">15</xref>] [<xref ref-type="bibr" rid="hanspub.21075-ref16">16</xref>] ，并通过多标签学习算法快速完成了过程纹理语义预测模型的建立工作，基本过程按照图3进行。</p><p>选择多标签学习的方法开展过程纹理的语义预测工作是有依据的。多标签排序方法需要满足以下两个条件：第一、每个样本<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x11_hanspub.png" xlink:type="simple"/></inline-formula>的多个标签可以分为相关标签集合<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x12_hanspub.png" xlink:type="simple"/></inline-formula>和不相关标签集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x13_hanspub.png" xlink:type="simple"/></inline-formula>；第二、样本<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x14_hanspub.png" xlink:type="simple"/></inline-formula>含有的多个标签，其强度值是有序的，即<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x15_hanspub.png" xlink:type="simple"/></inline-formula>。本文的语义标签满足以上条件；而且，利用多标签学习算法学习一个可以拟合过程纹理语义数据的潜在分布，可以有效地解决过程纹理语义预测的问题。</p><sec id="s5_1"><title>3.1. 构建语义分布模型</title><p>多标签学习的目标是从有限的样本集的基础上学习一个映射，使其能够输出样本多个标签的排序。从收集数据的过程的角度来讲，本文认为给出这样一种假设是合理的，即：每位心理物理学实验的参与者都是有意或者无意地在某种程度上依照大脑中某种潜在的概率分布来给出语义描述的。于是，本文将纹理图像的语义数值在整体上看作某种潜在的概率分布。这个潜在的概率分布，本文称之为语义分布。当给定一幅纹理图像<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x16_hanspub.png" xlink:type="simple"/></inline-formula>，多标签学习的目的就是构建和学习一个分布模型，使其能够在最大程度上为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x17_hanspub.png" xlink:type="simple"/></inline-formula>预测到多个语义属性及其表现强度值，借此表示该纹理样本的语义分布。要解决这个问题，本文可以分为两个步骤:第一步就是通过某种方法从样本中学习一种最能够拟合所有样本整体分布的分布；第二步就是学习一个从样本空间到上述分布的一个映射模型。当这个映射模型被很好地学习出来之后，纹理的语义预测模型也就随之实现了。</p><p>1) 学习语义分布模型</p><p>图2. STD-basic纹理数据集语义描述示例</p><p>图3. 基于多标签学习的纹理语义预测</p><p>受上述多标签学习方法的启发，本文探索了STD-basic数据的语义分布。首先，本文生成了一个语义分布P来最大限度地拟合纹理图像的语义数值，然后本文通过学习这个分布P，来建立能够为更多纹理标注语义数据的模型。分布P在此处便起到了类似于预测器的作用。分布P必须满足以下条件：</p><disp-formula id="hanspub.21075-formula166"><label>(1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/5-1540782x20_hanspub.png"  xlink:type="simple"/></disp-formula><p>此处，D是语义分布P与每个样本各自的分布P<sub>i</sub>之间的距离度量函数。除了上述条件之外，分布P与P<sub>i</sub>也应该受到以下约束：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x21_hanspub.png" xlink:type="simple"/></inline-formula>，这里的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x22_hanspub.png" xlink:type="simple"/></inline-formula>指的是第i个纹理样本的第j个语义属性的数值，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x23_hanspub.png" xlink:type="simple"/></inline-formula>。这里，c是语义属性的个数，而n是数据样本的个数。</p><p>公式(1)中的函数D有多种可选择的度量函数，这在论文 [<xref ref-type="bibr" rid="hanspub.21075-ref17">17</xref>] 中早已经被探究过，例如常用的欧氏距离，KL散度和卡方距离等等。本文主要采用KL散度进行计算，并且将整个学习问题通过以下方法进行优化：</p><disp-formula id="hanspub.21075-formula167"><label>(2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/5-1540782x24_hanspub.png"  xlink:type="simple"/></disp-formula><p>这个学习优化过程是要针对每一个训练样本X<sub>i</sub>的，如此便可以建立起符合过程纹理语义数据概率分布的语义分布P。</p><p>2) 优化语义分布模型</p><p>要学习分布P来完成预测过程纹理语义数据的模型。总体来讲，数据集已经转换成如下格式：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x25_hanspub.png" xlink:type="simple"/></inline-formula>，这里的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x26_hanspub.png" xlink:type="simple"/></inline-formula>是第i个样本数据的语义向量。于是，整个模型优化问题就是要在上述数据的基础上学习一个参数化的条件概率分布模型<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x27_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x28_hanspub.png" xlink:type="simple"/></inline-formula>正是需要求解的参数向量。于是，整个学习问题就转化成了一个学习参数向量<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x29_hanspub.png" xlink:type="simple"/></inline-formula>的问题，而<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x30_hanspub.png" xlink:type="simple"/></inline-formula>恰恰满足使得概率分布<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x31_hanspub.png" xlink:type="simple"/></inline-formula>能够与真实语义数据的潜在概率分布非常相近。参数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x32_hanspub.png" xlink:type="simple"/></inline-formula>的最优解可以被定义成以下形式：</p><disp-formula id="hanspub.21075-formula168"><label>(3)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/5-1540782x33_hanspub.png"  xlink:type="simple"/></disp-formula><p>与论文 [<xref ref-type="bibr" rid="hanspub.21075-ref18">18</xref>] 中的工作相似，本文假设上述条件概率模型符合最大熵模型：</p><disp-formula id="hanspub.21075-formula169"><label>(4)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/5-1540782x34_hanspub.png"  xlink:type="simple"/></disp-formula><p>其<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x35_hanspub.png" xlink:type="simple"/></inline-formula>中是正则化向量，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x36_hanspub.png" xlink:type="simple"/></inline-formula>是参数向量<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x37_hanspub.png" xlink:type="simple"/></inline-formula>其中的一个元素，x<sub>k</sub>是样本x的第k个语义属性。将公式(4)插入公式(3)中，就可以得到以下目标优化函数：</p><disp-formula id="hanspub.21075-formula170"><label>(5)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/5-1540782x38_hanspub.png"  xlink:type="simple"/></disp-formula><p>要求解这个公式的最小值<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x39_hanspub.png" xlink:type="simple"/></inline-formula>，可以借助于准牛顿方法(quasi-Newton)成功求解。而参数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x40_hanspub.png" xlink:type="simple"/></inline-formula>就是在<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x41_hanspub.png" xlink:type="simple"/></inline-formula>取最小值时所对应的值。一旦本文成功求解出分布模型<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/5-1540782x42_hanspub.png" xlink:type="simple"/></inline-formula>，就可以利用该模型为STD-add中的纹理图像预测语义描述，这样一来，含有语义描述数据的纹理数据集从1800幅扩增至含有1800幅与8800幅纹理两个数据集：STD-basic与STD-add，这样便丰富了本文的数据，扩大了数据量。</p></sec><sec id="s5_2"><title>3.2. 实验结果及分析</title><p>在这一部分，本文将从实验所采用的图像特征开始，逐步描述过程纹理图像的语义预测实验。</p><p>1) 纹理图像的特征表示</p><p>本文通过提取纹理图像不同的特征进行对比和验证实验效果。在语义预测实验过程中，本文所提取的图像特征有以下两种：</p><p>&#183; Gabor特征：Gabor特征是一种符合人类视觉感知的特征，在图像处理中，Gabor函数是一个线性的滤波器，专门用于检测图像边缘。Gabor函数的频率和方向表示类似于人类视觉系统，而且其已经被证实特别适用于纹理的表示与判别。本文种，Gabor变换采用了4个尺度6个方向的滤波器所提取得到，因此最终用来进行纹理图像特征表示的是48维Gabor特征；</p><p>&#183; 深度卷积神经网络提取的特征(Deep Convolutional Neural Network, CNN特征)：该特征是直接利用AlexNet网络架构 [<xref ref-type="bibr" rid="hanspub.21075-ref19">19</xref>] ，在2012年ILSVRC竞赛最优的model的基础上，提取了其fc7的输出特征。AlexNet网络结构包括5个卷积层和3个全连接层，是Krizhevsky等人在2012年的ImageNet图像分类竞赛上提出的网络结构。</p><p>2) 语义预测模型训练与测试</p><p>多标签学习方法的效果在STD-basic数据集上进行了验证。该过程是利用matlab工具实现的，其中1700样本作为训练样本，其余的100个作为测试样本。预测模型分别在Gabor特征和CNN特征上测试。结果如图4 所示。左图为Gabor特征训练所得模型测试结果；右图为CNN特征训练所得模型的测试结果。图中红色折线表示预测模型预测的语义数据，绿色折线表示测试样本本身真实的语义数据。</p><p>从图4来看，本文发现：CNN特征不如Gabor特征得到的效果优越。在用Gabor特征表示的数据上，预测模型可以很好地拟合纹理语义数据的语义分布，并能够符合期望地预测测试样本的语义数据；而在用CNN特征对纹理进行表示的数据上训练得到的模型所预测到的语义数据却产生了比较大的抖动，无法达到期望的效果。同时，表1展示了由不同特征所预测的语义数据与真实语义数据之间的距离关系。在这个表格数据中本文同样可以看出，用Gabor特征表示纹理进行语义预测模型的训练要相对于用CNN特征来表示纹理进行语义预测模型的训练，效果要更好。</p><p>图4. 分别用Gabor特征和CNN特征训练得到的模型各自的测试结果</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Distance metric result between predicted semantic and real semantic dat</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >距离</th><th align="center" valign="middle" >KL距离</th><th align="center" valign="middle" >欧式距离</th><th align="center" valign="middle" >索伦森距离</th><th align="center" valign="middle" >卡方距离</th></tr></thead><tr><td align="center" valign="middle" >Gabor特征</td><td align="center" valign="middle" >0.0564</td><td align="center" valign="middle" >0.0195</td><td align="center" valign="middle" >0.0557</td><td align="center" valign="middle" >0.0564</td></tr><tr><td align="center" valign="middle" >CNN特征</td><td align="center" valign="middle" >0.1114</td><td align="center" valign="middle" >0.0671</td><td align="center" valign="middle" >0.1380</td><td align="center" valign="middle" >0.1095</td></tr></tbody></table></table-wrap><p>表1. 预测的语义数据与真实数据的平均距离度量结果</p><p>总体来讲，Gabor特征在预测过程纹理的语义描述实验中，效果要优于流行的深度卷积神经网络模型直接得到的特征，因此，本文使用Gabor特征描述和表示纹理图像，进行语义预测模型的训练，然后利用该模型为STD-add数据集中的纹理图像预测其语义描述。最终，过程纹理的语义数据集就包括两个分别含有1800幅和8800幅纹理的STD-basic和STD-add。</p></sec></sec><sec id="s6"><title>4. 结束语</title><p>纹理的语义分析等相关研究在计算机图形学、计算机视觉等很多领域都是非常重要的课题。本文针对纹理的语义描述进行了研究，构建出了具有语义描述的过程纹理数据集，并通过多标签学习算法建立了有效的语义预测模型。试验结果表明，通过提取过程纹理图像的Gabor特征进行语义预测的效果是最好的，为图像与人的理解描述建立了沟通的桥梁，为根据人的描述得到合适的过程纹理图像打下了坚实的基础。</p></sec><sec id="s7"><title>基金项目</title><p>青岛农业大学高层次人才启动资金项目编号1117006基于图像的棉花品级自动分类研究；</p><p>国家自然科学基金项目编号61271405符合视觉感知机理的自然纹理生成模式研究；</p><p>教育部博士点专项基金项目编号20120132110018基于视觉感知特征创意的纹理生成模型研究。</p></sec><sec id="s8"><title>文章引用</title><p>王丽娜,刘 君,孙 鑫,董军宇,杨占宾. 过程纹理的语义描述与预测Semantic Descriptions and Prediction for Procedural Texture[J]. 计算机科学与应用, 2017, 07(06): 537-545. http://dx.doi.org/10.12677/CSA.2017.76064</p></sec><sec id="s9"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.21075-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Leung, T. and Malik, J. (2001) Representing and Recognizing the Visual Appearance of Materials Using Three- Dimensional Textons. International Journal of Computer Vision, 43, 29-44. &lt;br&gt;https://doi.org/10.1023/A:1011126920638</mixed-citation></ref><ref id="hanspub.21075-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Schwartz, G. and Nishino, K. (2013) Visual Material Traits: Recognizing Per-Pixel Material Context. IEEE International Conference on Computer Vision Workshops, Sydney, 1-8 December 2013, 883-890. 
&lt;br&gt;https://doi.org/10.1109/iccvw.2013.121</mixed-citation></ref><ref id="hanspub.21075-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Sharan, L., Liu, C., Rosenholtz, R. and Adelson, E.H. (2013) Recognizing Materials Using Perceptually Inspired Features. International Journal of Computer Vision, 103, 348-371. &lt;br&gt;https://doi.org/10.1007/s11263-013-0609-0</mixed-citation></ref><ref id="hanspub.21075-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Dana, K.J., et al. (1999) Reflectance and Texture of Real-World Surfaces. ACM Transactions on Graphics, 18, 1-34. &lt;br&gt;https://doi.org/10.1145/300776.300778</mixed-citation></ref><ref id="hanspub.21075-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Ojala, T., Pietikäinen, M. and Mäenpää, T. (2002) Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 24, 971-987. 
&lt;br&gt;https://doi.org/10.1109/TPAMI.2002.1017623</mixed-citation></ref><ref id="hanspub.21075-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Oxholm, G., Bariya, P. and Nishino, K. (2012) The Scale of Geometric Texture: Springer Berlin Heidelberg. European Conference on Computer Vision, Firenze, 7-13 October 2012, 58-71.  
&lt;br&gt;https://doi.org/10.1007/978-3-642-33718-5_5</mixed-citation></ref><ref id="hanspub.21075-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Amadasun, M. and King, R. (1989) Textural Features Corresponding to Textural Properties. IEEE Transactions on Systems, Man, and Cybernetics, 19, 1264-1274. &lt;br&gt;https://doi.org/10.1109/21.44046</mixed-citation></ref><ref id="hanspub.21075-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Gårding, J. (1992) Shape from Texture for Smooth Curved Surfaces. Journal of Mathematical Imaging and Vision, 2, 327-350. &lt;br&gt;https://doi.org/10.1007/BF00121877</mixed-citation></ref><ref id="hanspub.21075-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Ferrari, V. and Zisserman, A. (2008) Learning Visual Attributes. Conference on Neural Information Processing Systems, Vancouver, 8-10 December 2008, 433-440.</mixed-citation></ref><ref id="hanspub.21075-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Bhushan, N., Rao, A.R. and Lohse, G.L. (1997) The Texture Lexicon: Understanding the Categorization of Visual Texture Terms and Their Relationship to Texture Images. Cognitive Science, 21, 219-246. 
&lt;br&gt;https://doi.org/10.1207/s15516709cog2102_4</mixed-citation></ref><ref id="hanspub.21075-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S. and Vedaldi, A. (2014) Describing Textures in the Wild. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Columbus, 23-28 June 2014, 3606-3613. 
&lt;br&gt;https://doi.org/10.1109/cvpr.2014.461</mixed-citation></ref><ref id="hanspub.21075-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Perronnin, F. and Dance, C. (2007) Fisher Kernels on Visual Vocabularies for Image Categorization. IEEE Conference on Computer Vision and Pattern Recognition, Minneapolis, 17-22 June 2007, 1-8.  
&lt;br&gt;https://doi.org/10.1109/cvpr.2007.383266</mixed-citation></ref><ref id="hanspub.21075-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., et al. (2013) DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. Computer Science, 50, 815-830.</mixed-citation></ref><ref id="hanspub.21075-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Jun, L. (2015) Procedural Textures[EB/OL].  
&lt;br&gt;https://figshare.com/articles/procedural_textures/1289700, 2017-6-12.</mixed-citation></ref><ref id="hanspub.21075-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Brinker, K., Fürnkranz, J. and Hüllermeier, E. (2006) A Unified Model for Multilabel Classification and Ranking. European Conference on Artificial Intelligence, Riva Del Garda, 29 August-1 September 2006, 489-493.</mixed-citation></ref><ref id="hanspub.21075-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Geng, X. and Luo, L. (2014) Multilabel Ranking with Inconsistent Rankers. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Columbus, 23-28 June 2014, 3742-3747. 
&lt;br&gt;https://doi.org/10.1109/cvpr.2014.478</mixed-citation></ref><ref id="hanspub.21075-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Cha, S. (2007) Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions. City, 1, 1.</mixed-citation></ref><ref id="hanspub.21075-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Geng, X., Yin, C. and Zhou, Z. (2013) Facial Age Estimation by Learning from Label Distributions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 2401-2412. &lt;br&gt;https://doi.org/10.1109/TPAMI.2013.51</mixed-citation></ref><ref id="hanspub.21075-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky, A., Sutskever, I. and Hinton, G.E. (2012) Imagenet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems, Harrahs and Harveys, Lake Tahoe, 3-8 December 2012, 1097- 1105.</mixed-citation></ref></ref-list></back></article>