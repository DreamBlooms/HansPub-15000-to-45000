<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.1012255</article-id><article-id pub-id-type="publisher-id">CSA-39552</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20201200000_99548132.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于多尺度融合的卷积神经网络的杂草幼苗识别
  Weed Seeding Recognition Based on Multi-Scale Fusion Convolutional Neutral Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>吕</surname><given-names>昊宇</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>方</surname><given-names>睿</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>成都信息工程大学，计算机学院，四川 成都</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>12</month><year>2020</year></pub-date><volume>10</volume><issue>12</issue><fpage>2406</fpage><lpage>2418</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对传统网络ALexNet识别精度不高、内存需求量大、特征尺度单一等问题，该文提出了一种多尺度并行融合的轻量级模块PL-SE模块。该模块将上层特征经过两个不同尺度的卷积核和一个最大池化，融合之后得到新的特征信息，之后再经过一个SE模块，最后进行残差学习。同时，对于下采样部分，该文提出一种SR-SE模块代替传统网络的池化层，在降维的同时进行特征提取。使用PL-SE模块和SR-SE模块对ALexNet模型改进得到一种新的模型，用于对25种杂草幼苗进行训练识别。改进后的模型识别准确率达到了96.32%，相较于传统的ALexNet模型提高了8个百分点，参数总量减少约56.7 M (Million)。除此之外，与ResNet、GoogleNet、MobileNet等经典网络相比，改进后的模型在准确率和参数量方面都具有优势。 For the problems that the traditional network AlexNet recognition accuracy is not high, memory demand is large, the characteristic scale is single and so on, this paper presents a multi-scale parallel fusion lightweight module PL-SE module. In this module, the upper features are processed by two convolution kernels with different scales and a maximum pooling. After fusion, new feature information is obtained, followed by an SE module, and finally residual learning is conducted. At the same time, for the lower sampling part, this paper proposes a SR-SE module instead of the pooling layer of traditional network, which can extract features while reducing dimensions. Using PL-SE module and SR-SE module to improve ALexNet model, a new model was developed for training and identification of 25 weed seedlings. The improved model recognition accuracy reaches 96.32%, 8 percentage points higher than the traditional ALexNet model, and the total number of parameters is reduced by about 56.7 m (Million). In addition, compared with classic networks such as ResNet, GoogleNet and MobileNet, the improved model has advantages in accuracy and number of parameters. 
  
 
</p></abstract><kwd-group><kwd>AlexNet，PL-SE，SR-SE，多尺度并行融合，轻量级，杂草识别, AlexNet</kwd><kwd> PL-SE</kwd><kwd> SR-SE</kwd><kwd> Multiscale Parallel Fusion</kwd><kwd> Lightweight Class</kwd><kwd> Weeding Identification</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>针对传统网络ALexNet识别精度不高、内存需求量大、特征尺度单一等问题，该文提出了一种多尺度并行融合的轻量级模块PL-SE模块。该模块将上层特征经过两个不同尺度的卷积核和一个最大池化，融合之后得到新的特征信息，之后再经过一个SE模块，最后进行残差学习。同时，对于下采样部分，该文提出一种SR-SE模块代替传统网络的池化层，在降维的同时进行特征提取。使用PL-SE模块和SR-SE模块对ALexNet模型改进得到一种新的模型，用于对25种杂草幼苗进行训练识别。改进后的模型识别准确率达到了96.32%，相较于传统的ALexNet模型提高了8个百分点，参数总量减少约56.7 M (Million)。除此之外，与ResNet、GoogleNet、MobileNet等经典网络相比，改进后的模型在准确率和参数量方面都具有优势。</p></sec><sec id="s2"><title>关键词</title><p>AlexNet，PL-SE，SR-SE，多尺度并行融合，轻量级，杂草识别</p></sec><sec id="s3"><title>Weed Seeding Recognition Based on Multi-Scale Fusion Convolutional Neutral Network</title><p>Haoyu Lv, Rui Fang</p><p>School of Computer Science, Chengdu University of Information Technology, Chengdu Sichuan</p><p><img src="//html.hanspub.org/file/30-1541946x4_hanspub.png" /></p><p>Received: Nov. 29<sup>th</sup>, 2020; accepted: Dec. 23<sup>rd</sup>, 2020; published: Dec. 30<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/30-1541946x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>For the problems that the traditional network AlexNet recognition accuracy is not high, memory demand is large, the characteristic scale is single and so on, this paper presents a multi-scale parallel fusion lightweight module PL-SE module. In this module, the upper features are processed by two convolution kernels with different scales and a maximum pooling. After fusion, new feature information is obtained, followed by an SE module, and finally residual learning is conducted. At the same time, for the lower sampling part, this paper proposes a SR-SE module instead of the pooling layer of traditional network, which can extract features while reducing dimensions. Using PL-SE module and SR-SE module to improve ALexNet model, a new model was developed for training and identification of 25 weed seedlings. The improved model recognition accuracy reaches 96.32%, 8 percentage points higher than the traditional ALexNet model, and the total number of parameters is reduced by about 56.7 m (Million). In addition, compared with classic networks such as ResNet, GoogleNet and MobileNet, the improved model has advantages in accuracy and number of parameters.</p><p>Keywords:AlexNet, PL-SE, SR-SE, Multiscale Parallel Fusion, Lightweight Class, Weeding Identification</p><disp-formula id="hanspub.39552-formula34"><graphic xlink:href="//html.hanspub.org/file/30-1541946x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/30-1541946x7_hanspub.png" /> <img src="//html.hanspub.org/file/30-1541946x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>我国是一个拥有14亿人口的农业大国，农业的发展影响到社会的方方面面。在现代化农业生产过程中，杂草依然是制约农作物产量和质量的重要因素之一。杂草会与农作物争肥争水，增加病虫害的传播，最终导致农作物产量减少。尽管可以使用农药来控制杂草的生长，但是鉴于杂草种类繁多，同时一些杂草外观特征与农作物极其相似，若是以人眼识别难以区分，而且粗放式扩大喷洒农药，会造成资源浪费以及环境污染。随着精细农业的提出，为了合理使用农药，精准识别农作物和杂草变得尤为重要。</p><p>同时，随着我国现代化农村的提出，计算机与农业生产相结合是必然趋势。利用深度学习进行杂草识别，不仅可以提高杂草识别的准确率，合理使用除草方法，同时也可以对农业知识掌握不完全的人起到帮助。</p><p>2009年，Camargo [<xref ref-type="bibr" rid="hanspub.39552-ref1">1</xref>] 等通过提取出植物病害的特征参数，之后将特征输入支持向量机进行识别，获得了较高的准确度。2017年，Edmund J. Sadgrove [<xref ref-type="bibr" rid="hanspub.39552-ref2">2</xref>] 等对HSV、RGB和YUV三种不同颜色体系输入图像进行测试，并与标准灰度图像进行了时间和精度的比较，结果表明YUV在所有数据集上表现最好，可以达到84%的准确率。2016年，C Potena等将RGB和NIR图像分别输入不同的卷积神经网络，利用浅层神经网络对作物和杂草图像进行特征提取，然后利用深层卷积网络进行分类。2014年，HAUG [<xref ref-type="bibr" rid="hanspub.39552-ref3">3</xref>] 等提出四通道图像训练分割模型，平均准确率达到94%。2017年，Alessandro dos Santos Ferreira [<xref ref-type="bibr" rid="hanspub.39552-ref4">4</xref>] 等利用超像素算法生成图像，并利用卷积网络进行识别。MCCOOL [<xref ref-type="bibr" rid="hanspub.39552-ref5">5</xref>] 等利用卷积网络对作物与杂草进行识别，并使用模型压缩技术减少了模型参数。之后，Philipp Lottes [<xref ref-type="bibr" rid="hanspub.39552-ref6">6</xref>] 等利用马尔科夫随机场模型对随机森林算法进行优化进行杂草识别，并通过农场机器人在不同甜菜地进行实地实验，取得了较好的效果。</p><p>2007年，毛文华 [<xref ref-type="bibr" rid="hanspub.39552-ref7">7</xref>] 等根据作物及杂草的位置特征，提出种子填充算法识别行间杂草，适用于早期行间杂草的识别。2013年，何东健 [<xref ref-type="bibr" rid="hanspub.39552-ref8">8</xref>] 等针对单一特征识别率低、稳定性差的问题，提出多特征融合杂草识别方法，提取形状、纹理以及分形维数三类特征，稳定性和准确率相对提高。2014年，孟庆宽等针对图像处理易受光照等因素影响、算法实时性、稳定性较差等问题，提出了YUV颜色模型进行图像分割，取得较好效果。2015年，张新明 [<xref ref-type="bibr" rid="hanspub.39552-ref9">9</xref>] 等针对传统概率神经网络在杂草识别中存在识别精度低和实时性能差等缺陷，提出了一种基于改进概率神经网络的玉米与杂草识别方法。2016年，翟志强 [<xref ref-type="bibr" rid="hanspub.39552-ref10">10</xref>] 等针对识别算法在复杂农田环境下，立体匹配精度低、图像处理速度慢等问题，提出一种基于Census变换的识别算法，运用改进的超绿–超红方法灰度化图像提取特征值，取得了较好的效果。2016年，王璨 [<xref ref-type="bibr" rid="hanspub.39552-ref11">11</xref>] 等利用杂草的单目图像特征，融合杂草的高度信息，实现了较高准确率的识别。2017年，唐晶磊等利用K-means算法进行预训练，构建杂草识别模型，通过迁移学习训练新模型并在新数据集上进行微调，获得了较高的识别率。</p><p>该文就传统网络识别准确率不高，模型复杂等问题，基于传统Alexnet和文献中的模型提出了一种新的多尺度并行融合的轻量级模型，并与Alexnet、MobileNet等模型进行对比，为以后的杂草识别研究提供参考。</p></sec><sec id="s6"><title>2. 实验数据</title><p>该文使用由丹麦的奥胡斯大学提供的杂草幼苗图片 [<xref ref-type="bibr" rid="hanspub.39552-ref12">12</xref>] 作为数据集。该数据集包含了25种植物种类，一共38,000张RGB图像，同时每一种植物都是在三种不同的生长条件下培育的，在视觉外观方面提供了高度的多样性。实验图片见图1。</p><p>图1. 实验数据</p><p>为了使数据集种类均匀，保证模型的性能，该文通过旋转、翻转、改变图像的对比度、亮度等操作对数量较少的种类的图像进行扩充，保证每种植物的图像数量在1900~2000之间，扩充后的图像一共49,991张，之后将图像按照4:1的比例划分为训练集和测试集，最后将划分后的图像的尺寸通过线性插值法统一修改为224 &#215; 224 dpi。</p></sec><sec id="s7"><title>3. 杂草幼苗识别模型</title><sec id="s7_1"><title>3.1. AlexNet</title><p>AlexNet [<xref ref-type="bibr" rid="hanspub.39552-ref13">13</xref>] 是2012年ImageNet竞赛冠军获得者Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，更多的更深的神经网络被提出，比如优秀的vgg, GoogLeNet。</p><p>AlexNet [<xref ref-type="bibr" rid="hanspub.39552-ref11">11</xref>] 成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout [<xref ref-type="bibr" rid="hanspub.39552-ref13">13</xref>]。在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。除此之外，提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p><p>AlexNet [<xref ref-type="bibr" rid="hanspub.39552-ref11">11</xref>] 主要由5个卷积层和三个全连接层组成，其中3个卷积层进行了最大池化。其概述图见图2。</p><p>图2. AlexNet概述图</p></sec><sec id="s7_2"><title>3.2. 多尺度并行融合</title><p>在传统的CNN网络结构中，通常是由卷积层和池化层交替依次连接构成，对图像逐层进行特征提取和降维，这种结构特征提取比较单一，如果想要提升性能，只能通过改变卷积核大小或者提高网络层数，这会消耗大量的计算资源和造成过拟合等问题。在一系列Inception [<xref ref-type="bibr" rid="hanspub.39552-ref14">14</xref>] 中都表明引入最大池化表现效果比较好，且不会增加额外的参数量。因此，该文采用一个最大池化和两个不同尺度的卷积核构成并行结构对上层特征图进行特征特提取，之后将三个通道输出的特征信息进行融合。</p><p>如图3和图4所示，在图3中，上层输入只经过一层卷积层，比较单一，不易进行修改，而在图4中，上层输入经过不同大小的卷积核，不仅能够提取不同的特征，而且能够根据需求对卷积核进行不同的组合，更加灵活，使用较小的卷积核也不会增加额外的参数量。</p><p>图3. 传统卷积</p><p>图4. 多尺度融合卷积</p></sec><sec id="s7_3"><title>3.3. 深度可分离卷积</title><p>深度可分离卷积 [<xref ref-type="bibr" rid="hanspub.39552-ref15">15</xref>] 分为逐通道卷积和逐点卷积两个部分。逐通道卷积即一个卷积核负责一个通道，一个通道只被一个卷积核卷积。而逐点卷积的运算与常规卷积非常相似，它的卷积核为 1 &#215; 1 &#215; M ，M为上一层通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。</p><p>对于一个尺寸为 N &#215; N &#215; 3 &#215; M 的常规卷积，在深度可分离卷积中的参数量为 N &#215; N &#215; 3 + 1 &#215; 1 &#215; 3 &#215; M ，那么它与常规卷积的参数量之比为 1 / M + 1 / N &#215; N 。由此可见，深度可分离卷积可有效的减少参数量，对于模型的轻量化有着较大的帮助。</p></sec><sec id="s7_4"><title>3.4. SE (Squeeze-and-Excitation)模块</title><p>该模块的主要功能是对各个通道进行权重的分配，帮助网络把重要的信息学习到。该模块主要分为Squeeze、Excitation、融合三个操作 [<xref ref-type="bibr" rid="hanspub.39552-ref9">9</xref>]。基本框架图见图5。</p><p>图5. SE模块</p><p>Squeeze [<xref ref-type="bibr" rid="hanspub.39552-ref16">16</xref>] 操作是利用全局的池化，将大小为 C &#215; H &#215; W 的输入特征综合为 C &#215; 1 &#215; 1 的特征描述(description)，对于一张特征图来说，计算如下：</p><p>z c = F s q ( u c ) 1 H &#215; W ∑ i = 1 H ∑ j = 1 W u c ( i , j ) (1)</p><p>经过上述的Squeeze操作后，网络仅仅得到了一个全局描述，这个描述并不能作为该通道的权重。因此Excitation操作主要的目的是比较全面的获取到通道级别的依赖。同时应该满足灵活和能够学习非互斥强调的能力。基于上述目的和要求，该操作包含了两个全连接层和Sigmoid激活函数。全连接层能够很好的融合全部的输入特征信息，而Sigmoid函数也能够很好的将输入映射到0~1区间。计算如下：</p><p>s = F e x ( z , W ) = σ ( g ( z , W ) ) = σ ( W 2 δ ( W 1 ) ) (2)</p><p>经过上述的Exacitation [<xref ref-type="bibr" rid="hanspub.39552-ref16">16</xref>] 操作之后，就获得了特征图U各个通道的权重，最后就是将权重和原始的特征融合，就是简单的乘法运算：</p><p>x ˜ = F s c a l e ( u c , S c ) = S c ∗ U c (3)</p></sec><sec id="s7_5"><title>3.5. 线性瓶颈</title><p>在常规的深度学习中，一般使用Relu作为中间隐藏神经元的激活函数，AlexNet中提出的用Relu替代传统激活函数是深度学习的一大进步。相较于传统激活函数，Relu函数计算简单，具有加快网络训练速度、增加网络的非线性、防止梯度消失等优点。Relu的定义所示：</p><p>f ( x ) = max ( 0 , x ) = { x , x &gt; 0 0 , x ≤ 0 (4)</p><p>虽然Relu函数有诸多优点，但是Relu也有缺点。从以上定义可以看出，由于ReLU在x &lt; 0时梯度为0，这样就导致负的梯度在这个ReLU被置零，而且这个神经元有可能再也不会被任何数据激活。如果这个情况发生了，那么这个神经元之后的梯度就永远是0了，也就是ReLU神经元坏死了，不再对任何数据有所响应。为了避免这种问题的发生，该文提出了在深度卷积中去掉Relu函数而采用线性输出，这样可以在保证计算量不增大的同时保留各个通道的信息。</p><p>如图6所示，这是一个深度可分离结构，本文在第一个DW操作后使用Linear替换掉了Relu激活函数以避免在训练过程中神经元坏死造成梯度消失。</p><p>图6. 线性瓶颈</p></sec><sec id="s7_6"><title>3.6. 全局池化</title><p>对于传统的神经网络，往往会出现一两层全连接层，全连接一般会把卷积输出的二维特征图通过flatten转化成一个一维的向量，全连接层的每一个节点都与上一层每个节点连接，是把前一层的输出特征都综合起来，所以该层的权值参数是最多的，可能会造成过拟合，而全局池化常见的做法是把每个通道feature map输出一个神经元，大幅度减少了参数量，同时可以看到全局池化 [<xref ref-type="bibr" rid="hanspub.39552-ref17">17</xref>] 会根据需要产生神经元，神经元个数可控，可调。而flatten方式就是一个硬链接，无法在flatten的时候调整链接数目。全连接结构图见图7，全局池化结构图见图8。</p></sec><sec id="s7_7"><title>3.7. 多尺度并行轻量级模块</title><p>该文中为了得到更多的图像特征，提高识别准确率，使用了不同尺度的卷积核，同时对每个分支引入深度可分离卷积用于减少参数量，并且之后串联了一个SE模块，得到了一个新的结构，本文称之为PL-SE-Module。</p><p>图7. 全连接</p><p>图8. 全局池化</p><p>如图9所示，上层特征图首先经过三个分支，采用3 &#215; 3和5 &#215; 5两种不同尺度的卷积核分别进行深度可分离卷积操作和一个3 &#215; 3最大池化，然后将三个分支的输出特征在通道上进行融合 [<xref ref-type="bibr" rid="hanspub.39552-ref18">18</xref>]。为了方便对输出特征进行升维或者降维，之后再进行了一轮1 &#215; 1卷积，让三条支路的信息更好的融合在一起，接着为了进一步提高识别率，串联了一个SE-Module，最后将输出信息与原始特征图进行一个残差学习，防止过拟合。</p><p>图9. PL-SE-Module结构</p><p>除此之外，该文还提出了一个SR-SE-Module对图像进行下采样。</p><p>如图10所示，该模块首先采用了一个大小为3 &#215; 3，步长为2的深度可分离卷积和一个3 &#215; 3的池化操作并行提取特征和降维，最后使得融合后的特征信息通过一个SE模块。</p></sec><sec id="s7_8"><title>3.8. 模型设置</title><p>本文的模型设计以AlexNet为基础，使用PL-SE-module替换掉AlexNet中的卷积操作，同时为了保证有更多的信息传入下一层，使用SR-SE-Module替换掉池化层，以达到降维的同时进行特征提取的目的，除此之外，为了更多的减少参数，AlexNet的全连接层使用全局池化进行替代，并且在最后加入Dropout [<xref ref-type="bibr" rid="hanspub.39552-ref11">11</xref>] 以防止过拟合。模型结构见表1。</p><p>图10. SR-SE-Module结构</p><table-wrap-group id="1"><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Model structur</title></caption><table-wrap id="1_1"><table><tbody><thead><tr><th align="center" valign="middle" >Global-Avg-Pool</th><th align="center" valign="middle" >−</th><th align="center" valign="middle" >1 &#215; 1 &#215; 128</th></tr></thead><tr><td align="center" valign="middle" >BN</td><td align="center" valign="middle" >−</td><td align="center" valign="middle" >1 &#215; 1 &#215; 128</td></tr><tr><td align="center" valign="middle" >Dropout</td><td align="center" valign="middle" >−</td><td align="center" valign="middle" >1 &#215; 1 &#215; 128</td></tr><tr><td align="center" valign="middle" >Softmax</td><td align="center" valign="middle" >−</td><td align="center" valign="middle" >1 &#215; 1 &#215; 25</td></tr></tbody></table></table-wrap><table-wrap id="1_2"><table><tbody><thead><tr><th align="center" valign="middle" >Type</th><th align="center" valign="middle" >Filter shape/Stride</th><th align="center" valign="middle" >Output Size</th></tr></thead><tr><td align="center" valign="middle" >Input</td><td align="center" valign="middle" >−</td><td align="center" valign="middle" >224 &#215; 224 &#215; 3</td></tr><tr><td align="center" valign="middle" >PL-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 d w / 4 , 1 &#215; 1 / 1 5 &#215; 5 d w / 4 , 1 &#215; 1 / 1 3 &#215; 3 m a x p o o l / 4 ] , 1 &#215; 1 / 1</td><td align="center" valign="middle" >56 &#215; 56 &#215; 96</td></tr><tr><td align="center" valign="middle" >3&#215;PL-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 d w / 1 , 1 &#215; 1 / 1 5 &#215; 5 d w / 1 , 1 &#215; 1 / 1 3 &#215; 3 m a x p o o l / 1 ] , 1 &#215; 1 / 1</td><td align="center" valign="middle" >56 &#215; 56 &#215; 96</td></tr><tr><td align="center" valign="middle" >SR-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 d w / 2 , 1 &#215; 1 / 1 3 &#215; 3 m a x p o o l / 2 ]</td><td align="center" valign="middle" >28 &#215; 28 &#215; 96</td></tr><tr><td align="center" valign="middle" >PL-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 d w / 1 , 1 &#215; 1 / 1 5 &#215; 5 d w / 1 , 1 &#215; 1 / 1 3 &#215; 3 m a x p o o l / 1 ] , 1 &#215; 1 / 1</td><td align="center" valign="middle" >28 &#215; 28 &#215; 128</td></tr><tr><td align="center" valign="middle" >SR-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 d w / 2 , 1 &#215; 1 / 1 3 &#215; 3 m a x p o o l / 2 ]</td><td align="center" valign="middle" >14 &#215; 14 &#215; 128</td></tr><tr><td align="center" valign="middle" >Inception-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 m a x p o o l / 1 , 1 &#215; 1 / 1 1 &#215; 1 / 1 , 3 &#215; 3 / 1 1 &#215; 1 / 1 , 3 &#215; 3 / 1 , 3 &#215; 3 / 1 1 &#215; 1 / 1 ]</td><td align="center" valign="middle" >14 &#215; 14 &#215; 128</td></tr><tr><td align="center" valign="middle" >2&#215;PL-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 d w / 1 , 1 &#215; 1 / 1 5 &#215; 5 d w / 1 , 1 &#215; 1 / 1 3 &#215; 3 m a x p o o l / 1 ] , 1 &#215; 1 / 1</td><td align="center" valign="middle" >14 &#215; 14 &#215; 192</td></tr><tr><td align="center" valign="middle" >SR-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 d w / 2 , 1 &#215; 1 / 1 3 &#215; 3 m a x p o o l / 2 ]</td><td align="center" valign="middle" >7 &#215; 7 &#215; 192</td></tr><tr><td align="center" valign="middle" >PL-SE-Module</td><td align="center" valign="middle" >[ 3 &#215; 3 d w / 1 , 1 &#215; 1 / 1 5 &#215; 5 d w / 1 , 1 &#215; 1 / 1 3 &#215; 3 m a x p o o l / 1 ] , 1 &#215; 1 / 1</td><td align="center" valign="middle" >7 &#215; 7 &#215; 128</td></tr></tbody></table></table-wrap></table-wrap-group><p>表1. 模型结构</p></sec></sec><sec id="s8"><title>4. 实验分析</title><sec id="s8_1"><title>4.1. 实验平台</title><p>实验平台为Ubuntu16.04系统，采用Keras框架。计算机内存为32GB，GPU使用英伟达TITAN RTX，显存类型GDDR6，容量24 GB，CUDA核心数4608，Tensor核心数576，核心频率为1350~1770 MHz。</p></sec><sec id="s8_2"><title>4.2. 实验参数设置(表2)</title><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Parameter settin</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >参数</th><th align="center" valign="middle" >参数值</th></tr></thead><tr><td align="center" valign="middle" >Batch size</td><td align="center" valign="middle" >60</td></tr><tr><td align="center" valign="middle" >Learning rate</td><td align="center" valign="middle" >0.01</td></tr><tr><td align="center" valign="middle" >Training epochs</td><td align="center" valign="middle" >70</td></tr><tr><td align="center" valign="middle" >Optimizer</td><td align="center" valign="middle" >Adam</td></tr><tr><td align="center" valign="middle" >Dropout rate</td><td align="center" valign="middle" >0.5</td></tr><tr><td align="center" valign="middle" >Activation</td><td align="center" valign="middle" >Relu、Linear</td></tr></tbody></table></table-wrap><p>表2. 参数设置</p></sec><sec id="s8_3"><title>4.3. 模型识别准确率和迭代次数关系</title><p>为了测试该文中改进模型的收敛性，对比了准确率和迭代次数关系。将该文的改进模型、原始AlexNet、VGG16以及文献 [<xref ref-type="bibr" rid="hanspub.39552-ref17">17</xref>] 中的模型在该文图片集的测试集上进行准确率和迭代次数的比较，结果图见图11。</p><p>图11. 模型准确率与迭代次数关系</p><p>图11是使用tensorboard记录的每种模型每一次迭代和准确率的关系。从图11可以明显看到，该文提出的改进后的模型在迭代六次左右准确率就达到了90%，12次左右趋于稳定，而其他模型在20次之后才开始趋于稳定，说明改进模型收敛快，准确率高。</p></sec><sec id="s8_4"><title>4.4. SE-Module对准确率的影响</title><p>本文对AlexNet、文献 [<xref ref-type="bibr" rid="hanspub.39552-ref17">17</xref>] 模型、VGG16以及改进模型在引入SE-Module前后的准确率进行对比。从下表可以看出AlexNet、文献 [<xref ref-type="bibr" rid="hanspub.39552-ref17">17</xref>] 模型、VGG16以及改进模型在引入SE-Module之后准确率分别提升了0.97、0.25、0.88、1.04个百分点，说明SE-Module通过为每个通道分配权重，网络学习到重要信息，有效的提升了模型的识别精度。实验结果见表3。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> SE-Module and test accurac</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型 Module</th><th align="center" valign="middle" >准确率(未引入SE-Module) Accuracy/%</th><th align="center" valign="middle" >准确率(引入SE-Module) Accuracy/%</th></tr></thead><tr><td align="center" valign="middle" >原始AlexNet</td><td align="center" valign="middle" >87.47</td><td align="center" valign="middle" >88.44</td></tr><tr><td align="center" valign="middle" >文献 [<xref ref-type="bibr" rid="hanspub.39552-ref17">17</xref>] 模型</td><td align="center" valign="middle" >90.82</td><td align="center" valign="middle" >91.07</td></tr><tr><td align="center" valign="middle" >VGG16</td><td align="center" valign="middle" >89.50</td><td align="center" valign="middle" >90.38</td></tr><tr><td align="center" valign="middle" >改进模型</td><td align="center" valign="middle" >95.28</td><td align="center" valign="middle" >96.32</td></tr></tbody></table></table-wrap><p>表3. SE-Module与测试准确率</p></sec><sec id="s8_5"><title>4.5. 综合性能分析和对比</title><p>本文将AlexNet原始卷积核替换为两种尺度不同的融合卷积方式，同时引入SE模块以及使用融合卷积替代池化层进行下采样，大大提高了网络提取特征和获取信息的能力。除此之外，对所有卷积核采用深度可分离卷积和使用全局池化替换全连接层，既增加了网络深度也大幅度减少了参数量。实验结果见表4。</p><p>参数量少说明模型占用空间更少，更加轻量级。轻量级的网络可以用于部署在移动设备等小型设备之上，更加方便使用。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Comprehensive performance compariso</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型 Module</th><th align="center" valign="middle" >准确率 Accuracy/%</th><th align="center" valign="middle" >参数量 Parameters/Million</th><th align="center" valign="middle" >运行时间 Time/Second</th></tr></thead><tr><td align="center" valign="middle" >原始Alexnet</td><td align="center" valign="middle" >88.44</td><td align="center" valign="middle" >60.96</td><td align="center" valign="middle" >2.36</td></tr><tr><td align="center" valign="middle" >文献 [<xref ref-type="bibr" rid="hanspub.39552-ref16">16</xref>] 模型</td><td align="center" valign="middle" >91.07</td><td align="center" valign="middle" >1.62</td><td align="center" valign="middle" >2.09</td></tr><tr><td align="center" valign="middle" >VGG16</td><td align="center" valign="middle" >90.38</td><td align="center" valign="middle" >14.72</td><td align="center" valign="middle" >3.50</td></tr><tr><td align="center" valign="middle" >改进模型</td><td align="center" valign="middle" >96.32</td><td align="center" valign="middle" >1.79</td><td align="center" valign="middle" >3.58</td></tr><tr><td align="center" valign="middle" >ResNet</td><td align="center" valign="middle" >94.99</td><td align="center" valign="middle" >25.5</td><td align="center" valign="middle" >3.49</td></tr><tr><td align="center" valign="middle" >InceptionV2</td><td align="center" valign="middle" >96.29</td><td align="center" valign="middle" >11.26</td><td align="center" valign="middle" >4.30</td></tr><tr><td align="center" valign="middle" >MobileNetV2</td><td align="center" valign="middle" >95.21</td><td align="center" valign="middle" >0.98</td><td align="center" valign="middle" >4</td></tr></tbody></table></table-wrap><p>表4. 综合性能对比</p><p>从表4可以看出，原始AlexNet、文献 [<xref ref-type="bibr" rid="hanspub.39552-ref17">17</xref>] 模型以及VGG16在本文数据集上的表现一般。表现较好的模型为改进模型、InceptionV2和MobileNetV2，其中改进模型和InceptionV2的准确率都达到了96%以上，虽然改进模型和InceptionV2在准确率上比较接近，但是改进模型的参数量远小于InceptionV2，参数量减少了大约9.47 (Million)。</p><p>改进模型由于将传统卷积转化为1 &#215; 1的点卷积和通道卷积和在每个模块后级联了SE-Module，这样虽然缩小了参数量和提高了准确率，但是加深了网络深度，所以在运行速度方面表现一般。</p></sec></sec><sec id="s9"><title>5. 结束语</title><p>在数据集方面，大部分杂草识别有关文献中使用的数据集比较小，易导致过拟合，泛化效果不佳，针对这个问题，本文采用包含了25个杂草种类的数据集，并通过转换、改变图像对比度等方式对图像进行扩充，使得图片集数量达到49,991张。</p><p>在模型方面，本文采用多尺度卷积和SE模块结合提出PL-SE模块和SR-SE模块去替换掉AlexNet中的卷积核和池化层，以获取更重要的和更多的特征信息，同时由于多尺度卷积采用深度可分离卷积，因此并不会过多的增加计算量，且用全局池化层代替全连接层，模型参数量约为AlexNet的4%，性能得到较大提升。</p><p>该文目前主要仍是在单一背景下的杂草图像识别，实际田间的杂草状况要更加复杂，之后会继续探索在复杂背景下的杂草识别研究，同时将以此为基础与目标检测相结合，以满足更多实际田间复杂的需求。</p></sec><sec id="s10"><title>基金项目</title><p>国家自然科学基金(61806029)。</p></sec><sec id="s11"><title>文章引用</title><p>吕昊宇,方 睿. 基于多尺度融合的卷积神经网络的杂草幼苗识别Weed Seeding Recognition Based on Multi-Scale Fusion Convolutional Neutral Network[J]. 计算机科学与应用, 2020, 10(12): 2406-2418. https://doi.org/10.12677/CSA.2020.1012255</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.39552-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Camargo, A. and Smith, J.S. (2009) Image Pattern Classification for the Identification of Disease Causing Agents in Plants. Computers and Electronics in Agriculture, 66, 121-125. 
&lt;br&gt;https://doi.org/10.1016/j.compag.2009.01.003</mixed-citation></ref><ref id="hanspub.39552-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Sadgrove, E.J., Falzon, G., Miron, D. and Lamb, D. (2017) Fast Object Detection in Pastoral Landscapes Using a Colour Feature Extreme Learning Machine. Computers and Elec-tronics in Agriculture, 139, 204-212. 
&lt;br&gt;https://doi.org/10.1016/j.compag.2017.05.017</mixed-citation></ref><ref id="hanspub.39552-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Haug, S., Michaels, A., Biber, P. and Ostermann, J. (2014) Plant Classification System for Crop/Weed Discrimination without Segmentation. IEEE Winter Conference on Applica-tions of Computer Vision, Steamboat Springs, 24-26 March 2014, 1142-1149. &lt;br&gt;https://doi.org/10.1109/WACV.2014.6835733</mixed-citation></ref><ref id="hanspub.39552-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Dos Santos Ferreira, A., Matte Freitas, D., Gonçalves da Silva, G., Pistori, H. and Theophilo Folhes, M. (2017) Weed Detection in Soybean Crops Using ConvNets. Computers and Electronics in Agriculture, 143, 314-324. 
&lt;br&gt;https://doi.org/10.1016/j.compag.2017.10.027</mixed-citation></ref><ref id="hanspub.39552-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Mccool, C., Perez, T. and Upcroft, B. (2017) Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics. IEEE Robotics and Automation Letters, 2, 1344-1351. 
&lt;br&gt;https://doi.org/10.1109/LRA.2017.2667039</mixed-citation></ref><ref id="hanspub.39552-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Sa, I., Popović, M., Khanna, R., Chen, Z.T., Lottes, P., Liebisch, F., Nieto, J., Stachniss, C., Walter, A. and Siegwart, R. (2018) WeedMap: A Large-Scale Semantic Weed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming. Remote Sensing, 10, 1423. &lt;br&gt;https://doi.org/10.3390/rs10091423</mixed-citation></ref><ref id="hanspub.39552-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">王一鸣, 毛文华, 张小超. 基于纹理和位置特征的麦田杂草识别方法[J]. 农业机械学报, 2007, 38(4): 107-110. 
http://dx.chinadoi.cn/10.3969/j.issn.1000-1298.2007.04.028</mixed-citation></ref><ref id="hanspub.39552-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">何东健, 乔永亮, 李攀, 高瞻, 李海洋, 唐晶磊. 基于SVM-DS多特征融合的杂草识别[J]. 农业机械学报, 2013, 44(2): 182-187. http://dx.chinadoi.cn/10.6041/j.issn.1000-1298.2013.02.034</mixed-citation></ref><ref id="hanspub.39552-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">张新明, 涂强, 冯梦清. 基于改进概率神经网络的玉米与杂草识别[J]. 山西大学学报(自然科学版), 2015, 38(3): 432-438. http://dx.chinadoi.cn/10.13451/j.cnki.shanxi.univ(nat.sci.).2015.03.008</mixed-citation></ref><ref id="hanspub.39552-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">翟志强, 朱忠祥, 杜岳峰, 张硕, 毛恩荣. 基于Census变换的双目视觉作物行识别方法[J]. 农业工程学报, 2016, 32(11): 205-213. http://dx.chinadoi.cn/10.11975/j.issn.1002-6819.2016.11.029</mixed-citation></ref><ref id="hanspub.39552-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">王璨, 武新慧, 李志伟. 基于卷积神经网络提取多尺度分层特征识别玉米杂草[J]. 农业工程学报, 2018, 34(5): 144-151. http://dx.chinadoi.cn/10.11975/j.issn.1002-6819.2018.05.019</mixed-citation></ref><ref id="hanspub.39552-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Leminen Madsen, S., Mathiassen, S.K., Dyrmann, M., Laursen, M.S., Paz, L.-C. and Jørgensen, R.N. (2020) Open Plant Pheno-Type Database of Common Weeds in Denmark. Remote Sensing, 12, 1246. 
&lt;br&gt;https://doi.org/10.3390/rs12081246</mixed-citation></ref><ref id="hanspub.39552-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky, A., Sutskever, I. and Hinton, G.E. (2017) ImageNet Classi-fication with Deep Convolutional Neural Networks. Communications of the ACM, 60, 84-90. &lt;br&gt;https://doi.org/10.1145/3065386</mixed-citation></ref><ref id="hanspub.39552-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Liu, W., Jia, Y.Q., Sermanet, P., Reed, S., Anguelov, D., et al. (2015) Going Deeper with Convolutions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-nition, Boston, 7-12 June 2015, 1-9. 
&lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298594</mixed-citation></ref><ref id="hanspub.39552-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Howard, A.G., Zhu, M.L., Chen, B., Kalenichenko, D., Wang, W.J., Weyand, T., Andreetto, M. and Adam, H. (2017) Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv Preprint, arXiv:1704.04861.</mixed-citation></ref><ref id="hanspub.39552-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Hu, J., Shen, L. and Sun, G. (2018) Squeeze-and-Excitation Networks. Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 7132-7141. 
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00745</mixed-citation></ref><ref id="hanspub.39552-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">孙俊, 何小飞, 谭文军, 武小红, 沈继锋, 陆虎. 空洞卷积结合全局池化的卷积神经网络识别作物幼苗与杂草[J]. 农业工程学报, 2018, 34(11): 159-165. http://dx.chinadoi.cn/10.11975/j.issn.1002-6819.2018.11.020</mixed-citation></ref><ref id="hanspub.39552-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">孙俊, 谭文军, 武小红, 沈继锋, 芦兵, 戴春霞. 多通道深度可分离卷积模型实时识别复杂背景下甜菜与杂草[J]. 农业工程学报, 2019, 35(12): 184-190. http://dx.chinadoi.cn/10.11975/j.issn.1002-6819.2019.12.022</mixed-citation></ref></ref-list></back></article>