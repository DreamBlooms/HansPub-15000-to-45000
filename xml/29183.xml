<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AIRR</journal-id><journal-title-group><journal-title>Artificial Intelligence and Robotics Research</journal-title></journal-title-group><issn pub-type="epub">2326-3415</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AIRR.2019.82005</article-id><article-id pub-id-type="publisher-id">AIRR-29183</article-id><article-categories><subj-group subj-group-type="heading"><subject>AIRR20190200000_20850874.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject><subject> 工程技术</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于证据K近邻的目标识别新方法
  A New Target Recognition Method Based on Evidence Theoretic K-NN Rule
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>关</surname><given-names>欣</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>赵</surname><given-names>静</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>海桥</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>海军航空大学，山东 烟台</addr-line></aff><pub-date pub-type="epub"><day>08</day><month>03</month><year>2019</year></pub-date><volume>08</volume><issue>02</issue><fpage>37</fpage><lpage>45</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   本文提出了一种基于证据K近邻的目标识别新方法，在Zouhal改进KNN算法的基础上增加了训练修正步骤。首先，求得每一个目标类别的参考最近邻距离，使训练样本中该目标类别的样本在经验风险最小化的前提下与其他样本完成分离；然后，利用求得的参考最近邻距离和证据理论结合得出初始的识别分类结果；第三，设置混淆矩阵P，通过神经网络寻优迭代，获得P矩阵参数，用于Zouhal分类结果修正；最后，通过多数据集验证了P矩阵的泛化能力，通过与经典算法的分类精度对比验证了新方法的可行性和有效性。 This paper presents a new target recognition method based on evidence theoretic K-NN rule. A training correction step was added on the basis of Zouhal’s improved KNN algorithm. Firstly, a reference nearest neighbor distance of every target class should be computed in order to separate samples of one class from other samples with least error rate. Secondly, the initial classification result can be got through the reference nearest neighbor distance and DS evidence theory. Thirdly, setting up confusion matrix P and optimizing iteration through neural network to obtain matrix parameters for Zouhal’s classification result correction. Finally, the generalization ability of the matrix P is verified through multiple data sets, and the feasibility and effectiveness of the new method are verified by comparing with the classification accuracy of the classical algorithm. 
 
</p></abstract><kwd-group><kwd>证据K近邻，混淆矩阵，目标识别, Evidence Theoretic K-NN Rule</kwd><kwd> Confusion Matrix</kwd><kwd> Target Recognition</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于证据K近邻的目标识别新方法<sup> </sup></title><p>关欣，赵静，刘海桥</p><p>海军航空大学，山东 烟台</p><p><img src="//html.hanspub.org/file/1-2610149x1_hanspub.png" /></p><p>收稿日期：2019年2月6日；录用日期：2019年3月1日；发布日期：2019年3月8日</p><disp-formula id="hanspub.29183-formula1"><graphic xlink:href="//html.hanspub.org/file/1-2610149x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>本文提出了一种基于证据K近邻的目标识别新方法，在Zouhal改进KNN算法的基础上增加了训练修正步骤。首先，求得每一个目标类别的参考最近邻距离，使训练样本中该目标类别的样本在经验风险最小化的前提下与其他样本完成分离；然后，利用求得的参考最近邻距离和证据理论结合得出初始的识别分类结果；第三，设置混淆矩阵P，通过神经网络寻优迭代，获得P矩阵参数，用于Zouhal分类结果修正；最后，通过多数据集验证了P矩阵的泛化能力，通过与经典算法的分类精度对比验证了新方法的可行性和有效性。</p><p>关键词 :证据K近邻，混淆矩阵，目标识别</p><disp-formula id="hanspub.29183-formula2"><graphic xlink:href="//html.hanspub.org/file/1-2610149x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/1-2610149x7_hanspub.png" /> <img src="//html.hanspub.org/file/1-2610149x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>证据理论具有较强的理论基础，既能处理随机性所导致的不确定性，又能处理模糊性所导致的不确定性 [<xref ref-type="bibr" rid="hanspub.29183-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.29183-ref2">2</xref>] ，利用Dempster组合规则可以对不同传感器提供的目标识别证据进行空间域判决融合 [<xref ref-type="bibr" rid="hanspub.29183-ref3">3</xref>] ，还可以在时间域对传感器提供的目标识别证据进行时间域融合 [<xref ref-type="bibr" rid="hanspub.29183-ref4">4</xref>] ，证据理论可以依靠证据的积累，不断缩小假设集 [<xref ref-type="bibr" rid="hanspub.29183-ref5">5</xref>] 。K近邻(K-Nearest Neighbor, KNN)算法是一种比较成熟的分类算法，也是最简单的机器学习算法之一 [<xref ref-type="bibr" rid="hanspub.29183-ref6">6</xref>] 。近年来利用K近邻算法进行的识别与分类被广泛应用于各行各业，例如文献 [<xref ref-type="bibr" rid="hanspub.29183-ref7">7</xref>] 将其与支持向量机相融合应用于天气识别领域，文献 [<xref ref-type="bibr" rid="hanspub.29183-ref8">8</xref>] 将其应用于醉酒驾驶识别领域，等等。经典的K近邻算法仅适用于边界可分和类分布为椭圆和高斯分布的情况 [<xref ref-type="bibr" rid="hanspub.29183-ref9">9</xref>] ，一旦突破这个前提，K近邻分类器的分类效果就会不理想。针对这个问题，很多专家学者对经典K近邻算法进行了改进。文献 [<xref ref-type="bibr" rid="hanspub.29183-ref10">10</xref>] 中，Zouhal利用统计学习的方法对经典K近邻算法进行了优化，用一个基本概率赋值(Basic Probability Assignment, BPA)函数表示一个测试样本属于某一目标类的可能性，并且将该函数随距离的变化用负指数函数表示。</p><p>本文所提出的基于证据K近邻的目标识别新方法就是在Zouhal方法基础上的改进，当利用求得的参考最近邻距离和证据理论得出初始的识别分类结果后，引入混淆矩阵 P ，利用初始分类结果与训练样本真值之间的偏差对结果进行修正，使得分类结果更加准确，识别率提高，然后，通过多数据集验证了 P 矩阵的泛化能力，通过与经典算法的分类精度对比验证了新方法的可行性和有效性。</p></sec><sec id="s4"><title>2. K近邻算法</title><p>K近邻(K-Nearest Neighbor, KNN)算法是在1968年由Cover和Hart提出来的 [<xref ref-type="bibr" rid="hanspub.29183-ref11">11</xref>] ，它是一种理论比较成熟的分类算法，也是最简单的机器学习算法之一。</p><sec id="s4_1"><title>2.1. 算法思路及特点</title><p>该算法的思路是：如果一个样本在特征空间中的K个最相似(也就是特征空间中的K个最近邻)样本中的大多数属于某一识别类，则认为该样本属于此识别类。</p><p>在K-NN算法中，所选择的邻居都是已经正确分类的对象，该算法在分类决策上只依据最近邻的一个或者几个样本的类别来决定待测样本的类别。</p><p>该算法的特点是：①基于实例之间距离和投票表决的分类②适合多分类③大多数情况下比贝叶斯和中心向量法好④当给定训练集、距离度量、K值及分类决策函数时，结果唯一确定。</p></sec><sec id="s4_2"><title>2.2. 算法描述</title><p>假设训练数据集 T = { ( x i , y i ) , i = 1 , 2 , 3 , ⋯ , N } ，其中， x i ∈ R n 为实例的特征向量， y i ∈ { c i , i = 1 , 2 , 3 , ⋯ , K } 为实例的分类。根据给定的距离度量 d i j (使用的距离度量不同，K近邻的结果也会不同，本文取Euclid距离)即</p><p>d i j = ∑ l = 1 n ( x i ( l ) − x j ( l ) ) 2 (1)</p><p>其中， x i , x j ∈ R n ， x i = ( x i ( 1 ) , x i ( 2 ) , x i ( 3 ) , ⋯ , x i ( n ) ) ， x j = ( x j ( 1 ) , x j ( 2 ) , x j ( 3 ) , ⋯ , x j ( n ) ) 。</p><p>在训练集T中找出与实例向量 X 最近的K个点，涵盖着K个点的 X 的邻域记作 N K ( X ) ，在 N K ( X ) 中根据分类决策规则(如投票表决)决定 X 所属的类别y，即</p><p>y = arg max c j ∑ x i ∈ N K ( X ) I ( y i = c j ) (2)</p><p>其中， i = 1 , 2 , 3 , ⋯ , N ， j = 1 , 2 , 3 , ⋯ , K ，I为指示函数，即当 y i = c j 时，I为1，否则为0。</p><p>因此，K-NN算法中，当训练集、距离度量、K值及分类规则确定后，对于任意一个输入实例 X ，它所属的目标类y是唯一确定的。</p></sec></sec><sec id="s5"><title>3. 证据K近邻规则</title><p>K近邻算法的一个显而易见的改进是对K个近邻的贡献进行加权 [<xref ref-type="bibr" rid="hanspub.29183-ref12">12</xref>] ，根据它们相对待识别样本 X 的距离，将较大的权值赋给较近的近邻，因此，引入证据理论中，基本概率赋值函数的规律应该随距离的增大而减小，这里取负指数函数来表达 [<xref ref-type="bibr" rid="hanspub.29183-ref10">10</xref>] ，即</p><p>α = α 0 e − d k (3)</p><p>其中， k = 1 , 2 , 3 , ⋯ , K ， α 0 根据经验设定取0.95， 为第k个近邻到实例向量 X 的距离。则输出的识别分类结果为</p><p>y = arg max c j ∑ x i ∈ N K ( X ) α I ( y i = c j ) (4)</p><p>按照上述距离加权的K近邻算法是一种非常有效的归纳推理算法，它对训练中的噪声有很好的鲁棒性 [<xref ref-type="bibr" rid="hanspub.29183-ref13">13</xref>] ，并且当给定足够大的训练集合时，它也是非常有效的，本论文将在后续小节进行验证。</p><p>基于距离加权的K近邻算法引入到证据理论中，可以将权重系数作为证据理论中相关信息的基本概率赋值，则有</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/1-2610149x36_hanspub.png" xlink:type="simple"/></inline-formula>， α ∈ [ 0 , 1 ] (5)</p><p>证据K近邻规则的思路可以概括为：利用每个近邻的基本概率赋值作为证据理论的BPA赋值，并完成最终分类。即假设现有一个待识别样本 x s ，它的K个近邻为 F s ，根据式(3)和(5)可以得到每个近邻的基本概率赋值 ，再利用Dempster组合规则进行合成，算出 x s 属于某个目标类的置信度完成最终分类。</p></sec><sec id="s6"><title>4. 新算法</title><sec id="s6_1"><title>4.1. 新算法的思想</title><p>距离 d k 所包含的信息不仅与其所对应的训练样本中属于目标类y的那部分训练样本有关，还与训练样本中不属于目标类y的那部分训练样本有关。对于任意一个目标类y的样本而言，假设<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/1-2610149x43_hanspub.png" xlink:type="simple"/></inline-formula>是它与目标类y内的最近邻之间的距离，若 d i ≥ d k ，则认为该样本不属于目标类y；若<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/1-2610149x45_hanspub.png" xlink:type="simple"/></inline-formula>，则认为该样本属于目标类y。证据K近邻的目标就是在误分配率最小化的前提下完成样本的分类，因此，在一定程度上，距离信息 可以看作是一个阈值，以距离信息 d k 为门限在误分配率最小化的前提下进行样本分类。这里就存在一个误分配率的问题，它所代表的物理意义是目标类y与其他目标类的混淆程度，即使是在其最小化的前提下进行的分类，它也会对最终的结果造成不良的影响，本文就是从该误分配率着手，引入混淆矩阵 P ，利用初始分类结果与训练样本真值之间的偏差对结果进行修正，使分类结果更加精确。</p></sec><sec id="s6_2"><title>4.2. 混淆矩阵P的定义与求解</title><p>混淆矩阵也称作误差矩阵 [<xref ref-type="bibr" rid="hanspub.29183-ref14">14</xref>] ，在人工智能中它被看作是一种可视化工具，可用于监督学习 [<xref ref-type="bibr" rid="hanspub.29183-ref15">15</xref>] ，在图像精度评价中，主要用于比较分类结果和实际测得值 [<xref ref-type="bibr" rid="hanspub.29183-ref16">16</xref>] ，可以把分类结果的精度显示在一个混淆矩阵里面 [<xref ref-type="bibr" rid="hanspub.29183-ref17">17</xref>] ，利用这一思想，本文引入一个混淆矩阵 P 如下：</p><p>P = [ P 11 P 12 ⋯ P 1 j P 21 P 22 ⋯ P 2 j ⋮ ⋮ ⋱ ⋮ P i 1 P i 2 ⋯ P i j ] (6)</p><p>其中， P i j 表示实际属于第j个目标类却被误分配给第i个目标类的概率。</p><p>首先，利用BP神经网络对训练样本进行训练，找到目标样本的K个近邻，用 x k ( k = 1 , 2 , ⋯ , K ) 表示，通过预先设定的分类器C对这K个近邻进行分类输出，其中任意一个样本 x k 经过分类器输出后得到的结果为 P k = [ P k ( 1 ) , P k ( 2 ) , ⋯ , P k ( C ) ] T ， P k ( i ) = P ( L ( x k ) = y i ) 表示通过设定的分类器得到目标样本的近邻 属于类别 y i ( i = 1 , 2 , ⋯ , C ) 的概率。假设 x k 的真实分类为 y t ，则用 T k = [ T k ( 1 ) , T k ( 2 ) , ⋯ , T k ( C ) ] T 表示期望的分类器输出值。</p><p>假设共有C个目标类记作 { y 1 , y 2 , ⋯ y C } ，某待测样本 x s 有K个近邻，通过证据K近邻规则(见第2节)算得待测样本 x s 归为目标类 y i ( i = 1 , 2 , ⋯ , C ) ，并且通过式(3)和(5)得出权重系数 α ，即</p><p>α = [ α 1 , α 2 , ⋯ , α C ] (7)</p><p>然后，在K个近邻中找出被分给目标 y i ( i = 1 , 2 , ⋯ , C ) 类的样本，假设有N个，计算这N个近邻被修正后的值(即 α P )与训练样本真值T之间的偏差和(这里取Euclid距离)，即</p><p>∑ j = 1 N d j = ‖ α P − T ‖ (8)</p><p>将式(8)最小化算得 P ，即为所求混淆矩阵。</p></sec><sec id="s6_3"><title>4.3. 新算法的步骤</title><p>基于上述算法思想及相关基础解析，新算法的步骤可以概括为：</p><p>①选取数据集样本进行训练，得出初始目标类和聚类中心；</p><p>②利用证据K近邻规则计算待测样本的初始分类结果；</p><p>③由式(8)最小化算得混淆矩阵 P ；</p><p>④利用混淆矩阵 P 对结果进行修正，得出最终分类结果 m ( y i ) = α P ，其中 i = 1 , 2 , ⋯ , C 。</p></sec></sec><sec id="s7"><title>5. 实验仿真与分析</title><p>为了验证第3节所说的“当给定足够大的训练集合时本文算法依然非常有效”的说法，这里选取Satimage数据集来进行仿真验证和深入分析。为了验证混淆矩阵 P 的泛化能力还选取了其他6组数据集进行验证，并将本文方法与经典Bayes及经典ENN方法进行了对比。</p><sec id="s7_1"><title>5.1. 仿真步骤</title><p>Satimage数据集共有6435个样本，这里将样本一分为二，一半作为训练样本，一半作为测试样本。</p><p>图1. 新算法MATLAB仿真流程图</p><p>首先，构建神经网络对训练样本进行训练并得到初始的分类类别和聚类中心；其次，通过证据K近邻规则算出训练样本中每一个样本属于某一目标类别的BPA，得到每一目标类别的BPA矩阵；第三，计算每一个目标类的权重系数 α ，得出混淆矩阵 P (见4.2节)；第四，利用混淆矩阵 P 对初始结果进行修正，计算修正后的识别精确度；第五，重复迭代，将修正后的识别精确度大于初始识别精确度的权重系数 α 保存下来，构成最终的修正矩阵 P ′ ；第六，一方面，测试样本根据证据K近邻规则算出初始分类结果，另一方面，用训练样本算出的最终的修正矩阵 对测试样本的初始分类结果进行修正，得到修正后的分类结果；最后，将测试样本初始分类识别精确度与修正后的识别精确度进行比较并作图。</p><p>Matlab仿真流程如图1所示。</p></sec><sec id="s7_2"><title>5.2. 实验结果分析</title><p>通过本文所提的新算法，利用Satimage数据集进行实验仿真后，可以得到初始分类识别精确度与修正后分类识别精确度如表1所示，对比图如图2所示。</p><p>由表1及图2可以形象直观地看出，新方法在引入混淆矩阵<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/1-2610149x83_hanspub.png" xlink:type="simple"/></inline-formula>对初始分类结果进行修正后所得到的</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison of initial classification recognition accuracy and corrected classification identification accurac</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >聚类数目</th><th align="center" valign="middle" >修正前精确度</th><th align="center" valign="middle" >修正后精确度</th></tr></thead><tr><td align="center" valign="middle" >12</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >81.87%</td></tr><tr><td align="center" valign="middle" >13</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >81.83%</td></tr><tr><td align="center" valign="middle" >14</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >81.92%</td></tr><tr><td align="center" valign="middle" >15</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >81.90%</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >82.25%</td></tr><tr><td align="center" valign="middle" >17</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >82.27%</td></tr><tr><td align="center" valign="middle" >18</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >82.30%</td></tr><tr><td align="center" valign="middle" >19</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >82.27%</td></tr><tr><td align="center" valign="middle" >20</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >82.30%</td></tr><tr><td align="center" valign="middle" >21</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.24%</td></tr><tr><td align="center" valign="middle" >22</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.20%</td></tr><tr><td align="center" valign="middle" >23</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.25%</td></tr><tr><td align="center" valign="middle" >24</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.15%</td></tr><tr><td align="center" valign="middle" >25</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.19%</td></tr><tr><td align="center" valign="middle" >26</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.21%</td></tr><tr><td align="center" valign="middle" >27</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.55%</td></tr><tr><td align="center" valign="middle" >28</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.76%</td></tr><tr><td align="center" valign="middle" >29</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >83.81%</td></tr><tr><td align="center" valign="middle" >30</td><td align="center" valign="middle" >52.43%</td><td align="center" valign="middle" >84.10%</td></tr></tbody></table></table-wrap><p>表1. 初始分类识别精确度与修正后分类识别精确度对比表</p><p>图2. 修正前后分类识别精确度对比图</p><p>目标识别分类结果的精确度要远远高于仅利用证据K近邻规则所得的目标识别分类结果的精确度。修正前的分类精度因为没有对样本进行训练和修正的过程，所以其精度不会随着聚类数目的增多而发生变化。而本文所提的新方法利用混淆矩阵 P 对初始分类结果进行不断修正且有对样本进行训练的过程，所以该新方法在聚类数目增多的情况下，分类精度会随之增高。</p><p>为了验证混淆矩阵 P 的泛化能力，随机选取了其他6组数据集(Sonar、Vehicle、Page、Vowel、Wine-red和Segment)进行验证，并将本文方法与经典Bayes及经典ENN方法进行了对比。见表2。</p><p>由表2可知， P 阵具有很好的泛化能力，对训练样本依赖不大，不光是Satimage数据集，其他数据</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of classification accuracy of different classification idea</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >Bayes</th><th align="center" valign="middle" >ENN</th><th align="center" valign="middle" >本文方法</th></tr></thead><tr><td align="center" valign="middle" >Sonar</td><td align="center" valign="middle" >69.23%</td><td align="center" valign="middle" >73.08%</td><td align="center" valign="middle" >78.85%</td></tr><tr><td align="center" valign="middle" >Vehicle</td><td align="center" valign="middle" >46.93%</td><td align="center" valign="middle" >45.72%</td><td align="center" valign="middle" >61.17%</td></tr><tr><td align="center" valign="middle" >Page</td><td align="center" valign="middle" >90.51%</td><td align="center" valign="middle" >89.77%</td><td align="center" valign="middle" >94.76%</td></tr><tr><td align="center" valign="middle" >Vowel</td><td align="center" valign="middle" >71.59%</td><td align="center" valign="middle" >60.17%</td><td align="center" valign="middle" >94.44%</td></tr><tr><td align="center" valign="middle" >Wine-red</td><td align="center" valign="middle" >44.32%</td><td align="center" valign="middle" >45.59%</td><td align="center" valign="middle" >47.12%</td></tr><tr><td align="center" valign="middle" >Segment</td><td align="center" valign="middle" >80.32%</td><td align="center" valign="middle" >71.04%</td><td align="center" valign="middle" >90.76%</td></tr><tr><td align="center" valign="middle" >Satimage</td><td align="center" valign="middle" >80.29%</td><td align="center" valign="middle" >71.46%</td><td align="center" valign="middle" >90.78%</td></tr></tbody></table></table-wrap><p>表2. 不同分类思想分类精度对比</p><p>集也同样适用。通过实验结果对比可以看出基于证据K近邻的目标识别新方法较Bayes及经典ENN算法分类精度有较大提高，具有现实意义且行之有效。</p></sec></sec><sec id="s8"><title>6. 结论</title><p>本文提出了一种基于证据K近邻的目标识别新方法，在Zouhal改进KNN算法的基础上增加了训练修正步骤。首先，求得每一个目标类别的参考最近邻距离，使训练样本中该目标类别的样本在经验风险最小化的前提下与其他样本完成分离；然后，利用求得的参考最近邻距离和证据理论结合得出初始的识别分类结果；第三，设置混淆矩阵 P ，通过神经网络寻优迭代，获得 P 矩阵参数，用于Zouhal分类结果修正；最后，通过多数据集验证了 P 矩阵具有很好的泛化能力，对训练样本依赖不大，且通过实验结果对比可以看出基于证据K近邻的目标识别新方法较Bayes及经典ENN算法分类精度有较大提高，具有现实意义且行之有效。</p></sec><sec id="s9"><title>基金项目</title><p>国防科技卓越青年人才基金(2017-JCJQ-ZQ-003)，泰山学者工程专项经费(ts201712072)，国家自然科学基金(61501488)资助课题。</p></sec><sec id="s10"><title>文章引用</title><p>关 欣,赵 静,刘海桥. 基于证据K近邻的目标识别新方法A New Target Recognition Method Based on Evidence Theoretic K-NN Rule[J]. 人工智能与机器人研究, 2019, 08(02): 37-45. https://doi.org/10.12677/AIRR.2019.82005</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.29183-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">何友, 王国宏, 关欣, 等. 信息融合理论及应用[M]. 北京: 电子工业出版社, 2010.</mixed-citation></ref><ref id="hanspub.29183-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">郭强, 关欣, 潘丽娜, 孙贵东. 一种基于混合参数和DSmT的证据网络多连通结构推理方法[J]. 中国电子科学研究院学报, 2015, 10(1): 67-74.</mixed-citation></ref><ref id="hanspub.29183-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Yu, X.H., Zhou, Q.-J., Li, Y.-L., An, J. and Liu, Z.-C. (2014) A New Self-Adaptive Fusion Algorithm Based on DST and DSmT. Proceedings of 17th International Conference on Information Fusion, Salamanca, 7-10 July 2014.</mixed-citation></ref><ref id="hanspub.29183-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Tan, J.W., Zhan, H., Wen, Y. and Zhan, W.X. (2014) New Method for Multiple Cues Fusion Combined DST and DSmT. Information Technology Journal, 13, 393-396. &lt;br&gt;https://doi.org/10.3923/itj.2014.393.396</mixed-citation></ref><ref id="hanspub.29183-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Tchamova, A. and Dezert, J. (2012) On the Behavior of Dempster’s Rule of Combination and the Foundations of Dempster-Shafer Theory. 6th IEEE International Conference Intelligent Systems, Sofia, 6-8 September 2012.</mixed-citation></ref><ref id="hanspub.29183-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Rahmati, Z., King, V. and Whitesides, S. (2015) Kinetic Reverse k-Nearest Neighbor Problem. Lecture Notes in Computer Science, 8986, 307-317.</mixed-citation></ref><ref id="hanspub.29183-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">张红艳, 李茵茵, 万伟. 改进K近邻和支持向量机相融合的天气识别[J]. 计算机工程与应用, 2014, 50(14): 148-151.</mixed-citation></ref><ref id="hanspub.29183-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">李振龙, 韩建龙, 赵晓华, 等. 基于K近邻和支持向量机的醉酒驾驶识别方法的对比分析[J]. 交通运输系统工程与信息, 2015, 15(5): 246-251.</mixed-citation></ref><ref id="hanspub.29183-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Jiao, L.M. and Pan, Q. (2015) Evidential Editing K-Nearest Neighbor Classifier. Lecture Notes in Computer Science, 9161, 461-471. &lt;br&gt;https://doi.org/10.1007/978-3-319-20807-7_42</mixed-citation></ref><ref id="hanspub.29183-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Zouhal, L.M. and Denoeux, T. (1998) An Evidence-Theoretic k-NN Rule with Parameter Optimization. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 28, 263-271. &lt;br&gt;https://doi.org/10.1109/5326.669565</mixed-citation></ref><ref id="hanspub.29183-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Emrich, T., Kriegel, H.-P., et al. (2015) On Reverse-K-Nearest-Neighbor Joins. GeoInformatica, 19, 299-330. &lt;br&gt;https://doi.org/10.1007/s10707-014-0215-5</mixed-citation></ref><ref id="hanspub.29183-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Dubey, H. and Pudi, V. (2013) Class Based Weighted K-Nearest Neighbor over Imbalance Dataset. Lecture Notes in Computer Science, 7819, 317-328.</mixed-citation></ref><ref id="hanspub.29183-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Xu, Y.T. and Wang, L.S. (2014) K-Nearest Neighbor-Based Weighted Twin Support Vector Regression. Applied Intelligence, 41, 299-309. &lt;br&gt;https://doi.org/10.1007/s10489-014-0518-0</mixed-citation></ref><ref id="hanspub.29183-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Lutu, P.E.N. (2011) Using Confusion Matrices and Confusion Graphs to De-sign Ensemble Classification Models from Large Datasets. Lecture Notes in Computer Science, 6862, 301-315. &lt;br&gt;https://doi.org/10.1007/978-3-642-23544-3_23</mixed-citation></ref><ref id="hanspub.29183-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Burduk, R. and Trajdos, P. (2013) Construction of Sequential Classifier Using Confusion Matrix. Lecture Notes in Computer Science, 8104, 408-419. &lt;br&gt;https://doi.org/10.1007/978-3-642-40925-7_37</mixed-citation></ref><ref id="hanspub.29183-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Jiang, N. and Liu, H.B. (2013) Understand System’s Relative Effectiveness Using Adapted Confusion Matrix. Lecture Notes in Computer Science, 8012, 303-311. &lt;br&gt;https://doi.org/10.1007/978-3-642-39229-0_32</mixed-citation></ref><ref id="hanspub.29183-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Reyes-Vargas, M., Sanchez-Gutierrez, M., Rufiner, L., et al. (2013) Hier-archical Clustering and Classification of Emotions in Human Speech Using Confusion Matrices. Lecture Notes in Computer Science, 8113, 170-180. &lt;br&gt;https://doi.org/10.1007/978-3-319-01931-4_22</mixed-citation></ref></ref-list></back></article>