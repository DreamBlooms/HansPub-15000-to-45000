<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.116166</article-id><article-id pub-id-type="publisher-id">CSA-42869</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210600000_68268023.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于注意力机制与双向LSTM的行为识别
  Action Recognition Based on Attention and Bi-LSTM
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>玉铭</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>吴</surname><given-names>克伟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>金</surname><given-names>依珂</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>周</surname><given-names>龙辉</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>合肥工业大学计算机与信息学院，安徽 合肥</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>02</day><month>06</month><year>2021</year></pub-date><volume>11</volume><issue>06</issue><fpage>1607</fpage><lpage>1616</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   采用光流作为运动特征进行行为识别需要预先计算并存储光流，需要巨大的计算成本和存储资源，并且由于光流特征主要表征了相邻帧之间的运动特征，导致行为识别中存在长依赖问题。针对这些问题，本文提出了一种新的运动特征建模方式以取代光流特征，并且提出了一种长依赖时序运动建模模块。实验结果表明，本文提出的方法在增加极低的计算成本的情况下，能更好的对远距离图像帧间的时序上下文信息建模，显著提高行为识别的准确度。 Using optical flow as motion features for action recognition requires pre-computation and storage of optical flow, which requires huge computational cost and storage resources. And optical flow features mainly characterize the motion features between adjacent frames, which leads to long-dependency problems in action recognition. To address these problems, this paper proposes a new way of modeling motion features to replace optical flow features and proposes a long-dependency temporal motion modeling module. Experimental results show that the proposed method in this paper can better model the temporal context information between long-range frames and significantly improve the accuracy of action recognition with very low increase in computational cost. 
  
 
</p></abstract><kwd-group><kwd>行为识别，光流，运动特征，长依赖问题，时序上下文信息, Action Recognition</kwd><kwd> Optical Flow</kwd><kwd> Motion Features</kwd><kwd> Long-Dependency Problems</kwd><kwd> Temporal Context Information</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>采用光流作为运动特征进行行为识别需要预先计算并存储光流，需要巨大的计算成本和存储资源，并且由于光流特征主要表征了相邻帧之间的运动特征，导致行为识别中存在长依赖问题。针对这些问题，本文提出了一种新的运动特征建模方式以取代光流特征，并且提出了一种长依赖时序运动建模模块。实验结果表明，本文提出的方法在增加极低的计算成本的情况下，能更好的对远距离图像帧间的时序上下文信息建模，显著提高行为识别的准确度。</p></sec><sec id="s2"><title>关键词</title><p>行为识别，光流，运动特征，长依赖问题，时序上下文信息</p></sec><sec id="s3"><title>Action Recognition Based on Attention and Bi-LSTM<sup> </sup></title><p>Yuming Zhang, Kewei Wu, Yike Jin, Longhui Zhou</p><p>School of Computer Science and Information Engineering, Hefei University of Technology, Hefei Anhui</p><p><img src="//html.hanspub.org/file/1-1542099x4_hanspub.png" /></p><p>Received: May 1<sup>st</sup>, 2021; accepted: May 26<sup>th</sup>, 2021; published: Jun. 2<sup>nd</sup>, 2021</p><p><img src="//html.hanspub.org/file/1-1542099x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Using optical flow as motion features for action recognition requires pre-computation and storage of optical flow, which requires huge computational cost and storage resources. And optical flow features mainly characterize the motion features between adjacent frames, which leads to long-dependency problems in action recognition. To address these problems, this paper proposes a new way of modeling motion features to replace optical flow features and proposes a long-dependency temporal motion modeling module. Experimental results show that the proposed method in this paper can better model the temporal context information between long-range frames and significantly improve the accuracy of action recognition with very low increase in computational cost.</p><p>Keywords:Action Recognition, Optical Flow, Motion Features, Long-Dependency Problems, Temporal Context Information</p><disp-formula id="hanspub.42869-formula3"><graphic xlink:href="//html.hanspub.org/file/1-1542099x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/1-1542099x7_hanspub.png" /> <img src="//html.hanspub.org/file/1-1542099x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>行为识别是计算机视觉领域非常有挑战性的课题，研究的是视频中目标的行为动作，在视频监控、人机交互等领域有着重要的应用。在行为识别任务中，不仅要分析图像中目标的空间信息，还要提取时间维度上的时序特征。近几年，针对行为识别的研究主要围绕双流法和3D卷积进行展开。双流法包括两个并行分支：RGB空间流与时序流。在时序流中，通常采用光流作为运动特征，这也导致双流法需要预先计算并存储光流。因此，双流法需要巨大的计算和存储资源。3D卷积在2D空间卷积的基础上增加时间维度，学习时序特征，维度的增加也导致了计算量的增长。此外，双流法与3D卷积只能对局部的时间段内的时序特征进行提取，对长时视频中的长依赖问题存在较大的局限性。</p><p>针对以上存在的问题，我们提出了两个模块：通道级运动特征编码模块(CME)与长依赖时序运动建模模块(LDTM)。CME模块采用帧差法的思想，对相邻帧之间的运动特征进行编码，取代光流特征。利用注意力机制在通道级对运动特征通道进行激励，对背景通道进行抑制。LDTM模块利用双向LSTM网络，在增加较低的计算成本代价下，对远距离图像帧之间进行时序上下文关系建模，对解决行为识别中的长依赖问题提供了一种解决思路。</p></sec><sec id="s6"><title>2. 相关工作</title><p>传统的行为识别方法一般是通过人工观察和设计，手动设计出能够表征动作特征的特征提取方法，比如方向梯度直方图(HOG)、光流直方图(HOF)、光流梯度直方图(MBH)、轨迹特征(Trajectories)以及人体骨骼特征等。在早期研究工作中，iDT (improved Dense Trajectories) [<xref ref-type="bibr" rid="hanspub.42869-ref1">1</xref>] 算法是性能最好的算法之一。近几年，随着深度学习的兴起与快速发展，越来越多基于深度学习的方法被提出应用于行为识别，例如比较经典的双流网络架构以及近几年兴起的3D卷积网络及其变种。</p><sec id="s6_1"><title>2.1. 双流网络架构</title><p>2014年，Karen Simonyan等学者提出了双流(Two-Stream) [<xref ref-type="bibr" rid="hanspub.42869-ref2">2</xref>] 卷积网络，利用帧图像和光流图像作为CNN的输入，将行为识别中的特征提取分为两个分支，一个是RGB图像分支提取空间特征，另一个是光流图像分支提取时间上的光流特征，最后结合两种特征进行行为识别。在此之后，许多基于双流架构的先进算法陆续被提出，比如TSN、TRN [<xref ref-type="bibr" rid="hanspub.42869-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.42869-ref4">4</xref>] 等。TSN解决了传统双流法无法解决的长视频问题，提出将长视频切割成等长的若干视频段，每个视频段随机选取一帧作为输入提取特征，最后再进行特征融合。双流法的性能超越了传统手工特征提取方法，但是由于双流法需要预先计算光流提取光流特征，因此带来巨大的计算成本和存储需求，于是许多学者开始探寻不需要计算光流的方法，典型的方法例如3D卷积。</p></sec><sec id="s6_2"><title>2.2. 3D卷积网络</title><p>C3D [<xref ref-type="bibr" rid="hanspub.42869-ref5">5</xref>] 作为3D卷积的早期尝试，将VGG16模型中的2D卷积全都换成3D卷积。C3D虽然极具开创性，但是并没取得令人满意的效果，I3D [<xref ref-type="bibr" rid="hanspub.42869-ref6">6</xref>] 提出将2D卷积训练权重在时间维度重复，用ImageNet预训练的权重初始化3D卷积，大幅提升了3D卷积网络的性能。3D卷积是2D卷积的扩展，虽然取得了较大的性能提升，证明了3D卷积网络建模时序特征的可行性，但是3D网络架构相比于2D卷积网络多了时间维度，因此也带来了巨大的计算量。为了降低3D卷积的计算量，许多工作对3D卷积操作进行改进以实现更少的参数量和更高的性能。P3D [<xref ref-type="bibr" rid="hanspub.42869-ref7">7</xref>] 提出将3 &#215; 3 &#215; 3的3D卷积拆分成1 &#215; 3 &#215; 3的2D空间卷积和3 &#215; 1 &#215; 1的1D时序卷积，前者建模空间特征，后者建模时序特征，并采用不同的排列方式和残差连接，设计出多种变形结构。ECO [<xref ref-type="bibr" rid="hanspub.42869-ref8">8</xref>] 在网络浅层采用2D卷积编码图像帧，在网络深层再用3D卷积对拼接起来的特征图进行编码。X3D [<xref ref-type="bibr" rid="hanspub.42869-ref9">9</xref>] 从2D卷积的架构延伸，选择某个需要扩张的维度并固定住其他维度，将2D卷积核从不同维度扩展成3D卷积核。此外，X3D采用了通道可分离卷积网络，从而得到一个轻量级的网络模型。</p><p>除了双流模型与3D卷积以外，也有一些其他的工作聚焦于解决行为识别中的计算资源巨大、长距离依赖建模困难等问题。Chao-YuanWu等学者发现多数研究直接使用视频作为模型输入，从而导致数据量过大，因此他们参考视频压缩的技术，对视频进行压缩 [<xref ref-type="bibr" rid="hanspub.42869-ref10">10</xref>]。为了避免光流的计算和储存，Hidden Two-Stream [<xref ref-type="bibr" rid="hanspub.42869-ref11">11</xref>] 让模型先产生类似光流特征，再参考双流网络架构模型，分开处理时序特征和RGB特征。TSM [<xref ref-type="bibr" rid="hanspub.42869-ref12">12</xref>] 提出在2D卷积中沿时间轴交替拼接特征通道，这个操作不需要任何额外计算成本和参数，解决了3D卷积网络计算成本高的问题，同时也保证了时序特征的连续性。</p><p>此外，一些学者提出用新的运动特征编码方式来取代光流特征，降低计算成本。Boyuan Jiang等人提出了一个STM [<xref ref-type="bibr" rid="hanspub.42869-ref13">13</xref>] 模块，它包含一个通道级的时空模块(CSTM)来呈现时空特征和一个通道级的运动模块(CMM)来有效地编码运动特征。Yan Li等人认为时序建模是行为识别的关键，通常考虑短程运动和远程聚集，提出了一个时序激发和聚集模块(TEA) [<xref ref-type="bibr" rid="hanspub.42869-ref14">14</xref>]，包括一个运动激发(ME)模块和一个多时序聚集(MTA)模块，专门用于捕获短距离和长距离的时序运动特征。除了取代光流特征，也有学者开始将研究方向投向更加轻量级的模型架构设计，例如采用2D + 1D [<xref ref-type="bibr" rid="hanspub.42869-ref15">15</xref>] 卷积、shifting或注意力机制等等。</p></sec><sec id="s6_3"><title>2.3. 长短期记忆网络</title><p>在行为识别任务中，输入的视频段可以看作是若干视频帧按一定时序进行排列得到，当我们的模型在学习某一帧视频帧特征信息时，我们需要模型将前面视频帧所学的特征信息与当前帧联系起来，即学习视频中的上下文信息。传统的神经网络无法做到这一点，循环神经网络(RNNs)解决了这一点，循环神经网络带有自循环结构，使得特征信息可以持久保存。但是当视频帧增多，视频时段较长时，我们需要学习更远距离的上下文，循环神经网络的效果就不是很明显了。长短期记忆网络(LSTM)便是针对视频理解、自然语言处理及语音识别等时序任务中存在的长依赖问题而提出的一种网络模型。长短期记忆网络是一种特殊的循环神经网络，由Hochreiter和Schmidhuber (1997)提出，它能够学习视频、语音或文本等数据中的长距离依赖关系。在随后的发展中，许多研究人员针对计算机视觉任务中出现的不同问题，对LSTM进行改进和推广，出现很多LSTM变种，例如双向LSTM、GRU等，它们在许多问题上起到很好的效果，得到了广泛使用。</p><p>在本文中，我们的主要贡献总结如下：</p><p>(1) 提出了一个基于注意力机制的通道级运动特征编码模块(CME)，学习相邻帧间的时序上下文，在小幅增加计算成本的情况下显著提高模型性能；</p><p>(2) 提出了一个长依赖时序运动建模模块(LDTM)，对行为识别中远距离帧间的上下文关系建模，有效降低了长依赖问题对行为识别的影响。</p></sec></sec><sec id="s7"><title>3. 方法论</title><sec id="s7_1"><title>3.1. 整体网络模型架构</title><p>在这一节中，我们提出了两个模块(通道级运动特征编码模块和长距离时序运动特征建模模块)来提取视频运动特征，其中，通道级运动特征编码模块提取视频段相邻帧间短时运动特征，长依赖时序运动建模模块(LDTM)提取长距离时序运动特征。</p><p>如图1，展示了我们的整体网络模型结构图。我们选用ResNet50作为卷积网络，并用CME Block替换ResNet50中每一个卷积层的Block，用以提取相邻帧间的运动特征；此外，我们在Conv Layer2、Conv Layer3及Conv Layer4后引出一个并行分支接入长依赖时序运动建模模块(LDTM)，并与每一个卷积层的输出进行融合，作为后续卷积或者全连接网络的输入。在接下来的小节中，我们将详细介绍我们提出的通道级运动特征编码模块(CME)以及长依赖时序运动建模模块(LDTM)。</p><p>图1. 整体网络模型结构图</p></sec><sec id="s7_2"><title>3.2. 通道级运动特征编码模块(CME)</title><p>在之前的大量工作中，通常是通过提取光流特征来作为运动特征进行行为识别，但是计算光流特征需要巨大的计算资源和存储资源，效率比较低。因此，我们借鉴了帧差法检测运动目标的思想，提出了一个通道级运动特征编码模块CME (Channel-wise Motion Encoding)来提取运动特征并激励运动信道，以取代光流特征。</p><p>如图2所示，在CME模块中，我们首先将输入的视频帧经过一个1 &#215; 1的2D空间卷积降低特征通道维度，假设放缩因子为r，输入维度为[N, T, C, H, W]，经过卷积后得到输出维度为[N, T, C/r, H, W]的特征图。将得到的特征图沿时间维度T拆分成单独的每一帧，然后将相邻帧进行差分，用后一帧图像特征 X t + 1 减去前一帧 X t 。由于视频中前景目标的运动，导致两帧之间目标位置发生变化，直接作差可能会导致特征混乱。因此，为了消除特征混淆，我们先对后一帧用3x3的2D卷积进行空间特征变换，然后再用得到的变换特征与前一帧做差，从而得到相邻两帧之间的运动表征。我们假设第t帧到第t+1帧的运动特征为 M t ，那么：</p><p>图2. 通道级运动特征编码模块(CME)结构图</p><p>M t = C o n v 3 &#215; 3 ( X t + 1 ) − X t ， M t ∈ R N &#215; C / r &#215; H &#215; W (1)</p><p>其中， X t 表示第t帧图像特征， X t + 1 为第t + 1帧图像特征， C o n v 3 &#215; 3 ( &#183; ) 表示3 &#215; 3的2D空间卷积。将得到的相邻帧间运动特征拼接，得到输入视频的运动特征表示为M，即：</p><p>M = C o n c a t ( M 1 , M 2 , ⋯ , M T ) ， M ∈ R N &#215; T &#215; C / r &#215; H &#215; W (2)</p><p>由于输入特征X经过了卷积通道降维，因此得到的视频运动特征M的维度为[N, T, C/r, H, W]。我们将得到的视频运动特征M输入两个并行分支，运动编码分支与运动信道激励分支。在运动编码分支，我们设计一个1 &#215; 1的2D卷积对运动特征M进行编码，并将通道维度从C/r恢复到C，确保特征信息不会损失，同时也便于与输入特征融合。假设运动编码后得到的特征为 M e n c o d e ，则有：</p><p>M e n c o d e = C o n v 1 &#215; 1 ( M ) ， M e n c o d e ∈ R N &#215; T &#215; C &#215; H &#215; W (3)</p><p>其中， C o n v 1 &#215; 1 ( &#183; ) 表示1 &#215; 1的2D卷积操作。运动编码分支得到编码后的相邻帧之间的运动特征，但是特征通道包含了前景目标的运动特征以及背景特征。因此，我们采用注意力机制，对拼接后得到运动特征学习其通道的注意力权重，对于运动特征通道进行激励，对背景特征通道进行抑制。具体地，考虑到通道特征权重与空间位置无关，我们先将运动特征进行空间上的全局平均池化，得到空间无关的运动特征，记为 M p o o l i n g ，即：</p><p>M p o o l i n g = P o o l i n g ( M ) ， M p o o l i n g ∈ R N &#215; T &#215; C / r &#215; 1 &#215; 1 (4)</p><p>其中 P o o l i n g ( &#183; ) 表示空间池化操作，得到的 M p o o l i n g 维度为[N, T, C/r, W, H]。接下来，将 M p o o l i n g 经过卷积升维，并输入到sigmoid激活函数，得到注意力权重矩阵记为A，即：</p><p>A = s i g m o i d ( C o n v 1 &#215; 1 ( M p o o l i n g ) ) ， A ∈ R N &#215; T &#215; C &#215; 1 &#215; 1 (5)</p><p>其中 s i g m o i d ( &#183; ) 表示sigmoid激活函数， C o n v 1 &#215; 1 ( &#183; ) 同上，表示1 &#215; 1的2D卷积操作。将得到的通道注意力权重A与运动编码特征 M e n c o d e 相乘，激励其中的运动特征通道，同时加入残差连接，保持原有输入特征不丢失，即：</p><p>Y = X + A ⋅ M e n c o d e ， Y ∈ R N &#215; T &#215; C &#215; H &#215; W (6)</p><p>其中，X为输入特征，Y为模块的最终输出特征。显然，X与Y具有相同的维度，即CME模块并不改变输入特征的输出维度，这允许我们可以将该模块嵌入模型的任何位置进行端对端的学习，在我们的实验中，我们将其嵌入ResNet50网络，并用以替换每一个卷积层Block。</p></sec><sec id="s7_3"><title>3.3. 长依赖时序运动建模(LDTM)</title><p>运动特征编码模块(CME)可以提取相邻帧之间的运动特征，激励运动特征通道，但对于时长较长的视频段时序运动特征提取存在局限性。因此，我们提出一个长依赖时序运动建模模块(Long-Dependency Temporal Motion)，如图3所示。</p><p>图3. 长依赖时序运动建模(LDTM)模块结构图</p><p>首先，我们对输入的视频帧序列通过一个1 &#215; 1的2D卷积对通道进行降维，缩放因子取r = 16，然后将得到的特征经过维度变形转换后，输入一个双向LSTM网络(Bi-LSTM)。假设输入的视频帧序列为 x ∈ R N &#215; T &#215; C &#215; H &#215; W ，经1 &#215; 1卷积后特征维度变为 N &#215; T &#215; C / 16 &#215; H &#215; W ，然后通过Reshape操作将特征维度转换成 N H W &#215; T &#215; C / 16 。我们将若将 N H W 看作一个新的Batchsize，则等价于将每一帧特征向量 x t ∈ R 1 &#215; C / r 输入双向LSTM网络。在图3所示的结构图中，我们详细展示了双向LSTM模块的结构。具体地，我们将每一帧 x t ∈ R 1 &#215; C / r 输入到一个前向LSTM网络和一个反向LSTM网络，将两者的输出特征进行融合得到每一帧的隐藏层输出为 y t ∈ R 1 &#215; C / r 。显然，双向LSTM网络并不改变输入特征的输出维度。因此，我们将得到的双向LSTM输出特征进行逆操作(Reshape和1 &#215; 1卷积)后，并可将特征维度恢复与输入特征保持一致。我们将LDTM模块的操作流程表达如下：</p><p>y = C o n v 1 &#215; 1 ( R s h ( B i L S T M ( R s h ( C o n v 1 &#215; 1 ( x ) ) ) ) ) , y ∈ R N &#215; T &#215; C &#215; H &#215; W (7)</p><p>其中， C o n v 1 &#215; 1 ( &#183; ) 表示1x1的空间卷积， R s h ( &#183; ) 表示Reshape操作， B i L S T M ( &#183; ) 表示双向LSTM的输出，x与y分别代表输入及输出特征。</p></sec></sec><sec id="s8"><title>4. 实验结果与分析</title><sec id="s8_1"><title>4.1. 实验设置及数据集</title><p>实验设置。我们的实验选用ResNet50作为我们的基础卷积网络，并用我们提出的模块对ResNet50中的网络构件进行替换。为了便于与其他模型进行比较，我们在Something-Something-v1数据集上进行了实验。实验模型用2块2080Ti显卡进行训练，共训练50个epoch。训练数据的Batchsize大小设定为16，初始学习率设定为0.005，并分别在第30，40以及45个epoch时将学习率缩小10倍。</p><p>数据集。Something-Something-v1数据集是一个大型的带标签的视频段集合，这些视频段主要包括了人们对日常物品进行预定义的一些基本动作。该数据集包含108499个视频，并分为174个类别，其中训练集有86017个，验证集有11522个，测试集有10960个。</p></sec><sec id="s8_2"><title>4.2. 与不同模型的性能对比</title><p>我们将我们的方法与当前最先进的一些行为识别模型在Something-Something-v1公开数据集上进行了实验对比。如表1，可以发现，在相同的实验参数设置条件下，比如相同的帧采样、Backbone网络以及预训练等，我们的方法相较于TSN、TSM、GST等优秀模型都有一定的性能提升。</p><table-wrap-group id="1"><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison with state-of-the-art models on Something-Something-v</title></caption><table-wrap id="1_1"><table><tbody><thead><tr><th align="center" valign="middle" >Method</th><th align="center" valign="middle" >Backbone</th><th align="center" valign="middle" >Frames &#215; Crops &#215; Clips</th><th align="center" valign="middle" >Flops</th><th align="center" valign="middle" >Param</th><th align="center" valign="middle" >Pre-train</th><th align="center" valign="middle" >Top-1 val (%)</th><th align="center" valign="middle" >Top-5 val (%)</th></tr></thead><tr><td align="center" valign="middle" >TSN</td><td align="center" valign="middle" >ResNet50</td><td align="center" valign="middle" >8 &#215; 1 &#215; 1</td><td align="center" valign="middle" >33G &#215; 1 &#215; 1</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >19.70</td><td align="center" valign="middle" >46.60</td></tr><tr><td align="center" valign="middle" >TRN</td><td align="center" valign="middle" >BNInception</td><td align="center" valign="middle" >8 &#215; 10 &#215; N/A</td><td align="center" valign="middle" >16G &#215; 10 &#215; N/A</td><td align="center" valign="middle" >18.3M</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >34.40</td><td align="center" valign="middle" >63.20</td></tr><tr><td align="center" valign="middle" >TRN Two-Stream</td><td align="center" valign="middle" >BNInception</td><td align="center" valign="middle" >(8 + 8) &#215; 10 &#215; N/A</td><td align="center" valign="middle" >32G &#215; 10 &#215; N/A</td><td align="center" valign="middle" >36.6M</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >42.00</td><td align="center" valign="middle" >/</td></tr><tr><td align="center" valign="middle" >I3D</td><td align="center" valign="middle" >3D ResNet50</td><td align="center" valign="middle" >32 &#215; 3 &#215; 2</td><td align="center" valign="middle" >153G &#215; 3 &#215; 2</td><td align="center" valign="middle" >28.0M</td><td align="center" valign="middle" >ImageNet + Kinetics400</td><td align="center" valign="middle" >41.60</td><td align="center" valign="middle" >72.20</td></tr><tr><td align="center" valign="middle" >NL-I3D</td><td align="center" valign="middle" >3D ResNet50</td><td align="center" valign="middle" >32 &#215; 3 &#215; 2</td><td align="center" valign="middle" >168G &#215; 3 &#215; 2</td><td align="center" valign="middle" >35.3M</td><td align="center" valign="middle" >ImageNet + Kinetics400</td><td align="center" valign="middle" >44.40</td><td align="center" valign="middle" >76.00</td></tr><tr><td align="center" valign="middle" >NL-I3D&amp;GCN</td><td align="center" valign="middle" >3D ResNet50</td><td align="center" valign="middle" >32 &#215; 3 &#215; 2</td><td align="center" valign="middle" >303G &#215; 3 &#215; 2</td><td align="center" valign="middle" >62.2M</td><td align="center" valign="middle" >ImageNet + Kinetics400</td><td align="center" valign="middle" >46.10</td><td align="center" valign="middle" >76.80</td></tr></tbody></table></table-wrap><table-wrap id="1_2"><table><tbody><thead><tr><th align="center" valign="middle" >TSM</th><th align="center" valign="middle" >ResNet50</th><th align="center" valign="middle" >8 &#215; 1 &#215; 1</th><th align="center" valign="middle" >33G &#215; 1 &#215; 1</th><th align="center" valign="middle" >24.3M</th><th align="center" valign="middle" >ImageNet</th><th align="center" valign="middle" >45.60</th><th align="center" valign="middle" >74.20</th></tr></thead><tr><td align="center" valign="middle" >TSM</td><td align="center" valign="middle" >ResNet50</td><td align="center" valign="middle" >16 &#215; 1 &#215; 1</td><td align="center" valign="middle" >65G &#215; 1 &#215; 1</td><td align="center" valign="middle" >24.3M</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >47.30</td><td align="center" valign="middle" >77.10</td></tr><tr><td align="center" valign="middle" >TSM&amp;En</td><td align="center" valign="middle" >ResNet50</td><td align="center" valign="middle" >(8+16) &#215; 1 &#215; 1</td><td align="center" valign="middle" >98G &#215; 1 &#215; 1</td><td align="center" valign="middle" >48.6M</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >49.70</td><td align="center" valign="middle" >78.50</td></tr><tr><td align="center" valign="middle" >STM</td><td align="center" valign="middle" >ResNet50</td><td align="center" valign="middle" >8 &#215; 3 &#215; 10</td><td align="center" valign="middle" >33G &#215; 3 &#215; 10</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >49.20</td><td align="center" valign="middle" >79.30</td></tr><tr><td align="center" valign="middle" >STM</td><td align="center" valign="middle" >ResNet50</td><td align="center" valign="middle" >16 &#215; 3 &#215; 10</td><td align="center" valign="middle" >67G &#215; 3 &#215; 10</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >50.70</td><td align="center" valign="middle" >80.40</td></tr><tr><td align="center" valign="middle" >GST</td><td align="center" valign="middle" >ResNet50</td><td align="center" valign="middle" >16 &#215; 1 &#215; 1</td><td align="center" valign="middle" >59G &#215; 1 &#215; 1</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >48.60</td><td align="center" valign="middle" >77.90</td></tr><tr><td align="center" valign="middle" >GSM</td><td align="center" valign="middle" >BNInception</td><td align="center" valign="middle" >16 &#215; 1 &#215; 1</td><td align="center" valign="middle" >33G &#215; 1 &#215; 1</td><td align="center" valign="middle" >N/A</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >49.56</td><td align="center" valign="middle" >/</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >我们的方法</td><td align="center" valign="middle" >ResNet50</td><td align="center" valign="middle" >8 &#215; 1 &#215; 1</td><td align="center" valign="middle" >33G &#215; 1 &#215; 1</td><td align="center" valign="middle" >24.3M</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >47.89</td><td align="center" valign="middle" >76.78</td></tr><tr><td align="center" valign="middle" >ResNet50</td><td align="center" valign="middle" >16 &#215; 1 &#215; 1</td><td align="center" valign="middle" >67G &#215; 1 &#215; 1</td><td align="center" valign="middle" >24.3M</td><td align="center" valign="middle" >ImageNet</td><td align="center" valign="middle" >49.74</td><td align="center" valign="middle" >79.03</td></tr></tbody></table></table-wrap></table-wrap-group><p>表1. 与当前先进的行为识别模型在Something-Something-v1数据集上进行对比</p><p>此外，我们也尝试了在ResNet50不同的层上加入LDTM模块，得到的对比实验数据如表2。由于ResNet50网络的浅层特征图尺寸较大，语义信息较少，我们考虑跳过第一层卷积，而从第二个Stage开始输入LDTM模块。我们发现，在第3个卷积层(一个卷积层表示ResNet50网络的一个Stage)后加入我们的LDTM模块，可以取得更好的效果。在较浅层或者更深层单独加入LDTM模块，由于浅层语义特征较少，而深层网络分辨率信息丢失，导致性能都有所下降。当然，在每一个Stage都加入LDTM，能取得最好的效果，但也不可避免带来更大的计算量。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison after adding LDTM modules to different layers of ResNet5</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Stage</th><th align="center" valign="middle" >Top-1 val (%)</th><th align="center" valign="middle" >Top-5 val (%)</th></tr></thead><tr><td align="center" valign="middle" >Conv2</td><td align="center" valign="middle" >42.6</td><td align="center" valign="middle" >71.8</td></tr><tr><td align="center" valign="middle" >Conv3</td><td align="center" valign="middle" >45.3</td><td align="center" valign="middle" >74.3</td></tr><tr><td align="center" valign="middle" >Conv4</td><td align="center" valign="middle" >44.6</td><td align="center" valign="middle" >73.4</td></tr><tr><td align="center" valign="middle" >Conv3 + Conv4</td><td align="center" valign="middle" >46.9</td><td align="center" valign="middle" >76.1</td></tr><tr><td align="center" valign="middle" >Conv2 + Conv3 + Conv4</td><td align="center" valign="middle" >47.4</td><td align="center" valign="middle" >76.2</td></tr></tbody></table></table-wrap><p>表2. 在ResNet50网络不同卷积层加入LDTM模块后的性能比较</p></sec><sec id="s8_3"><title>4.3. 可视化分析</title><p>我们将我们的方法与TSM模型在ResNet50骨干网络第五层卷积Conv5后得到的特征图可视化。对比可以发现，我们的方法可以在长距离帧间进行关联的上下文学习。如图4，在我们的方法中，第1帧与第4、5帧特征图中，人手在拉动物体时，前景目标区域的特征被分配了更大的权重和关注度，明显区别于背景目标。这也证明，我们的方法对长依赖时序运动特征建模是有效的。</p><p>图4. “拉某物的两端但什么都没发生”行为视频段在Conv5后的特征热图，其中(a)表示我们的方法，(b) 表示TSM模型</p></sec></sec><sec id="s9"><title>5. 结束语</title><p>在行为识别任务中，短时相邻帧间采用光流特征作为运动特征进行行为识别存在着计算成本高、效率低等问题，而长时视频由于长依赖问题、远距离帧间上下文关系无法有效建模导致行为识别效果很差。针对以上的问题，我们提出了两个模块：通道级运动特征编码模块(CME)与长依赖时序运动建模模块(LDTM)。CME模块在采用帧差法的思想，对相邻帧间的运动特征进行编码，并利用注意力机制在通道级对运动特征通道进行激励，对背景通道进行抑制，从而得到更加有效的短时运动特征。LDTM模块利用双向LSTM网络，对远距离的帧间进行时序上的上下文关系学习，对解决行为识别中的长依赖问题提供了一种解决思路。CME模块与LDTM模块均不改变输入特征的输出维度，这允许我们将模块嵌入到模型任何位置进行端对端的学习，在增加较低的计算成本代价下，显著增加了模型的识别性能。本文主要从短时运动特征编码与长时依赖问题两方面对行为识别进行思考，后续我们针对这两方面问题，将进一步优化提出的方法，以获得更好的性能。</p></sec><sec id="s10"><title>文章引用</title><p>张玉铭,吴克伟,金依珂,周龙辉. 基于注意力机制与双向LSTM的行为识别Action Recognition Based on Attention and Bi-LSTM[J]. 计算机科学与应用, 2021, 11(06): 1607-1616. https://doi.org/10.12677/CSA.2021.116166</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.42869-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Wang, H., et al. (2011) Action Recognition by Dense Trajectories. The 24th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2011, Colorado Springs, 20-25 June 2011, 3169-3176.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2011.5995407</mixed-citation></ref><ref id="hanspub.42869-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan, K. and Zisserman, A. (2014) Two-Stream Convolu-tional Networks for Action Recognition in Videos. 28th Annual Conference on Neural Information Processing Systems (NIPS 2014), Montreal, 8-13 December 2014, 568-576.</mixed-citation></ref><ref id="hanspub.42869-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Wang, L., et al. (2016) Temporal Segment Networks: To-wards Good Practices for Deep Action Recognition.</mixed-citation></ref><ref id="hanspub.42869-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Zhou, B., et al. (2018) Temporal Relational Reasoning in Videos.</mixed-citation></ref><ref id="hanspub.42869-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Tran, D., et al. (2015) Learning Spatiotemporal Features with 3D Convolutional Networks. 2015 IEEE In-ternational Conference on Computer Vision (ICCV), Santiago, 7-13 December 2015, 4489-4497.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2015.510</mixed-citation></ref><ref id="hanspub.42869-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Carreira, J. and Zisserman, A. (2017) Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 4724-4733.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.502</mixed-citation></ref><ref id="hanspub.42869-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Qiu, Z.F., et al. (2017) Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks. 2017 IEEE International Conference on Computer Vision (ICCV), Venice, 22-29 October 2017, 5534-5542.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2017.590</mixed-citation></ref><ref id="hanspub.42869-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Danelljan, M., et al. (2017) ECO: Efficient Convolution Operators for Tracking. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 6931-6939.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.733</mixed-citation></ref><ref id="hanspub.42869-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Feichtenhofer, C. (2020) X3D: Expanding Architectures for Efficient Video Recognition. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, 14-19 June 2020, 200-210.  
&lt;br&gt;https://doi.org/10.1109/CVPR42600.2020.00028</mixed-citation></ref><ref id="hanspub.42869-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Wu, C.-Y., et al. (2018) Compressed Video Action Recog-nition. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 6026-6035. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00631</mixed-citation></ref><ref id="hanspub.42869-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, Y., et al. (2018) Hidden Two-Stream Convolu-tional Networks for Action Recognition. 14th Asian Conference on Computer Vision, Perth, 2-6 December 2018, 363-378.</mixed-citation></ref><ref id="hanspub.42869-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Lin, J., et al. (2019) TSM: Temporal Shift Module for Efficient Video Understanding. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, 27-28 October 2019, 7082-7092.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2019.00718</mixed-citation></ref><ref id="hanspub.42869-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Jiang, B.Y., et al. (2019) STM: SpatioTemporal and Motion En-coding for Action Recognition. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, 27-28 October 2019, 2000-2009.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2019.00209</mixed-citation></ref><ref id="hanspub.42869-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Li, Y., et al. (2020) TEA: Temporal Excitation and Aggregation for Action Recognition. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, 14-19 June 2020, 906-915.  
&lt;br&gt;https://doi.org/10.1109/CVPR42600.2020.00099</mixed-citation></ref><ref id="hanspub.42869-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Tran, D., et al. (2018) A Closer Look at Spatiotemporal Convolutions for Action Recognition. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 6450-6459.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00675</mixed-citation></ref></ref-list></back></article>