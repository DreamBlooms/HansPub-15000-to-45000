<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.109162</article-id><article-id pub-id-type="publisher-id">CSA-37430</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200900000_67676549.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  用于无人机巡线的图像分类模型选择算法研究
  Research on Algorithm of Image Classification Model Selection for UAV Patrol
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>金</surname><given-names>明磊</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>明</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>赵</surname><given-names>文璇</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>国家海洋技术中心，天津</addr-line></aff><aff id="aff2"><addr-line>天津航天中为数据系统科技有限公司，天津</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>02</day><month>09</month><year>2020</year></pub-date><volume>10</volume><issue>09</issue><fpage>1541</fpage><lpage>1548</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   随机森林算法作为经典的分类算法，应用广泛，分类的准确度高。但在分类的过程之中，各个决策树的分类性能和两两决策树之间的差异性是影响最终分类效果的两个重要因素，当部分决策树有相似的错误分类情况，在最终利用决策树的结果进行投票时，将降低模型最终的分类效果。针对该问题，本文将误差矩阵引入分类树的相似性度量当中。该方法考虑了不同类别的树的数量、分类正确错误的情况，以便选出相似度弱的决策树，然后，剔除分类能力差的决策树，最终选择出分类能力强的分类器集合。实验结果显示，本文提出的方法在3类数据集中，平均分类正确率高于原算法，且稳定性更高。 As a classic classification algorithm, random forest algorithm is widely used and has high classification accuracy. However, in the process of classification, the classification performance of each decision tree and the difference between two decision trees are two important factors that affect the final classification effect. When some decision trees have similar misclassifications, and they are used in the final voting on the results of the decision tree, the final classification effect of the model will be reduced. Aiming at this problem, this paper proposes a method for measuring the similarity of decision trees based on confusion Matrix. This method takes into account the number of different categories of trees and the correct and incorrect classification, in order to select decision trees with weak similarity, and then remove the decision trees with poor classification results, and finally complete the model selection of random forest. Experimental results show that the method pro-posed in this paper has a higher average classification accuracy rate and higher stability in the three types of datasets. 
  
 
</p></abstract><kwd-group><kwd>集成分类器，随机森林，误差矩阵, Integrated Classifier</kwd><kwd> Random Forest</kwd><kwd> Confusion Matrix</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>随机森林算法作为经典的分类算法，应用广泛，分类的准确度高。但在分类的过程之中，各个决策树的分类性能和两两决策树之间的差异性是影响最终分类效果的两个重要因素，当部分决策树有相似的错误分类情况，在最终利用决策树的结果进行投票时，将降低模型最终的分类效果。针对该问题，本文将误差矩阵引入分类树的相似性度量当中。该方法考虑了不同类别的树的数量、分类正确错误的情况，以便选出相似度弱的决策树，然后，剔除分类能力差的决策树，最终选择出分类能力强的分类器集合。实验结果显示，本文提出的方法在3类数据集中，平均分类正确率高于原算法，且稳定性更高。</p></sec><sec id="s2"><title>关键词</title><p>集成分类器，随机森林，误差矩阵</p></sec><sec id="s3"><title>Research on Algorithm of Image Classification Model Selection for UAV Patrol</title><p>Minglei Jin<sup>1</sup>, Ming Li<sup>1</sup>, Wenxuan Zhao<sup>2</sup></p><p><sup>1</sup>Tianjin Zhongwei Aerospace Data System Technology Co. Ltd., Tianjin</p><p><sup>2</sup>National Ocean Technology Center, Tianjin</p><p><img src="//html.hanspub.org/file/1-1541853x4_hanspub.png" /></p><p>Received: Aug. 15<sup>th</sup>, 2020; accepted: Aug. 26<sup>th</sup>, 2020; published: Sep. 2<sup>nd</sup>, 2020</p><p><img src="//html.hanspub.org/file/1-1541853x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>As a classic classification algorithm, random forest algorithm is widely used and has high classification accuracy. However, in the process of classification, the classification performance of each decision tree and the difference between two decision trees are two important factors that affect the final classification effect. When some decision trees have similar misclassifications, and they are used in the final voting on the results of the decision tree, the final classification effect of the model will be reduced. Aiming at this problem, this paper proposes a method for measuring the similarity of decision trees based on confusion Matrix. This method takes into account the number of different categories of trees and the correct and incorrect classification, in order to select decision trees with weak similarity, and then remove the decision trees with poor classification results, and finally complete the model selection of random forest. Experimental results show that the method proposed in this paper has a higher average classification accuracy rate and higher stability in the three types of datasets.</p><p>Keywords:Integrated Classifier, Random Forest, Confusion Matrix</p><disp-formula id="hanspub.37430-formula3"><graphic xlink:href="//html.hanspub.org/file/1-1541853x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/1-1541853x7_hanspub.png" /> <img src="//html.hanspub.org/file/1-1541853x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>随着无人机电力线路巡检的发展，无人机拍摄了大量的图像信息，数据将增长非常快，目前已经发展到了几个PB的海量数据。为了便于处理数据，各种分类算法层出不穷。其中，随机森林算法是以分类集成思想为基础的回归模型。该算法被应用于气象分析 [<xref ref-type="bibr" rid="hanspub.37430-ref1">1</xref>]、医学 [<xref ref-type="bibr" rid="hanspub.37430-ref2">2</xref>]、大数据推荐 [<xref ref-type="bibr" rid="hanspub.37430-ref3">3</xref>] 等。由于随机森林良好的分类能力，也被用来进行数据的处理，并应用于分布式当中 [<xref ref-type="bibr" rid="hanspub.37430-ref4">4</xref>] - [<xref ref-type="bibr" rid="hanspub.37430-ref9">9</xref>]。该算法被进行多次改进。如通过将新的理论引入随机森林，得到算法效果的提升。文献 [<xref ref-type="bibr" rid="hanspub.37430-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.37430-ref11">11</xref>] 将随机森林算法与Hough Transform相结合，应用于目标检测，效果较好。文献 [<xref ref-type="bibr" rid="hanspub.37430-ref12">12</xref>] 把survival forests与随机森林相结合，提升了算法的性能。谢晓东 [<xref ref-type="bibr" rid="hanspub.37430-ref13">13</xref>] 等利用梯度提升算法森林模型进行了改进，提高了模型的分类准确性。魏正涛 [<xref ref-type="bibr" rid="hanspub.37430-ref14">14</xref>] 通过对抽样结果增加约束条件来改进重抽样方法，加强了算法的分算类能力。王诚 [<xref ref-type="bibr" rid="hanspub.37430-ref15">15</xref>] 等对随机森林算法中存在的面对特征纬度高且不平衡的数据时，算法性能低下的问题提出了改进的算法。该算法先对数据集的特征按照正负类分类能力赋予不同的权值，然后删除冗余的低权值的特征值，得到性能良好的特征子集构造随机森林。文献 [<xref ref-type="bibr" rid="hanspub.37430-ref16">16</xref>] 使用聚类的方法，将效果良好的分类器进行聚合；文献 [<xref ref-type="bibr" rid="hanspub.37430-ref17">17</xref>] 将基分类器进行划分，选择出效果良好的分类器。通过筛选出来的决策树进行最终的投票过程中，如果各个决策树的相似性过高，决策树的种类过于单一，那么最终的分类效果会变差。同时，现有的决策树选择策略带来的计算量比较复杂，分类效果欠佳。为了解决该问题，本文将误差矩阵引入随机森林算法中，删选出种类更多的决策树，提升了随机森林算法的分类能力。</p></sec><sec id="s6"><title>2. 集成学习</title>集成分类器的差异性度量<p>集成学习，即集成分类器，指的是通过构建若干分类器，然后用某种方法将这些分类器的分类结果结合起来进行学习，完成任务。</p><p>集成分类器有两种方式进行分类。第一种是选择不具有强依赖关系的分类器进行学习，该方法将分类器进行综合和分析较为困难。第二种是选择有依赖关系的分类器进行学习，对于大多数分类方法都倾向于该方法。对于总的分类器的错误率和单分类器错误率之间的关系 [<xref ref-type="bibr" rid="hanspub.37430-ref18">18</xref>]，如公式(1)所示。</p><p>E t o t a l = E b a y + 1 + ρ ( N − 1 ) N E e a c h (1)</p><p>其中， E t o t a l 及 E e a c h 分别代表总的错误率和单分类器的错误率， ρ 表示单分类器的错误相关性，N是总分类器的规模， E b a y 是基于已知分布和Bayes规律的分类错误率。</p><p>对于差异性的度量有两种方法。第一种是非成对差异性度量，即直接计算集成系统的差异性值。第二种是成对差异性度量，通过计算每一对分类器的差异性值，然后用平均值衡量总的差异值。常用的方式 [<xref ref-type="bibr" rid="hanspub.37430-ref19">19</xref>] [<xref ref-type="bibr" rid="hanspub.37430-ref20">20</xref>] 有分歧度量，双误度量、评判间一致度 [<xref ref-type="bibr" rid="hanspub.37430-ref21">21</xref>] 等。本文采用的就是第二种方法。</p></sec><sec id="s7"><title>3. 随机森林算法</title><p>随机森林算法具体的步骤如下所示。</p><p>1) 从训练集中采用bootstrap法，即自助抽样法，有放回地抽取若干样本，作为一个训练子集。</p><p>2) 对于训练子集，从特征集中无放回地随机抽取若干特征，作为决策树的每个节点的分裂的依据。</p><p>3) 重复步骤1)和步骤2)，得到若干训练子集，并生产若干决策树，将决策树组合起来，形成随机森林。</p><p>4) 将测试集的样本输入随机森林中，让每个决策树对样本进行决策，得到结果后，采用投票方法对结果投票，得到样本的分类结果。</p><p>5) 重复步骤4)，直到测试集分类完成。</p></sec><sec id="s8"><title>4. 随机森林模型选择</title><sec id="s8_1"><title>4.1. 本文提出的模型选择方法</title><p>本文提出一种基于误差矩阵的随机森林改进方法，主要思想在于将误差矩阵引入决策树的相似性度量方法中，以删选出合适的分类树。随机森林算法的总流程如图1所示，本文重点是利用误差矩阵对决策树就进行删选。</p><sec id="s8_1_1"><title>4.1.1. 基于误差矩阵判断分类树相似性</title><p>判断两棵分类树相似度的方法通常主要有两类，第一种是根据两棵树之间的结构的差异性来判断相似性，常用的方法是利用一些算法 [<xref ref-type="bibr" rid="hanspub.37430-ref22">22</xref>] 直接判断结构的差异性，通常该方法的计算量较大。第二种是通过分类树的分类结果来判断相似性。如果两棵树的分类准确率相近则认为是相似的，或者两棵树对样本集进行分类时，将其分在同一类，也认为是相似的。本文采用的是第二种方法，即将分类结果中类别之间的关系进行分析，在分类树的误差矩阵上进行相似性的判别。</p><p>误差矩阵常作为分类结果的可视化工具 [<xref ref-type="bibr" rid="hanspub.37430-ref16">16</xref>]，在监督学习中应用广泛。对于误差矩阵，每一列代表数据的预测类别，每一列的总数代表预测为该类别的样本的数目，每一行代表数据的真实归属类别。每一行的总数表示该类别的数目。 X = { x 1 , x 2 , ⋯ , x N } 表示N个样本数据， Y = { y 1 , y 2 , ⋯ , y M } 表示M种分类的类别，通过矩阵 C N 表示N个样本数据在分类之后的结果，如公式(2)所示。</p><p>C N = [ c n 11 ⋯ c n 1 m ⋮ c n i i ⋮ c n m 1 ... c n m m ] (2)</p><p>式中 c n i j 表示样本数据X中真实类别为i的数据被分为类别j的数据的总数量。显然 c n i i 表示的是类别为i的数据被正确分类的数量。</p><p>本文使用矩阵的距离测度和向量夹角作为两棵树的相似性度量。当分类树类似时，则矩阵就相接近。同理，当矩阵距离较远，则分类树的差距就较大。</p><p>差值矩阵 D C N ( i , j ) 是两个误差矩阵 C N ( i ) 和 C N ( j ) 之差( i , j 代表两棵决策树)。 D C N ( i , j ) 的大小是 M &#215; M ，如公式(3)所示。</p><p>D C N ( i , j ) = C N ( i ) − C N ( j ) = [ c n 11 ( i ) − c n 11 ( j ) c n 12 ( i ) − c n 12 ( j ) ⋯ c n 1 N ( i ) − c n 1 M ( j ) c n 21 ( i ) − c n 21 ( j ) c n 22 ( i ) − c n 22 ( j ) ⋯ c n 2 N ( i ) − c n 2 M ( j ) ⋮ ⋮ ⋱ ⋮ c n M 1 ( i ) − c n M 1 ( j ) c n M 2 ( i ) − c n M 2 ( j ) ⋯ c n M M ( i ) − c n M M ( j ) ] (3)</p><p>当不同类别的样本的数量有较大的差距时，数量大的类别会影响到矩阵距离的计算，最终使随机森林分类器更多地趋于多值类别。因此，考虑到这个因素，本文使对差值矩阵 D C N ( i , j ) 进行归一化处理，得到矩阵 D C N u ( i , j ) ，其元素为 d c n ′ m n ，具体的计算如公式(4)、(5)所示。</p><p>d c n ′ m n = d c n m n m a x m (4)</p><p>m a x m = m a x n ( d c n m n ) (5)</p><p>其中 m a x m 表示差值矩阵第m行的最大值。</p><p>定义规模为l的随机森林的相似性度量矩阵为 R F ， R F 的大小和树的数量有关，是<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/1-1541853x35_hanspub.png" xlink:type="simple"/></inline-formula>的方阵。其元素 r f i j 与归一化差值矩阵 D C N u ( i , j ) , i , j = 1 , ⋯ , l 的关系如公式(6)所示。</p><p>r f i j = { 0 ,     ( i ≥ j ) ‖ D C N ′ i , j ‖ F = ∑ m M ∑ n M d c n ′ m n 2 ,     ( i &lt; j ) (6)</p><p>当 r f i j 越小，则树i与树j的相似度越高，两个分类器对样本的分类结果越接近。</p><p>图1. 随机森林选择模型</p></sec><sec id="s8_1_2"><title>4.1.2. 基于“删劣”策略选择模型</title><p>对于常用选择策略，采用的是选优策略，即从分类器之中选择出若干代表性强的分类器。这种方法对分类器的分类效果和分类器之间的关联性都有要求，是多对多的关系，因此用该方法进行选择较为复杂 [<xref ref-type="bibr" rid="hanspub.37430-ref23">23</xref>]。本文选择另一种方法，即删除效果不佳的分类器。从基础分类器之中将相关度高的，分类能力差的分类器删除，然后将剩下的分类器集中在一起，组成新的模型。</p><p>这种删除策略只需要应用于相关度高的分类器之间，当相关度超过所设的阈值时，就将其剔除，因而该方法计算量将大大降低计算量。同时该方法还降低了总体分类器之间的相关度，从而提升分类能力。</p></sec><sec id="s8_1_3"><title>4.1.3. 模型选择算法描述</title><p>具体步骤如算法1所示。</p><p>算法1随机森林模型选择算法</p><p>输入：决策树相似度阈值t、分类准确度阈值 β</p><p>输出：随机森林模型RF</p><p>1：通过决策树对测试样本进行分类预测；</p><p>2：根据分类结果，为决策树创建误差矩阵 C N</p><p>3：创建相似度度量矩阵 R F ：</p><p>4：for (( i , j = 1 to l) &amp; ( i &lt; j ))</p><p>利用公式(3)计算 D C N ( i , j )</p><p>利用公式(4)和公式(5)计算 D C N u ( i , j )</p><p>利用公式(6)计算 D C N u ( i , j ) 的范数，即 R F 在该处的元素值</p><p>5：令 m i j 为 R F 中最小的非零元素</p><p>6：for ( m i j &lt; t )</p><p>if (决策树i分类效果 &lt; β )</p><p>将 R F 中的树i清除</p><p>m i j = R F 中下一个最小非零元素</p><p>7：否则结束，未删除的决策树组成随机森林RF。</p></sec></sec></sec><sec id="s9"><title>5. 实验与分析</title><sec id="s9_1"><title>5.1. 实验数据说明</title><p>本文使用的数据集是UCI机器学习数据集的部分数据，具体信息如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Experimental data set taken from UC</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >UCI数据集</th><th align="center" valign="middle" >属性类别</th><th align="center" valign="middle" >特征维数</th><th align="center" valign="middle" >样本个数</th><th align="center" valign="middle" >类别个数</th></tr></thead><tr><td align="center" valign="middle" >Iris</td><td align="center" valign="middle" >连续</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >150</td><td align="center" valign="middle" >3</td></tr><tr><td align="center" valign="middle" >Breast-cancer</td><td align="center" valign="middle" >离散</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >286</td><td align="center" valign="middle" >2</td></tr><tr><td align="center" valign="middle" >Anneal</td><td align="center" valign="middle" >离散连续</td><td align="center" valign="middle" >38</td><td align="center" valign="middle" >798</td><td align="center" valign="middle" >8</td></tr></tbody></table></table-wrap><p>表1. 取自UCI的实验数据集</p></sec><sec id="s9_2"><title>5.2. 实验结果与分析</title><p>本实验通过从10到100内，不同的十个随机森林规模，然后对原始的随机森林(RF)和基于误差矩阵的随机森林(CM-RF)的分类结果进行对比采用的指标为平均分类准确率。实验结果如图2~4示。</p><p>由实验结果看出，对于iris和anneal数据集，在初始树规模不同的情况下，基于误差矩阵的随机森林的平均分类准确率均高于传统的随机森林模型。对于glass数据集，随着随机森林在建数目的增加，传统的随机森林算法和本文提出的模型都出现了分类准确率下降的情况，但是，基于随机矩阵的森林的分类准确率下降更加缓慢，从而本文算法保持了一定的鲁棒性。因此，进一步说明本文提出的基于误差矩阵的随机森林模型的有效性。</p><p>图2. Iris数据集上Accuracy对比结果</p><p>图3. Breast-cancer数据集上Accuracy对比结果</p><p>图4. Aneal数据集上Accuracy对比结果</p></sec></sec><sec id="s10"><title>6. 结束语</title><p>本文提出了一种基于误差矩阵的随机森林分类模型选择方法。将误差矩阵应用于决策树的相似性度量中，通过使用矩阵的距离测度和向量夹角判断两棵树的相似性，考虑到树的数量占比问题，对矩阵每行进行归一化处理，之后结合决策树的分类性能，采用“删劣”思想完成随机森林模型的选择。由实验结果可知，该方法提高了分类的准确度。</p></sec><sec id="s11"><title>基金项目</title><p>赛尔网络下一代互联网技术创新项目(NGII20170104)。</p></sec><sec id="s12"><title>文章引用</title><p>金明磊,李 明,赵文璇. 用于无人机巡线的图像分类模型选择算法研究Research on Algorithm of Image Classification Model Selection for UAV Patrol[J]. 计算机科学与应用, 2020, 10(09): 1541-1548. https://doi.org/10.12677/CSA.2020.109162</p></sec><sec id="s13"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.37430-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Babar, B., Luppino, L.T., Boström, T. and Anfinsen, S.N. (2020) Random Forest Regression for Improved Mapping of Solar Irradiance at High Latitudes. Solar Energy, 198, 81-92. &lt;br&gt;https://doi.org/10.1016/j.solener.2020.01.034</mixed-citation></ref><ref id="hanspub.37430-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Li, J., Tian, Y., Zhu, Y., Zhou, T.S., Li, J., Ding, K.F. and Li, J.S. (2020) A Multicenter Random Forest Model for Effective Prognosis Prediction in Collaborative Clinical Research Network. Artificial Intelligence in Medicine, 103, Article ID: 101814. &lt;br&gt;https://doi.org/10.1016/j.artmed.2020.101814</mixed-citation></ref><ref id="hanspub.37430-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Hammou, B.A., Lahcen, A.A. and Mouline, S. (2019) An Effective Distributed Predictive Model with Matrix Factorization and Random Forest for Big Data Recommendation Systems. Expert Systems with Applications, 137, 253-265. 
&lt;br&gt;https://doi.org/10.1016/j.eswa.2019.06.046</mixed-citation></ref><ref id="hanspub.37430-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Çifçi, M.A., Ertugrul, D.Ç. and Elçi, A. (2016) A Search Service for Food Consumption Mobile Applications via Hadoop and MapReduce Technology. 2016 IEEE 40th Annual Comput-er Software and Applications Conference (COMPSAC), Atlanta, 10-14 June 2016, 77-82. &lt;br&gt;https://doi.org/10.1109/COMPSAC.2016.35</mixed-citation></ref><ref id="hanspub.37430-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">刘迎春, 陈梅玲. 流式大数据下随机森林方法及应用[J]. 西北工业大学学报, 2015, 33(6): 1055-1061.</mixed-citation></ref><ref id="hanspub.37430-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Yousfi, S. and Chiadmi, D. (2015) Big Data-as-a-Service Solution for Building Graph Social Networks. 2015 International Conference on Cloud Technologies and Applications (CloudTech), Marrakech, 2-4 June 2015, 1-6. 
&lt;br&gt;https://doi.org/10.1109/CloudTech.2015.7337009</mixed-citation></ref><ref id="hanspub.37430-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">韩伟, 张学庆, 陈旸. 基于MapReduce的图像分类方法[J]. 计算机应用, 2014, 34(6): 1600-1603.</mixed-citation></ref><ref id="hanspub.37430-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Rajagopalan, M.R. and Vellaipandiyan, S. (2013) Big Data Framework for National E-Governance Plan. 2013 11th International Conference on ICT and Knowledge Engineering (ICT&amp;KE), Bangkok, 20-22 November 2013, 1-5. 
&lt;br&gt;https://doi.org/10.1109/ICTKE.2013.6756283</mixed-citation></ref><ref id="hanspub.37430-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">孙悦, 袁健. 基于Spark的改进随机森林算法[J]. 电子科技, 2019, 32(4): 60-63+67.</mixed-citation></ref><ref id="hanspub.37430-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Gall, J., Yao, A., Razavi, N., Cool, L.V. and Lempitsky, V. (2011) Hough Forests for Ob-ject Detection, Tracking, and Action Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33, 2188-2202. 
&lt;br&gt;https://doi.org/10.1109/TPAMI.2011.70</mixed-citation></ref><ref id="hanspub.37430-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Gall, J. and Lempitsky, V. (2009) Class-Specific Hough Forests for Object Detection. 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Miami, 20-25 June 2009, 1022-1029. 
&lt;br&gt;https://doi.org/10.1109/CVPR.2009.5206740</mixed-citation></ref><ref id="hanspub.37430-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Ishwaran, H., Kogalur, U.B., Xi C. and Minn, A.J. (2011) Random Survival Forests for High-Dimensional Data. Statistical Analysis and Data Mining, 4, 115-132. &lt;br&gt;https://doi.org/10.1002/sam.10103</mixed-citation></ref><ref id="hanspub.37430-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">谢晓龙, 叶笑冬, 董亚明. 梯度提升随机森林模型及其在日前出清电价预测中的应用[J]. 计算机应用与软件, 2018, 35(9): 327-333.</mixed-citation></ref><ref id="hanspub.37430-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">魏正韬. 基于非平衡数据的随机森林算法研究[D]: [硕士学位论文]. 西安: 西安电子科技大学, 2017.</mixed-citation></ref><ref id="hanspub.37430-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">王诚, 高蕊. 基于特征约减的随机森林改进算法研究[J/OL]. 计算机技术展, 2020, 30(3): 40-45.</mixed-citation></ref><ref id="hanspub.37430-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">雍凯. 随机森林的特征选择和模型优化算法研究[D]: [硕士学位论文]. 哈尔滨: 哈尔滨工业大学, 2008.</mixed-citation></ref><ref id="hanspub.37430-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">毕凯, 王晓丹, 姚旭, 等. 一种基于Bagging和混淆矩阵的自适应选择性集成[J]. 电子学报, 2014(4): 711-716.</mixed-citation></ref><ref id="hanspub.37430-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Tumer, K. and Ghosh, J. (1996) Error Correlation and Error Reduc-tion in Ensemble Classifiers. Connection Science, 8, 385-340. &lt;br&gt;https://doi.org/10.1080/095400996116839</mixed-citation></ref><ref id="hanspub.37430-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Faria, F.A., Dos Santos, J.A., Sarkar, S., et al. (2013) Classifier Selection Based on the Correlation of Diversity Measures: When Fewer Is More. 2013 XXVI Conference on Graphics, Patterns and Images, Arequipa, 5-8 August 2013, 16-23. &lt;br&gt;https://doi.org/10.1109/SIBGRAPI.2013.12</mixed-citation></ref><ref id="hanspub.37430-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Shi, H.L., Ferguson, D., Beagley, J. and Huyck, M. (2008) Work in Progress—Improving Interrater Agreement Used to Measure Learning Outcomes. 2008 38th Annual Frontiers in Ed-ucation Conference, Saratoga Springs, 22-25 October 2008, F2B-7-F2B-8. &lt;br&gt;https://doi.org/10.1109/FIE.2008.4720398</mixed-citation></ref><ref id="hanspub.37430-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Löfström, T., Johansson, U. and Boström, H. (2008) On the Use of Accuracy and Diversity Measures for Evaluating and Selecting Ensembles of Classifiers. 2008 7th International Con-ference on Machine Learning and Applications (ICLMLA), San Diego, 11-13 December 2008, 127-132. &lt;br&gt;https://doi.org/10.1109/ICMLA.2008.102</mixed-citation></ref><ref id="hanspub.37430-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">乔少杰, 唐常杰, 陈瑜, 等. 基于树编辑距离的层次聚类算法[J]. 计算机科学与探索, 2007, 1(3): 282-292.</mixed-citation></ref><ref id="hanspub.37430-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">谢元澄, 杨静宇. 删除最差基学习器来层次修剪Bagging集成[J]. 计算机研究与发展, 2009, 46(2): 261-267.</mixed-citation></ref></ref-list></back></article>