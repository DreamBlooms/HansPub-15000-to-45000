<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.1010201</article-id><article-id pub-id-type="publisher-id">CSA-38444</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20201000000_45412438.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于GAN的超分辨率重建算法研究
  A Study of Super Resolution Algorithm Basing on Generative Adversarial Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>吴</surname><given-names>熹鹏</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>黄</surname><given-names>溪</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>先兵</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>何</surname><given-names>涛</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>吴</surname><given-names>中鼎</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>柴</surname><given-names>婉秋</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>中国科学院计算技术研究所，北京</addr-line></aff><aff id="aff2"><addr-line>武汉大学国家网络安全学院，湖北 武汉</addr-line></aff><aff id="aff4"><addr-line>贵阳铝镁设计研究院有限公司，贵州 贵阳</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>10</day><month>10</month><year>2020</year></pub-date><volume>10</volume><issue>10</issue><fpage>1908</fpage><lpage>1920</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   图像超分辨率重建是图像复原的重要分支之一，算法任务是进行图像或者视频的修复或重建，目标是提高原始图像或视频帧的分辨率，目前很多超分辨率重建算法使用生成对抗模型来提升分辨率。本文拟采用密集型卷积网络对生成对抗模型的生成器部分进行改进，并提供一个将高层次特征与低层次特征联合起来的方法，将每一层的特征图传播到后续卷积层，从而在模型不同卷积层之间建立一条捷径，使得梯度信息在层次较深的网络模块中实现回传。然后在此基础上采用生成对抗网络生成高频信息，用来补充图像或视频帧细节，获得更加真实的高分辨率图像，达到提升重建结果的目标。 Image super-resolution is one of the most important branches of image restoration, which is used for restoring or reconstructing images or videos, thus enhancing the resolution of the original images or frames of video. At present, most image super-resolution algorithms are using the generative adversarial network. In this paper, DenseNet is used to improve the generator part of the GAN model, and a method of combining high-level features and low-level features is provided to spread the feature map of each layer to the subsequent convolution layer, so as to establish a shortcut be-tween different convolution layers of the model, making gradient information in the deeper net-work module propagate back. On this basis, the high-frequency information is generated by the GAN, which is used to supplement the image or video frame details, and obtain more real high-resolution images, thus improving the reconstruction results. 
  
 
</p></abstract><kwd-group><kwd>图像超分辨率，生成对抗模型，密集卷积网络，深度学习, Super-Resolution</kwd><kwd> Generative Adversarial Network</kwd><kwd> DenseNet</kwd><kwd> Deep Learning</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>图像超分辨率重建是图像复原的重要分支之一，算法任务是进行图像或者视频的修复或重建，目标是提高原始图像或视频帧的分辨率，目前很多超分辨率重建算法使用生成对抗模型来提升分辨率。本文拟采用密集型卷积网络对生成对抗模型的生成器部分进行改进，并提供一个将高层次特征与低层次特征联合起来的方法，将每一层的特征图传播到后续卷积层，从而在模型不同卷积层之间建立一条捷径，使得梯度信息在层次较深的网络模块中实现回传。然后在此基础上采用生成对抗网络生成高频信息，用来补充图像或视频帧细节，获得更加真实的高分辨率图像，达到提升重建结果的目标。</p></sec><sec id="s2"><title>关键词</title><p>图像超分辨率，生成对抗模型，密集卷积网络，深度学习</p></sec><sec id="s3"><title>A Study of Super Resolution Algorithm Basing on Generative Adversarial Network<sup> </sup></title><p>Xipeng Wu<sup>1*</sup>, Xi Huang<sup>1</sup>, Xianbing Wang<sup>1#</sup>, Tao He<sup>2</sup>, Zhongding Wu<sup>3</sup>, Wanqiu Chai<sup>3</sup></p><p><sup>1</sup>School of Cyber Science and Engineering, Wuhan University, Wuhan Hubei</p><p><sup>2</sup>Institute of Computing Technology, Chinese Academy of Sciences, Beijing</p><p><sup>3</sup>Guiyang Aluminum Magnesium Design and Research Institute Co., Ltd., Guiyang Guizhou</p><p><img src="//html.hanspub.org/file/22-1541865x5_hanspub.png" /></p><p>Received: Oct. 8<sup>th</sup>, 2020; accepted: Oct. 23<sup>rd</sup>, 2020; published: Oct. 30<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/22-1541865x6_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Image super-resolution is one of the most important branches of image restoration, which is used for restoring or reconstructing images or videos, thus enhancing the resolution of the original images or frames of video. At present, most image super-resolution algorithms are using the generative adversarial network. In this paper, DenseNet is used to improve the generator part of the GAN model, and a method of combining high-level features and low-level features is provided to spread the feature map of each layer to the subsequent convolution layer, so as to establish a shortcut between different convolution layers of the model, making gradient information in the deeper network module propagate back. On this basis, the high-frequency information is generated by the GAN, which is used to supplement the image or video frame details, and obtain more real high-resolution images, thus improving the reconstruction results.</p><p>Keywords:Super-Resolution, Generative Adversarial Network, DenseNet, Deep Learning</p><disp-formula id="hanspub.38444-formula29"><graphic xlink:href="//html.hanspub.org/file/22-1541865x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/22-1541865x8_hanspub.png" /> <img src="//html.hanspub.org/file/22-1541865x9_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>图像超分辨率(Super-resolution, SR)重建是图像复原的重要分支之一，通过算法对图像或视频帧进行修复，提升原始图像或视频帧的分辨率。目前，通过对单张低分辨率图像进行重建生成单张高分辨率图像的方法大致可以分为三类：基于插值的方法、基于重建的方法、基于学习的方法。</p><p>基于插值 [<xref ref-type="bibr" rid="hanspub.38444-ref1">1</xref>] 的方法。插值是比较容易想到也是最先用来进行图像重建的方法。该方法将输入图像信息作为算法的先验知识，算法可以得到较满意的重建效果。不过，因简单插值方法有限的表达能力，很容易在分辨率提升过程中产生振荡，并可能会产生锯齿状纹理，导致最终的重建图像比较模糊。</p><p>基于重建 [<xref ref-type="bibr" rid="hanspub.38444-ref2">2</xref>] 的方法。该方法则采取下采样，并强制约束平滑，使得原图像和重建图像一致。该方法有一个明显的缺陷，当放大倍数过大时，很多重要的高频细节可能会丢失，导致重建图像质量大幅度下降。</p><p>基于学习 [<xref ref-type="bibr" rid="hanspub.38444-ref3">3</xref>] 的方法。该方法是目前的研究热点，通过深度学习网络来解决上述两种方法产生的问题。该方法对训练样本中的低分辨率图像和相对的高分辨率图像进行字典学习 [<xref ref-type="bibr" rid="hanspub.38444-ref4">4</xref>]，得到图像之间统计学意义上的关联，然后利用关联来提升低分辨率图像的分辨率，最终得到高清重建图像。</p><p>SRGAN [<xref ref-type="bibr" rid="hanspub.38444-ref5">5</xref>] 将生成对抗网络(GAN)用于超分辨率SR重建问题，采用生成器重建低频图像内容，通过GANs补全图像高频细节内容，从而解决传统MSE损失函数缺少高频信息难题。虽然SRGAN的重建图像质量超过其他深度学习方法，但SRGAN更注重于感知相似性，即从图像风格上相似，而不是像素相似性，在细节上可能会出现某些不一致的情况。</p><p>本文基于密集网络和残差网络相结合的密集残差单元来改进SRGAN中的生成器结构，可以在使用更深的网络层次进行重建图像低频信息时，不会导致梯度消失，在此基础上优化目标函数，从而提高训练效率。训练好的模型在Set5、Set14、Urban100、BSD100等公开数据集中进行了测试，实验结果表明本文的改进方法有效，具有较明显提升。</p></sec><sec id="s6"><title>2. 相关理论</title><sec id="s6_1"><title>2.1. 生成对抗网络GAN</title><p>GAN是一种生成式神经网络 [<xref ref-type="bibr" rid="hanspub.38444-ref6">6</xref>]，它把训练过程比作两个参与者竞争的过程。其中，生成器通过学习与训练数据生成新样本；新样本则由判别器来判断是真实样本还是假样本。在此基础上不断进行改进和完善。</p></sec><sec id="s6_2"><title>2.2. 残差网络ResNet</title><p>残差网络(ResNet) [<xref ref-type="bibr" rid="hanspub.38444-ref7">7</xref>] 的提出是为了解决深度神经网络的梯度弥散问题。残差块通过在网络中加入短路连接实现梯度回传，如图1所示，假定想要的隐层输出为H(x)，通过加入一个短路连接identity mapping，将经过卷积层的输出F(x)，与输入x进行求和，即得到输出H(x) = F(x) + x。</p><p>图1. 残差结构</p><p>加入identity mapping之后，即使在深层网络，梯度也可以通过identity mapping回传，不像以前的网络通过单纯的权值连乘进行梯度回传，从而避免反向传播导致的梯度弥散。也可以理解为残差网络在做反向传播时通过短路缩短了所经过的网络层数。</p></sec><sec id="s6_3"><title>2.3. SRGAN</title><p>SRGAN将ResNet应用于超分辨率重建，生成器SRResNet中每个残差块中都由两个3 &#215; 3大小的卷积核以及64个卷积核的卷积层组成，每个卷积层之后接一个批规范化层，并使用PReLU作为激活函数。</p><p>SRGAN提出了感知损失用于解决重建结果感知质量不佳问题，采用基于预训练好的VGG19网络VGG loss。由于SRGAN使用GAN进行高分辨率重建，添加了一个判别器，因此不仅需要考虑内容损失，还需考虑对抗损失。</p></sec><sec id="s6_4"><title>2.4. 密集卷积网络DenseNet</title><p>如图2所示，密集卷积网络 [<xref ref-type="bibr" rid="hanspub.38444-ref8">8</xref>] 是一种更加简便的连接模式，通过连接具有相同大小的特征图，确保网络各层间信息流通。每一层卷积层都会接受之前所有卷积层的输出，同时会把自身层的输出传递给之后所有卷积层。与残差网络通过相加将特征整合不同，密集卷积网络则是通过堆叠相同大小的特征图。</p><p>由于密集网络要堆叠整个网络的特征图，而且要保存卷积层输出作为后续输入，资源消耗巨大。因此密集思想仅在小个块(DenseBlock)中使用，并在网络中减少特征图数量。</p><p>图2. 密集卷积网络结构</p></sec><sec id="s6_5"><title>2.5. 残差密集块</title><p>残差密集块 [<xref ref-type="bibr" rid="hanspub.38444-ref9">9</xref>] (Residual dense block, RDB)吸收DenseNet的优点，包括密集连接层、局部特征融合(LFF)和局部残差学习(LRL)。残差密集块能确保每一层卷积层能提取更多的特征图，并通过特征融合在全局范围保留分层特征，从而产生隐式深层监督。本文使用RDB替代残差网络来改进SRGAN的生成器，使用密集残差块来提取特征，与DenseNet相比可使用更多卷积核。</p></sec></sec><sec id="s7"><title>3. 训练框架</title><sec id="s7_1"><title>3.1. 生成器架构</title><p>本文采用RDB改进SRGAN的生成器架构，主要是利用DenseNet的局部密集连接，通过堆叠实现充分利用不同卷积层级之间的特征。改进的残差密集网络(RDN)生成器通过局部特征融合LFF来调整特征图数量，通过局部残差学习LRL整合上层RDB与本层RDB的信息。最后加入全局特征融合(GFF)，整合所有RDB提取的特征。</p><p>生成器网络(如图3所示)包括四个模块，1) RDB浅层特征提取的模块；2) 将RDB堆叠起来的分层特征提取模块；3) GFF全局特征整合模块；4) 亚元素卷积模块，通过图像放大得到最终重建图像。</p><p>本文将网络输入记作I<sub>LR</sub>，输出记作I<sub>SR</sub>。首先采用一个3 &#215; 3卷积层完成第一次浅层特征提取，卷积核数量为64个，浅层特征提取记作：</p><p>F − 1 = C o n v ( I L R ) (1)</p><p>F<sub>−1</sub>不仅会用于下一步的分层特征提取，还将用于全局残差学习。第二次的浅层特征提取与第一层相同，也是3 &#215; 3卷积层，64个卷积核。第二层浅层特征提取的输出F<sub>0</sub>将作为后续一系列RDB结构的输入。</p><p>F 0 = C o n v ( I L R ) (2)</p><p>假设生成器网络一共采用K个RDB，则第d个RDB的输出F<sub>d</sub>表示为：</p><p>F d = H R D B , d ( F d − 1 ) = H R D B , d ( H R D B , d − 1 ( ⋯ ( H R D B , 1 ( F 0 ) ) ⋯ ) ) (3)</p><p>其中H<sub>RDB</sub><sub>,d</sub>表示第d个RDB。</p><p>图3. 生成器模型</p><p>生成器网络一共使用了12个RDB进行堆叠。每个RDB结构如图4所示，由6个卷积层组成，每层包含64个3 &#215; 3卷积核，采用PReLU作为激活函数。每层卷积层的输入来自于前面所有卷积层的输出堆叠形成，最后堆叠所有层的输出，并通过一个由64个大小为1 &#215; 1的卷集核组成的卷积层调整特征图数量，并与上一个RDB的输出一起进行局部残差学习，作为这个RDB的输出。</p><p>图4. 残差密集块结构</p><p>生成器网络然后将12个RDB的输出堆叠在了一起，即将不同层的局部特征进行全局特征整合GFF。GFF由一个堆叠层和一个64个1 &#215; 1卷积核组成的卷积层组成，紧接着采用一个3 &#215; 3卷积层进一步提取特征，最后再与最开始产生的F<sub>−1</sub>进行全局残差学习。</p><p>生成器网络接下来用一个卷积层将形状提升到 H ⋅ W ⋅ C ⋅ r 2 ，然后用亚元素卷积层将放大图像到 r H ⋅ r W ⋅ C ，最后采用一个3 &#215; 3卷积层把图像压缩到RGB3通道，输出高分辨率图像。</p></sec><sec id="s7_2"><title>3.2. 判别器架构</title><p>判别器用于判别图像样本的真伪，判别器模型如图5所示。</p><p>图5. 判别器模型</p><p>判别器网络首先使用一个64个3 &#215; 3卷积核组成的卷积层提取特征，并使用Leaky ReLU作为激活函数，紧接着是一连串特征提取模块。每一个特征提取模块都由一层3 &#215; 3卷积层，紧跟一层批规范化层(Batch-normalization, BN)层，后面接LReLU作为激活函数。第一个模块卷积层有64个卷积核，步长为2。后面两个模块卷积层的卷积核数量均为128，步长分别为1与2。再后面两个模块卷积层的卷积核数量为256，步长一样分别为1与2。最后两个模块卷积层的卷积核数量为512，步长也分别为1与2。然后通过一个1024个神经元隐层平铺特征矩阵，再通过一个隐层得到最终输出，使用sigmoid激活函数获得真实图像概率，即可给出判别结果。</p></sec><sec id="s7_3"><title>3.3. 模型训练</title><p>模型训练流程如图6所示。模型训练使用ImageNet2014 [<xref ref-type="bibr" rid="hanspub.38444-ref10">10</xref>] 数据集，随机选取5万张高清图像，通过使用bicubic插值进行缩小4倍压缩，得到对应的低分辨率图像，然后在图像中随机裁取一个32 &#215; 32大小区域作为训练样本，固定训练样本大小的原因是因为判别器有全连接层。为了增加数据的多样性，对每张图像随机决定是否进行上下或者水平翻转，从而防止模型过拟合。</p><p>验证集则使用DIV2K高质量数据集 [<xref ref-type="bibr" rid="hanspub.38444-ref11">11</xref>]，该数据集有800张2 K分辨率图像，也通过bicubic插值等操作得到对应的低分辨率图像。训练过程中每训练1000次就使用验证集对模型效果进行评估，并将效果最好的参数保存下来。由于GAN在刚开始训练阶段，容易因达不到纳什平衡而得到无法预料的结果，因此在训练刚开始阶段需要随时观察实际重建结果，如果不正常，需要调参重新开始训练。</p><p>训练过程中先针对生成器单独训练，然后针对生成器使用GAN，将损失函数中内容损失设置为MSE，并将对抗损失系数设置为10<sup>−3</sup>，再进行训练。最后再将损失函数替换为感知损失，并将MSE损失也加入到内容损失中，再进行训练，得到最终模型。</p><p>图6. 训练流程</p><p>模型基于开源深度学习框架Keras2实现，深度学习框架采用tensorflow，python的版本为3.4，操作系统为Ubuntu16.04，显卡GTX1060 (6 G)。训练优化方法采用Adam [<xref ref-type="bibr" rid="hanspub.38444-ref12">12</xref>]，初始学习率设置为10<sup>−4</sup>，每训练1000轮将其缩减一半，训练耗时一天。</p><p>模型最终使用峰值信噪比(Peak Signal to Noise Ratio, PSNR)和结构相似性(SSIM)对重建结果进行评估，但需要先将图像从RGB空间转换到YCbCr空间中，在Y通道上计算SSIM和PSNR值。</p><p>图7. 预训练生成器 左图为loss右图为PSNR</p><p>图7左边图像是预训练生成器过程中loss的趋势图，图中可以看出预训练过程中loss震荡较严重，主要原因是GPU显存只有6G太小，每次样本数量不够多，导致loss发生比较明显的震荡，但总体仍呈下降趋势。横坐标代表训练轮数，每一轮包含1000次迭代。从图中可以看出前十几轮迭代loss下降非常明显，后面的训练，loss下降非常有限，而且产生剧烈震荡。</p><p>通过调整学习率模型可以进一步收敛，并期望最终收敛到局部最优值。为实现这一目标，我们采用Adam作为模型训练优化器，根据训练迭代数通过自适应的方式来调整学习率，其优化更新方法为：</p><p>m = β 1 ⋅ m + ( 1 − β 1 ) ⋅ d x (4)</p><p>m t = m ( 1 − β 1 t ) (5)</p><p>v = β 2 ⋅ v + ( 1 − β 2 ) ⋅ d x 2 (6)</p><p>v t = v ( 1 − β 2 t ) (7)</p><p>x + = − l r ⋅ m t n p . s q r t ( v t ) + e p s (8)</p><p>其中lr为学习率，eps是一个超参数用来防止分母为0，β<sub>1</sub>和β<sub>2</sub>为优化器中的超参数，其中β<sub>1</sub> = 0.9，β<sub>2</sub> = 0.999，eps = 1e−8，t为迭代轮数。</p><p>PSNR表示信号最大可能功率和影响其表示精度的破坏性噪声功率的比值。PSNR的值越大越好，会随着训练时间越变越大。从图7右边可以看出，PSNR图像的走向趋势与MSE走向相反，在前十几轮的迭代中迅速增加，后面有震荡趋于稳定。而且，PSNR值不高，最后稳定在21.3左右，究其原因应该是ImageNet样本图像分辨率不高，压缩后高频细节丢失严重。</p><p>图8是验证集训练结果，十几轮后验证集损失降低到一个较低水平，说明模型未产生过拟合现象。验证集使用DIV2K数据集，样本图像分辨率比ImageNet高，PSNR值提升较明显，因此在训练过程中PSNR值不高与样本图像原始分辨率较低存在关联关系，很明显过于模糊的图像会导致超分辨率重建变得困难。</p><p>图8. 预训练验证器 左图为loss 右图为PSNR</p><p>图9显示的是GANs训练过程中对抗损失与内容损失走向。由于先进行了生成预训练，内容损失刚开始下降，后面一直震荡。由于判别器刚开始还未训练，依靠随机猜测输出类别，对抗损失较低。后面随着训练的进行，对抗损失越来越高，后面随着生成器生成的样本越来越接近真实样本，对抗损失再度降低，后面震荡减小，模型逐渐收敛到局部最优。</p><p>图9. 生成对抗网络训练过程,左图为对抗损失，右图为内容损失</p><p>这个阶段PSNR震荡非常明显，应该是模型生成的高频细节并不是原始图像的真实细节，比如会多出一些假的纹理和细节。从而导致重建图像与真实图像间的MSE相差较大，MSE是PSNR值计算的衡量标准，最终导致明显的震荡。</p></sec></sec><sec id="s8"><title>4. 实验过程</title><sec id="s8_1"><title>4.1. 实验数据集</title><p>实验数据集选择Set5 [<xref ref-type="bibr" rid="hanspub.38444-ref13">13</xref>]，Set14 [<xref ref-type="bibr" rid="hanspub.38444-ref14">14</xref>]，BSD100 [<xref ref-type="bibr" rid="hanspub.38444-ref15">15</xref>]，Urban100 [<xref ref-type="bibr" rid="hanspub.38444-ref16">16</xref>] 这些经常被图像超分辨率重建用于作为结果对比的公开数据集。四个数据集分别包含5张、14张、100张、100张高分辨率图片及对应的低分辨率图片，图片格式均为png。实验采用python中的matplotlib包将这些图像读到内存，将图像像素为[0,255]的值域进行转换，低分辨率图像像素值域映射到[0,1]，高分辨率图像像素值域映射到[−1,1]。</p></sec><sec id="s8_2"><title>4.2. 实验设计</title><p>本文的对比实验包括：改进生成器之后的SRGAN与原文使用SRResNe生成器的SRGAN对比；与其它公开结果的模型进行对比实验，包括与最近邻插值法、双立方插值法 [<xref ref-type="bibr" rid="hanspub.38444-ref17">17</xref>]、SRCNN [<xref ref-type="bibr" rid="hanspub.38444-ref18">18</xref>]、SCSR [<xref ref-type="bibr" rid="hanspub.38444-ref19">19</xref>] 等进行对比实验；进行转置卷积与亚元素卷积的对比实验，分析两者对重建结果与训练速度的影响。</p><p>实验使用keras复现SRGAN的SRResNet生成器网络，设置的参数与RDB训练的网络一致，使用ImageNet数据集进行训练。</p><p>实验使用的最近邻插值法和双立方插值法属于传统插值超分辨率重建算法，优点体现在速度快，缺点也非常明显，因为丢失高频细节导致重建结果会出现模糊的情况。实验使用的SRCNN是最早几种采用卷积网络进行超分辨率重建的模型。模型使用的卷积层数较少，仅堆叠了三层卷积层，模型首先通过双立方插值进行图像放大，然后采用卷积网络进行特征提取和筛选，最后一层卷积输出作为重建结果。</p><p>本文为了验证模块数量对重建结果的影响，以及残差密集块对梯度弥散的减轻程度，采用堆叠不同数量RDB模块进行实验。因为显卡显存容量大小限制，本文分别将RDB堆叠数量设置为8，10，12进行对比实验，每个RDB模块采用6个卷积核数量为32的卷积层进行特征提取。模型使用的其它卷积层，包括最开始的浅层特征提取、LFF、GFF，均采用64个卷积核。实验结果显示在堆叠较多的RDB模块后，仍能起到促进作用。</p><p>本文针对DenseNet改进主要是在RDB结构中加了更多1 &#215; 1卷积，因此每层卷积层可以使用更多的卷积核进行特征提取。本文针对卷积核数量影进行了对比实验，分别将RDB中卷积层的卷积核数量设为16、32、64进行对比实验，分析卷积核数量对模型的影响。</p></sec><sec id="s8_3"><title>4.3. 实验结果</title><sec id="s8_3_1"><title>4.3.1. 堆叠RDB块数量对比实验</title><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The number of RDB’s influence on the networ</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Set5</th><th align="center" valign="middle" >Set14</th><th align="center" valign="middle" >训练时间</th><th align="center" valign="middle" >参数数量</th></tr></thead><tr><td align="center" valign="middle" >K = 8</td><td align="center" valign="middle" >29.437/0.848</td><td align="center" valign="middle" >26.809/0.816</td><td align="center" valign="middle" >161 ms</td><td align="center" valign="middle" >2,502,019</td></tr><tr><td align="center" valign="middle" >K = 10</td><td align="center" valign="middle" >29.536/0.883</td><td align="center" valign="middle" >26.855/0.816</td><td align="center" valign="middle" >193 ms</td><td align="center" valign="middle" >3,041,155</td></tr><tr><td align="center" valign="middle" >K = 12</td><td align="center" valign="middle" >29.717/0.895</td><td align="center" valign="middle" >27.005/0.821</td><td align="center" valign="middle" >223 ms</td><td align="center" valign="middle" >3,580,291</td></tr></tbody></table></table-wrap><p>表1. 堆叠RDB数量对网络的影响</p><p>表1中的K表示堆叠RDB块数量，训练时间是模型训练完一个batch所消耗的时间。表1中第2列与第3列数据显示，模型增加堆叠RDB块，重建图像的PSNR值与SSIM值会增加，K = 12的值比前两者的值都高，说明堆叠更多的RDB模块能够提升重建结果，原因是每堆叠一个RDB，网络会增加6个卷积层，从而提升网络表示能力，最终提升重建结果，但提升的精度细微。从表1的第4列与第5列可以看到，随着堆叠的RDB块增加，训练时间大大增加，带来细微精度的提升；模型参数数量也随着堆叠的RDB块增加而提高了不少，如果GPU的显存不够大，参数数量过多会导致显存不够训练无法继续，堆叠的RDB块不能太多，在对重建精度要求不高的使用场景，可以堆叠较少RDB提升训练效率和重建速度。</p></sec><sec id="s8_3_2"><title>4.3.2. 卷积核数量对比实验</title><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> The number of kernel’s influence on the networ</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Set5</th><th align="center" valign="middle" >Set14</th><th align="center" valign="middle" >参数数量</th></tr></thead><tr><td align="center" valign="middle" >G = 16</td><td align="center" valign="middle" >29.491/0.880</td><td align="center" valign="middle" >26.955/0.816</td><td align="center" valign="middle" >1,597,699</td></tr><tr><td align="center" valign="middle" >G = 32</td><td align="center" valign="middle" >29.717/0.895</td><td align="center" valign="middle" >27.005/0.821</td><td align="center" valign="middle" >3,580,291</td></tr><tr><td align="center" valign="middle" >G = 64</td><td align="center" valign="middle" >29.889/0.888</td><td align="center" valign="middle" >27.38/0.827</td><td align="center" valign="middle" >10,033,795</td></tr></tbody></table></table-wrap><p>表2. 卷积核数量对网络表现的影响</p><p>表2中G表示卷积层包含的卷积核数量，实验数据表明，增加卷积核数量可以提升模型能力，但提升不是很明显。与RDB堆叠数量相似，当特征图数量过多时参数数量增大，最终会导致显存不够无法继续训练。</p></sec><sec id="s8_3_3"><title>4.3.3. 卷积方法对比实验</title><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Compare transposed convolution with sub-pixel convolutio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Set5</th><th align="center" valign="middle" >Set14</th><th align="center" valign="middle" >训练时间</th></tr></thead><tr><td align="center" valign="middle" >转置卷积</td><td align="center" valign="middle" >29.887/0.885</td><td align="center" valign="middle" >27.39/0.832</td><td align="center" valign="middle" >381 ms</td></tr><tr><td align="center" valign="middle" >亚元素卷积</td><td align="center" valign="middle" >29.889/0.888</td><td align="center" valign="middle" >27.38/0.827</td><td align="center" valign="middle" >319 ms</td></tr></tbody></table></table-wrap><p>表3. 转置卷积与亚元素卷积对比</p><p>本文采用转置卷积与亚元素卷积两种不同的卷积方案进行了对比实验，实验结果如表3所示，两种不同的卷积方式的结果精度并没有太大的差别。但在训练时间上，亚元素卷积比转置卷积要快一些。</p></sec><sec id="s8_3_4"><title>4.3.4. 不同生成器对比实验</title><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Compare different generator</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Set5</th><th align="center" valign="middle" >Set14</th><th align="center" valign="middle" >Urban100</th><th align="center" valign="middle" >BSD100</th></tr></thead><tr><td align="center" valign="middle" >SRResNet</td><td align="center" valign="middle" >23.303/0.635</td><td align="center" valign="middle" >21.024/0.535</td><td align="center" valign="middle" >19.381/0.567</td><td align="center" valign="middle" >20.983/0.545</td></tr><tr><td align="center" valign="middle" >RDB</td><td align="center" valign="middle" >26.245/0.814</td><td align="center" valign="middle" >24.717/0.756</td><td align="center" valign="middle" >22.753/0.729</td><td align="center" valign="middle" >24.348/0.732</td></tr></tbody></table></table-wrap><p>表4. 不同生成器对比</p><p>图10. 左图为SRGAN (SRResNet)右图为SRGAN (RDB)</p><p>本文的核心工作是改进SRGAN的生成器，并对基于RDB改进生成器的SRGAN模型与原来使用SRResNet作为生成器的SRGAN模型在四个测试集上分别进行了超分辨率重建对比实验，表4显示的是实验结果，结果显示基于本文改进的RDB生成器模型，在每个数据集上，PSNR与SSIM均有较明显的提升，见图10。</p><p>实验结果显示将密集块与残差块结合起来，的确可以提升SRGAN的重建结果。由于密集网络充分利用了不同层级之间的特征，低频信息重建与高频细节重建都起到了帮助。虽然SRGAN主要依靠GAN补足高频细节，但是生成器的改进对重建也带来了帮助。</p></sec><sec id="s8_3_5"><title>4.3.5. 不同算法对比实验</title><p>本文最后将本文改进后的SRGAN模型与其它经典的超分辨率算法在各个公开测试集进行了对比实验，结果如表5所示。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Compare to other algorithm</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >Set5</th><th align="center" valign="middle" >Set14</th><th align="center" valign="middle" >Urban100</th><th align="center" valign="middle" >BSD100</th></tr></thead><tr><td align="center" valign="middle" >最近邻插值</td><td align="center" valign="middle" >24.607/0.731</td><td align="center" valign="middle" >22.982/0.646</td><td align="center" valign="middle" >20.733/0.619</td><td align="center" valign="middle" >23.728/0.629</td></tr><tr><td align="center" valign="middle" >双立方线性插值</td><td align="center" valign="middle" >26.694/0.789</td><td align="center" valign="middle" >24.247/0.686</td><td align="center" valign="middle" >21.701/0.651</td><td align="center" valign="middle" >24.654/0.661</td></tr><tr><td align="center" valign="middle" >SCSR</td><td align="center" valign="middle" >27.237/0.806</td><td align="center" valign="middle" >24.631/0.706</td><td align="center" valign="middle" >22.510/0.695</td><td align="center" valign="middle" >25.258/0.694</td></tr><tr><td align="center" valign="middle" >SRCNN</td><td align="center" valign="middle" >28.089/0/829</td><td align="center" valign="middle" >25.161/0.720</td><td align="center" valign="middle" >22.626/0.696</td><td align="center" valign="middle" >25.345/0.698</td></tr><tr><td align="center" valign="middle" >SRGAN(RDB)</td><td align="center" valign="middle" >26.245/0.814</td><td align="center" valign="middle" >24.717/0.756</td><td align="center" valign="middle" >22.753/0.729</td><td align="center" valign="middle" >24.348/0.732</td></tr></tbody></table></table-wrap><p>表5. 与其他算法结果对比</p><p>图11. 左一LR左二SRCNN右二SRGAN(RDB)右一HR</p><p>从表5的结果可以看出GAN模型的重构图像由于生成了许多假细节，在PSNR指标上比不过一些方法，但重构图像在真实度上有较大提升，更符合人眼观察到的高清图像，见图11。</p><p>因此，对于重构图像真实度要求不是特别高的场景，可以采用SRGAN进行超分辨率重建，比如复原哪些压缩严重的风景画或者照片，但对于那些对图像真实要求很高的医疗、遥感等图像的重构就不太适合。</p></sec></sec></sec><sec id="s9"><title>5. 结束语</title><p>本文采用密集型卷积网络对生成对抗模型的生成器进行改进，提升了SRGAN模型进行超分辨率重建能力。虽然本文研究达到了预期目的，但仍有很多改进的空间。因为堆叠RDB块造成网络深度过深，虽然密集网络能减轻梯度弥散，但训练和测试时间增长非常快。另外，利用GANs重建图像的真实感会有较大提升，但PSNR值却稍低，说明PSNR可能不是特别适合用来作为指标来衡量重建效果。由于GANs的训练是基于两个模型的对抗竞争，超参数设置比较敏感，易出现难以预料的结果。如何提升GANs训练的鲁棒性，尽量避免出现不收敛情况是未来继续研究的方向。</p></sec><sec id="s10"><title>基金项目</title><p>黔科合重大专项字[<xref ref-type="bibr" rid="hanspub.38444-ref2016">2016</xref>] 3012。</p></sec><sec id="s11"><title>文章引用</title><p>吴熹鹏,黄 溪,王先兵,何 涛,吴中鼎,柴婉秋. 基于GAN的超分辨率重建算法研究A Study of Super Resolution Algorithm Basing on Generative Adversarial Network[J]. 计算机科学与应用, 2020, 10(10): 1908-1920. https://doi.org/10.12677/CSA.2020.1010201</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.38444-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Zhou, F., Yang, W.M. and Liao, Q.M. (2012) Interpolation-Based Image Super-Resolution Using Multi-Surface Fitting. IEEE Transactions on Image Processing, 21, 3312-3318. &lt;br&gt;https://doi.org/10.1109/TIP.2012.2189576</mixed-citation></ref><ref id="hanspub.38444-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Lin, Z.C. and Shum, H.Y. (2004) Fundamental Limits of Reconstruction Based Super Resolution Algorithms under Local Transla-tion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26, 83-97.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2004.1261081</mixed-citation></ref><ref id="hanspub.38444-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Pan, Z.-X., Yu, J., Hu, S.-X. and Sun, W.-D. (2014) Single Image Super Resolution Based on Multi-Scale Structural Self-Similarity. Acta Automatica Sinica, 40, 594-603.</mixed-citation></ref><ref id="hanspub.38444-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Lian, Q.-S., Shi, B.-S. and Chen, S.Z. (2015) Research Advances on Dictionary Learning Models, Algorithms and Applications. Acta Automatica Sinica, 41, 240-260.</mixed-citation></ref><ref id="hanspub.38444-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Ledig, C., Theis, L., Huszar, F., et al. (2016) Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 4680-4690. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.19</mixed-citation></ref><ref id="hanspub.38444-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Goodfellow, I., Pouget-Abadie, J., Mirza, M., et al. (2014) Generative Adversarial Nets. Advances in Neural Information Processing Systems, Montreal, 8-13 December 2014, 2672-2680.</mixed-citation></ref><ref id="hanspub.38444-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Dong, C., Loy, C.C., He, K., et al. (2014) Learning a Deep Convolutional Network for Image Su-per-Resolution. In: European Conference on Computer Vision, Springer, Cham, 184-199. &lt;br&gt;https://doi.org/10.1007/978-3-319-10593-2_13</mixed-citation></ref><ref id="hanspub.38444-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Huang, G., Liu, Z., Weinberger, K.Q., et al. (2017) Densely Connected Convolutional Networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, Vol. 1, 3.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.243</mixed-citation></ref><ref id="hanspub.38444-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Y., Tian, Y., Kong, Y., et al. (2018) Residual Dense Network for Image Super-Resolution. IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 2472-2481.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00262</mixed-citation></ref><ref id="hanspub.38444-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Deng, J., Dong, W., Socher, R., et al. (2009) Imagenet: A Large-Scale Hierarchical Image Database. IEEE Conference on Computer Vision and Pattern Recognition, Miami, 20-25 June 2009, 248-255.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2009.5206848</mixed-citation></ref><ref id="hanspub.38444-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Agustsson, E. and Timofte, R. (2017) Ntire 2017 Challenge on Single Image Super-Resolution: Dataset and Study. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Honolulu, 21-26 July 2017, Vol. 3, 2. &lt;br&gt;https://doi.org/10.1109/CVPRW.2017.150</mixed-citation></ref><ref id="hanspub.38444-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Kingma, D.P. and Ba, J. (2014) Adam: A Method for Stochastic Optimization.</mixed-citation></ref><ref id="hanspub.38444-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Bevilacqua, M., Roumy, A., Guillemot, C., et al. (2012) Low-Complexity Single-Image Super-Resolution Based on Nonnegative Neighbor Embedding. Proceedings British Machine Vision Conference 2012, Surrey, 3-7 September 2012, 135.1-135.10. &lt;br&gt;https://doi.org/10.5244/C.26.135</mixed-citation></ref><ref id="hanspub.38444-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Zeyde, R., Elad, M. and Protter, M. (2010) On Single Image Scale-Up Using Sparse-Representations. In: International Conference on Curves and Surfaces, Springer, Berlin, Heidelberg, 711-730.  
&lt;br&gt;https://doi.org/10.1007/978-3-642-27413-8_47</mixed-citation></ref><ref id="hanspub.38444-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Martin, D., Fowlkes, C., Tal, D., et al. (2001) A Database of Human Segmented Natural Images and Its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics. Eighth IEEE International Conference on Computer Vision, Vol. 2, 416-423.</mixed-citation></ref><ref id="hanspub.38444-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Huang, J.B., Singh, A. and Ahuja, N. (2015) Single Image Super-Resolution from Transformed Self-Exemplars. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 5197-5206.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2015.7299156</mixed-citation></ref><ref id="hanspub.38444-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">De Boor, C. (1962) Bicubic Spline Interpolation. Journal of Mathematics &amp; Physics, 41, 212-218.  
&lt;br&gt;https://doi.org/10.1002/sapm1962411212</mixed-citation></ref><ref id="hanspub.38444-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Dong, C., Chen, C.L., He, K., et al. (2016) Image Super-Resolution Using Deep Convolutional Networks. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 38, 295-307. &lt;br&gt;https://doi.org/10.1109/TPAMI.2015.2439281</mixed-citation></ref><ref id="hanspub.38444-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Yang, J.C., Wright, J., Huang, T.S. and Ma, Y. (2010) Image Superresolution via Sparse Representation. IEEE Transactions on Image Processing, 19, 2861-2873. &lt;br&gt;https://doi.org/10.1109/TIP.2010.2050625</mixed-citation></ref></ref-list></back></article>