<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.113064</article-id><article-id pub-id-type="publisher-id">CSA-41152</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210300000_37463482.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于深度学习的命名实体识别算法
  Named Entity Recognition Algorithm Based on Deep Learning
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>娟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>卓薇</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>程</surname><given-names>良伦</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>广东工业大学，广东 广州</addr-line></aff><aff id="aff4"><addr-line>广东工业大学计算机学院，广东 广州</addr-line></aff><aff id="aff2"><addr-line>广东省信息物理融合系统重点实验室，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>03</month><year>2021</year></pub-date><volume>11</volume><issue>03</issue><fpage>628</fpage><lpage>634</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   命名实体识别(Named Entity Recognition, NER)的定义是从自由文本中识别出属于预定义类别的文本片段(如人名、地理位置名、机构组织名等)。命名实体识别一直是许多自然语言应用的基础，例如问题回答、提取文本摘要和知识库建立。早期的NER系统在实现良好性能方面取得了巨大的成功，其代价是人类工程在设计特定领域的特征和规则方面付出的代价。近年来，非线性处理的连续实值向量表示和语义组合使得深度学习在命名实体识别系统中发挥了很好的作用。在本文中，我们提供了一种基于深度学习的命名实体识别算法。首先我们随机初始化训练集中的每个字特征，并在获取该字典句子中每个字的特征之后，利用周期卷积来得到其固定长度的特征，以此作为句子特征;随后训练数据自动编码器，通过栈式自动编码器得到高层句子的特征;最后通过高层句特征与字特征的组合训练字的标注网络模型来得到未知字的标注值，再进行实体扩展(分类，属性，副标题)，最后利用马尔科夫逻辑网络优化整体识别效果。 Named entity recognition is the task of identifying rigid indicators from text belonging to predefined semantic types (such as person, location, organizations, and so on). NER has been the basis for many natural language applications, such as question answering, text summarization and machine translation. Early NER systems had great success in achieving good performance at the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning has been used in NER systems through continuous real-valued vector representations and semantic combinations of nonlinear processing, resulting in the most advanced performance. In this article, we provide an entity recognition technique based on deep learning. Firstly, each word feature in the training set is randomly initialized, and the feature of each word in the sentence is obtained based on the dictionary. Then, the fixed-length feature is obtained by periodic convolution of different length of sentence features, which are used as sentence features. Then the data autoencoder is trained to get the features of high-level sentences through the stack autoencoder. Finally, the combination of high level sentence features and word features is used to train the annotation network model of words, and the annotation value of unknown words is obtained based on the annotation model, and then the entity expansion (classification, attribute, subtitle) is carried out. Finally, the overall recognition effect is optimized by using Markov logic network. 
  
 
</p></abstract><kwd-group><kwd>知识图谱，深度学习，实体识别, Knowledge Graph</kwd><kwd> Deep Learning</kwd><kwd> Entity Recognition</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>命名实体识别(Named Entity Recognition, NER)的定义是从自由文本中识别出属于预定义类别的文本片段(如人名、地理位置名、机构组织名等)。命名实体识别一直是许多自然语言应用的基础，例如问题回答、提取文本摘要和知识库建立。早期的NER系统在实现良好性能方面取得了巨大的成功，其代价是人类工程在设计特定领域的特征和规则方面付出的代价。近年来，非线性处理的连续实值向量表示和语义组合使得深度学习在命名实体识别系统中发挥了很好的作用。在本文中，我们提供了一种基于深度学习的命名实体识别算法。首先我们随机初始化训练集中的每个字特征，并在获取该字典句子中每个字的特征之后，利用周期卷积来得到其固定长度的特征，以此作为句子特征;随后训练数据自动编码器，通过栈式自动编码器得到高层句子的特征;最后通过高层句特征与字特征的组合训练字的标注网络模型来得到未知字的标注值，再进行实体扩展(分类，属性，副标题)，最后利用马尔科夫逻辑网络优化整体识别效果。</p></sec><sec id="s2"><title>关键词</title><p>知识图谱，深度学习，实体识别</p></sec><sec id="s3"><title>Named Entity Recognition Algorithm Based on Deep Learning<sup> </sup></title><p>Juan Chen<sup>1</sup>, Zhuowei Wang<sup>2</sup>, Lianglun Cheng<sup>3</sup></p><p><sup>1</sup>Guangdong Provincial Key Laboratory of Cyber-Physics Fusion System, Guangzhou Guangdong</p><p><sup>2</sup>Guangdong University of Technology, Guangzhou Guangdong</p><p><sup>3</sup>School of Computer Science, Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/18-1542076x4_hanspub.png" /></p><p>Received: Feb. 23<sup>rd</sup>, 2021; accepted: Mar. 17<sup>th</sup>, 2021; published: Mar. 24<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/18-1542076x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Named entity recognition is the task of identifying rigid indicators from text belonging to predefined semantic types (such as person, location, organizations, and so on). NER has been the basis for many natural language applications, such as question answering, text summarization and machine translation. Early NER systems had great success in achieving good performance at the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning has been used in NER systems through continuous real-valued vector representations and semantic combinations of nonlinear processing, resulting in the most advanced performance. In this article, we provide an entity recognition technique based on deep learning. Firstly, each word feature in the training set is randomly initialized, and the feature of each word in the sentence is obtained based on the dictionary. Then, the fixed-length feature is obtained by periodic convolution of different length of sentence features, which are used as sentence features. Then the data autoencoder is trained to get the features of high-level sentences through the stack autoencoder. Finally, the combination of high level sentence features and word features is used to train the annotation network model of words, and the annotation value of unknown words is obtained based on the annotation model, and then the entity expansion (classification, attribute, subtitle) is carried out. Finally, the overall recognition effect is optimized by using Markov logic network.</p><p>Keywords:Knowledge Graph, Deep Learning, Entity Recognition</p><disp-formula id="hanspub.41152-formula18"><graphic xlink:href="//html.hanspub.org/file/18-1542076x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/18-1542076x7_hanspub.png" /> <img src="//html.hanspub.org/file/18-1542076x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>命名实体识别(Named Entity Recognition, NER)定义是从自由文本中识别出属于预定义类别的文本片段，如人名、地理位置名、机构组织名等 [<xref ref-type="bibr" rid="hanspub.41152-ref1">1</xref>]。NER不仅是信息抽取的独立工具，而且在各种自然语言处理(NLP)的分支中充当着重要角色，如文本分析、信息抽取，文本摘要生成，问答系统，建立知识图谱等。近年来，深度学习(DL，也称为深度神经网络)被各个领域广泛应用，并取得成功。从Collobert等人开始 [<xref ref-type="bibr" rid="hanspub.41152-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref4">4</xref>]，基于DL的NER系统，其具有最少的特征工程，正在迅猛的发展。在过去的几年中，NER慢慢引入深度学习，并得到了很好的效果 [<xref ref-type="bibr" rid="hanspub.41152-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref9">9</xref>]。这一现象鼓舞我们深入研究NER中的深度学习技术。Nadeau和Sekine [<xref ref-type="bibr" rid="hanspub.41152-ref3">3</xref>] 可以说是最成熟的技术，发表于2007年。该论文讲述了从手工制定的规则到机器学习的技术发展趋势。2013年，Patawar和Potey [<xref ref-type="bibr" rid="hanspub.41152-ref10">10</xref>] 在2015年进行了简短回顾。最近的两项研究分别涉及新领域 [<xref ref-type="bibr" rid="hanspub.41152-ref11">11</xref>] 和复杂的实体提及 [<xref ref-type="bibr" rid="hanspub.41152-ref12">12</xref>]。总之，现有的研究主要包括基于特征的机器学习模型，与这项工作更为紧密的是最近的两次调查。在2018年，Goyal等 [<xref ref-type="bibr" rid="hanspub.41152-ref13">13</xref>] 调查了NER的发展和进步。但是，它们不包括深度学习技术的最新进展。Yadav和Bethard [<xref ref-type="bibr" rid="hanspub.41152-ref12">12</xref>] 根据句子中的单词表示，对NER的最新进展进行了研究。这项研究主要技术是输入的分布式表示形式(例如，字符级和单词级嵌入)，而不是采用查看上下文编码器和标签解码器。在本文中，首先我们随机初始化训练集中的每个字特征，并在获取该字典句子中每个字的特征之后，利用周期卷积来得到其固定长度的特征，以此作为句特征;随后训练数据自动编码器，通过栈式自动编码器得到高层句子的特征；最后通过高层句特征与字特征的组合训练字的标注网络模型来得到未知字的标注值，再进行实体扩展(分类，属性)，最后利用马尔科夫逻辑网络优化整体识别效果。</p></sec><sec id="s6"><title>2. 命名实体识别研究历史</title><p>对英文文本的实体识别的研究领先于中文文本命名实体识别的研究，早在1991年，Rau在第七届IEEE人工智能应用会议上，发布了一个实体识别系统，该系统可以抽取出文本中的公司名称 [<xref ref-type="bibr" rid="hanspub.41152-ref14">14</xref>]，在当时引起了轰动。从此命名实体识别就开始被引入MUC-6 [<xref ref-type="bibr" rid="hanspub.41152-ref15">15</xref>]，MUC-7的MET-2 [<xref ref-type="bibr" rid="hanspub.41152-ref16">16</xref>] 等一系列会议中。在20世纪90年代初期，孙茂松等 [<xref ref-type="bibr" rid="hanspub.41152-ref17">17</xref>] 开始研究中文命名实体识别，最开始研究的范围比较窄，仅仅是对文本中的人名进行识别。而后张小衡等 [<xref ref-type="bibr" rid="hanspub.41152-ref18">18</xref>] 开始扩大实体识别的范围，建立了高校名数据集，并采用人工规则进行了实验，完成了对文本中的组织机构名称的抽取。2000年，ZHANG, ZHOU等 [<xref ref-type="bibr" rid="hanspub.41152-ref19">19</xref>] 在ACL会议上发表了一个抽取命名实体及它们之间关系的系统。在算法方面，命名实体识别最早期的方法是基于规则，基于字典。后来发展成传统机器学习的方法，比如CRF。经过专家们的不断努力，实体识别引入了深度学习的方法，比如RNN-CRF, CNN-CRF等模型 [<xref ref-type="bibr" rid="hanspub.41152-ref20">20</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref21">21</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref22">22</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref23">23</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref24">24</xref>]。</p></sec><sec id="s7"><title>3. 关键技术</title><sec id="s7_1"><title>3.1. 字词结合向量</title><sec id="s7_1_1"><title>3.1.1. 字特征向量</title><p>基于字特征向量的模型缺点是单独输入字符，并没有考虑到相邻字符，句子，段落之间所存在的语义关系。其模型结构如图1所示，在最开始的时候字向量公式为</p><p>X j c = e c ( c j )</p><p>h j c = [ h → j c ; h ← j c ]</p><p>将字符序列 c 1 , c 2 , ⋯ , c m ，输入Lstm-CRF模型中。每个字符 c j 用第一个公式表示， e c 表示字符嵌入查找表，即计算字向量的操作。一个双向LSTM应用于 x 1 , x 2 , ⋯ , x j ,可以得到双向lstm的两个方向隐含层的输出，即分别在从左到右和从右到左的方向上的两组不同的参数。根据第二个公式将其参数进行合并，得到的就是第j个字符的输出，再将其输入CRF中。2017年，CHEN [<xref ref-type="bibr" rid="hanspub.41152-ref25">25</xref>] 等人改进了该模型</p><p>如公式三所示</p><p>X j c = [ e c ( c j ) ; e b ( c j , c j + 1 ) ]</p><p>在计算输入向量的时候把这个字的向量和下一个字符的向量进行了合并，以此来加强字符间的语义关系。</p><p>图1. 字特征向量模型结构</p></sec><sec id="s7_1_2"><title>3.1.2. 词特征向量</title><p>词特征向量类似字特征，它需要词嵌入来表示每个词，其模型结构如图2所示，公式如下：</p><p>X i w = [ e w ( w i ) ; X i c ]</p><p>X i c = [ h → t ( i , l e n ( i ) ) c ; h ← t ( i , 1 ) c ]</p><p>该模型用了两层LSTM，第一层是公式七中计算每个词所有字向量的输出。第二层双向Lstm用来学习隐藏状态 h → t ( i , 1 ) c , ⋯ , h → t ( i , l e n ( i ) ) c ，len(i)表示w<sub>i</sub>中的字符数。公式中 h → t ( i , l e n ( i ) ) c 表示第i个词最后一个字的正向的隐含层h； h ← t ( i , 1 ) c 则表示第i个词第一个字的反向的隐含层h [<xref ref-type="bibr" rid="hanspub.41152-ref26">26</xref>]。</p><p>图2. 词特征向量模型结构</p></sec><sec id="s7_1_3"><title>3.1.3. 字词结合特征</title><p>单词–字符格模型可以看作是基于字符的模型的扩展，集成了基于单词的单元和用于控制信息流的其他门。其模型结构如图3所示，模型的输入是字符序列 c 1 , c 2 , ⋯ , c m 以及与词典D中的单词匹配的所有字符子序列。该模型涉及四种类型的向量，即输入向量，输出隐藏向量，单元向量和门向量。作为基本组成部分，字符输入向量用于表示每个字符 c j ，该模型的基本递归结构是使用字符单元向量 c j c 和一个隐藏元素构造的每个 c j 上的向量 h j c ，其中 c j c 用于记录从句子开头到 c j 的循环信息流， h j c 用于进行CRF序列标记 [<xref ref-type="bibr" rid="hanspub.41152-ref27">27</xref>]。该模型公式如下：</p><p>x j c = e c ( c j )</p><p>[ i j c o j c f j c c ˜ j c ] = [ σ σ σ tanh ] ( W c ⊤ [ x j c h j − 1 c ] + b c )</p><p>c j c = f j c ⊙ c j − 1 c + i j c ⊙ c ˜ j c</p><p>h j c = o j c ⊙ tanh ( c j c )</p><p>图3. 单词–字符格模型结构</p></sec></sec><sec id="s7_2"><title>3.2. 实体扩展</title><p>本文制作了中国地理特产数据集，实体扩展是指针对某一实体类别比如梨，给出了香梨，鸭梨等种子实体，输出是梨这个类别里其他未知实体，比如雪花梨，水晶梨等。本文采用的是实体扩展方法是基于模板的实体抽取，我们的目标实体(雪花梨，水晶梨)与种子实体(香梨，鸭梨)同属于梨这个语义类，首先我们预定义好指示上下文关系的语义模板，再分析种子实体(香梨，鸭梨)所处的上下文得到模板，然后基于Booststrapping策略，反复迭代，得到更多的种子模板，以模板为特征，计算候选实体的置信度。</p></sec><sec id="s7_3"><title>3.3. 马尔科夫逻辑网络(MLN)优化整体识别效果</title><p>马尔科夫逻辑网络是一种统计关系学习模型，之所以引入该网络，是因为我们的模型针对我们的实体，从分类属性方面进行了扩展，但是这两个方面的扩展是相互独立的，所以我们在扩展之后，引入了马尔科夫逻辑网络来提高我们实体识别的精确度。我们将模型得到的规则转化为子句的集合，将每一个子句看成一个节点，而每个集合中子句的关系即为连边，至此我们就构成了马尔科夫逻辑网。其概率计算的公式如下：</p><p>P ( X = x ) = 1 z exp ( ∑ i = 1 F w i n i ( x ) )</p><p>这个公式中， n i ( x ) 是某一个规则 F i 的取值为真的时候所对应闭规则的个数。如果我们的规则权重越大，那么必然 n i ( x ) 就会越大，也就说明我们所取的x在 F i 下越可信。然后我们把当前取值x在所有规则下的可信度相乘，再除以归一化因子，就得到了当前取值的概率 [<xref ref-type="bibr" rid="hanspub.41152-ref28">28</xref>] [<xref ref-type="bibr" rid="hanspub.41152-ref29">29</xref>]。</p></sec></sec><sec id="s8"><title>4. 实验结果分析</title><sec id="s8_1"><title>4.1. 实验数据</title><p>本文人工构建了一个中国地理特产介绍的命名实体识别数据集，我们的中国地理特产数据集包括训练集，开发集，测试集，训练集里有34906个字，开发集里有4396个字，测试集里有4768个字。</p></sec><sec id="s8_2"><title>4.2. 实验环境</title><p>本研究中的实验环境为ubuntu16.04操作系统，Python3.6，深度学习框架为Pytorch1.4.0。</p></sec><sec id="s8_3"><title>4.3. 实验设计与结果</title><p>本文模型主要识别中国地理特产网数据集中的特产名，地理位置名以及组织机构名，为了验证本文模型的性能，我们设计了三个实验，主要模型为Lattice LSTM-CNN-CRF，而对比实验模型我们用了LSTM-CNN-CRF模型和BiLSTM-CNN-CRF模型，本文实验结果是从准确率、召回率和F1值三个方面来进行。见下表1。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Experimental result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >准确率</th><th align="center" valign="middle" >召回率</th><th align="center" valign="middle" >F1值</th></tr></thead><tr><td align="center" valign="middle" >LSTM-CNN-CRF</td><td align="center" valign="middle" >88.90</td><td align="center" valign="middle" >87.90</td><td align="center" valign="middle" >88.10</td></tr><tr><td align="center" valign="middle" >BiLSTM-CNN-CRF</td><td align="center" valign="middle" >89.80</td><td align="center" valign="middle" >89.80</td><td align="center" valign="middle" >89.20</td></tr><tr><td align="center" valign="middle" >Lattice LSTM-CNN-CRF</td><td align="center" valign="middle" >92.82</td><td align="center" valign="middle" >92.10</td><td align="center" valign="middle" >92.56</td></tr></tbody></table></table-wrap><p>表1. 实验效果</p></sec></sec><sec id="s9"><title>5. 结束语</title><p>在本文中，首先我们随机初始化训练集中的每个字特征，并在获取该字典句子中每个字的特征之后，利用周期卷积来得到其固定长度的特征，以此作为句特征；随后训练数据自动编码器，通过栈式自动编码器得到高层句子的特征；最后通过高层句特征与字特征的组合训练字的标注网络模型来得到未知字的标注值，再进行实体扩展(分类，属性)，最后利用马尔科夫逻辑网络优化整体识别效果。</p></sec><sec id="s10"><title>基金项目</title><p>《工业过程数据实时获取与知识自动化》，国家自然科学基金委员会资助项目，项目编号：U17012621006336。</p></sec><sec id="s11"><title>文章引用</title><p>陈 娟,王卓薇,程良伦. 基于深度学习的命名实体识别算法Named Entity Recognition Algorithm Based on Deep Learning[J]. 计算机科学与应用, 2021, 11(03): 628-634. https://doi.org/10.12677/CSA.2021.113064</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41152-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Christian, B., Heath, T. and Berners-Lee, T. (2009) Linked Data—The Story So Far. International Journal on Semantic Web and Information Systems, 5, 1-22. &lt;br&gt;https://doi.org/10.4018/jswis.2009081901</mixed-citation></ref><ref id="hanspub.41152-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Bollacker, K., Cook, R. and Tufts, P. (2007) Freebase: A Shared Database of Structured General Human Knowledge. AAAI, 7, 1962-1963.</mixed-citation></ref><ref id="hanspub.41152-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Nadeau, D. and Sekine, S. (2007) A Survey of Named Entity Recognition and Classification. Lingvisticæ Investigationes, 30, 3-26. &lt;br&gt;https://doi.org/10.1075/li.30.1.03nad</mixed-citation></ref><ref id="hanspub.41152-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K. and Kuksa, P. (2011) Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12, 2493-2537.</mixed-citation></ref><ref id="hanspub.41152-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Huang, Z., Xu, W. and Yu, K. (2015) Bidirectional LSTM-CRF Models for Sequence Tagging. arXiv preprint arXiv:1508.01991.</mixed-citation></ref><ref id="hanspub.41152-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C. (2016) Neural Architectures for Named Entity Recognition. Proceedings of the 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-gies, San Diego, June 2016, 260-270.  
&lt;br&gt;https://doi.org/10.18653/v1/N16-1030</mixed-citation></ref><ref id="hanspub.41152-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Chiu, J.P. and Nichols, E. (2016) Named Entity Recognition with Bidi-rectional LSTM-CNNs. Transactions of the Association for Computational Linguistics, 4, 357-370. &lt;br&gt;https://doi.org/10.1162/tacl_a_00104</mixed-citation></ref><ref id="hanspub.41152-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Peters, M.E., Ammar, W., Bhagavatula, C. and Power, R. (2017) Semi-Supervised Sequence Tagging with Bidirectional Language Models. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 1, 1756-1765. &lt;br&gt;https://doi.org/10.18653/v1/P17-1161</mixed-citation></ref><ref id="hanspub.41152-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Patawar, M.L. and Potey, M. (2015) Approaches to Named Entity Recognition: A Survey. International Journal of Innovative Research in Computer and Communication Engineering, 3, 12201-12208.</mixed-citation></ref><ref id="hanspub.41152-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Saju, C.J. and Shaja, A. (2017) A Survey on Ef-ficient Extraction of Named Entities from New Domains Using Big Data Analytics. 2017 Second International Confer-ence on Recent Trends and Challenges in Computational Models (ICRTCCM), Tindivanam, 3-4 February 2017, 170-175. &lt;br&gt;https://doi.org/10.1109/ICRTCCM.2017.34</mixed-citation></ref><ref id="hanspub.41152-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Dai, X. (2018) Recognizing Complex Entity Mentions: A Review and Future Directions. Proceedings of ACL 2018, Student Research Workshop, Melbourne, July 2018, 37-44.  
&lt;br&gt;https://doi.org/10.18653/v1/P18-3006</mixed-citation></ref><ref id="hanspub.41152-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Yadav, V. and Bethard, S. (2018) A Survey on Recent Advances in Named Entity Recognition from Deep Learning Models. Proceedings of the 27th International Conference on Computa-tional Linguistics, Santa Fe, 20-26 August 2018, 2145-2158.</mixed-citation></ref><ref id="hanspub.41152-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Goyal, A., Gupta, V. and Kumar, M. (2018) Recent Named Entity Recognition and Classification Techniques: A Systematic Review. Computer Science Review, 29, 21-43. &lt;br&gt;https://doi.org/10.1016/j.cosrev.2018.06.001</mixed-citation></ref><ref id="hanspub.41152-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Rau, L.F. (1991) Extracting Company Names from Text. Pro-ceedings of the 7th IEEE Conference on Artificial Intelligence Applications, Miami Beach, 24-28 February 1991, 29-32.</mixed-citation></ref><ref id="hanspub.41152-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Grishman, R. and Sundheim, B. (1996) Message Understanding Conference-6: A Brief History. Proceed-ings of the 16th International Conference on Computational Linguistics, 1, 466-471. &lt;br&gt;https://doi.org/10.3115/992628.992709</mixed-citation></ref><ref id="hanspub.41152-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Chinchor, N.A. (1998) Overview of MUC-7/MET-2. Proceedings of the 7th Message Understanding Conference.</mixed-citation></ref><ref id="hanspub.41152-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">孙茂松, 黄昌宁, 高海燕, 等. 中文姓名的自动辨识[J]. 中文信息学报, 1995, 9(2): 16-27.</mixed-citation></ref><ref id="hanspub.41152-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">张小衡, 王玲玲. 中文机构名称的识别与分析[J]. 中文信息学报, 1997, 11(4): 21-32.</mixed-citation></ref><ref id="hanspub.41152-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Y. and Zhou, J.F. (2000) A Trainable Method for Extracting Chinese Entity Names and Their Rela-tions. In: Proceedings of the 2nd Chinese Language Processing Workshop, Hong Kong, October 2000, 66-76.  
&lt;br&gt;https://doi.org/10.3115/1117769.1117780</mixed-citation></ref><ref id="hanspub.41152-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Bikel, D.M., Schwarta, R. and Weischedel, R.M. (1999) An Algo-rithm that Learns What’s in a Name. Machine Learning, 34, 211-231. &lt;br&gt;https://doi.org/10.1023/A:1007558221122</mixed-citation></ref><ref id="hanspub.41152-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Liao, W. and Veeramachaneni, S. (2009) A Simple Semi-supervised Algorithm for Named Entity Recognition. Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, Boulder, June 2009, 58-65. &lt;br&gt;https://doi.org/10.3115/1621829.1621837</mixed-citation></ref><ref id="hanspub.41152-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Ratinov, L. and Roth, D. (2009) Design Challenges and Miscon-ceptions in Named Entity Recognition. Proceedings of the 13th Conference on Computational Natural Language Learn-ing, Boulder, 4-5 June 2009, 147-155.  
&lt;br&gt;https://doi.org/10.3115/1596374.1596399</mixed-citation></ref><ref id="hanspub.41152-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">冯元勇, 孙乐, 李文波, 等. 基于单字提示特征的中文命名实体识别快速算法[J]. 中文信息学报, 2008, 22(1): 105-110.</mixed-citation></ref><ref id="hanspub.41152-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">郑逢强, 林磊, 刘秉权, 等. 《知网》在命名实体识别中的应用研究[J]. 中文信息学报, 2008, 22(5): 97-101.</mixed-citation></ref><ref id="hanspub.41152-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Chen, X., Qiu, X., Zhu, C., Liu, P. and Huang, X. (2015) Long Short-Term Memory Neural Networks for Chinese Word Segmentation. Proceedings of the 2015 Confer-ence on Empirical Methods in Natural Language Processing, Lisbon, September 2015, 1197-1206. http://aclweb.org/anthology/D15-1141  
&lt;br&gt;https://doi.org/10.18653/v1/D15-1141</mixed-citation></ref><ref id="hanspub.41152-ref26"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Limsopatham, N. and Collier, N. (2016) Bidirectional LSTM for Named Entity Recognition in Twitter Messages. Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), Osaka, 11 December 2016, 145-152.</mixed-citation></ref><ref id="hanspub.41152-ref27"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Dong, C., Zhang, J., Zong, C., Hattori, M. and Di, H. (2016) Character-Based LSTM-CRF with Radical-Level Features for Chinese Named Entity Recognition. In: Lin, C.Y., Xue, N., Zhao, D., Huang, X. and Feng, Y., Eds., Natural Language Understanding and Intelligent Applications, Springer, Cham, 239-250.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-50496-4_20</mixed-citation></ref><ref id="hanspub.41152-ref28"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Y. and Yang, J. (2018) Chinese NER Using Lattice LSTM. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 1, 1554-1564. &lt;br&gt;https://doi.org/10.18653/v1/P18-1144</mixed-citation></ref><ref id="hanspub.41152-ref29"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">Ee, S. and Xiang, Y. (2017) Chinese Named Entity Recognition with Character-Word Mixed Embedding. Proceedings of the 2017 ACM on Conference on Information and Knowledge Man-agement, ACM, 2055-2058.</mixed-citation></ref></ref-list></back></article>