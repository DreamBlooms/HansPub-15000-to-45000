<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">HJDM</journal-id><journal-title-group><journal-title>Hans Journal of Data Mining</journal-title></journal-title-group><issn pub-type="epub">2163-145X</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/HJDM.2016.61010</article-id><article-id pub-id-type="publisher-id">HJDM-16872</article-id><article-categories><subj-group subj-group-type="heading"><subject>HJDM20160100000_26691799.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  面向移动WEB的微手势识别技术及系统实现
  The Implementation of the Micro-Gesture Recognition on the Mobile Web
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>美玉</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>嘉诚</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>伟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>简</surname><given-names>琤峰</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>浙江工业大学计算机学院，浙江 杭州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>14</day><month>01</month><year>2016</year></pub-date><volume>06</volume><issue>01</issue><fpage>81</fpage><lpage>89</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对面向移动终端的微手势识别需求，本文介绍了一种基于HTML5、Javascript和webRTC框架的移动web端的微手势识别系统。系统通过移动终端摄像头获取视频信息，并从视频中提取人手，再将手分为手掌域和手指域。其中手掌域用于定位手部位置，手指域通过肤色提取、区域分割、轮廓提取等操作提取手势中的手指信息。通过与手势预定义进行解析对比，从而响应相应的操作行为。 For the requirement of Micro-gesture recognition on mobile terminal, this paper introduces a Micro- gesture recognition system based on HTML5, Javascript and webRTC. It obtains video through the web camera, then extracts the hand from the video, and divides the hand into palm and fingers. The position of palm would indicate the location of whole hand, while the fingers would indicate the information of gesture after skin color extraction, region segmentation and contour extraction. Compared with the predefined command, the system would work according to the current com-mand as the gesture.
    
  
 
</p></abstract><kwd-group><kwd>移动WEB，webRTC，微手势识别, Web Client</kwd><kwd> webRTC</kwd><kwd> Micro-Gesture Recognition</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>面向移动WEB的微手势识别技术及系统实现<sup> </sup></title><p>张美玉，陈嘉诚，张伟，简琤峰</p><p>浙江工业大学计算机学院，浙江 杭州</p><disp-formula id="hanspub.16872-formula100"><graphic xlink:href="http://html.hanspub.org/file/10-1760086x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>收稿日期：2016年1月7日；录用日期：2016年1月26日；发布日期：2016年1月29日</p><disp-formula id="hanspub.16872-formula101"><graphic xlink:href="http://html.hanspub.org/file/10-1760086x7_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>针对面向移动终端的微手势识别需求，本文介绍了一种基于HTML5、Javascript和webRTC框架的移动web端的微手势识别系统。系统通过移动终端摄像头获取视频信息，并从视频中提取人手，再将手分为手掌域和手指域。其中手掌域用于定位手部位置，手指域通过肤色提取、区域分割、轮廓提取等操作提取手势中的手指信息。通过与手势预定义进行解析对比，从而响应相应的操作行为。</p><p>关键词 :移动WEB，webRTC，微手势识别</p><disp-formula id="hanspub.16872-formula102"><graphic xlink:href="http://html.hanspub.org/file/10-1760086x8_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s3"><title>1. 引言</title><p>移动硬件的普及，使得消费者在日常生活中所使用的应用渐渐集中到移动端上，从而移动互联网应用的开发逐渐成为趋势。而随着移动应用的发展，消费者已经不再满足于点按的交互方式。而当手势识别技术被应用到移动端时，成本大小和便捷与否就应该被优先考虑 [<xref ref-type="bibr" rid="hanspub.16872-ref1">1</xref>] - [<xref ref-type="bibr" rid="hanspub.16872-ref5">5</xref>] 。从数据的采集方式考虑，传统的采用数据手套等的方式由于成本较高以及使用时的不便，所以并不适用于移动平台。而Kinect等三维信息采集设备虽然具有极大的发展前景 [<xref ref-type="bibr" rid="hanspub.16872-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.16872-ref6">6</xref>] ，但就目前来说，并不适合用在移动平台上。由此，本文所介绍的系统只使用移动设备自带的网络摄像头获取视频信息。</p><p>当前市场上的移动平台并不统一，仅考虑身边最为常见的就有苹果和安卓这两大系统。这使得开发者有时需要针对不同系统开发不同版本的应用 [<xref ref-type="bibr" rid="hanspub.16872-ref7">7</xref>] 。所以，为了使系统具备跨平台工作的能力，本系统以移动WEB应用的方式进行实现。它将与运行的移动平台无关，切实解决平台局限性问题。</p><p>此外，现有手势识别往往会忽略了对于较小动作手势的识别，这也是本系统所重视的问题之一。</p></sec><sec id="s4"><title>2. 识别流程及关键技术</title><p>本文将介绍一种基于web的微手势识别系统。微手势识别的重点，在于对手指位置信息的确定。手指的外观形态随动作而变，而手掌的外观形态基本处于相似形状，故系统将手分为手指域和手掌域。系统对大量样本进行训练，得到基于Haar特征的级联分类器。这个分类器可以用来定位视频画面中的手掌位置，即定位到整个手势的位置。然后，系统获取手指的指尖和重心的位置，并通过追踪手指重心的前后位置差异，对手势进行解析。</p><p>基于移动web的微手势识别的流程如图1所示。首先通过网络摄像头获取手势图像；再使用已经训练完成的分类器，通过objectdetect.js对获取的图像进行对手掌区域的位置确定；然后根据手掌域位置的确定，获得手指域位置；接着根据YCrCb颜色空间的肤色模型 [<xref ref-type="bibr" rid="hanspub.16872-ref8">8</xref>] ，对图像进行二值化处理，而后再经过一定的形态学处理，去除一定噪声；最后对已分割出的图像，采取微手势识别算法，提取出其中的手掌与手指重心，解析手势信息。</p><sec id="s4_1"><title>2.1. 图像采集与定位</title><p>图像采集基于Google提出的WebRTC框架，它基于HTML5标准，使用Javascript API来实现对摄像头的调用和浏览器的无插件视频播放。目前的主流浏览器，如Chrome、Opera以及Firefox等的最新稳定版本都支持这一标准。</p><p>对于网络摄像头所获取的视频信息，系统利用 &lt;canvas&gt; 获取视频的每一帧图像，用来检测视频中的手势。而在对视频中的手势进行检测和定位时，系统采用开源项目objectdetect.js来实现。它主要采用基于Haar特征的级联分类器对目标进行检测。</p><p>这里，本文的目标图像为手掌图像，通过对手掌域的定位，获取手势的大致位置。在大致位置(兴趣</p><p>图1. 微手势识别的流程</p><p>区域ROI)的确定之后，后续的图像处理工作将被定在这块区域内。这样不仅排除了非手势区域的干扰信息，并且大大加快了处理速度，提高了系统的工作效率。</p></sec><sec id="s4_2"><title>2.2. 肤色分割</title><p>相对RGB，YCrCb颜色空间可以将色度和亮度分离，从而在肤色分割的过程中降低亮度的干扰。其中，Y表示亮度，Cr表示光源中红色分量，Cb表示光源中蓝色分量。RGB向YCrCb的转换公式如下：</p><disp-formula id="hanspub.16872-formula103"><graphic xlink:href="http://html.hanspub.org/file/10-1760086x10_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.16872-formula104"><graphic xlink:href="http://html.hanspub.org/file/10-1760086x11_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.16872-formula105"><graphic xlink:href="http://html.hanspub.org/file/10-1760086x12_hanspub.png"  xlink:type="simple"/></disp-formula><p>一般来说，根据某点Cr和Cb的值满足<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x13_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x14_hanspub.png" xlink:type="simple"/></inline-formula>，那么该点可以被判断为肤色点，否则不是。根据这个标准，肤色区域像素点的灰度值被设为255，非肤色区域像素点的灰度值被设为0，生成肤色二值图像。</p></sec><sec id="s4_3"><title>2.3. 手指识别</title><p>手指识别的作用在于对所得的手势图像进行解析，获得其所表达的手势信息。它是整个流程中最重要的部分。目前手势识别的方法很多，大致可分为基于模板匹配的方法和基于机器学习的方法。本系统的检测部分属于机器学习类算法。整个过程包括悬停判断，肤色区域编号，轮廓统计，重心相对位置计算，手势信息确定。其中，重心相对法利用手掌域重心，手指域重心的相对位置，判断手指相对于手掌的运动情况，再结合手指域重心的前后两者变化，从而解析出手势信息。</p></sec></sec><sec id="s5"><title>3. 手势识别算法的研究</title><p>本系统的重点在于对手指的识别与追踪，需要具有一定的高效、实时性。</p><sec id="s5_1"><title>3.1. 算法流程</title><p>算法流程图2所示。</p></sec><sec id="s5_2"><title>3.2. 算法实现</title><sec id="s5_2_1"><title>3.2.1. 获取手掌域重心</title><p>在处理过的肤色二值图像中，肤色像素的灰度被设置为255。这意味着手掌域内的肤色像素的(x, y)坐标点已经被确定，然后手掌域的重心可以根据下方的公式算出。</p><p>图2. 算法流程</p><disp-formula id="hanspub.16872-formula106"><graphic xlink:href="http://html.hanspub.org/file/10-1760086x16_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x17_hanspub.png" xlink:type="simple"/></inline-formula>为重心坐标，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x18_hanspub.png" xlink:type="simple"/></inline-formula>为肤色像素点的横坐标，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x19_hanspub.png" xlink:type="simple"/></inline-formula>为肤色像素点的纵坐标，N为肤色像素点的总数目。</p><p>3.2.2. 悬停判断</p><p>web端的手势识别，在追求高效、实时性的同时，会失去相应的准确性，为了达到效率与精度的相平衡，采用了一种悬停判断的方法。此方法的前提是手掌域的图像已经被识别，且手掌域重心已经被获取到。将此时手掌重心的坐标记为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x20_hanspub.png" xlink:type="simple"/></inline-formula>。随着时间推移，视频的每一帧的重心坐标可以组成一个序列，即<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x21_hanspub.png" xlink:type="simple"/></inline-formula>。若其中有30个坐标处于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x22_hanspub.png" xlink:type="simple"/></inline-formula>的邻域内，则该手势在这个位置保持了一定时间。它所代表的含义即为“选中，执行下一步操作”。</p><p>3.2.3.肤色区域编号</p><p>在经过肤色二值化操作后，肤色像素在图中呈区域的形态。为了有效地处理其中的肤色信息，系统对肤色像素区域进行编码，从中提取出所需要的区域，并排除不需要的区域。编码过程的大体准则为：首先初始化整个手势图像的值；接着按照从左到右、从上到下的方式对肤色像素区域进行编码。</p><p>(1) 初始化编号值</p><p>图像区域如图3所示：其中Total为整个图像区域；Pre为事先获得的手势图像区域；Ext为将手势图像区域边界延伸2个像素大小所得的图像区域。</p><p>图3. 区域示意图</p><p>图4. 编码初始化</p><p>图像初始化是将Ext区域内的所有像素点的编号值初始化为0，如图4所示：其中黑色边框为Pre区域，灰色框代表肤色像素点，白色框代表非肤色像素点</p><p>(2) 编号方法</p><p>如图所示，整个编码过程依照从左到右、从上到下的原则进行。假设肤色点的坐标为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x25_hanspub.png" xlink:type="simple"/></inline-formula>，码值为num。系统需要判定<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x26_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x27_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x28_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x29_hanspub.png" xlink:type="simple"/></inline-formula>这四个点是否已被更改编码：若四个点的码值都是0，则认定该像素点所处区域未被重新编码，然后将当前点的码值重新赋值为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x30_hanspub.png" xlink:type="simple"/></inline-formula>；若其中至少有一个点被重新编号，则当前点的值应被赋值为那个被重新编号的码值。根据以上原则，如图4中所示，首先遇到的肤色像素点为P点，当前num = 0。因为P点四个方向上的点编号值均为0，故P点的编号值应为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x31_hanspub.png" xlink:type="simple"/></inline-formula>，即为1。同理，对于图中Q点，系统需判定Q点的左前方、左上方、上方、友上方四个点是否被重新编号，由于P点为Q点的左前方点，又因为P点已被重新编号为1，那么Q点的编号值应与P点的编号值相同，即仍为1；最终，图4中肤色区域编码后的结果如图5。</p></sec><sec id="s5_2_2"><title>3.2.4. 轮廓统计</title><p>在不同区域的肤色被用不同的数字进行了编号之后，由于肤色分割存在一定缺陷，它会将与肤色相</p><p>近的颜色也误认为是肤色。所以在经过肤色二值化后，图像中仍会存在或大或小的干扰区域。而轮廓统计目的在于筛选出系统所需要的肤色轮廓。</p><p>此步骤首先对编号过的各个区域进行编号值统计，分别统计各个编号值的数量，通过数量上的大小，可以得知这个区域的大小。根据实验统计，手指轮廓的大小，所占目标区域的比例约为5%~15%。然后根据轮廓大小，系统就可以得到所感兴趣的肤色轮廓区域，并排除了过大或过小的干扰区域。</p></sec><sec id="s5_2_3"><title>3.2.5. 重心相对位置识别法</title><p>经过前面几个步骤，系统已经可以得出手指区域。与获取手掌域重心一样，系统采用相同的方法获取手指域重心坐标，并将重心点抽象出来，如图6所示。其中“状态一”指“手部运动开始前且通过悬浮判定成功获取重心坐标的状态”，“状态二”指“手部运动结束后且通过悬浮判定成功获取重心坐标的状态”。</p></sec><sec id="s5_2_4"><title>3.2.6. 算法具体实现方法</title><p>(1) 构建重心坐标三角形</p><p>过掌心A点做一条垂线L，过手指重心B点做L的垂线L'且两条线交于点C，得<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x32_hanspub.png" xlink:type="simple"/></inline-formula>，记A角度数为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x33_hanspub.png" xlink:type="simple"/></inline-formula>。同理可得<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x34_hanspub.png" xlink:type="simple"/></inline-formula>，记A'角度数为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x35_hanspub.png" xlink:type="simple"/></inline-formula>。如图7所示</p><p>(2) 计算手指长度</p><p>在手指域部分，假设手指区域纵坐标最小点为E，纵坐标最大点为F，分别过E、F点做水平线，记水平线间距为d，如图8所示。手指分为伸长和弯曲情况，弯曲情况下的手指长度大体为伸长情况下的二分之一。故根据这一点，假设状态一的手指长度为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x36_hanspub.png" xlink:type="simple"/></inline-formula>，状态二的手指长度为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x37_hanspub.png" xlink:type="simple"/></inline-formula>，max为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x38_hanspub.png" xlink:type="simple"/></inline-formula>、<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x39_hanspub.png" xlink:type="simple"/></inline-formula>中的较</p><p>图5. 编码结果</p><p>图6. 手部运动状态</p><p>图7. 手部运动状态形式化</p><p>图8. 手指弯曲</p><p>大者，设<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x45_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x46_hanspub.png" xlink:type="simple"/></inline-formula>，则认为手指前后发生了弯曲动作；若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x47_hanspub.png" xlink:type="simple"/></inline-formula>，则认为手指未进行弯曲动作，此时，系统需要从角度变化方面入手来解析手指的运动信息</p><p>(3) 计算角<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x48_hanspub.png" xlink:type="simple"/></inline-formula>、<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x49_hanspub.png" xlink:type="simple"/></inline-formula></p><p>通过事先构建的坐标三角形，系统可以利用三角信息计算出相应的夹角。假设A点坐标为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x50_hanspub.png" xlink:type="simple"/></inline-formula>，B点坐标为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x51_hanspub.png" xlink:type="simple"/></inline-formula>，C点坐标为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x52_hanspub.png" xlink:type="simple"/></inline-formula>，由此可得：</p><p><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x53_hanspub.png" xlink:type="simple"/></inline-formula>，同理可得<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x54_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x55_hanspub.png" xlink:type="simple"/></inline-formula>，则认为手指前后未进行左右摆动，并且事先已知手指前后长度未改变，此时可认为手部微小动作前后未发生改变。</p><p>当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x56_hanspub.png" xlink:type="simple"/></inline-formula>，若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x57_hanspub.png" xlink:type="simple"/></inline-formula>，则认为手指部分进行了向右的摆动运动；若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/10-1760086x58_hanspub.png" xlink:type="simple"/></inline-formula>，则认为手指部分进行了向左的摆动运动。</p><p>综上，系统得到了手指的4个运动情况：弯曲、保持、向左和向右。然后系统将手指的4个动作与手掌部重心坐标相结合起来，就可以解析出多种事先定义的手势信息。</p></sec></sec></sec><sec id="s6"><title>4. 系统示例</title><sec id="s6_1"><title>4.1. “复制”命令</title><p>当视频中手指做一次向左移动，系统解析为开启矩形复制功能，如图9。</p></sec><sec id="s6_2"><title>4.2. “移动”命令</title><p>当视频中手指做一次向右运动时，系统解析为开启拖动矩形的功能，然后矩形随手移动而移动，如图10。</p><p>图9. “复制”命令</p><p>图10. “移动”命令</p></sec></sec><sec id="s7"><title>5. 结束语</title><p>本系统通过网络摄像头获取手势视频，使用基于Haar特征的级联分类器定位手掌位置，再通过肤色分割、形态学处理等方法获取手部图像，然后根据手掌和手指的信息分析获得当前手势所代表的命令。系统从数据采集到获取命令的整个过程，都体现了低成本、高便捷性等特点，对于移动设备具有较好的适用性。</p></sec><sec id="s8"><title>基金项目</title><p>浙江省科技厅公益计划项目：支持移动WEB手势识别的草图协同创意设计平台(No.2014C31081)资助。</p></sec><sec id="s9"><title>文章引用</title><p>张美玉,陈嘉诚,张 伟,简琤峰. 面向移动WEB的微手势识别技术及系统实现 The Implementation of the Micro-Gesture Recognition on the Mobile Web[J]. 数据挖掘, 2016, 06(01): 81-89. http://dx.doi.org/10.12677/HJDM.2016.61010</p></sec><sec id="s10"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.16872-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Chen, L.C., Wang, F., Deng, H. and Ji, K.F. (2013) A Survey on Hand Gesture Recognition. International Conference on Computer Sciences and Applications, Wuhan, 14-15 December 2013, 14-15. &lt;br&gt;http://dx.doi.org/10.1109/csa.2013.79</mixed-citation></ref><ref id="hanspub.16872-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Panwar, M. (2012) Hand Gesture Recognition Based on Shape Para-meters. International Conference on Computing, Communication and Applications, Dindigul, 22-24 February 2012, 22-24.  
&lt;br&gt;http://dx.doi.org/10.1109/iccca.2012.6179213</mixed-citation></ref><ref id="hanspub.16872-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Yu, C.L., Wang, X., Huang, H.J., et al. (2010) Vision-Based Hand Gesture Recognition Using Combinational Features. 2010 Sixth International Conference on Intelligent Informa-tion Hiding and Multimedia Signal Processing, Darmstadt, 15-17 October 2010, 543-546. &lt;br&gt;http://dx.doi.org/10.1109/IIHMSP.2010.138</mixed-citation></ref><ref id="hanspub.16872-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Mei, K.Z., Xu, L., Li, B.L., et al. (2015) A Real-Time Hand Detection System Based on Multi-Feature. Neurocomputing, 158, 184-193. &lt;br&gt;http://dx.doi.org/10.1016/j.neucom.2015.01.049</mixed-citation></ref><ref id="hanspub.16872-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">张秋余, 王道东, 张墨逸, 刘景满. 基于特征包支持向量机的手势识别[J]. 计算机应用, 2012, 32(12): 3392-3396.</mixed-citation></ref><ref id="hanspub.16872-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Raheja, J.L., Chaudhary, A. and Singal, K. (2011) Tracking of Fingertips and Centers of Palm Using KINECT. 2011 Third International Conference on Computational Intelligence Modelling Simulation, Langkawi, 20-22 September 2011, 248-252. &lt;br&gt;http://dx.doi.org/10.1109/CIMSim.2011.51</mixed-citation></ref><ref id="hanspub.16872-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">赵健, 张冬泉. 基于OpenCV的数字手势识别算法[J]. 计算机应用, 2013, 33(S2): 193-196.</mixed-citation></ref><ref id="hanspub.16872-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">朱志亮, 刘富国, 陶向阳, 刘晓山. 基于积分图和粒子群优化的肤色分割[J]. 计算机工程与应用, 2014(21): 171- 174.</mixed-citation></ref></ref-list></back></article>