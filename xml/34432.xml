<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.103042</article-id><article-id pub-id-type="publisher-id">CSA-34432</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200300000_95416920.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  简单语义单元的语义修饰关系图模型及其求解
  Graph Model of the Descriptive Semantic Relationship for the Simple Semantic Units and Its Solution
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>运通</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>世佳</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>南阳理工学院计算机与信息工程学院，河南 南阳</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>28</day><month>02</month><year>2020</year></pub-date><volume>10</volume><issue>03</issue><fpage>408</fpage><lpage>417</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   为了有效地使用语义信息来进行自然语言处理，提出了一个简单语义单元的语义修饰关系图模型并对其求解方法进行了研究；首先提出了简单语义单元的概念，分析了简单语义单元的特点；在生成其语义修饰关系图的基础上，通过近似算法，求取语义修饰关系完全图的最大生成树；从而实现对简单语义单元的语法分析和语义消歧。实验表明，该方法具有一定的可行性。 In order to use semantics more effectively in natural language processing, a graph model of the descriptive semantic relationship for the simple semantic units was proposed and its solution was researched. Firstly the definition of the simple semantic unit was given and its characteristics were discussed. Then the graph of descriptive semantic relations could be created for a simple semantic unit, and the maximum spanning tree of the graph could be got by an approximate algorithm. And the simple semantic unit could be parsed and the word sense could be disambiguated. Finally experiments were finished and the results suggested that the method might be feasible. 
  
 
</p></abstract><kwd-group><kwd>语义相关度，简单语义单元，语义修饰关系图，最大生成树变量, Semantic Relevancy</kwd><kwd> Simple Semantic Units</kwd><kwd> Graph of the Descriptive Semantic Relations</kwd><kwd> The Variable of the Maximum Spanning Tree</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>简单语义单元的语义修饰关系图模型及其求解<sup> </sup></title><p>刘运通<sup>*</sup>，陈世佳</p><p>南阳理工学院计算机与信息工程学院，河南 南阳</p><p>收稿日期：2020年2月14日；录用日期：2020年3月2日；发布日期：2020年3月9日</p><disp-formula id="hanspub.34432-formula9"><graphic xlink:href="//html.hanspub.org/file/3-1541682x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>为了有效地使用语义信息来进行自然语言处理，提出了一个简单语义单元的语义修饰关系图模型并对其求解方法进行了研究；首先提出了简单语义单元的概念，分析了简单语义单元的特点；在生成其语义修饰关系图的基础上，通过近似算法，求取语义修饰关系完全图的最大生成树；从而实现对简单语义单元的语法分析和语义消歧。实验表明，该方法具有一定的可行性。</p><p>关键词 :语义相关度，简单语义单元，语义修饰关系图，最大生成树变量</p><disp-formula id="hanspub.34432-formula10"><graphic xlink:href="//html.hanspub.org/file/3-1541682x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-1541682x8_hanspub.png" /> <img src="//html.hanspub.org/file/3-1541682x9_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>当前，自然语言处理研究日益成为人工智能研究的热点，研究的主要技术手段是基于统计学或基于深度神经网络机器学习这两种技术，并取得了许多非常有价值的研究成就。但是，这两种方法没有有效地利用语句中的语义信息，没有语义分析计算环节，影响了其分析处理的效果。</p><p>为了在自然语言处理中更加有效地利用语义分析，取得更好的研究效果，不少学者在原有技术的基础上，通过引入语义分析计算的方法来进行自然语言处理的研究。陈谦研究了基于循环神经网络的自然语言语义表征方法，利用了注意力机制来关注篇章的特定区域、内容，同时设计了分散力机制去遍历篇章的不同内容，从而更好地为摘要获取篇章的全局信息 [<xref ref-type="bibr" rid="hanspub.34432-ref1">1</xref>]；彭敏等研究了一个基于双向LSTM语义强化的概率主题模型来实现文档宏观语义的嵌入表示，采用文档–主题和词汇–词汇双语义强化机制以及LSTM来刻画参数推断过程中的吉布斯采样过程，可以提高文本语义特征表达方面的有效性 [<xref ref-type="bibr" rid="hanspub.34432-ref2">2</xref>]；贾玉祥等研究了采用神经网络技术的语义选择知识获取模型，设计了引入预训练词向量的单隐层前馈网络和两层maxout网络，在汉语和英语的伪消歧实验中神经网络模型取得了较好的效果 [<xref ref-type="bibr" rid="hanspub.34432-ref3">3</xref>]；Mikolov等通过词向量模型工具Word2vec来描述语义信息，具有较好的语言学特性，可在很多应用和任务改善自然语言处理效果 [<xref ref-type="bibr" rid="hanspub.34432-ref4">4</xref>]；Mikilov等人对词向量的研究非常具有开创性，引起了围绕词向量训练的热潮。Word2vec基于词频和上下文建立语言模型和训练词向量，有研究者考虑用更复杂的方式来训练，利用统计出的语言学规律，给词向量赋予更多的语义信息 [<xref ref-type="bibr" rid="hanspub.34432-ref5">5</xref>]；Pennington等提出GloVe模型，综合运用词的上下文统计信息，用共现矩阵来生成对数线性语言模型和词向量表示 [<xref ref-type="bibr" rid="hanspub.34432-ref6">6</xref>]；孟凡擎等通过依存句法分析，获取语料上下文知识，构建上下文词汇语义消歧图，进行依存句法分析，完成歧义词的消歧处理 [<xref ref-type="bibr" rid="hanspub.34432-ref7">7</xref>]；唐善成等研究了采用Seq2Seq模型的非受限词义消歧方法，输入词上下文序列，经过编码器编码得到潜在语义向量，再经过解码器解码输出词义序列 [<xref ref-type="bibr" rid="hanspub.34432-ref8">8</xref>]；夏小强等通过将PV-DM模型作为句向量训练基本模型，添加关系信息知识约束条件，能够学习到文本中词语之间的关系，进一步整合语义关系约束信息，能够更好地表示文本 [<xref ref-type="bibr" rid="hanspub.34432-ref9">9</xref>]。</p><p>虽然这些研究已经获得了许多重要的学术成果，但迄今为止，并没有形成一个系统、有效的自然语言处理语义模型，其计算方法，依然有很大的改进余地。为了在自然语言处理更有效地使用语义信息，本文作者已经做了一些研究，可以通过语义计算，获取语句的语义修饰关系图 [<xref ref-type="bibr" rid="hanspub.34432-ref10">10</xref>]，在该文中，提出了简单语义单元的概念，但并没有得到很好地解决其语义修饰关系图的求解问题。为了解决这个问题，本文对简单语义单元的语义修饰关系图的求解方法进行了进一步的研究，设计近似算法，可以求出语义修饰关系图的最大生成树；实现了简单语义单元的语法、语义分析，并实现语义消歧。</p></sec><sec id="s4"><title>2. 理论基础</title><sec id="s4_1"><title>2.1. 简单语义单元的定义</title><p>假设有一个短文本L，其词汇序列为L = W<sub>m</sub> … W<sub>0</sub>，且W<sub>i</sub>是L中的一个实词，词汇W<sub>Gi</sub>是词汇W<sub>i</sub>的语义修饰目标，函数match(W<sub>i</sub>, W<sub>Gi</sub>)来表示W<sub>i</sub>和W<sub>Gi</sub>之间的语义相关度，假如L满足如下的语义特征，则称L为一个简单语义单元：</p><p>i) ∀ W i ( ( W i ∈ L ) ∩ ( W G i ∈ L ) )         ( if   i ≠ 0 ) ；</p><p>ii) L中不包含从句，也不包含谓语；</p><p>iii) L中不包含形式为W<sub>L1</sub> … W<sub>L2</sub>…，或… W<sub>L3</sub> …的虚词W<sub>L1</sub>、W<sub>L2</sub>、W<sub>L3</sub>；这些虚词具有连接、分段作用。</p></sec><sec id="s4_2"><title>2.2. 简单语义单元所具有的语义特征</title><p>通过简单语义单元的定义进行分析，可以知道简单语义单元具有如下的语义特征：</p><p>i) 除了最后一个词汇W<sub>0</sub>，任何词汇W<sub>i</sub>的语义修饰目标W<sub>Gi</sub>，都在简单语义单元内部；</p><p>ii) 简单语义单元内部，没有复杂的内部语法结构需要处理；</p><p>iii) 进行语法、语义分析时，可以把简单语义单元作为一个整体来处理，其内部语法、语义结构对语句其余部分的分析结果没有影响。</p></sec><sec id="s4_3"><title>2.3. 简单语义单元在自然语言处理中的作用</title><p>在自然语言处理过程中，可以把语句转化为多个简单语义单元，再进行处理，其方法如下：</p><sec id="s4_3_1"><title>2.3.1. 分割语句并获取简单语义单元</title><p>假设语句C<sub>S</sub>的语义、语义结构可以表示为：C<sub>S</sub>→LSLOLVL，并假定每个分段L是已经被分割成为“不包含从句”的程度，就可以根据分段L内各部分语义修饰目标的不同，将L划分为：前置定语、后置定语、状语(分别用A<sub>B</sub>、A<sub>A</sub>、P<sub>D</sub>表示)。就可以把L就转化为三个简单语义单元：A<sub>B </sub>+ S、A <sub>A</sub>+ S、P<sub>D </sub>+ V (见图1)。</p><p>图1. 把语句转化为简单语义单元的方法</p></sec><sec id="s4_3_2"><title>2.3.2. 含有分段、连接词的简单语义单元</title><p>经过2.3.1中方法的处理，假如某个简单语义单元中还包含有形式为W<sub>L1</sub> … W<sub>L2</sub> …，或… W<sub>L3</sub> …的虚词。则可以用如下的方法将其转化为多个简单语义单元。</p><p>假设简单语义单元内包含了形式为W<sub>L1</sub> … W<sub>L2</sub> …的虚词；那么，简单语义单元内就被划分出两个语义块B<sub>1</sub>、B<sub>2</sub>；语义块B<sub>1</sub>的范围显而易见，B<sub>2</sub>的后界限W<sub>i</sub>，可以通过式(1)来确定(见图2)：</p><p>match ( W , W i ) = max { match ( W , W 1 ) , match ( W , W 2 ) , ⋯ , match ( W , W i ) , ⋯ , match ( W , W p ) } (1)</p><p>其中的W<sub>1</sub>~W<sub>p</sub>与W的词性相同。</p><p>图2. 分段功能的虚词的处理方法</p><p>先将B<sub>1</sub>、B<sub>2</sub>当作两个简单语义单元处理；然后，再把W<sub>L1</sub> B<sub>1</sub>W<sub>L2</sub> B<sub>2</sub>的当作一个整体，用词汇W来替代即可。</p></sec></sec><sec id="s4_4"><title>2.4. 简单语义单元的语义修饰关系图模型</title><sec id="s4_4_1"><title>2.4.1. 简单语义单元的语义修饰关系图</title><p>自然语言语句中的词汇，大多数都是多义词，对简单语义单元来说，只要构造出它的语义修饰关系图，就求出了它的语法、语义分析结果，并实现了语义消歧。</p><p>假设简单语义单元的词汇序列为 L = W m ⋯ W i ⋯ W ⋯ W p ⋯ W ⋯ W 0 。L有许多不同的语法分析方案 { A n ⋯ A t ⋯ A 0 } ，图3描述了它的第t语法分析方案A<sub>t</sub>，多义词W<sub>i</sub>的第j个义项是 W i j ，多义词W<sub>p</sub>的第q个义项是 W p q ，并且义项 W i j 语义修饰于义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x19_hanspub.png" xlink:type="simple"/></inline-formula>；则在图中的义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x20_hanspub.png" xlink:type="simple"/></inline-formula>和义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x21_hanspub.png" xlink:type="simple"/></inline-formula>之间，画一条有向边，该边的权值等于<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x22_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>图3. 简单语义单元的语义修饰关系图</p><p>在图3中，每个词的所有义项构成一个义项集合，每个义项都是其中的一个元素；每个词的义项集合都可以被看成一个“广义顶点”，每个义项都可以被看成一个“义项顶点”，则有如下特点：</p><p>i) 最后的词汇W<sub>0</sub>只具有一个义项，不存在语义修饰目标，其义项可以通过由语句级别处理过程确定；</p><p>ii) 每个“广义顶点”只具有一个语义修饰目标，集合的总出度是1；</p><p>iii) 每种语法分析方案，本质上都是一个语义修饰关系图，该图必然是“广义顶点完全图”的一颗“生成树”；</p><p>iv) 在任意“义项集合”中，其“生成树”的每条边，必定关联于同一个“义项顶点”。</p></sec><sec id="s4_4_2"><title>2.4.2. 图语义修饰关系图最佳解的求解原理</title><p>自然语言语句中的词汇，大多数都是多义词，对简单语义单元来说，只要构造出它的语义修饰关系图，就求出了它的语法、语义分析结果，并实现了语义消歧。</p><p>假设某个简单语义单元，具有n种不同的语法分析方案<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x24_hanspub.png" xlink:type="simple"/></inline-formula>，在它的第t语法分析方案A<sub>t</sub>中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x25_hanspub.png" xlink:type="simple"/></inline-formula>是W<sub>i</sub>的第j个义项，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x26_hanspub.png" xlink:type="simple"/></inline-formula>是<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x27_hanspub.png" xlink:type="simple"/></inline-formula>的语义修饰目标义项，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x28_hanspub.png" xlink:type="simple"/></inline-formula>是<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x29_hanspub.png" xlink:type="simple"/></inline-formula>的语义修饰目标义项；用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x30_hanspub.png" xlink:type="simple"/></inline-formula>表示它们之间的语义相关度，则针对语法分析方案A<sub>t</sub>，总的语义相关度f<sub>At</sub>可以用式(2)来计算：</p><disp-formula id="hanspub.34432-formula11"><label>(2)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/3-1541682x31_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x32_hanspub.png" xlink:type="simple"/></inline-formula>用来表示义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x33_hanspub.png" xlink:type="simple"/></inline-formula>、<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x34_hanspub.png" xlink:type="simple"/></inline-formula>之间的语义相关度，针对一个词汇语义库(本文使用hownet词汇语义库)，可用式(3)计算。</p><disp-formula id="hanspub.34432-formula12"><label>(3)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/3-1541682x35_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x36_hanspub.png" xlink:type="simple"/></inline-formula>表示义原之间的最大语义相似度，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x37_hanspub.png" xlink:type="simple"/></inline-formula>表示义原之间的最大语义关联度，且系数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x38_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>最佳解选择规则：在简单语义单元的n种不同的语法、语义分析方案<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x39_hanspub.png" xlink:type="simple"/></inline-formula>中，满足式(4)中条件的语法、语义分析方案A<sub>i</sub>就是最佳方案：</p><disp-formula id="hanspub.34432-formula13"><label>(4)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/3-1541682x40_hanspub.png"  xlink:type="simple"/></disp-formula><p>最佳语法分析方案的语义修饰关系图是简单语义单元“广义顶点完全图”的“最大生成树”。</p><p>值得注意的是：代词、副词、量词、数词被按照实词计算，虚词不参与计算。</p></sec></sec></sec><sec id="s5"><title>3. 语义修饰关系图模型的近似求解</title><p>假定简单语义单元具有m个词汇，一个词汇平均具有t个义项。假如采用穷举法来求取最优解，其计算复杂度将是<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x41_hanspub.png" xlink:type="simple"/></inline-formula>，属于指数级别时间复杂度，实际上不可行。</p><p>为此，我们提出了一种近似算法，可以降低求解的计算复杂度，较快地求出近似解。</p><sec id="s5_1"><title>3.1. 最大生成树变量</title><p>采用动态规划方法，可以求出模型的近似解。先定义一个“最大生成树变量”(见图4)，其含义为：针对词汇W<sub>i</sub>，其第j个义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x42_hanspub.png" xlink:type="simple"/></inline-formula>有多个生成树，其中各边权值之和最大的生成树，用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x43_hanspub.png" xlink:type="simple"/></inline-formula>表示。</p></sec><sec id="s5_2"><title>3.2. 求最大生成树的近似算法</title><p>计算过程中的所涉及的两个变量：</p><p>集合V = {已经计算出最大的生成树M<sub>T</sub>的“广义顶点”}</p><p>目标词汇W<sub>i</sub>：即将被加入V的下一个词汇；</p><p>图4. 简单语义单元的最大生成树变量</p><p>动态规划算法(算法1)：</p><p>步骤1：设定V的初始值</p><p>设置<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x45_hanspub.png" xlink:type="simple"/></inline-formula>，则很容易算出：</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x46_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x47_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>其中Num是词汇的义项个数，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x48_hanspub.png" xlink:type="simple"/></inline-formula>表示义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x49_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x50_hanspub.png" xlink:type="simple"/></inline-formula>之间的边。</p><p>步骤2：计算目标词汇W<sub>i</sub>的最大生成树变量</p><p>针对W<sub>i</sub>，其每个义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x51_hanspub.png" xlink:type="simple"/></inline-formula>与V中的任意义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x52_hanspub.png" xlink:type="simple"/></inline-formula>之间，都可以计算出一个语义相关度，按照式(5)，可以计算出<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x53_hanspub.png" xlink:type="simple"/></inline-formula>的最大生成树变量(见图5)。</p><disp-formula id="hanspub.34432-formula14"><label>(5)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/3-1541682x54_hanspub.png"  xlink:type="simple"/></disp-formula><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x55_hanspub.png" xlink:type="simple"/></inline-formula>表示义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x56_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x57_hanspub.png" xlink:type="simple"/></inline-formula>之间的边。</p><p>图5. 计算义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x60_hanspub.png" xlink:type="simple"/></inline-formula>的最大生成树变量</p><p>步骤3：对所有的<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x61_hanspub.png" xlink:type="simple"/></inline-formula>进行逆排序</p><p>在步骤2的计算过程中，可以计算出所有的<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x62_hanspub.png" xlink:type="simple"/></inline-formula>的值，按由大到小的顺序，将其保存到线性表List中。</p><p>步骤4：更新V中义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x63_hanspub.png" xlink:type="simple"/></inline-formula>的最大生成树变量</p><p>如图6所示，已知V中义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x64_hanspub.png" xlink:type="simple"/></inline-formula>的最大生成树，可定义变化函数f：</p><disp-formula id="hanspub.34432-formula15"><label>(6)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/3-1541682x65_hanspub.png"  xlink:type="simple"/></disp-formula><p>其含义为：假设把最大生成树进行一个义项改动，由义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x66_hanspub.png" xlink:type="simple"/></inline-formula>改为义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x67_hanspub.png" xlink:type="simple"/></inline-formula>，其权值之和的变化量。</p><p>假定边<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x68_hanspub.png" xlink:type="simple"/></inline-formula>所关联的顶点为<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x69_hanspub.png" xlink:type="simple"/></inline-formula>，把最大生成树上的义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x70_hanspub.png" xlink:type="simple"/></inline-formula>改为义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x71_hanspub.png" xlink:type="simple"/></inline-formula>；则其变化函数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x72_hanspub.png" xlink:type="simple"/></inline-formula>的值必然小于0。</p><p>假如按照由大到小的顺序，处理List中的所有边<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x73_hanspub.png" xlink:type="simple"/></inline-formula>，有以下两种情况：</p><p>i)<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x74_hanspub.png" xlink:type="simple"/></inline-formula>位于最大生成树上，则List中后续的任何边都不可能是最大生成树的边，因此就无需处理了。</p><p>ii)<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x75_hanspub.png" xlink:type="simple"/></inline-formula>不位于最大生成树上，则按照下属算法2求取义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x76_hanspub.png" xlink:type="simple"/></inline-formula>的最大生成树。</p><p>算法2：</p><p>----------------------------</p><p>For all <inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x77_hanspub.png" xlink:type="simple"/></inline-formula> in List</p><p>{</p><p>If (<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x78_hanspub.png" xlink:type="simple"/></inline-formula>不在最大生成树上)</p><p>{</p><p>计算变化函数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x79_hanspub.png" xlink:type="simple"/></inline-formula>的值；</p><p>(A) 找到<img src="//html.hanspub.org/file/3-1541682x80_hanspub.png" />所指的下一个顶点(<img src="//html.hanspub.org/file/3-1541682x81_hanspub.png" />)</p><p>{</p><p>针对<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x82_hanspub.png" xlink:type="simple"/></inline-formula>的所有义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x83_hanspub.png" xlink:type="simple"/></inline-formula>，计算变化函数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x84_hanspub.png" xlink:type="simple"/></inline-formula>的值；</p><p>选取能够使得<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x85_hanspub.png" xlink:type="simple"/></inline-formula>最大的义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x86_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>将最大生成树的上的义项由<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x87_hanspub.png" xlink:type="simple"/></inline-formula>修改为<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x88_hanspub.png" xlink:type="simple"/></inline-formula>；</p><p>}</p><p>返回(A)，循环处理至最后一个顶点；</p><p>按下式计算总变化量：</p><p><inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x89_hanspub.png" xlink:type="simple"/></inline-formula>，即对所有的变化函数求和，并加上边<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x90_hanspub.png" xlink:type="simple"/></inline-formula>的值。</p><p>}</p><p>}</p><p>选择能够使总变化量最大的边<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x91_hanspub.png" xlink:type="simple"/></inline-formula>，将其修改为最大生成树的边。</p><p>---------------------------</p><p>图6. 更新义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x94_hanspub.png" xlink:type="simple"/></inline-formula>的最大生成树变量</p><p>步骤5：把词汇W<sub>i</sub>加入集合V</p><p>针对V中的所有义项，更新其最大生成树变量，完成后把W<sub>i</sub>加入集合V。</p><p>步骤6：循环</p><p>返回步骤2，循环处理，当所有顶点都加入集合V后，选出词汇W<sub>m</sub>的最大生成树，就是所求的近似解。</p></sec><sec id="s5_3"><title>3.3. 计算结果分析</title><p>在算法1的步骤4中，使用了贪心法，只选择能够使得<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x95_hanspub.png" xlink:type="simple"/></inline-formula>最大的义项<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x96_hanspub.png" xlink:type="simple"/></inline-formula>进行计算，而没有对所有<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x97_hanspub.png" xlink:type="simple"/></inline-formula>的所有义项进行穷举；故此所求的解不是“理论最优解”，只是近似解，其时间复杂度约为<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/3-1541682x98_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec></sec><sec id="s6"><title>4. 实验结果及分析</title><p>实验中，以hownet作为语义计算的依据。在人民日报2014语料库中选取了1200个语句进行求解实验，先将它们转化为4729个简单语义单元，通过算法进行求解，并与正确结果进行对比。实验环境为：windows 10，i5 9400F六核，主频2.9 GHz，32 G内存。</p><p>实验情况见表1、图7、图8，其中正确率标准有两个：</p><p>1) 语义消歧正确率大于阀值0.5 (义项选择正确就是语义消歧正确)；</p><p>2) 语法分析正确率大于阀值0.8 (义项的语义修饰目标正确就是语法分析正确)。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The resulting data of experimen</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >长度</th><th align="center" valign="middle" >个数</th><th align="center" valign="middle" >平均歧义个数</th><th align="center" valign="middle" >穷举法计算时间(ms)</th><th align="center" valign="middle" >穷举法正确率</th><th align="center" valign="middle" >近似算法计算时间(ms)</th><th align="center" valign="middle" >近似算法正确率</th></tr></thead><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >697</td><td align="center" valign="middle" >3.17</td><td align="center" valign="middle" >1848</td><td align="center" valign="middle" >87.7%</td><td align="center" valign="middle" >1838</td><td align="center" valign="middle" >79.2%</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >887</td><td align="center" valign="middle" >3.86</td><td align="center" valign="middle" >10,977</td><td align="center" valign="middle" >83.6%</td><td align="center" valign="middle" >2820</td><td align="center" valign="middle" >79.8%</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >1153</td><td align="center" valign="middle" >3.90</td><td align="center" valign="middle" >97,017</td><td align="center" valign="middle" >79.6%</td><td align="center" valign="middle" >5713</td><td align="center" valign="middle" >74.4%</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >769</td><td align="center" valign="middle" >3.59</td><td align="center" valign="middle" >370,284</td><td align="center" valign="middle" >68.1%</td><td align="center" valign="middle" >7303</td><td align="center" valign="middle" >60.8%</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >447</td><td align="center" valign="middle" >2.95</td><td align="center" valign="middle" >695,010</td><td align="center" valign="middle" >58.9%</td><td align="center" valign="middle" >7420</td><td align="center" valign="middle" >53.2%</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >349</td><td align="center" valign="middle" >2.47</td><td align="center" valign="middle" >815,967</td><td align="center" valign="middle" >49.9%</td><td align="center" valign="middle" >7289</td><td align="center" valign="middle" >47.6%</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >194</td><td align="center" valign="middle" >2.19</td><td align="center" valign="middle" >993,451</td><td align="center" valign="middle" >46.9%</td><td align="center" valign="middle" >7871</td><td align="center" valign="middle" >40.0%</td></tr><tr><td align="center" valign="middle" >&gt;8</td><td align="center" valign="middle" >233</td><td align="center" valign="middle" >1.81</td><td align="center" valign="middle" >1,749,055</td><td align="center" valign="middle" >41.8%</td><td align="center" valign="middle" >8707</td><td align="center" valign="middle" >34.5%</td></tr></tbody></table></table-wrap><p>表1. 实验数据表</p><p>图7. 正确率对比图</p><p>图8. 穷举法时间/近似算法时间</p><p>从实验结果可以看出：</p><p>1) 两种算法均可进行语法分析和语义消歧；</p><p>2) 穷举法的正确率略高于近似算法；</p><p>3) 正确率随着简单语义单元长度的增加有逐渐下降的趋势；</p><p>4) 当简单语义单元长度增加时，近似算法所需时间，在可接受的范围内，属于实践可行的算法，而穷举法计算所需的时间急剧增加，实践上不可行。</p></sec><sec id="s7"><title>5. 结语</title><p>本文对简单语义单元的语义修饰关系图模型及其求解方法进行了研究；通过把语句转化为简单语义单元，生成其语义修饰关系图，通过近似算法对模型进行求解；可以实现对简单语义单元的语法分析和语义消歧；并通过实验，验证了本方法的计算效果。由于问题自身难度所限，本文没有能够找具有“非指数级别计算复杂度”的求取最佳理论解的方法。另外，在语义计算时，由于所依据的hownet词汇语义库的准确度、细致程度等有很多欠缺和不足，使得语义相关度计算结果不够准确；这也是计算效果不够理想的关键因素。这些问题，将是我们下一步的研究的目标。</p></sec><sec id="s8"><title>基金项目</title><p>河南省教育厅科学技术研究重点项目(18A520002)。</p></sec><sec id="s9"><title>文章引用</title><p>刘运通,陈世佳. 简单语义单元的语义修饰关系图模型及其求解Graph Model of the Descriptive Semantic Relationship for the Simple Semantic Units and Its Solution[J]. 计算机科学与应用, 2020, 10(03): 408-417. https://doi.org/10.12677/CSA.2020.103042</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.34432-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">陈谦. 基于神经网络的自然语言语义表征方法研究[D]: [博士学位论文].合肥: 中国科学技术大学, 2018.</mixed-citation></ref><ref id="hanspub.34432-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">彭敏, 杨绍雄, 朱佳晖. 基于双向LSTM语义强化的主题建模[J]. 中文信息学报, 2018, 32(4): 40-49.</mixed-citation></ref><ref id="hanspub.34432-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">贾玉祥, 许鸿飞, 昝红英. 基于神经网络的语义选择限制知识自动获取[J]. 中文信息学报, 2017, 31(1): 155-161.</mixed-citation></ref><ref id="hanspub.34432-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Mikolov, T., Chen, K., Corrado, G., et al. (2013) Efficient Estimation of Word Representations in Vector Space.  
http://cn.bing.com/academic/profile?id=6b535fdb1fad7c57b8421d3a81b00af5&amp;encoded=0&amp;v=paper_preview&amp;mkt=zh-cn</mixed-citation></ref><ref id="hanspub.34432-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Wan, S., Lan, Y., Guo, J., et al. (2016) A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations.  
&lt;br&gt;https://www.researchgate.net/publication/285271115_A_Deep_Architecture_for_Semantic 
_Matching_with_Multiple_Positional_Sentence_Representations</mixed-citation></ref><ref id="hanspub.34432-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Pennington, J., Socher, R. and Manning, C. (2014) Glove: Global Vectors for Word Representation. Proceedings  
of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25-29 October 2014, 1532-1543.  
&lt;br&gt;https://doi.org/10.3115/v1/D14-1162</mixed-citation></ref><ref id="hanspub.34432-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">孟凡擎, 鹿文鹏, 张旭, 等. 基于HowNet的图模型词义消歧方法[J]. 齐鲁工业大学学报, 2018, 32(6): 69-76.</mixed-citation></ref><ref id="hanspub.34432-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">唐善成, 马付玉, 张镤月, 等. 采用Seq2Seq模型的非受限词义消歧方法[J]. 西北大学学报(自然科学版), 2019, 49(3): 351-355.</mixed-citation></ref><ref id="hanspub.34432-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">夏小强, 邵堃. 基于语义关系约束和词语关系信息的句向量研究[J]. 计算机应用研究, 2019, 36(7): 2023-2026.</mixed-citation></ref><ref id="hanspub.34432-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Liu, Y.T. and Sun, H. (2015) Word Sense Disambiguation for Chinese Based on Semantics Calculation. Mathematical Problems in Engineering, 2015, Article ID: 235096. &lt;br&gt;https://doi.org/10.1155/2015/235096</mixed-citation></ref></ref-list></back></article>