<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AIRR</journal-id><journal-title-group><journal-title>Artificial Intelligence and Robotics Research</journal-title></journal-title-group><issn pub-type="epub">2326-3415</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AIRR.2021.101003</article-id><article-id pub-id-type="publisher-id">AIRR-40397</article-id><article-categories><subj-group subj-group-type="heading"><subject>AIRR20210100000_41543281.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject><subject> 工程技术</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于视觉的多自主移动机器人动态编队技术研究
  Research on Dynamic Formation Technology of Multiple Autonomous Mobile Robots Based on Vision
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>云龙</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>清珍</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>郑州科技学院，河南 郑州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>29</day><month>01</month><year>2021</year></pub-date><volume>10</volume><issue>01</issue><fpage>29</fpage><lpage>36</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  目的：针对多自主移动机器人动态编队过程中，机器人产生的位置和角度误差问题，提出一整套基于视觉相机的空间坐标计算方法。方法：首先搭建出一套相机测距定位系统，介绍系统中各部分的构成及对应的职能；其次，构建出三位空间坐标系，详细介绍坐标系中关键数据的测算过程和计算公式；最后，搭建出含有多自主移动机器人的编队系统，对上述坐标角度计算方法进行测试和验证。结果：实验结果表明，机器人坐标角度的计算符合设计预期，良好地实现了整个编队过程。结论：基于视觉的多自主移动机器人动态编队技术符合工程应用要求。
   The work aims to propose a set of spatial coordinate calculation methods based on vision cameras to solve the problem of position and angle errors generated by the robots during the dynamic for-mation of multiple autonomous mobile robots. Firstly, a set of camera ranging and positioning sys-tem is built to introduce the composition and corresponding functions of each part in the system. Secondly, a three-dimensional space coordinate system is constructed, and the measurement pro- cess and calculation formula of key data in the coordinate system are introduced in detail. Finally, a formation system with multiple autonomous mobile robots is built to test and verify the above cal-culation method. The experimental results show that the calculation results of the robot coordi-nates meet the design expectations, and the entire formation process is well realized. It may be concluded that the dynamic formation technology of multiple autonomous mobile robots satisfies the requirements on engineering applications.
 
</p></abstract><kwd-group><kwd>坐标系，定位，机器人，动态编队, Coordinate System</kwd><kwd> Positioning</kwd><kwd> Robot</kwd><kwd> Dynamic Formation</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>目的：针对多自主移动机器人动态编队过程中，机器人产生的位置和角度误差问题，提出一整套基于视觉相机的空间坐标计算方法。方法：首先搭建出一套相机测距定位系统，介绍系统中各部分的构成及对应的职能；其次，构建出三位空间坐标系，详细介绍坐标系中关键数据的测算过程和计算公式；最后，搭建出含有多自主移动机器人的编队系统，对上述坐标角度计算方法进行测试和验证。结果：实验结果表明，机器人坐标角度的计算符合设计预期，良好地实现了整个编队过程。结论：基于视觉的多自主移动机器人动态编队技术符合工程应用要求。</p></sec><sec id="s2"><title>关键词</title><p>坐标系，定位，机器人，动态编队</p></sec><sec id="s3"><title>Research on Dynamic Formation Technology of Multiple Autonomous Mobile Robots Based on Vision<sup> </sup></title><p>Yunlong Zhang, Qingzhen Wang</p><p>Zhengzhou University of Science and Technology, Zhengzhou Henan</p><p><img src="//html.hanspub.org/file/3-2610219x4_hanspub.png" /></p><p>Received: Jan. 15<sup>th</sup>, 2021; accepted: Jan. 29<sup>th</sup>, 2021; published: Feb. 19<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/3-2610219x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>The work aims to propose a set of spatial coordinate calculation methods based on vision cameras to solve the problem of position and angle errors generated by the robots during the dynamic formation of multiple autonomous mobile robots. Firstly, a set of camera ranging and positioning system is built to introduce the composition and corresponding functions of each part in the system. Secondly, a three-dimensional space coordinate system is constructed, and the measurement pro- cess and calculation formula of key data in the coordinate system are introduced in detail. Finally, a formation system with multiple autonomous mobile robots is built to test and verify the above calculation method. The experimental results show that the calculation results of the robot coordinates meet the design expectations, and the entire formation process is well realized. It may be concluded that the dynamic formation technology of multiple autonomous mobile robots satisfies the requirements on engineering applications.</p><p>Keywords:Coordinate System, Positioning, Robot, Dynamic Formation</p><disp-formula id="hanspub.40397-formula35"><graphic xlink:href="//html.hanspub.org/file/3-2610219x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-2610219x7_hanspub.png" /> <img src="//html.hanspub.org/file/3-2610219x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>工业生产中，如何利用机器人完成定位和抓取一直是一个热门话题 [<xref ref-type="bibr" rid="hanspub.40397-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.40397-ref2">2</xref>]。倪鹤鹏等人提出基于时间与物品位置的图像去重复算法，旨在判断去除重复图像信息 [<xref ref-type="bibr" rid="hanspub.40397-ref3">3</xref>]；陈海永等人提出定位起始点和判断位置的视觉控制算法，在保持较好鲁棒性的前提下实现精确定位控制 [<xref ref-type="bibr" rid="hanspub.40397-ref4">4</xref>]。上述文献在研究过程中，解决了图像提取、处理工作，实现了精准定位的功能。这些功能技术不仅仅可以适用于工业中的机器人抓取动作上，还能够在多机器人动态编队中发挥一定的价值。</p><p>多机器人系统主要研究由多个机器人组成的队伍通过相互协调配合，完成单一机器人难以实现的复杂操作，因而其适用范围更为广泛 [<xref ref-type="bibr" rid="hanspub.40397-ref5">5</xref>]。原魁等人介绍了在机器人编队系统中的编队控制问题 [<xref ref-type="bibr" rid="hanspub.40397-ref6">6</xref>]。Balch T等人提出了基于行为的编队控制算法 [<xref ref-type="bibr" rid="hanspub.40397-ref7">7</xref>]；杨丽等人提出了未知环境下的多机器人动态编队控制方法，将全局控制问题转化为各个机器人的自我调整问题 [<xref ref-type="bibr" rid="hanspub.40397-ref8">8</xref>]。</p><p>在多机器人动态编队过程中，由于机器人是存在于空间中的三维物体，因此许多工业中的视觉定位技术在应用时无法直接套用。为了解决此问题，本文在前人研究基础上设计出能基于视觉的多自主移动机器人动态编队定位技术。通过创建视觉三维坐标系，仅需将相机捕捉到的关键点数据发给云端进行处理计算，就能获得目标机器人的中心点坐标和面朝角度。将相应数据与理想路径数据进行对比，得到位置及角度误差，并通过基站控制进行误差调整，使整个动态编队过程表现出较好的鲁棒性。</p></sec><sec id="s6"><title>2. 相机测距定位系统的搭建</title><p>整个系统包含有MCU、智能相机和多个参与编队的自主移动机器人，如图1所示。</p><p>智能相机拍射范围可网络化为5 &#215; 6的阵列，机器人放置在网络的中心位置。其中在编队时每个机器人按照匀速v的速度运动；机器人运动过程中首先经过智能相机，根据本文设计的算法，允许智能相机自上而下正对机器人运动区域，也允许智能相机位于机器人运动区域侧方，以倾斜的角度拍摄记录区域内的机器人位置情况；数据传输至MCU后，MCU根据捕捉到的相应数据算出目标运动机器人中心点M的坐标 ( x M , y M , z M ) 、机器人在编队过程中产生的摆放角度误差 θ e ，最后根据计算得到的最终应在的目标位置坐标。</p><p>图1. 相机测距定位系统</p></sec><sec id="s7"><title>3. 视觉坐标系的创建与定位算法</title><p>为描述清楚相机视觉下的空间坐标系，构建OXYZ三维坐标系，如图2所示。细节描述如下：以相机为点S，坐标为 ( 0 , 0 , h 1 ) ， h 1 为已知相机的悬挂高度；记S在OXY平面上的投影为点O，坐标为 ( 0 , 0 , 0 ) ；以机器人运动方向为X轴，构建右手坐标系下的Y轴和Z轴。</p><p>图2. 相机视觉下三维空间坐标系</p><sec id="s7_1"><title>3.1. 相机拍摄所得数据分析</title><p>当目标移动机器人编队时，由相机进行拍摄抓取。假设机器人为立方体，共8个顶点，相机可抓取得到距离点O最近的顶点A，而点A位于OXY平面上；同时可抓取到得到距离O点最远的顶点C在相机视角方向上位于OXY平面上的投影C'。</p><p>已知相机的成像比例为k，由相机拍摄结果可以获得向量 O A 的长度 l 1 及偏转距离 θ 1 ，同时可以获得向量 O C ′ 的长度 l 2 及偏转距离 θ 2 。各个数据的对应关系已在图2中标记表示。</p></sec><sec id="s7_2"><title>3.2. 相机数据处理过程</title><p>此刻向量 O A 于空间坐标系下可表示为：</p><p>( l 1 ∗ cos θ 1 , l 1 ∗ sin θ 1 , 0 ) (1)</p><p>向量 O C 于空间坐标系下可表示为：</p><p>( l 2 ∗ cos θ 2 , l 2 ∗ sin θ 2 , 0 ) (2)</p><p>由图2可知，为求得目标移动机器人的中心点M的坐标 ( x M , y M , z M ) 及角度误差 θ e ，关键在于求得顶点B坐标，即向量 O B 。</p><p>分析点O、点S、点C、点C'、点B之间的关系，如图3所示。在实际多自主移动机器人编队过程中，目标移动机器人的高度已知为 h 2 ，因此可由图3获得向量 O B 的计算方法为</p><p>图3. 空间坐标系中部分关键点间关系图</p><p>从而向量 O B 于空间坐标系下的长度关系可表示为：</p><p>C ′ B C ′ O = BC OS (3)</p><p>向量 O B 可表示为：</p><p>( h 1 − h 2 h 1 ∗ l 2 ∗ cos θ 2 , h 1 − h 2 h 1 ∗ l 2 ∗ sin θ 2 , 0 ) (4)</p><p>结合向量 O A ，求得目标物体中心点M坐标为：</p><p>( 1 2 ∗ ( l 1 ∗ cos θ 1 + h 1 − h 2 h 1 ∗ l 2 ∗ cos θ 2 ) , 1 2 ∗ ( l 1 ∗ sin θ 1 + h 1 − h 2 h 1 ∗ l 2 ∗ sin θ 2 ) , h 2 2 ) (5)</p><p>假设此时目标物体的理想角度为 θ 0 ，误差角度 θ e 计算方式为：</p><p>θ ε = arctan ( y B − y A x B − x A ) − θ 0 (6)</p><p>考虑到机器人与工业中的产品不同，存在着正面与反面，因此可以对式(6)进行修正，具体实现方式为：以机器人的正面为对称轴，将左侧四个点(含点A在内)全部标记为红色，将右侧四个点(含点B和点C在内)全部标记为蓝色，通过判断相应点的颜色来判断是否对式(6)进行修正。当红色点距离原点O较近，说明一切正常，无需修正；当蓝色点距离原点O较近，而红色点距离原点较远时，说明此时机器人面朝方向正好相仿，需要对式(6)进行 π 的修正。</p><p>考虑到机器人以匀速 v 沿X运动，经过任意时间t后，应产生位移向量 O S 为 ( v ∗ t , 0 , 0 ) ，在此情况下，目标运动机器人中心点M应沿向量 O S 移动至空间坐标系的点M'，M'坐标为：</p><p>( v ∗ t + 1 2 ∗ ( l 1 ∗ cos θ 1 + h 1 − h 2 h 1 ∗ l 2 ∗ cos θ 2 ) , 1 2 ∗ ( l 1 ∗ sin θ 1 + h 1 − h 2 h 1 ∗ l 2 ∗ sin θ 2 ) , h 2 2 ) (7)</p><p>当相机捕捉到移动机器人经过时，记录下文中所述分析过程中所需的对应数据，并发送给MCU，MCU处理计算后，得到空间坐标系下机器人为完成捕捉动作应移动至的准确坐标，并根据计算得到的 θ e 进行偏转，从而在经过时间t后准确将目标移动机器人捕捉。</p></sec></sec><sec id="s8"><title>4. 动态编队的试验及结果分析</title><sec id="s8_1"><title>4.1. 多自主移动机器人编队队形</title><p>多自主移动机器人系统由云服务器、MCU+摄像头、多自主移动机器人组成。编队采用领航跟随算法(Leader−Follower)实现，把动态编队过程的运算部分存放在云端，定位的信息传输到云端进行数据库实时更新，在云端存储和计算 [<xref ref-type="bibr" rid="hanspub.40397-ref9">9</xref>]。其中的编队队形如图4所示进行规划。</p><p>图4. 队形变换图</p><p>多机器人编队采用领航跟随算法，位置坐标的校正通过视觉定位实现。机器人的编队通过两步完成，第一步云端传输队形信息，即每个机器人当前的实际位置坐标和队形变换后的目标位置坐标，进行路径规划计算，给出每个机器人前进、后退、左移、右移的距离；第二步MCU把云端传来的控制信息一次性下发给每个机器人并确保机器人接收到信息；第三步机器人自身的控制器驱动机器人移动目标位置；第四步摄像头检测机器人到达的位置坐标与实际坐标是否一致，不同时通过前后左右移动到实际坐标位置，进行位置误差的微调。当机器人变换队形时，云端服务器发送编队指令控制领航者移动到对应的坐标，跟随者随着领航者发送的信息开始移动，云端调用数据库中的坐标与实际坐标进行判断，检测每个机器人的位置是否正确。编队变换坐标变化如表1所示，单位用cm。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Coordinate change table of team changin</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >编号</th><th align="center" valign="middle" >①</th><th align="center" valign="middle" >②</th><th align="center" valign="middle" >③</th><th align="center" valign="middle" >④</th><th align="center" valign="middle" >⑤</th><th align="center" valign="middle" >⑥</th><th align="center" valign="middle" >⑦</th><th align="center" valign="middle" >⑧</th><th align="center" valign="middle" >⑨</th></tr></thead><tr><td align="center" valign="middle" >心型</td><td align="center" valign="middle" >(0, 40)</td><td align="center" valign="middle" >(−40, 20)</td><td align="center" valign="middle" >(40, 20)</td><td align="center" valign="middle" >(−60, 0)</td><td align="center" valign="middle" >(−40, −20)</td><td align="center" valign="middle" >(−20, 0)</td><td align="center" valign="middle" >(20, 0)</td><td align="center" valign="middle" >(40, −20)</td><td align="center" valign="middle" >(60, 0)</td></tr><tr><td align="center" valign="middle" >“K”字</td><td align="center" valign="middle" >(0, 40)</td><td align="center" valign="middle" >(0, 20)</td><td align="center" valign="middle" >(40, 20)</td><td align="center" valign="middle" >(0, −40)</td><td align="center" valign="middle" >(0, −20)</td><td align="center" valign="middle" >(0, 0)</td><td align="center" valign="middle" >(20, 0)</td><td align="center" valign="middle" >(20, −20)</td><td align="center" valign="middle" >(40, −40)</td></tr></tbody></table></table-wrap><p>表1. 变队坐标变化表</p><p>可以看出，当心型变换为K字形编队时，① ③ ⑦号坐标不变，②右移，④下移右移，⑤ ⑥右移，⑧左移，⑨下移左移，按照图4移动，其中一格为10 cm。由于机器人同时进行移动操作，考虑防碰撞问题，经过最优移动路径规划编队，再结合云端大数据存储和计算功能，使机器人编队效率更高。</p></sec><sec id="s8_2"><title>4.2. 视觉定位</title><p>在机器人编队视觉定位的过程中，领航者位置坐标信息由智能相机进行检测，形成坐标定位 [<xref ref-type="bibr" rid="hanspub.40397-ref8">8</xref>]，视觉定位主要运用MCU进行处理，MCU利用OPencv库对图像进行处理，通过检测物体标签的形状和颜色，对所需颜色以及形状进行分析识别，识别计算所得的坐标信息即时存储于云端，检测过程中用不同颜色的正方体代表不同的机器人进行位置检测，图5(a)为模拟摄像头的定位检测图，图5(b)为实际摄像头的定位检测图。</p><p>图5. 定位检测图</p><p>多组检测坐标与实际坐标对比如表2所示。选择① ② ③号进行数据比对，检测所得的坐标单位为毫米(mm)，检测的数据与实际坐标之间的误差范围为0~54 mm。误差在可行的范围内，对机器人的编队不会有太大影响。</p><table-wrap-group id="2"><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison table of detection coordinates and actual coordinate</title></caption><table-wrap id="2_1"><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="4"  >① (mm)</th><th align="center" valign="middle" >检测坐标1</th><th align="center" valign="middle" >(−2800, 2916)</th></tr></thead><tr><td align="center" valign="middle" >实际坐标1</td><td align="center" valign="middle" >(−2700, 2500)</td></tr><tr><td align="center" valign="middle" >检测坐标2</td><td align="center" valign="middle" >(−3700, 5360)</td></tr><tr><td align="center" valign="middle" >实际坐标2</td><td align="center" valign="middle" >(−3500, 5300)</td></tr></tbody></table></table-wrap><table-wrap id="2_2"><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="4"  >② (mm)</th><th align="center" valign="middle" >检测坐标1</th><th align="center" valign="middle" >(1600, 2741)</th></tr></thead><tr><td align="center" valign="middle" >实际坐标1</td><td align="center" valign="middle" >(1600, 2550)</td></tr><tr><td align="center" valign="middle" >检测坐标2</td><td align="center" valign="middle" >(3650, 5541)</td></tr><tr><td align="center" valign="middle" >实际坐标2</td><td align="center" valign="middle" >(3650, 5000)</td></tr><tr><td align="center" valign="middle"  rowspan="4"  >③ (mm)</td><td align="center" valign="middle" >检测坐标1</td><td align="center" valign="middle" >(−500, −2800)</td></tr><tr><td align="center" valign="middle" >实际坐标1</td><td align="center" valign="middle" >(−500, −2400)</td></tr><tr><td align="center" valign="middle" >检测坐标2</td><td align="center" valign="middle" >(1800, −5016)</td></tr><tr><td align="center" valign="middle" >实际坐标2</td><td align="center" valign="middle" >(1700, −4700)</td></tr></tbody></table></table-wrap></table-wrap-group><p>表2. 检测坐标与实际坐标对比表</p></sec><sec id="s8_3"><title>4.3. 编队队形变换仿真</title><p>与采用backsteeping (逐步后推，反推)的控制(简记做BSMC)做对照，以误差浮动，收敛速度等做评价指标进行分析。对三角队形进行仿真，理想编队队形控制与基于视觉的编队队形控制(VS)中某一个机器人的误差分析如图6所示。</p><p>图6. 编队队形变换仿真结果</p><p>三个机器人完成从随机状态形成三角形编队并在外界干扰下沿直线轨迹保持编队运动。从仿真图中可以看出，在BSMC控制或VS控制的作用下，多移动机器人可从初始状态生成期望编队队形的L = 60 cm正三角形，并保持队形前行。在运行过程中能够完成从L = 60 cm至L = 150 cm的正三角形的编队队形的切换，并且在此过程中机器人并未出现掉队的现象。从仿真图中可以看出，变换队形时，跟随者1和跟随者2的角速度和线速度会发生较大跳变，这是由于瞬间队形的改变，导致距离误差过大造成的，当每一次队形变换结束后，线速度和角速度均会趋于稳定，均与领航者保持相同的速度运动，达到一致性。同样改进的VS控制采用误差补偿因此线速度和角速度的变化相对比较平稳。</p></sec></sec><sec id="s9"><title>5. 结论</title><p>本文提出了一套基于视觉的多自主移动机器人动态编队技术，在工业抓取技术之上构建出完整的三维空间坐标系，利用视觉相机抓取某时刻目标移动机器人在空间坐标系中关键点的视距与角度，传送给云端，由云端计算目标移动机器人中心点M的准确空间坐标，并推得该机器人最终应在的目标位置坐标。为了实现该目的，本文设计出基于智能相机的定位测距方法，搭建了完善的试验系统。通过实际数据，证明系统的实际可行性和适用价值。下一步，将在本文已有的研究成果基础上，继续研究空间坐标系下相应关键点和向量的数据分析，改进相应算法，使整个系统能够更为广泛地适用于各种工业产品模型，同时使系统对各种形状的机器人编队具备更强的适用性。</p></sec><sec id="s10"><title>基金项目</title><p>2020年度河南省高等学校重点科研项目20B120003。</p></sec><sec id="s11"><title>文章引用</title><p>张云龙,王清珍. 基于视觉的多自主移动机器人动态编队技术研究Research on Dynamic Formation Technology of Multiple Autonomous Mobile Robots Based on Vision[J]. 人工智能与机器人研究, 2021, 10(01): 29-36. https://doi.org/10.12677/AIRR.2021.101003</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.40397-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">张德红, 代艳霞. 机器人视觉搬运系统构建与软件开发[J]. 包装工程, 2019, 40(1): 149-155.</mixed-citation></ref><ref id="hanspub.40397-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">翟敬梅, 董鹏飞, 张铁. 基于视觉引导的工业机器人定位抓取系统设计[J]. 制造业自动化, 2014, 30(5): 45-49.</mixed-citation></ref><ref id="hanspub.40397-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">倪鹤鹏, 刘亚男. 基于机器视觉的Delta机器人分拣系统算法[J]. 机器人, 2016, 38(18): 49-55.</mixed-citation></ref><ref id="hanspub.40397-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">陈海永, 方灶军, 徐德, 等. 基于视觉的薄钢板焊接机器人起始点识别与定位控制[J]. 机器人, 2013, 35(1): 90-97.</mixed-citation></ref><ref id="hanspub.40397-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">谭民, 王硕, 曹志强. 多机器人系统[M]. 北京: 清华大学出版社, 2005.</mixed-citation></ref><ref id="hanspub.40397-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">原魁, 李园, 房立新. 多移动机器人系统研究发展近况[J]. 自动化学报, 2007, 33(8): 785-794.</mixed-citation></ref><ref id="hanspub.40397-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Balch, T. and Arkin, R.C. (1998) Behavior-Based Formation Control for Multirobot Teams. IEEE Transactions on Robotics and Automation, 14, 926-939. &lt;br&gt;https://doi.org/10.1109/70.736776</mixed-citation></ref><ref id="hanspub.40397-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">杨丽, 曹志强, 谭民. 不确定环境下多机器人的动态编队控制[J]. 自动化学报, 2010, 32(2): 283-288.</mixed-citation></ref><ref id="hanspub.40397-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">潘力, 高伟强, 刘建群, 等. 基于云计算的喷涂机器人远程监控诊断系统的研究[J]. 组合机床与自动化加工技术, 2019(12): 52-56.</mixed-citation></ref></ref-list></back></article>