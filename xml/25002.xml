<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2018.85076</article-id><article-id pub-id-type="publisher-id">CSA-25002</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20180500000_27540005.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于深度学习的多元文本情感研究与分析
  Research and Analysis of Textual Multi-Emotion Based on Deep Learning
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>楠</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陈</surname><given-names>进才</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>卢</surname><given-names>萍</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff2"><addr-line>华中科技大学武汉光电国家研究中心，湖北 武汉</addr-line></aff><pub-date pub-type="epub"><day>03</day><month>05</month><year>2018</year></pub-date><volume>08</volume><issue>05</issue><fpage>669</fpage><lpage>686</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   文本情感分析主要是通过文本挖掘技术对带有倾向性的文本进行情感分析和处理，识别其中主观性文本的倾向是正面、负面、中性的过程，这种关于文本情感颗粒的划分是不充分的，不全面的，显得过于生硬和暴力，不仅不能有效地体现出不同的文本情感颗粒的强度和大小，而且还需要大量的人工标注。本文针对此问题，提出和构建了基于Co-Training半监督训练的多元文本情感数据集，并且结合情感词频、情感词典、情感语义信息构建了D &amp; W、T &amp; W、SSW三种情感词向量，最后利用CNN和LSTM神经网络结构模型分别对构建的多元数据集进行了情感词向量的对比训练和模型优化，从而验证了情感词向量的有效性，而且提升了文本情感分类的准确度。 Text emotional analysis is mainly based on text mining technology for emotional analysis and processing of the text with tendentiousness, which is the subjective text tendency recognition process of positive and negative, neutral. This text about emotional particle division is not sufficient, not comprehensive, is too stiff and violence, and not only cannot effectively reflect the text sentiment granules with different strength and size, but also needs a lot of manual annotation. This paper proposes and constructs the multiple text sentiment data Co-Training based on semi supervised training set, and combines with the emotion of frequency, emotion dictionary, emotion semantic in-formation to construct the D &amp; W, T &amp; W, SSW three kinds of emotion word vector. Finally, CNN and LSTM neural network structure model is used to construct multivariate data sets that were com-pared with the training and optimization model of emotion word vector, which verifies the validity of the emotional word vector, but also improves the accuracy of text sentiment classification.
    
  
 
</p></abstract><kwd-group><kwd>情感分析，文本分类，多元情感，情感词向量, Emotion Analysis</kwd><kwd> Text Classification</kwd><kwd> Multiple Emotion</kwd><kwd> Emotional Word Vector</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于深度学习的多元文本情感研究与分析<sup> </sup></title><p>陈楠，陈进才，卢萍</p><p>华中科技大学武汉光电国家研究中心，湖北 武汉</p><p><img src="//html.hanspub.org/file/12-1540975x1_hanspub.png" /></p><p>收稿日期：2018年4月29日；录用日期：2018年5月16日；发布日期：2018年5月23日</p><disp-formula id="hanspub.25002-formula10"><graphic xlink:href="//html.hanspub.org/file/12-1540975x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>文本情感分析主要是通过文本挖掘技术对带有倾向性的文本进行情感分析和处理，识别其中主观性文本的倾向是正面、负面、中性的过程，这种关于文本情感颗粒的划分是不充分的，不全面的，显得过于生硬和暴力，不仅不能有效地体现出不同的文本情感颗粒的强度和大小，而且还需要大量的人工标注。本文针对此问题，提出和构建了基于Co-Training半监督训练的多元文本情感数据集，并且结合情感词频、情感词典、情感语义信息构建了D &amp; W、T &amp; W、SSW三种情感词向量，最后利用CNN和LSTM神经网络结构模型分别对构建的多元数据集进行了情感词向量的对比训练和模型优化，从而验证了情感词向量的有效性，而且提升了文本情感分类的准确度。</p><p>关键词 :情感分析，文本分类，多元情感，情感词向量</p><disp-formula id="hanspub.25002-formula11"><graphic xlink:href="//html.hanspub.org/file/12-1540975x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2018 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/12-1540975x7_hanspub.png" /> <img src="//html.hanspub.org/file/12-1540975x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>随着移动互联网和大数据时代的到来，爆炸式的海量信息急需要我们去处理分析，基于文本的情感分析技术也越来越成为研究热点，作为互联网的主体，每一条文本都带着我们的主观情绪、每种情绪也是各不相同的，比如喜爱，愤怒，悲伤，难受，赞扬，中立等。情感分析又称作情感挖掘或者意见挖掘，它主要包含的研究内容是情感信息分类任务、情感信息抽取任务等，传统的情感分析方法主要包括基于机器学习算法的研究和基于情感词典的构建的研究，基于情感词典的情感分析方法存在覆盖率不足的缺点，基于机器学习算法研究的情感分析会存在特征选择困难，人工标注训练集困难，模型简单，系统可扩展性不足，准确率低等缺点。</p><p>为了定量地得到文本内容的情感倾向，避免情感二元极性划分带来的情感不充分的，不全面的问题，本文构建了针对多元情感分析的语义情感数据集，结合Co-Training标注方法，减少了单纯进行人工标注带来的误差，增强了模型数据的健壮性和泛化能力，为了更加有效地结合文本的情感信息，抽取更加深层次的语义情感信息，本文构建了更加有效的情感词向量模型算法，将情感词典和情感词频-逆文档概率，情感词向量三者有效地结合起来，充分地考虑到了三者的优点，提出了结合情感词向量和情感词典的D &amp; W词向量、结合情感词向量和情感词频的T &amp; W词向量、结合情感词频和情感词典以及情感词向量的SSW情感语义词向量，并且将其应用在深度学习模型的词向量特征表示上面，进行模块叠加训练，实验证明对比基准词向量模型，其能够在文本情感分类任务中取得更加优秀的效果。</p></sec><sec id="s4"><title>2. 相关研究</title><sec id="s4_1"><title>2.1. 深度学习</title><p>近些年，深度学习在语音识别、自然语言处理、机器视觉、图像处理等领域取得了巨大成功，在1986年，Hinton [<xref ref-type="bibr" rid="hanspub.25002-ref1">1</xref>] 提出了非常著名且沿用至今的反向传播算法，使得基于深度神经网络的深度学习(Deep Learning)的方法应运而生，从此神经网络变得非常流行起来，利用神经网络来建立语言模型的研究思路逐渐走向成熟，大大提升了文本的特征质量。2003年，Bengio等提出用神经网络的方法去构建二元语言模型 [<xref ref-type="bibr" rid="hanspub.25002-ref2">2</xref>] 。2008年，Ronan Collobert和Jason Weston推出SENNA系统 [<xref ref-type="bibr" rid="hanspub.25002-ref3">3</xref>] ，并将其应用到自然语言处理领域中，利用词向量的方法完成了其中的词性标注、命名实体识别、短语识别、语义角色标注等多种任务。2013年，随着Hinton提出word embedding的概念 [<xref ref-type="bibr" rid="hanspub.25002-ref4">4</xref>] ，以及Mikolov对该理论的进一步实现 [<xref ref-type="bibr" rid="hanspub.25002-ref5">5</xref>] ，这种全新的文本特征表示已经被越来越多的研究者所认可。基于word embedding的特征表示方法不但能够避免“维度灾难”现象，还能够从更高的语义层面上描述词与词之间的关系。梁军等人通过采用自动编码器，实现了利用半监督学习的方法对微博的文本数据进行情感分析，大量减少了人工标注的工作量 [<xref ref-type="bibr" rid="hanspub.25002-ref6">6</xref>] 。陈翠平引入了深度学习的思想来完成文本分类任务，利用深度信念网络自动提取文本特征 [<xref ref-type="bibr" rid="hanspub.25002-ref7">7</xref>] 。Yoon等尝试利用卷积神经网络结构来解决情感分析和问题分类等若干自然语言处理任务，获得了非常好的效果 [<xref ref-type="bibr" rid="hanspub.25002-ref8">8</xref>] 。</p></sec><sec id="s4_2"><title>2.2. 情感分析</title><p>情感分析的目的是将具有情感倾向的主观性文本识别出来，并且分为褒义和贬义两类。其中在传统的情感分析方法中，主要采用基于规则的方法，需要相当一部分人力和物力作为支撑，所以，现在情感分析研究领域的学者纷纷转向了基于统计的学习方法，该方法主要根据特征的分布对文本的情感类别做出正确的判断。Pang等在对电影评论数据进行褒贬二分类的研究中，使用了包括一元词、二元词、词性标注等若干特征 [<xref ref-type="bibr" rid="hanspub.25002-ref9">9</xref>] ，Davidov等利用在Twitter中的标签元素和笑脸符号来作为特征，从而对Twitter进行情感分类 [<xref ref-type="bibr" rid="hanspub.25002-ref10">10</xref>] ，李婷婷等尝试从文本数据中人工构建若干特征，再利用传统的机器学习方法进行文本分类 [<xref ref-type="bibr" rid="hanspub.25002-ref11">11</xref>] ；李荣陆等人利用最大熵模型实现了中文文本分类 [<xref ref-type="bibr" rid="hanspub.25002-ref12">12</xref>] 。Taboada等 [<xref ref-type="bibr" rid="hanspub.25002-ref13">13</xref>] 采用的是基于词库的方法，文本的最后情感值采用集约化的方法计算，进而确定文本的最后情感倾向。Hu等 [<xref ref-type="bibr" rid="hanspub.25002-ref14">14</xref>] 在文章中提出采用Bootstrapping策略，句子中所有情感词的情感倾向性分数总和决定最后该句子的情感倾向。以上方法本质上均属于机器学习范畴，其分类效果严重依赖所构建特征的质量和模型参数的调优，整个过程非常耗时耗力，往往需要大量的领域内知识，因此最终的分类效果并不稳定。</p></sec></sec><sec id="s5"><title>3. 多元情感数据集的构建</title><sec id="s5_1"><title>3.1. 多元情感分析</title><p>文本情感分析是指对于网络用户的喜爱、观点和意见的分析和挖掘，获得用户对于事件的主观性情感倾向评论，我们通常用情感权重来代表某个词语的情感极性，如果情感权重大于0则代表用户的积极正面的情感倾向，如果情感极性权重小于0则代表主观用户的负面消极的情感倾向，这样简单高效的划分情感便于我们直接的得到情感极性和做出最终的判断任务，同样也带来了关于情感划分颗粒不充分，不全面的问题，情感之间的划分过于暴力和生硬，没有有效的区分不同的文本情感颗粒强度和大小。</p><p>关于情感(Sentiment)狭义的定义：情感是对于一个实体或者事件等事物评价的极性，又称极性情感，情感的主要类别包含正面、负面、中性情感颗粒，但是情感之间出现相互影响，相互交割带来的情感交叉现象，对于我们最终的情感分类模型造成相应的误判，基于此问题，本文在基于Ekman et al. (1982)的基础上面考虑加入了相关的中性极性情感，提出了基于情感极性的细颗粒情感分类模型，在基本的正向，负向，中性三种极性情感的基础上将情感极性强度划分成8中基本的情感颗粒，分别是高兴、生气、厌恶、悲伤、害怕、吃惊、轻视、中性8中基本情感强度，如表1所示，最大化的减少文本极性情感强度之间的干扰和误判，我们提出了基于多元情感分析的细颗粒划分指标，充分反应情感持有者的情感强度，不仅包含文本情感的极性分析，而且包含了文本情感的极性强度，将二者充分的进行了融合。</p></sec><sec id="s5_2"><title>3.2. 情感文本数据集构建</title><p>情感数据集对于最终的多元情感分类任务有着重要的表现，考虑到单纯的情感语料库的标签都是针对二元情感分类任务的，不能够满足多元情感分类的要求，因此我们首先要满足构建多元情感任务的情感文本标签。</p><p>为了构建多元情感的极性文本，我们提出了借助表情包文字识别技术来构筑多元的情感极性文本，表情已经充斥在网络的各个角落，现在用户都流行 “能发图就不打字”的习惯，表情包文字本身充满了多元的情感极性，我们需要借助这些表情包，从形形色色的网络表情中找出对应的文字，从这些表情中提取出的文字，可用于我们后续的文本分析，情感预测，语义理解等任务。</p><p>目前主流的文字识别方法都差不多。主要分为两个模块，一个模块定位文字位置，另外一个模块针对定位后的文字进行识别。针对这两个模块，我们使用的是Faster RCNN + CTC的方案。文字定位部分本文使用了Faster RCNN技术，如图1所示，其是从RCNN逐渐演变过来的。相对于它的前辈RCNN以及Fast RCNN，Faster RCNN提出了RPN (Region Proposal Network)网络。通过RPN输出Anchor Box Proposals，再通过NMS和其他一些方法进行Proposals Reduction。该方法对比以往的方案，性能更优，减少了selective search里面繁琐的计算。目前目标检测还有其他state of art的定位方案，例如YOLOv2，Mask RCNN等，其中Mask RCNN更多聚焦在image segmentation上。关于文字识别部分的结果，如图2和图3所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title>Basic emotion and extended emotio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Basic</th><th align="center" valign="middle" >Extended</th></tr></thead><tr><td align="center" valign="middle" >高兴(Happiness)</td><td align="center" valign="middle" >Depressed(郁闷)</td></tr><tr><td align="center" valign="middle" >生气(Anger)</td><td align="center" valign="middle" >boring(无聊)</td></tr><tr><td align="center" valign="middle" >厌恶(Disgust)</td><td align="center" valign="middle" >Lonely(孤独)</td></tr><tr><td align="center" valign="middle" >悲伤(Sadness)</td><td align="center" valign="middle" >irritated(烦躁)</td></tr><tr><td align="center" valign="middle" >害怕(Fear)</td><td align="center" valign="middle" >envy(嫉妒)</td></tr><tr><td align="center" valign="middle" >吃惊(Surprise)</td><td align="center" valign="middle" >Disappointed(失望)</td></tr><tr><td align="center" valign="middle" >轻视(Contempt)</td><td align="center" valign="middle" >regret(后悔)</td></tr><tr><td align="center" valign="middle" >中性(Neutral)</td><td align="center" valign="middle" >Like(喜欢) hope(希望)</td></tr></tbody></table></table-wrap><p>表1.基本情感和扩充情感</p><p>图1. Faster RCNN的基本结构</p><p>图2. 文字定位后截取的图片</p><p>图3. CTC模型文字输出</p></sec><sec id="s5_3"><title>3.3. 基于半监督的多元情感数据集的构建</title><p>由于直接使用无监督聚类机器学习算法带来的人工工作量大，耗时的问题，本文提出了借助Co-training思想和半监督思想来进行数据集的构建。</p><p>Co-training是目前很流行的一种半指导机器学习的方法，它的基本思想是：构造两个不同的分类器，利用小规模的标注语料，对大规模的未标注语料进行标注的方法。Co-training方法最大的优点是不用人工干涉，能够从未标注的语料中自动学习到知识。Co-training方法，是无监督和有监督机器学习两者的一个折中办法，它的原则是：在不牺牲性能的前提下，尽量多的使用未带标数据，它从一个小规模的带标的语料库开始，同时使用大规模的未带标语料来进行学习。这里面，我们Co-training使用的文本特征分别是Word2vec语义特征样本选择和加权的Word2vec文本语义特征：Word2vec * TF-idf，这两种文本特征选择来分别对文本进行特征选择过程，分类器使用的是SVM分类器，得到的情感类别比重如图4所示。</p><p>其中要先选择表情包图片数据集和种子图片的类别和数量；表情包的图片包含了二次元、斗图、纯文字、动物、彩字祝福、真人、未知、七个大的类别，一共大约有25万张，每种表情包的数量如表2、表3和表4所示：</p><p>具体的Co-training针对文本特征选择过程和分类标签如下：</p><p>1) 先选择一些标签种子文本，这部分文本可以通过手工标注的方式获得；</p><p>2) 然后根据预训练的Word2vec词向量特征对种子文本进行分类训练；</p><p>3) 得到了训练好的分类器；</p><p>4) 接着根据训练的Word2vec * TF-IDF特征结合训练好的分类器接着对剩下的文本进行标签预测；</p><p>5) 所使用的分类器是SVM分类器；</p><p>6) 这样就得到了待分类文本的标签集。</p><p>图4. 无监督情感标签分布示意图</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Expression package picture data se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >一级分类</th><th align="center" valign="middle" >数量</th><th align="center" valign="middle" >比例</th></tr></thead><tr><td align="center" valign="middle" >二次元</td><td align="center" valign="middle" >31,639</td><td align="center" valign="middle" >12.48%</td></tr><tr><td align="center" valign="middle" >斗图</td><td align="center" valign="middle" >70,585</td><td align="center" valign="middle" >27.85%</td></tr><tr><td align="center" valign="middle" >纯文字</td><td align="center" valign="middle" >991</td><td align="center" valign="middle" >0.39%</td></tr><tr><td align="center" valign="middle" >动物</td><td align="center" valign="middle" >20,408</td><td align="center" valign="middle" >8.05%</td></tr><tr><td align="center" valign="middle" >彩字祝福</td><td align="center" valign="middle" >14,985</td><td align="center" valign="middle" >5.91%</td></tr><tr><td align="center" valign="middle" >真人</td><td align="center" valign="middle" >52,685</td><td align="center" valign="middle" >20.79%</td></tr><tr><td align="center" valign="middle" >未知</td><td align="center" valign="middle" >62,157</td><td align="center" valign="middle" >24.52%</td></tr><tr><td align="center" valign="middle" >合计</td><td align="center" valign="middle" >253,504</td><td align="center" valign="middle" >100%</td></tr></tbody></table></table-wrap><p>表2. 表情包图片数据集</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Emoticons seed pictures data se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >一级分类</th><th align="center" valign="middle" >数量</th><th align="center" valign="middle" >比例</th></tr></thead><tr><td align="center" valign="middle" >二次元</td><td align="center" valign="middle" >167</td><td align="center" valign="middle" >4.2%</td></tr><tr><td align="center" valign="middle" >斗图</td><td align="center" valign="middle" >176</td><td align="center" valign="middle" >4.4%</td></tr><tr><td align="center" valign="middle" >纯文字</td><td align="center" valign="middle" >17</td><td align="center" valign="middle" >0.43%</td></tr><tr><td align="center" valign="middle" >动物</td><td align="center" valign="middle" >155</td><td align="center" valign="middle" >3.96%</td></tr><tr><td align="center" valign="middle" >彩字祝福</td><td align="center" valign="middle" >79</td><td align="center" valign="middle" >2.02%</td></tr><tr><td align="center" valign="middle" >真人</td><td align="center" valign="middle" >431</td><td align="center" valign="middle" >1.10%</td></tr><tr><td align="center" valign="middle" >未知</td><td align="center" valign="middle" >2891</td><td align="center" valign="middle" >73.82%</td></tr><tr><td align="center" valign="middle" >合计</td><td align="center" valign="middle" >3916</td><td align="center" valign="middle" >100%</td></tr></tbody></table></table-wrap><p>表3. 表情包种子图片数据集</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Text seed datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Emotion</th><th align="center" valign="middle" >Size</th><th align="center" valign="middle" >Percent</th></tr></thead><tr><td align="center" valign="middle" >despise</td><td align="center" valign="middle" >293</td><td align="center" valign="middle" >7.5%</td></tr><tr><td align="center" valign="middle" >neutral</td><td align="center" valign="middle" >1656</td><td align="center" valign="middle" >42.3%</td></tr><tr><td align="center" valign="middle" >angry</td><td align="center" valign="middle" >178</td><td align="center" valign="middle" >4.6%</td></tr><tr><td align="center" valign="middle" >sad</td><td align="center" valign="middle" >413</td><td align="center" valign="middle" >10.6%</td></tr><tr><td align="center" valign="middle" >disgust</td><td align="center" valign="middle" >266</td><td align="center" valign="middle" >6.81%</td></tr><tr><td align="center" valign="middle" >surprise</td><td align="center" valign="middle" >158</td><td align="center" valign="middle" >4.04%</td></tr><tr><td align="center" valign="middle" >happy</td><td align="center" valign="middle" >1211</td><td align="center" valign="middle" >30.9%</td></tr></tbody></table></table-wrap><p>表4. 文本种子数据集</p></sec></sec><sec id="s6"><title>4. 情感语义词向量的构建</title><p>在传统的词向量模型基础上，为了更加有效地结合文本的情感信息，抽取更加深层次的语义情感信息，本文构建了更加有效的情感词向量模型算法，将情感词典和情感词频-逆文档概率，情感词向量三者有效地结合起来，充分地考虑到了三者的优点，本文提出了结合情感词向量和情感词典的D &amp; W词向量、结合情感词向量和情感词频的T &amp; W词向量、结合情感词频和情感词典以及情感词向量的SSW情感语义词向量。</p><sec id="s6_1"><title>4.1. 基于机器算法的情感词典的构建</title><p>目前的情感词典主要是包含正向和负向两种情感，为了满足多元情感分析任务，本文根据之前提出的在基本的正向，负向，中性三种极性情感的基础上将情感极性强度划分成8中基本的情感颗粒，结合机器学习的相应方法，减少人工手工标注的工作量，自动化构建相应的多元情感词典。</p><p>本文提出了结合词向量的相似性大小，用词聚类的方法来判断文本词语的情感极性大小，具体的，就是先通过人工选择一些情感词语作为我们的种子词语，然后通过词语之间的相似性判断来对情感词典进行扩充，利用种子词语和新的词语之间的关系来计算词语的情感极性大小。</p><p>本文的多元情感词典参考了大连理工大学的中文情感词典的构建方法，具体的种子词典的构建是先选择8种情感大类，在这8种基本的大类情感下面再进一步的选择其中的情感文本关键词作为我们的情感词典基准词，每种情感类别下面的情感基准词的大小数量不一样，但是他们的情感极性得分是统一的，得分的范围是：[−0.8, 0.8]，情感文本基准词的最终构建结果如表5所示。</p></sec><sec id="s6_2"><title>4.2. 基于Word2vec与Tf-idf的T &amp; W词向量研究</title><p>我们在文本情感特征的选择和文本情感特征的加权基础上面，结合基于机器学习的特征选择和基于深度学习词向量的特征选择方法，对于文本情感特征的特征进行加权，构建了基于Word2vec和tf-idf的加权词向量特征构建方法，然后对文本的情感极性进行判断。</p><p>这里面把整个tf-idf作为衡量整个词语的重要程度权重，Tf-IDF表示的是一个词语在整个文档中的重要程度，其主要有两个部c分组成：词频TF和逆文档概率IDF，主要公式：</p><p>Tf-IDF = TF ∗ IDF</p><p>对于word2vec词向量部分，我们显示使用word2vec中的CBOW模型训练语料库，得到相应的词向量模型，然后将文档中的所有对应的相同单词的词向量进行叠加求和得到所有词语的词向量表示：</p><p>R ( d j ) = ∑ t word2vec ( t )       where   t ∈ d j</p><p>其中： word2vec ( t ) 表示词语的词向量表示。</p><p>接下来我们先后分别取得对应语料库的Word2vec词向量和TF-IDF词向量的特征表示方法，再将二者有效的进行结合即可得到新的基于Word2vec与Tf-idf的词向量表示。</p><p>W_R ( d j ) = ∑ t word2vec ( t ) ∗ tfidf i , j</p><p>这样，我们就结合了Word2vec的词向量表示的词语语义信息，同时，我们又将tf-idf表达的词频信息进行了有效的结合起来，这样新的词向量用来提高文本的多元情感分类的准确率。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Emotion dictionary example</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >情感类别</th><th align="center" valign="middle" >情感词语</th><th align="center" valign="middle" >情感极性</th><th align="center" valign="middle" >情感数量</th></tr></thead><tr><td align="center" valign="middle" >高兴(Happiness)</td><td align="center" valign="middle" >喜悦、开心、欢喜、称心、 振奋、快活、欢乐、快乐、 起劲、兴奋、愉悦、安乐、</td><td align="center" valign="middle" >0.8</td><td align="center" valign="middle" >12</td></tr><tr><td align="center" valign="middle" >生气(Anger)</td><td align="center" valign="middle" >愤怒、起火、生机、活气、 发怒、不满、负气、活力、 朝气、发火、赌气、动怒</td><td align="center" valign="middle" >-0.6</td><td align="center" valign="middle" >14</td></tr><tr><td align="center" valign="middle" >厌恶(Disgust)</td><td align="center" valign="middle" >憎恶、嫌恶、恨恶、厌烦、 可恶、讨厌、憎恨、腻烦</td><td align="center" valign="middle" >-0.8</td><td align="center" valign="middle" >8</td></tr><tr><td align="center" valign="middle" >悲伤(Sadness)</td><td align="center" valign="middle" >心酸、悲哀、哀思、酸楚、 难过、痛苦、伤心、痛心、</td><td align="center" valign="middle" >-0.5</td><td align="center" valign="middle" >8</td></tr><tr><td align="center" valign="middle" >害怕(Fear)</td><td align="center" valign="middle" >惊恐、畏惧、畏缩、畏怯、</td><td align="center" valign="middle" >-0.7</td><td align="center" valign="middle" >12</td></tr><tr><td align="center" valign="middle" >吃惊(Surprise)</td><td align="center" valign="middle" >惊讶、惊异、惊奇、受惊、 惊诧、惊呀、惊愕、诧异</td><td align="center" valign="middle" >0.2</td><td align="center" valign="middle" >8</td></tr><tr><td align="center" valign="middle" >轻视(Contempt)</td><td align="center" valign="middle" >小看、渺视、鄙夷、看不起、 藐视、轻蔑、忽视、歧视、 小瞧、轻茂、鄙视、看轻、</td><td align="center" valign="middle" >-0.4</td><td align="center" valign="middle" >24</td></tr><tr><td align="center" valign="middle" >中性(Neutral)</td><td align="center" valign="middle" >生活、爱情、活动、情况…</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >100</td></tr></tbody></table></table-wrap><p>表5. 情感词典实例</p></sec><sec id="s6_3"><title>4.3. 基于Word2vec与情感词典结合的D &amp; W词向量研究</title><p>本文基于上面介绍的情感词典得到了情感词语的情感极性得分大小，同时，基于情感的词向量得到了情感词语语义表示，为了将二者有效的进行结合，本文提出了基于情感词向量和基于情感词典的词向量构建过程，将情感词语的情感极性强度和情感词语的词向量结合起来，改进了传统的词向量只包含情感词语单纯的语义信息的构建模型，以此来希望提升我们最后的文本情感分析准确率。</p><p>本文结合词向量的相似性大小，用词聚类的方法来判断文本词语的情感极性大小，具体的，就是先通过人工选择一些情感词语作为我们的种子词语，然后通过词语之间的相似性判断来对情感词典进行扩充，利用种子词语和新的词语之间的关系来计算词语的情感极性大小得到了文本词语的情感词典语料，然后结合情感词语的词向量构建了最终的情感语义词向量。</p><p>具体的词向量构建过程为：</p><p>1) 将语料库的文本词语进行预训练过程；</p><p>2) 对于语料库里面的词语选中一部分作为种子词典，并且给出这些种子语料的情感极性得分词典；</p><p>3) 对于每种情感分类的情感种子词语进行加和求平均得到每种情感的中心词向量；</p><p>4) 将每种情感得到的平均词向量作为虚拟的中心词语聚类中心，计算每个新词与聚类中心的词向量相似度大小，</p><p>5) 并且将每个新词与聚类中心的词向量相似度大小进行从小到大的排序；</p><p>6) 选择上一步得到的最大相似度的情感类别作为新词的情感类别；</p><p>7) 并且根据新词与聚类中心词语的相似度大小与种子文本得情感极性得分得到最终的新词的情感极性权重得分；</p><p>8) 这样就得到了新词的情感极性得分。</p></sec><sec id="s6_4"><title>4.4. 基于Word2vec与Tf-idf、情感词典结合的SSW词向量研究</title><p>传统的词向量模型根据中心词的上下文来表示中心词，表达出了词向量的语义信息，这样的分布式词向量可以简单的处理基本的文本任务，但是在基于文本的情感分析任务处理中并不能够有效的结合文本的情感信息，所以这样的词向量语义信息是一种不包含情感信息的浅层语义信息，所以为了更加有效的结合文本的情感信息，抽取更加深层次的语义情感信息，本文提出的基于情感词典和Tf-idf (词频-逆文档)概率，word2vec词向量三者有效的结合起来，充分的考虑到了三者优点，将词频，词义，词性三者结合起来，并且将其应用在深度学习模型的词向量特征表示上面，实验证明对比以前的词向量模型，有效的提升了文本分类效果。</p><p>Word2vec的词向量模型虽然在更高的层次上面得到了文本的语义特征词向量，但是Word2vec的两种模型：基于CBOW和Skip-gram的词向量算法都没有能够将文本的情感信息有效的蕴含起来，所以，在构建新的语义情感的词向量上面需要先将情感词典，文本词向量的语义信息有效的结合起来，本文先分别基于机器学习算法构建了新的情感词典，对比以前的情感词典构建过程更加的直接有效，然后结合基本的词频-逆文档特征选择算法，Word2vec算法将文本的词频信息，情感信息，语义信息有效的结合起来并且将新构建的情感语义词向量与原来的词向量进行了对比试验，有效的证明了本文提出的情感语义词向量的有效性。</p><p>具体的词向量构建过程为：</p><p>1) 将语料库的文本词语进行预训练过程；选择相应的文本特征进行特征训练过程，分别得到对应的TF-IDF和Word2vec特征词向量；</p><p>2) 对于语料库里面的词语选中一部分作为种子词典，并且给出这些种子语料的情感极性得分词典；同时改进相应的TF-IDF特征选择过程，得到新的改进的TF-IDF特征选择词向量；</p><p>3) 分别将得到的文本的改进的TF-IDF特征词向量与Word2vec特征词向量的结果保存起来；</p><p>4) 对于每种情感分类的情感种子词语进行加和求平均得到每种情感的中心词向量；</p><p>5) 将每种情感得到的平均词向量作为虚拟的中心词语聚类中心，计算每个新词与聚类中心的词向量相似度大小，并且进行从小到大的排序</p><p>6) 选择上一步得到的最大相似度的情感类别作为新词的情感类别；</p><p>7) 并且根据新词与聚类中心词语的相似度大小与种子文本得情感极性得分得到最终的新词的情感极性权重得分。</p><p>8) 将之前得到的情感极性得分与改进的TF-IDF特征词向量、Word2vec特征词向量的乘积作为最终的情感语义特征词向量。</p></sec></sec><sec id="s7"><title>5. 基于深度学习的多元情感分析实验</title><p>传统的CNN和RNN模型只能解决简单的句子级别文本情感分类，而我们的神经网络模型需要结合文本的情感和语义信息，进行单词词向量的联合训练，因此本文提出了新的修改升级的神经网络模型，将文本的情感和语义信息都包含进行，进行叠加模块训练，因此新的CNN和RNN神经网络包含3个层次模块的文本信息，分别是文本的词频特征信息、情感特征信息、语义特征信息、对应的相关新的CNN模型分别是CNN-DW、CNN-TW、CNN-SSW，同样的，相应的RNN模型分别是RNN-DW、RNN-TW、RNN-SSW，同时改进了相关的卷积神经网络结构和递归神经网络结构，提升了实验效果，提高了准确率，从而验证了结合情感词向量和情感词典的D &amp; W词向量、结合情感词向量和情感词频的T &amp; W词向量、结合情感词频和情感词典以及情感词向量的SSW情感语义词向量的有效性。</p><sec id="s7_1"><title>5.1. 实验数据集</title><p>为了比较本文提出的三种词向量模型在多元情感分类模型任务上的效果，我们结合前面第四章得到的多元情感文本数据集进行了相关的实验分析，结合深度学习的CNN和RNN等模型进行了对比实验效果。</p><p>该实验数据为多元的情感类别文本，具体的是多元情感的语义情感数据集标签是与我们之前提出的多元情感分类一致的，一共包含8中基本的情感颗粒，最大化的减少文本极性情感强度之间的干扰和误判，这里面我们提出了基于多元情感分析的细颗粒划分指标，充分反应情感持有者的情感强度，不仅包含文本情感的极性分析，而且包含了文本情感的极性强度分析，将二者充分的进行了融合。</p><p>我们需要先定义好各种情感极性对应的情感标签，其对应过程如表6、表7和表8所示。</p><p>其中统计得到相应的情感标签数量如下。</p></sec><sec id="s7_2"><title>5.2. 实验基准系统</title><p>为了比较本文提出的将CNN和RNN模型与文本的情感和语义信息都包含进来，进行叠加模块训练效果，需要将新的CNN和RNN神经网络包含3个层次模块的文本信息，分别是文本的词频特征信息、情感特征信息、语义特征信息、对应的相关新的CNN模型分别是CNN-DW、CNN-TW、CNN-SSW，同样的，相应的RNN模型分别是RNN-DW、RNN-TW、RNN-SSW。为了比较相应的词向量模型效果，需要先设置基准系统，方便我们进行对比观察结果，相关的实验标准参照词向量模型是将如下两个基本词向量模型进行平均计算：</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Emotional tag classificatio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >情感标签</th><th align="center" valign="middle" >情感内容</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >高兴</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >生气</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >厌恶</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >悲伤</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >害怕</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >吃惊</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >轻视</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >中性</td></tr></tbody></table></table-wrap><p>表6. 情感标签分类</p><table-wrap id="table7" ><label><xref ref-type="table" rid="table7">Table 7</xref></label><caption><title> Various emotional tag statistic</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Emotion</th><th align="center" valign="middle" >Size</th></tr></thead><tr><td align="center" valign="middle" >Despise</td><td align="center" valign="middle" >10,309</td></tr><tr><td align="center" valign="middle" >Neutral</td><td align="center" valign="middle" >109,634</td></tr><tr><td align="center" valign="middle" >Angry</td><td align="center" valign="middle" >13,305</td></tr><tr><td align="center" valign="middle" >Sad</td><td align="center" valign="middle" >31,811</td></tr><tr><td align="center" valign="middle" >Disgust</td><td align="center" valign="middle" >7397</td></tr><tr><td align="center" valign="middle" >Surprise</td><td align="center" valign="middle" >10,936</td></tr><tr><td align="center" valign="middle" >Fear</td><td align="center" valign="middle" >2358</td></tr><tr><td align="center" valign="middle" >Happy</td><td align="center" valign="middle" >82,442</td></tr></tbody></table></table-wrap><p>表7. 各种情感标签统计图</p><table-wrap id="table8" ><label><xref ref-type="table" rid="table8">Table 8</xref></label><caption><title> Emotional polarity distributio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >情感极性</th><th align="center" valign="middle" >文本数量</th></tr></thead><tr><td align="center" valign="middle" >中性</td><td align="center" valign="middle" >109, 634 (34%)</td></tr><tr><td align="center" valign="middle" >非中性</td><td align="center" valign="middle" >158,558 (66%)</td></tr></tbody></table></table-wrap><p>表8. 情感极性分布图</p><p>1) CBOW：根据输入的上下文来预测中心词构建的语义词向量模型，</p><p>2) Skip-gram：根据输入的中心词的上下文来构建中心词对应的上下文语义的词向量模型。</p></sec><sec id="s7_3"><title>5.3. 基于CNN模型的多元情感分析实验</title><p>针对基于卷积神经网络CNN模型的多元文本情感分类实验，我们采用的卷积神经网络结构依次如下是：Embedding &#174; Convection &#174; Pooling &#174; Activation，其中Embedding和Convection之间的Dropout、以及Pooling和Activation之间的Dropout，我们都将其大小设置为0.20，其中输入层词向量维度大小是100，并且卷积层的滤波器将相应的输入层词向量矩阵分成3个region，每个region分别设置32个滤波器，一共采用的数量是96个滤波器，滤波器的大小是(2, 3, 4)，就是说将滤波器设置成3种状况，分别是2 * 3 * 32，2 * 4 * 32，3 * 4 * 32的3种状况，每种状况下的滤波器设置得到的每个region是32维度的特征向量，池化层，采用的是最大池化层策略，激活层通过Softmax激活函数得到8中情感类别的概率大小，再与标准的情感类别比较得到误差反向传播即可，如图5所示。</p></sec><sec id="s7_4"><title>5.4. 基于RNN模型的多元情感分析实验</title><p>因为CNN神经网络模型无法获取对应的很长的文本序列信息，这里面我们选择使用双向的LSTM长短记忆单元神经网络模型来进行文本词向量的对比试验，Bi-LSTM在序列标注、命名体识别、seq2seq等模型有很多场景都有应用，它能够更好的表达文本上下文信息，而且Bi-LSTM可以获取变长且双向的的n元语法信息，更加适合拟合文本序列信息，如图6所示。</p><p>我们的网络结构依次是：Embedding &#174; Convection &#174; Pooling &#174; Activation，其中相应的Embedding和Convection之间的Dropout、以及Pooling和Activation之间的Dropout，我们都将其大小设置为0.25，其中输入层词向量维度大小是100，激活函数同样选择Softmax激活函数，分别结合RNN-DW、RNN-TW、RNN-SSW三种词向量模型得到相应的情感分类结果。</p><p>图7是Bi-LSTM用于分类问题的网络结构原理示意图，其中LSTM节点分别是前向和后向的双向RNN的输出表示，这里面需要注意的是LSTM单元之间的相互连接和FC层的表示，同样，最终的输出层使用的是Softmax函数输出。</p></sec><sec id="s7_5"><title>5.5. 实验结果对比</title><p>通过与基准实验的对比，获取了多元情感类别在3种词向量下面分别通过深度学习实验得到的相应结果如下图8~图15所示。</p><p>图5. 基于CNN模型的多元情感分析实验设置</p><p>图6. LSTM结构示意图</p><p>图7. 基于Bi-LSTM模型的多元情感分析实验设置</p><p>图8. Despise实验对比结果</p><p>图9. Angry实验对比结果</p><p>图10. Neutral实验对比结果</p><p>图11. Sad实验对比结果</p><p>图12. Disgust实验对比结果</p><p>图13. Surprise实验对比结果</p><p>图14. Fear实验对比结果</p><p>为了更加方便的突出我们的实验对比结果，我们计算了多元情感类别在3中词向量模型下面的深度学习文本分类实验中的平均F1值、Precision、Recall值作为我们的实验对比指标，得到了相应的实验对比分析结果如下图16~图18所示。</p></sec><sec id="s7_6"><title>5.6. 实验结论总结</title><p>根据上面的词向量实验对比分析，可以简单的得出以下结论：</p><p>1) 在构建多元情感数据公开集上面，本文提出的基于情感词典和情感语义的几种词向量在CNN和RNN深度学习模型上面都取得比基准词向量要好的分类效果，说明将情感信息和词向量信息有效的结合起来，能够在文本情感分类任务中取得更加优秀的效果。</p><p>图15. Happy实验对比结果</p><p>图16. 深度学习分别在3种词向量下的文本情感分类F1值效果对比图</p><p>图17. 深度学习分别在3种词向量下的文本情感分类Precision值效果对比图</p><p>图18. 深度学习分别在3种词向量下的文本情感分类Recall值效果对比图</p><p>2) 在本章提出的基于深度学习的几种词向量上面：结合情感词向量和情感词典的D &amp; W词向量、结合情感词向量和情感词频和T &amp; W词向量、结合情感词频和情感词典以及情感词向量的SSW情感语义词向量对比中，可以看出SSW的效果要比其他两种词向量的效果好很多，说明将文本情感信息和语义信息结合起来对比其他的词向量模型对于文本情感分类任务可以提升分类效果。</p><p>3) 可以看出在基于深度学习的几种词向量模型的情感分类任务中，基于语义词向量学习模型的分类效果是最差的，说明仅仅将文本的词义信息作为情感特征去进行情感分类的效果是最差的，不能够有效的得到情感的上下文语序语义信息以及情感信息，因此其分类效果对比其他词向量进行的特征选择效果相比很差。</p><p>4) 在深度学习实验中，对比基于情感词典和情感词向量的D &amp; W词向量模型、结合情感词向量和情感词频的T &amp; W词向量模型，二者相对比基本的基于词频的特征词向量BOW模型和单纯的词向量CBOW和Skip-gram提升了分类效果，因为二者加入了情感信息和词频信息，比单纯的词频信息和词向量信息好很多，说明需要在情感分类任务中加入多层次的词向量信息。</p></sec></sec><sec id="s8"><title>6. 全文总结</title><p>本文根据构建的多元文本情感数据集，结合深度学习对其进行特征抽取和多元情感分类任务，同时改进了相关的卷积神经网络结构和递归神经网络结构，提升了实验效果，提高了准确率，从而验证了结合情感词向量和情感词典的D &amp; W词向量、结合情感词向量和情感词频的T &amp; W词向量、结合情感词频和情感词典以及情感词向量的SSW情感语义词向量的有效性。</p></sec><sec id="s9"><title>文章引用</title><p>陈 楠,陈进才,卢 萍. 基于深度学习的多元文本情感研究与分析 Research and Analysis of Textual Multi-Emotion Based on Deep Learning[J]. 计算机科学与应用, 2018, 08(05): 669-686. https://doi.org/10.12677/CSA.2018.85076</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.25002-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Rumelhart, D.E., Hinton, G.E. and Williams, R.J. (1986) Learning Internal Representations by Error Propagation. MIT Press, Nature, 3l8-362.</mixed-citation></ref><ref id="hanspub.25002-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Bengio, Y., Ducharme, R., Vincent, P., et al. (2003) A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3, 1137-1155.</mixed-citation></ref><ref id="hanspub.25002-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Collobert, R. and Weston, J. (2008) A Unified Ar-chitecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. Proceedings of the Twen-ty-Fifth International Conference on Machine Learning (ICML 2008), Helsinki, Finland, 160-167.</mixed-citation></ref><ref id="hanspub.25002-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Hinton, G.E. (1986) Learning Distributed Representations of Concepts. Proceedings of the Eighth Annual Conference of the Cognitive Science Society, 1, 12.</mixed-citation></ref><ref id="hanspub.25002-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Mikolov, T., Sutskever, T., et al. (2013) Distributed Representations of Words and Phrases and Their Compositionality. Advances in Neural Information Processing Systems, 26, 3111-3119.</mixed-citation></ref><ref id="hanspub.25002-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">梁军, 柴玉梅, 原慧斌, 等. 基于深度学习的微博情感分析[J]. 中文信息学报, 2014, 28(5): 155-161.</mixed-citation></ref><ref id="hanspub.25002-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">陈翠平. 基于深度信念网络的文本分类算法[J]. 计算机系统应用, 2015, 24(2): 121-126.</mixed-citation></ref><ref id="hanspub.25002-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Kim, Y. (2014) Convolutional Neural Net-works for Sentence Classification. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, 25-29 October 2014, 1746-1751. 
&lt;br&gt;https://doi.org/10.3115/v1/D14-1181</mixed-citation></ref><ref id="hanspub.25002-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Pang, B., Lee, L. and Vaithyanathan, S. (2002) Sentiment Classification Using Machine Learning Techniques. Proceedings of the Conference on Empirical Methods in Natural Language Pro-cessing (EMNILP), July 2002, Philadelphia, 79-86.</mixed-citation></ref><ref id="hanspub.25002-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Davidov, D., Tsur, O. and Rappoport, A. (2010) Enhanced Sentiment Learning Using Twitter Hashtags and Smileys. International Conference on Computational Linguistics: Post-ers, Paris, 241-249.</mixed-citation></ref><ref id="hanspub.25002-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">李婷婷, 姬东鸿. 基于SVM和CRF多特征组合的微博情感分析[J]. 计算机应用研究, 2015, 32(4): 978-981.</mixed-citation></ref><ref id="hanspub.25002-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">李荣陆, 王建会, 陈晓云, 等. 使用最大熵模型进行中文文本分类[J]. 计算机研究与发展, 2005, 42(1): 94-101.</mixed-citation></ref><ref id="hanspub.25002-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Taboada, M., Brooke, J., Tofiloski, M., et al. (201l) Lexicon-Based Methods for Sen-timent Analysis. Computational Linguistics, 37, 267-307.</mixed-citation></ref><ref id="hanspub.25002-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Hu, M. and Liu, B. (2004) Mining and Summarizing Customer Reviews. Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Seattle, Washington DC, USA, August, 168-177.</mixed-citation></ref></ref-list></back></article>