<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114115</article-id><article-id pub-id-type="publisher-id">CSA-41958</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_65347508.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于交叉重构的领域自适应算法
  Cross Reconstruction-Based Domain Adaptation
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郭</surname><given-names>蔚颖</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>null</addr-line></aff><aff id="aff2"><label>1</label><addr-line>广东工业大学计算机学院，广东 广州</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>1113</fpage><lpage>1122</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   领域自适应是解决跨域识别的有效方法，它是迁移学习在计算机视觉方面的有效应用，将源域学到的知识迁移到目标域的识别任务中，有效解决目标域标注数据不足的问题。本文提出了一种新的基于交叉重构的领域自适应方法(Cross Reconstruction-based Domain Adaptation, CRDA)，通过对原始源域和目标域的交叉重构来构造新的源域与目标域，使得同类数据相互交织，缩短同类数据间的距离。并通过对重构矩阵施加低秩约束，将两个域的同类数据对齐，以此来充分挖掘源域和目标域同类数据之间的内在结构信息，并利用该结构信息来学习分类器，从而取得更好的跨域识别效果。在五个公开数据集上的实验结果表明CRDA有着较高的跨域识别准确率。 Domain adaptation is an effective method to solve the problem of cross-domain recognition. It is an effective application of transfer learning in computer vision, which transfers the knowledge learned in the source domain to the recognition task of the target domain, and effectively solves the problem of insufficient labeled data in the target domain. In this paper, a new Cross Reconstruction-based Domain Adaptation (CRDA) method is proposed, which constructs a new source domain and target domain through the cross reconstruction of the original source domain and target domain, so as to make the same kind of data intersect with each other and shorten the distance between the same kind of data. By applying low-rank constraints to the reconstruction matrix, the same kind of data in the two domains are aligned, so as to fully mine the internal structure information between the same kind of data in the source domain and the target domain, and use the structure information to learn the classifier, so as to achieve better cross-domain recognition effect. The experimental results on five open datasets show that CRDA has a high cross-domain recognition accuracy. 
  
 
</p></abstract><kwd-group><kwd>领域自适应，交叉重构，跨域识别, Domain Adaptation</kwd><kwd> Cross Reconstruction</kwd><kwd> Cross-Domain Recognition</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>领域自适应是解决跨域识别的有效方法，它是迁移学习在计算机视觉方面的有效应用，将源域学到的知识迁移到目标域的识别任务中，有效解决目标域标注数据不足的问题。本文提出了一种新的基于交叉重构的领域自适应方法(Cross Reconstruction-based Domain Adaptation, CRDA)，通过对原始源域和目标域的交叉重构来构造新的源域与目标域，使得同类数据相互交织，缩短同类数据间的距离。并通过对重构矩阵施加低秩约束，将两个域的同类数据对齐，以此来充分挖掘源域和目标域同类数据之间的内在结构信息，并利用该结构信息来学习分类器，从而取得更好的跨域识别效果。在五个公开数据集上的实验结果表明CRDA有着较高的跨域识别准确率。</p></sec><sec id="s2"><title>关键词</title><p>领域自适应，交叉重构，跨域识别</p></sec><sec id="s3"><title>Cross Reconstruction-Based Domain Adaptation</title><p>Weiying Guo</p><p>School of Computers, Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/36-1542122x4_hanspub.png" /></p><p>Received: Mar. 28<sup>th</sup>, 2021; accepted: Apr. 21<sup>st</sup>, 2021; published: Apr. 28<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/36-1542122x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Domain adaptation is an effective method to solve the problem of cross-domain recognition. It is an effective application of transfer learning in computer vision, which transfers the knowledge learned in the source domain to the recognition task of the target domain, and effectively solves the problem of insufficient labeled data in the target domain. In this paper, a new Cross Reconstruction-based Domain Adaptation (CRDA) method is proposed, which constructs a new source domain and target domain through the cross reconstruction of the original source domain and target domain, so as to make the same kind of data intersect with each other and shorten the distance between the same kind of data. By applying low-rank constraints to the reconstruction matrix, the same kind of data in the two domains are aligned, so as to fully mine the internal structure information between the same kind of data in the source domain and the target domain, and use the structure information to learn the classifier, so as to achieve better cross-domain recognition effect. The experimental results on five open datasets show that CRDA has a high cross-domain recognition accuracy.</p><p>Keywords:Domain Adaptation, Cross Reconstruction, Cross-Domain Recognition</p><disp-formula id="hanspub.41958-formula48"><graphic xlink:href="//html.hanspub.org/file/36-1542122x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/36-1542122x7_hanspub.png" /> <img src="//html.hanspub.org/file/36-1542122x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>随着在线自媒体和短视频的快速发展，对图像和其他多媒体数据的自动识别和分析的需求越来越大。然而，由于科技的飞速发展，很多领域却没有足量的有效标注数据，而通过人为进行数据的标注是十分昂贵并且耗费时间的，这导致了传统机器学习的局限性。因此，利用现有领域中充足的有标注数据来促进相关目标领域的模型学习的迁移学习方法是更经济、高效的方法。领域自适应是迁移学习在计算机视觉中对图片进行跨域识别的一种十分有效的方法 [<xref ref-type="bibr" rid="hanspub.41958-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.41958-ref2">2</xref>]。</p></sec><sec id="s6"><title>2. 相关工作</title><p>在这一节中，将介绍和本文相关的工作，潜在的低秩表征方法(Latent low-rank representation, LatLRR) [<xref ref-type="bibr" rid="hanspub.41958-ref3">3</xref>]，并且会详细介绍数据的交叉重构方法。</p><sec id="s6_1"><title>2.1. 潜在低秩表征</title><p>潜在低秩表征(Latent low-rank representation, LatLRR)是一种基于低秩表征的子空间学习方法，它能利用大量未观测样本来更好地表示原样本，它的原始目标函数如下：</p><p>min Z ‖ Z ‖ * s . t .       X = [ X O , X H ] Z (1)</p><p>其中， X O 代表能够观察到的数据， X H 代表着不能被直接观察到的数据，通过贝叶斯引理 [<xref ref-type="bibr" rid="hanspub.41958-ref4">4</xref>]，我们知道X可以被表示为 X = X Z + C X ，则LatLRR的目标函数如下：</p><p>min Z , E ‖ Z ‖ * + λ ‖ C ‖ 1 s . t .       X = X Z + C X (2)</p><p>其中， ‖   ⋅   ‖ * 表示矩阵的核范数，它的值是矩阵所有奇异值之和， ‖   ⋅   ‖ 1 表示矩阵的 l 1 范数，它的值为矩阵中的每个元素绝对值之和。Z是低秩约束的重构矩阵，C是投影矩阵，为了减少噪声的影响，LatLRR中引入了噪声矩阵E，最终的目标函数如下：</p><p>min Z , C , E ‖ Z ‖ * + ‖ C ‖ * + λ ‖ E ‖ 1 s . t .       X = X Z + C X + E (3)</p></sec><sec id="s6_2"><title>2.2. 交叉重构方法</title><p>在传统的领域自适应方法中，一般是把源域和目标域投影到一个子空间中，在这个子空间中，它们的分布差异较小 [<xref ref-type="bibr" rid="hanspub.41958-ref5">5</xref>]。然而这些方法都没有很充分的利用原始数据，对数据潜在的关系挖掘不够充分。我们采取了一种新的数据重构方法来使得数据能够被有效利用。 X s 和 X t 分别表示原始的源域和目标域，本文构造两个新的数据域X和Y，我们分别从原始数据域 X s 和 X t 的每一类中随机抽取相同数目的样本数据，然后交叉混合作为新域的每一类数据。如果 X s ∈ R m &#215; n s ， X t ∈ R m &#215; n t (其中m 表示样本数据的维度， n s 和 n t 分别表示源域和目标域中样本的数量)。我们让 X s i ∈ R m &#215; n 和 X t i ∈ R m &#215; n 分别表示从原始源域 X s 和 X t 的第i个类中抽取出的样本。则我们新构造的目标域可以被表示为： X = [ X s 1 X t 1 , X s 2 X t 2 , ⋯ , X s c X t c ] ∈ R m &#215; ( 2 ∗ n ∗ c ) 和 Y = [ X t 1 X s 1 , X t 2 X s 2 , ⋯ , X t c X s c ] ∈ R m &#215; ( 2 ∗ n ∗ c ) 。例如，在手写数字识别数据集USPS和MNIST中包含10个类，我们实验的时候从原始源域和目标域的每个类中抽取5个样本构成我们新域中的一个包含10个样本特征的类。L表示我们所构建的两个域的标签(他们的标签是一样的)。</p><p>通过数据的交叉重构，原始数据的每一个类都得到了很好地对齐，再通过LatLRR方法，我们能够把源域和目标域的主要信息提取出来，使它们相同类可以做到相互表征，这样就很好的把源域和目标域数据中相同标签类数据做到了很好的局部对齐。如此，得到新构成数据域的相互表示 X = Y Z + C Y + Z 。只要再把分类器的学习统一到框架中，就得到了我们的交叉重构领域自适应方法。</p></sec></sec><sec id="s7"><title>3. 交叉重构的领域自适应方法</title><p>这一章节，将详细介绍交叉重构的领域自适应(Cross Reconstruction-based Domain Adaptation, CRDA)方法。第一小节介绍它的目标函数。第二小节介绍它的优化方法和主要步骤。</p><sec id="s7_1"><title>3.1. CRDA的目标函数</title><p>由于低秩矩阵C能够很好的提取出数据的显著特征，考虑用C来作为的跨域识别分类器。于是，得到CRDA的目标函数如下：</p><p>min P , Q , Z , E , W ‖ L − C Y ‖ F 2 + λ 1 ‖ Z ‖ * + λ 2 ‖ E ‖ 2 , 1   + λ 3 ‖ C ‖ F 2 s .   t .       X = Y Z + C Y + E (4)</p><p>其中， ‖   ⋅   ‖ F 表示矩阵的Frobenius范数，值为矩阵中每个元素的平方和再开平方的值。 ‖   ⋅   ‖ 2 , 1 表示矩阵的2,1范数，它的值是矩阵每一行的 l 2 范数之和 [<xref ref-type="bibr" rid="hanspub.41958-ref6">6</xref>]。 λ 1 , λ 2 , λ 3 ≥ 0 是惩罚系数，用于调整每一项的权重。</p></sec><sec id="s7_2"><title>3.2. CRDA的优化计算方法</title><p>基于凸优化理论，可知本文的目标函数整体是非凸的，但是每个变量 Z , C , E 的优化都是凸优化问题 [<xref ref-type="bibr" rid="hanspub.41958-ref7">7</xref>]。因此本文使用迭代更新方式来优化目标函数(4)。为方便优化，我们引入辅助变量 A , H 替代 C , Z 。将(4)中的目标函数重写为以下形式：</p><p>min A , H , Z , C , E ‖ L − A Y ‖ F 2 + λ 1 ‖ H ‖ * + λ 2 ‖ E ‖ 2 , 1   + λ 3 ‖ A ‖ F 2 s .   t .       X = Y Z + C Y + E ,       Z = H , C = A (5)</p><p>式子(5)的增广拉格朗日函数为：</p><p>F ( A , H , Z , C , E ) = ‖ L − A Y ‖ F 2 + λ 1 ‖ H ‖ * + λ 2 ‖ E ‖ 2 , 1   + λ 3 ‖ A ‖ F 2 + 〈 τ 1 , X − Y Z − C Y − E 〉   + 〈 τ 2 , Z − H 〉 + 〈 τ 3 , C − A 〉 + μ 2 ( ‖ X − Y Z − C Y − E ‖ F 2 + ‖ Z − H ‖ F 2 + ‖ C − A ‖ F 2 ) (6)</p><p>其中， τ 1 , τ 2 , τ 3 是拉格朗日乘子， μ &gt; 0 是惩罚系数。变量通过交替方向乘子法(Alternating Direction Method of Multipliers, ADMM) [<xref ref-type="bibr" rid="hanspub.41958-ref8">8</xref>] 更新，更新一个系数时，会保证其他的系数固定不变。迭代优化步骤如下：</p><p>步骤1 (更新A)：固定H，Z，C，E，并求解以下式子：</p><p>min A ‖ L − A Y ‖ F 2 + λ 3 ‖ A ‖ F 2 + μ 2 ‖ C − A + τ 2 μ ‖ F 2 (7)</p><p>令 F ( A ) = ‖ L − A Y ‖ F 2 + λ 3 ‖ A ‖ F 2 + μ 2 ‖ C − A + τ 2 μ ‖ F 2 ，通过令偏导 ∂ F ( A ) ∂ A = 0 ，可以得到A的解如下：</p><p>A = [ 2 L Y T + μ ( C + τ 3 μ ) ] [ 2 Y Y T + ( 2 λ 3 + μ ) E ] − 1</p><p>步骤2 (更新H)：固定A，Z，C，E，可以通过求解以下式子：</p><p>min H λ 1 ‖ H ‖ * + μ 2 ‖ Z − H + τ 2 μ ‖ F 2 (8)</p><p>可以得到H的解如下：</p><p>H = Θ λ 1 / μ ( Z + τ 2 μ )</p><p>步骤3 (更新Z)：固定A，H，C，E，并求解以下式子：</p><p>min Q μ 2 ( ‖ X − Y Z − C Y − E + τ 1 μ ‖ F 2 + ‖ Z − H + τ 2 μ ‖ F 2 ) (9)</p><p>令 F ( Z ) = min Q μ 2 ( ‖ X − Y Z − C Y − E + τ 1 μ ‖ F 2 + ‖ Z − H + τ 2 μ ‖ F 2 ) ，通过求 ∂ F ( Z ) ∂ Z = 0 ,得到Z的解为：</p><p>Z = ( Y T Y + E ) − 1 [ Y T ( X − C Y − E + τ 1 μ ) − τ 2 μ + H ]</p><p>步骤4 (更新C)：固定A，H，Z，E，并求解以下式子：</p><p>min C μ 2 ( ‖ X − Y Z − C Y − E + τ 1 μ ‖ F 2 + ‖ C − A + τ 3 μ ‖ F 2 ) (10)</p><p>令 F ( C ) = min Q μ 2 ( ‖ X − Y Z − C Y − E + τ 1 μ ‖ F 2 + ‖ C − A + τ 3 μ ‖ F 2 ) ，通过求 ∂ F ( C ) ∂ C = 0 得到C的解为：</p><p>C = [ ( Y Z + E − X − τ 1 μ ) Y T + A − τ 3 μ ] ( I + Y Y T ) − 1 (I为单位矩阵)</p><p>步骤5 (更新E)，固定A，H，Z，C，并求解以下式子：</p><p>min E λ 2 ‖ E ‖ 2 , 1 + μ 2 ‖ X − Y Z − C Y − E + τ 1 μ ‖ F 2 (11)</p><p>对于(11)，可以根据以下引理来求解 [<xref ref-type="bibr" rid="hanspub.41958-ref9">9</xref>]：</p><p>引理1：对于问题:</p><p>min w α ‖ W ‖ 2 , 1 + 1 2 ‖ W − Q ‖ F 2</p><p>其中Q是一个已知矩阵，如果该问题的最优解是 W * ，那么 W * 的第i列的值如下：</p><p>[ W * ] : , i { ‖ Q : , i ‖ 2 − α ‖ Q : , i ‖ 2 Q : , i ,         当 ‖ Q : , i ‖ 2 &gt; α   时 0 ,                                                                                                                                           其 他 情 况</p><p>用 τ 2 μ 替换其中的 α ，用 X − Y Z − C Y + τ 1 μ 替换其中的Q，就能够得到E的解</p><p>算法1总结了CRDA的优化框架如下：</p></sec></sec><sec id="s8"><title>4. 实验</title><p>为了验证交叉重构领域适应方法(CRDA)的有效性，这一节将让CRDA在COIL20, MNIST &amp; USPS, MSRC &amp; VOC2007, Office &amp; Caltech 和Office-Home这5个基准数据集上分别进行实验。从这些数据集的源域和目标域的每一类中随机抽取5个样本来构造新的数据域，剩下的数据作为测试样本使用。通过CRDA得到分类矩阵A，使用AX来作为最后的分类标签。如果第i个测试样本的特征向量为 x i ∈ R m ，通过计算得到它的标签向量 l i = A x i ∈ R c = { l i 1 , l i 2 , ⋯ , l i c } ，则标签向量 l i 中最大值所处位置为该样本所属标签类。例如，我们得到了第i个测试样本的标签向量 l i = { l i 1 , l i 2 , ⋯ , l i c } ∈ R c ，并且 max { l i 1 , l i 2 , ⋯ , l i c } = l i k ，其中 1 ≤ k ≤ c ，则我们把这个样本划分到第k类数据当中。实验在Matlab2019b, Intel(R) Core(TM) i7-6700 CPU @3.40GHz环境下进行，为了保证实验结果的有效性，CRDA算法的最终的识别效率为20次实验的平均值。</p><sec id="s8_1"><title>4.1. 数据集介绍</title><p>COIL 20数据集：该数据集包含20个不同对象以360度旋转成像。每旋转5度收集一张物体的图像，即每个物体有72幅图像，均为不同角度，共1440张图像。所有的图片裁剪并转换为32 &#215; 32像素的灰度图像。</p><p>MSRC &amp; VOC2007数据集：MSRC包含了18个层次的4323幅图像，而VOC 2007包含了20个概念的5011幅图像，它们分别共享了飞机、羊、汽车、牛、鸟、自行车6个语义类，并且将所有图像调整为256像素。</p><p>MNIST &amp; USPS数据集：该数据集中有7291幅训练图像和2007幅测试图像；MNIST数据集中有60,000幅训练图像和10,000幅测试图像，这两个数据集共有10个语义类，每个语义类对应数字0~9，所有的图片都被转换为16 &#215; 16像素的灰度图像。</p><p>Office &amp; Caltech256数据集：Office数据集是视觉对象识别的基准数据，包括来自三个不同领域的常见对象类别，即A(Amazon)、W(Webcam)和D(DSLR)，每个领域共有31个对象类别。例如笔记本电脑、键盘、显示器、自行车等，一共含有4652张图片。在Amazon域中，每个类别平均有90张图片，而在DSLR或Webcam域中，每个类别平均有30张图片。Caltech-256数据集是用于目标识别的标准数据集，有30,607幅图像和256个类别。</p><p>Office-Home数据集：该数据集由来自4个不同领域的图像组成:艺术图像、剪辑艺术、产品图像、现实世界图像，对于每个领域，数据集包含65个对象类别的图像，通常在办公室和家庭中发现。实验中的数据特征是由预先训练好的ResNet50模型提取得到的。</p></sec><sec id="s8_2"><title>4.2. 超参数设置</title><p>如式(5)所示，CRDA中包3个超参数，他们分别是 λ 1 , λ 2 , λ 3 ，它们的取值范围都设定为 { 1 e − 8 , 1 e − 7 , ⋯ , 1 } ，超参数的最后取值通过网格搜索策略确定。对于数据集COIL 20，最终参数选择为 λ 1 = 1 e − 6 , λ 2 = 1 e − 5 , λ 3 = 1 e − 3 ；对于数据集MSRC &amp; VOC2007，最终参数选择为 λ 1 = 1 e − 8 , λ 2 = 1 e − 6 , λ 3 = 1 ；对于数据集MNIST &amp; USPS，最终参数选择为 λ 1 = 1 e − 1 , λ 2 = 1 e − 5 , λ 3 = 1 - 3 ；对于数据集Office &amp; Caltech，最终参数的选择为 λ 1 = 1 e − 2 , λ 2 = 1 e − 6 , λ 3 = 1 ；对于Office-Home数据集，最终参数选择为 λ 1 = 1 e − 6 , λ 2 = 1 e − 6 , λ 3 = 1 e − 7 。</p></sec><sec id="s8_3"><title>4.3. 实验结果和分析</title><p>为了验证CRDA算法的有效性，我们选取了几个迁移学习和领域适应的基线方法做对比，它们分别是Geodesicflow Kernel (GFK) [<xref ref-type="bibr" rid="hanspub.41958-ref10">10</xref>], Low-rank Transfer Subspace Learning (LTSL) [<xref ref-type="bibr" rid="hanspub.41958-ref11">11</xref>], Fisher Discrimination Dictionary Learning (FDDL) [<xref ref-type="bibr" rid="hanspub.41958-ref12">12</xref>], Joint Geometrical and Statistical Alignment (JGSA) [<xref ref-type="bibr" rid="hanspub.41958-ref13">13</xref>], Weakly-Supervised Cross-Domain Dictionary Learning for Visual Recognition (WSCDDL) [<xref ref-type="bibr" rid="hanspub.41958-ref14">14</xref>], Visual Domain Adaptation with Manifold Embedded Distribution Alignment (MEDA) [<xref ref-type="bibr" rid="hanspub.41958-ref15">15</xref>]。</p><sec id="s8_3_1"><title>4.3.1. 实验结果</title><p>表1展示了CRDA与对比算法在COIL20、MSRC &amp; VOC2007、MNIST &amp; USPS这三个数据集上的识别准确率。表2展示了CRDA与对比算法在Office + Caltech256数据集上的识别准确率。表3展示了CRDA与对比算法在Office-Home数据集上的识别准确率。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Experimental results on three different dataset</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Dataset</th><th align="center" valign="middle" >GFK</th><th align="center" valign="middle" >LTSL</th><th align="center" valign="middle" >FDDL</th><th align="center" valign="middle" >JGSA</th><th align="center" valign="middle" >WSCDDL</th><th align="center" valign="middle" >MEDA</th><th align="center" valign="middle" >CRDA</th></tr></thead><tr><td align="center" valign="middle" >C O I L 1 → C O I L 2</td><td align="center" valign="middle" >91.02</td><td align="center" valign="middle" >36.38</td><td align="center" valign="middle" >41.93</td><td align="center" valign="middle" >91.83</td><td align="center" valign="middle" >84.90</td><td align="center" valign="middle" >90.00</td><td align="center" valign="middle" >92.68</td></tr><tr><td align="center" valign="middle" >C O I L 2 → C O I L 1</td><td align="center" valign="middle" >90.38</td><td align="center" valign="middle" >39.27</td><td align="center" valign="middle" >39.97</td><td align="center" valign="middle" >90.39</td><td align="center" valign="middle" >85.60</td><td align="center" valign="middle" >90.83</td><td align="center" valign="middle" >84.61</td></tr><tr><td align="center" valign="middle" >M S R C → V O C</td><td align="center" valign="middle" >29.04</td><td align="center" valign="middle" >24.62</td><td align="center" valign="middle" >29.97</td><td align="center" valign="middle" >29.98</td><td align="center" valign="middle" >30.40</td><td align="center" valign="middle" >36.08</td><td align="center" valign="middle" >33.01</td></tr><tr><td align="center" valign="middle" >V O C → M S R C</td><td align="center" valign="middle" >58.11</td><td align="center" valign="middle" >46.63</td><td align="center" valign="middle" >60.98</td><td align="center" valign="middle" >60.55</td><td align="center" valign="middle" >64.52</td><td align="center" valign="middle" >54.85</td><td align="center" valign="middle" >72.05</td></tr><tr><td align="center" valign="middle" >M N I S T → U S P S</td><td align="center" valign="middle" >72.03</td><td align="center" valign="middle" >36.32</td><td align="center" valign="middle" >75.85</td><td align="center" valign="middle" >72.90</td><td align="center" valign="middle" >74.05</td><td align="center" valign="middle" >39.94</td><td align="center" valign="middle" >78.40</td></tr><tr><td align="center" valign="middle" >U S P S → M N I S T</td><td align="center" valign="middle" >63.04</td><td align="center" valign="middle" >39.13</td><td align="center" valign="middle" >60.54</td><td align="center" valign="middle" >62.97</td><td align="center" valign="middle" >62.88</td><td align="center" valign="middle" >45.40</td><td align="center" valign="middle" >64.50</td></tr><tr><td align="center" valign="middle" >Average</td><td align="center" valign="middle" >67.33</td><td align="center" valign="middle" >37.06</td><td align="center" valign="middle" >51.54</td><td align="center" valign="middle" >63.73</td><td align="center" valign="middle" >66.97</td><td align="center" valign="middle" >59.52</td><td align="center" valign="middle" >70.88</td></tr></tbody></table></table-wrap><p>表1. 在三个不同数据集上的实验结果</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Experimental results on the Office + Caltech datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Dataset</th><th align="center" valign="middle" >GFK</th><th align="center" valign="middle" >LTSL</th><th align="center" valign="middle" >FDDL</th><th align="center" valign="middle" >JGSA</th><th align="center" valign="middle" >WSCDDL</th><th align="center" valign="middle" >MEDA</th><th align="center" valign="middle" >CRDA</th></tr></thead><tr><td align="center" valign="middle" >A → C</td><td align="center" valign="middle" >36.97</td><td align="center" valign="middle" >34.99</td><td align="center" valign="middle" >37.65</td><td align="center" valign="middle" >37.65</td><td align="center" valign="middle" >38.97</td><td align="center" valign="middle" >43.99</td><td align="center" valign="middle" >42.25</td></tr><tr><td align="center" valign="middle" >A → D</td><td align="center" valign="middle" >53.63</td><td align="center" valign="middle" >38.49</td><td align="center" valign="middle" >51.23</td><td align="center" valign="middle" >55.76</td><td align="center" valign="middle" >53.24</td><td align="center" valign="middle" >45.86</td><td align="center" valign="middle" >54.49</td></tr><tr><td align="center" valign="middle" >A → W</td><td align="center" valign="middle" >59.34</td><td align="center" valign="middle" >39.58</td><td align="center" valign="middle" >59.79</td><td align="center" valign="middle" >59.98</td><td align="center" valign="middle" >58.83</td><td align="center" valign="middle" >53.22</td><td align="center" valign="middle" >61.72</td></tr><tr><td align="center" valign="middle" >D → A</td><td align="center" valign="middle" >45.78</td><td align="center" valign="middle" >42.41</td><td align="center" valign="middle" >45.64</td><td align="center" valign="middle" >47.10</td><td align="center" valign="middle" >46.51</td><td align="center" valign="middle" >41.23</td><td align="center" valign="middle" >50.33</td></tr><tr><td align="center" valign="middle" >D → C</td><td align="center" valign="middle" >33.64</td><td align="center" valign="middle" >34.94</td><td align="center" valign="middle" >36.20</td><td align="center" valign="middle" >34.96</td><td align="center" valign="middle" >34.43</td><td align="center" valign="middle" >34.91</td><td align="center" valign="middle" >37.79</td></tr><tr><td align="center" valign="middle" >D → W</td><td align="center" valign="middle" >79.12</td><td align="center" valign="middle" >70.12</td><td align="center" valign="middle" >78.60</td><td align="center" valign="middle" >79.69</td><td align="center" valign="middle" >76.60</td><td align="center" valign="middle" >87.46</td><td align="center" valign="middle" >78.70</td></tr><tr><td align="center" valign="middle" >C → A</td><td align="center" valign="middle" >46.70</td><td align="center" valign="middle" >40.29</td><td align="center" valign="middle" >48.70</td><td align="center" valign="middle" >48.90</td><td align="center" valign="middle" >50.29</td><td align="center" valign="middle" >56.58</td><td align="center" valign="middle" >53.30</td></tr><tr><td align="center" valign="middle" >C → D</td><td align="center" valign="middle" >57.43</td><td align="center" valign="middle" >40.49</td><td align="center" valign="middle" >58.03</td><td align="center" valign="middle" >58.29</td><td align="center" valign="middle" >56.62</td><td align="center" valign="middle" >50.32</td><td align="center" valign="middle" >56.44</td></tr><tr><td align="center" valign="middle" >C → W</td><td align="center" valign="middle" >57.16</td><td align="center" valign="middle" >42.01</td><td align="center" valign="middle" >62.97</td><td align="center" valign="middle" >57.87</td><td align="center" valign="middle" >68.32</td><td align="center" valign="middle" >53.90</td><td align="center" valign="middle" >71.02</td></tr><tr><td align="center" valign="middle" >W → A</td><td align="center" valign="middle" >45.11</td><td align="center" valign="middle" >44.16</td><td align="center" valign="middle" >45.78</td><td align="center" valign="middle" >47.89</td><td align="center" valign="middle" >47.45</td><td align="center" valign="middle" >42.69</td><td align="center" valign="middle" >49.03</td></tr><tr><td align="center" valign="middle" >W → C</td><td align="center" valign="middle" >32.50</td><td align="center" valign="middle" >36.44</td><td align="center" valign="middle" >34.79</td><td align="center" valign="middle" >35.60</td><td align="center" valign="middle" >36.44</td><td align="center" valign="middle" >34.28</td><td align="center" valign="middle" >37.36</td></tr><tr><td align="center" valign="middle" >W → D</td><td align="center" valign="middle" >67.22</td><td align="center" valign="middle" >69.09</td><td align="center" valign="middle" >69.78</td><td align="center" valign="middle" >70.25</td><td align="center" valign="middle" >62.33</td><td align="center" valign="middle" >88.54</td><td align="center" valign="middle" >73.97</td></tr><tr><td align="center" valign="middle" >Average</td><td align="center" valign="middle" >51.22</td><td align="center" valign="middle" >43.75</td><td align="center" valign="middle" >52.41</td><td align="center" valign="middle" >52.93</td><td align="center" valign="middle" >52.68</td><td align="center" valign="middle" >52.75</td><td align="center" valign="middle" >55.53</td></tr></tbody></table></table-wrap><p>表2. 在Office + Caltech数据集上的实验结果</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Experimental results on the Office-Home datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Dataset</th><th align="center" valign="middle" >GFK</th><th align="center" valign="middle" >LTSL</th><th align="center" valign="middle" >FDDL</th><th align="center" valign="middle" >JGSA</th><th align="center" valign="middle" >WSCDDL</th><th align="center" valign="middle" >MEDA</th><th align="center" valign="middle" >CRDA</th></tr></thead><tr><td align="center" valign="middle" >A r → C l</td><td align="center" valign="middle" >39.74</td><td align="center" valign="middle" >33.30</td><td align="center" valign="middle" >43.89</td><td align="center" valign="middle" >44.51</td><td align="center" valign="middle" >45.56</td><td align="center" valign="middle" >39.62</td><td align="center" valign="middle" >47.51</td></tr><tr><td align="center" valign="middle" >A r → Pr</td><td align="center" valign="middle" >61.83</td><td align="center" valign="middle" >54.01</td><td align="center" valign="middle" >67.40</td><td align="center" valign="middle" >65.86</td><td align="center" valign="middle" >67.62</td><td align="center" valign="middle" >57.09</td><td align="center" valign="middle" >67.81</td></tr><tr><td align="center" valign="middle" >A r → R w</td><td align="center" valign="middle" >61.28</td><td align="center" valign="middle" >36.25</td><td align="center" valign="middle" >65.50</td><td align="center" valign="middle" >65.11</td><td align="center" valign="middle" >65.48</td><td align="center" valign="middle" >65.43</td><td align="center" valign="middle" >67.46</td></tr><tr><td align="center" valign="middle" >C l → A r</td><td align="center" valign="middle" >33.63</td><td align="center" valign="middle" >21.56</td><td align="center" valign="middle" >42.70</td><td align="center" valign="middle" >41.00</td><td align="center" valign="middle" >43.76</td><td align="center" valign="middle" >39.28</td><td align="center" valign="middle" >45.73</td></tr><tr><td align="center" valign="middle" >C l → Pr</td><td align="center" valign="middle" >54.16</td><td align="center" valign="middle" >42.52</td><td align="center" valign="middle" >57.63</td><td align="center" valign="middle" >56.31</td><td align="center" valign="middle" >59.72</td><td align="center" valign="middle" >50.45</td><td align="center" valign="middle" >60.38</td></tr><tr><td align="center" valign="middle" >C l → R w</td><td align="center" valign="middle" >49.67</td><td align="center" valign="middle" >31.04</td><td align="center" valign="middle" >55.64</td><td align="center" valign="middle" >53.40</td><td align="center" valign="middle" >54.28</td><td align="center" valign="middle" >52.10</td><td align="center" valign="middle" >55.75</td></tr><tr><td align="center" valign="middle" >Pr → A r</td><td align="center" valign="middle" >35.60</td><td align="center" valign="middle" >32.48</td><td align="center" valign="middle" >47.80</td><td align="center" valign="middle" >46.23</td><td align="center" valign="middle" >48.10</td><td align="center" valign="middle" >42.09</td><td align="center" valign="middle" >48.48</td></tr><tr><td align="center" valign="middle" >Pr → C l</td><td align="center" valign="middle" >36.42</td><td align="center" valign="middle" >32.08</td><td align="center" valign="middle" >42.89</td><td align="center" valign="middle" >43.74</td><td align="center" valign="middle" >43.74</td><td align="center" valign="middle" >31.83</td><td align="center" valign="middle" >44.25</td></tr><tr><td align="center" valign="middle" >Pr → R w</td><td align="center" valign="middle" >60.46</td><td align="center" valign="middle" >40.50</td><td align="center" valign="middle" >63.64</td><td align="center" valign="middle" >64.46</td><td align="center" valign="middle" >65.82</td><td align="center" valign="middle" >63.92</td><td align="center" valign="middle" >66.94</td></tr><tr><td align="center" valign="middle" >R w → A r</td><td align="center" valign="middle" >49.20</td><td align="center" valign="middle" >34.23</td><td align="center" valign="middle" >59.86</td><td align="center" valign="middle" >58.22</td><td align="center" valign="middle" >60.07</td><td align="center" valign="middle" >55.43</td><td align="center" valign="middle" >58.40</td></tr><tr><td align="center" valign="middle" >R w → C l</td><td align="center" valign="middle" >41.56</td><td align="center" valign="middle" >39.99</td><td align="center" valign="middle" >48.89</td><td align="center" valign="middle" >47.23</td><td align="center" valign="middle" >49.27</td><td align="center" valign="middle" >41.27</td><td align="center" valign="middle" >48.70</td></tr><tr><td align="center" valign="middle" >R w → Pr</td><td align="center" valign="middle" >69.56</td><td align="center" valign="middle" >54.63</td><td align="center" valign="middle" >72.65</td><td align="center" valign="middle" >72.37</td><td align="center" valign="middle" >73.42</td><td align="center" valign="middle" >70.44</td><td align="center" valign="middle" >73.32</td></tr><tr><td align="center" valign="middle" >Average</td><td align="center" valign="middle" >49.42</td><td align="center" valign="middle" >37.71</td><td align="center" valign="middle" >56.12</td><td align="center" valign="middle" >54.87</td><td align="center" valign="middle" >56.40</td><td align="center" valign="middle" >50.75</td><td align="center" valign="middle" >57.06</td></tr></tbody></table></table-wrap><p>表3. 在Office-Home数据集上的实验结果</p></sec><sec id="s8_3_2"><title>4.3.2. 实验结果分析</title><p>对于上述实验结果，我们进行如下分析：</p><p>1). 该方法在大多数数据集上的性能都优于与之比较的方法。这表明，使用改进的类PCA正则化项，CRDA能够很好地保存数据信息。此外，通过对重构矩阵施加低秩约束，可以将来自不同领域但共享同一标签的数据很好的对齐。这就保证了新的特征表示的识别性，因此我们的方法可以显著提高识别准确率。</p><p>2). 在高维数据集 Office-home上的实验结果表明，CRDA在大多数高维数据集上表现良好。结果表明，我们的方法能够很好的处理高维度数据。</p><p>3). 在半监督领域适应中，人工添加伪标签的对抗式方法是很常见的。在本文中，我们使用原始数据重构代替伪标签来提高分类精度。通过实验，我们发现CRDA的性能比MEDA更好，证明了我们方法的有效性。</p></sec><sec id="s8_3_3"><title>4.3.3. 模型收敛性分析</title><p>ADMM在只有两个或更少块时的收敛性已经被证明。算法1有7个分块，没有严格的理论收敛性支持。然而，一些理论可以扩展ADMM的收敛性的范围。例如，Jia等人在单个函数是强凸函数和线性函数复合 [<xref ref-type="bibr" rid="hanspub.41958-ref16">16</xref>] 的前提下证明了具有两个块的可分离凸规划的经典ADMM可以推广到三个或三个以上的块。Hong等人证明了在增广拉格朗日函数 [<xref ref-type="bibr" rid="hanspub.41958-ref17">17</xref>] 中惩罚参数足够大的情况下，经典ADMM收敛于平稳解集。Luo和Hong指出，当线性约束的非光滑凸可分离函数的和最小时，ADMM与n(n &gt; 2)变量块也可以收敛 [<xref ref-type="bibr" rid="hanspub.41958-ref18">18</xref>]。从图1的实验结果中可以证明，CRDA算法是有效收敛的。</p><p>图1. 模型收敛性分析</p></sec><sec id="s8_3_4"><title>4.3.4. 参数敏感性分析</title><p>从图2中我们可以发现，在CRDA中，随着 λ 1 的变化，识别准确率的变化不大，这是因为 λ 1 控制的是低秩矩阵的权重，而低秩矩阵本身就很小， λ 1 使得它更小，所以对识别准确率的影响不大。随着 λ 2 的增大，识别准确率会有一个先减小再增大再减小的趋势，这是因为 λ 2 控制的是噪声矩阵所占的比重，随着它的变化，分类器的比重会相应的变化，所以对识别准确率的影响是波动的。 λ 3 控制的是分类器权重，从图中我们可以知道，对于不同的数据集，随着 λ 3 的增长，准确率的变化是不同的，说明分类器矩阵和数据集是十分相关的。</p><p>图2. 超参数 λ 1 , λ 2 , λ 3 对识别准确率的影响</p></sec></sec></sec><sec id="s9"><title>5. 结论</title><p>本文提出了一种基于交叉重构的领域自适应算法用于跨域识别。该算法通过将源域和目标域数据进行交叉重构来使原始源域和目标域做到相互表示，同时对重构矩阵施加低秩约束来对齐同类数据，从而挖掘同类数据间的相似性，以最大程度保留数据主要信息。最后对分类矩阵施加稀疏约束，来去除数据冗余信息，达到提高模型性能的目的。在5个常用的数据集COIL20，MNIST &amp; USPS, MSRC&amp;VOC2007, Office &amp; Caltech, Office-Home上验证模型的有效性。实验结果显示，与其他相关的传统领域自适应方法相比，CRDA算法能更好地保留投影数据信息，取得更好的跨域识别效果。</p></sec><sec id="s10"><title>文章引用</title><p>郭蔚颖. 基于交叉重构的领域自适应算法Cross Reconstruction-Based Domain Adaptation[J]. 计算机科学与应用, 2021, 11(04): 1113-1122. https://doi.org/10.12677/CSA.2021.114115</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41958-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Weiss, K., Khoshgoftaar, T.M. and Wang, D.D. (2016) A Survey of Transfer Learning. Journal of Big Data, 3, Article No. 9. &lt;br&gt;https://doi.org/10.1186/s40537-016-0043-6</mixed-citation></ref><ref id="hanspub.41958-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Dai, W., Qiang, Y., Xue, G. and Yong, Y. (2007) Boost-ing for Transfer Learning. Proceedings of the 24th International Conference on Machine Learning, Corvallis, 22-24 June 2007, 193-200. &lt;br&gt;https://doi.org/10.1145/1273496.1273521</mixed-citation></ref><ref id="hanspub.41958-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Liu, G. and Yan, S. (2011) Latent Low-Rank Rep-resentation for Subspace Segmentation and Feature Extraction. IEEE International Conference on Computer Vision, Bar-celona, 6-13 November 2011, 1615-1622.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2011.6126422</mixed-citation></ref><ref id="hanspub.41958-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Pan, Z., Lin, Z. and Chao, Z. (2016) Integrated Low-Rank-Based Discriminative Feature Learning for Recognition. IEEE Transactions on Neural Networks and Learning Systems, 27, 1080-1093.  
&lt;br&gt;https://doi.org/10.1109/TNNLS.2015.2436951</mixed-citation></ref><ref id="hanspub.41958-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Pan, S.J., Tsang, I.W., Kwok, J.T. and Yang, Q. (2011) Do-main Adaptation via Transfer Component Analysis. IEEE Transactions on Neural Networks, 22, 199-210. &lt;br&gt;https://doi.org/10.1109/TNN.2010.2091281</mixed-citation></ref><ref id="hanspub.41958-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">屈磊, 方怡, 熊友玲, 唐俊. 基于L2,1模和图正则化的低秩迁移子空间学习[J]. 控制理论与应用, 2018, 35(12): 1738-1749.</mixed-citation></ref><ref id="hanspub.41958-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Georghiades, A.S., Belhumeur, P.N. and Kriegman, D.J. (2002) From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 23, 643-660. &lt;br&gt;https://doi.org/10.1109/34.927464</mixed-citation></ref><ref id="hanspub.41958-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">吴越, 曾向荣, 周典乐, 刘衍, 周家庆, 周经纶. 图像恢复中的稳健交替方向乘子法[J]. 国防科技大学学报, 2018, 40(2): 115-121.</mixed-citation></ref><ref id="hanspub.41958-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Yang, J., Yin, W., Zhang, Y. and Wang, Y. (2009) A Fast Algorithm for Edge-Preserving Variational Multichannel Image Restoration. SIAM Journal on Imaging Sciences, 2, 569-592. &lt;br&gt;https://doi.org/10.1137/080730421</mixed-citation></ref><ref id="hanspub.41958-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Gong, B., Yuan, S., Fei, S. and Grauman, K. (2015) Geodesic Flow Kernel for Unsupervised Domain Adaptation. 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, 16-21 June 2015, 2066-2073.</mixed-citation></ref><ref id="hanspub.41958-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Ming, S., Kit, D. and Yun, F. (2014) Generalized Transfer Subspace Learning through Low-Rank Constraint. International Journal of Computer Vision, 109, 74-93. &lt;br&gt;https://doi.org/10.1007/s11263-014-0696-6</mixed-citation></ref><ref id="hanspub.41958-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Yang, M., Zhang, L., Feng, X. and Zhang, D. (2014) Sparse Representation Based Fisher Discrimination Dictionary Learning for Image Classification. International Journal of Computer Vision, 109, 209-232.  
&lt;br&gt;https://doi.org/10.1007/s11263-014-0722-8</mixed-citation></ref><ref id="hanspub.41958-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Jing, Z., Li, W. and Ogunbona, P. (2017) Joint Geometrical and Statistical Alignment for Visual Domain Adaptation. CVPR 2017, Honolulu, 21-26 July 2017, 5150-5158.</mixed-citation></ref><ref id="hanspub.41958-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, F. and Shao, L. (2014) Weakly-Supervised Cross-Domain Dictionary Learning for Visual Recognition. International Jour-nal of Computer Vision, 109, 42-59. &lt;br&gt;https://doi.org/10.1007/s11263-014-0703-y</mixed-citation></ref><ref id="hanspub.41958-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Wang, J., Feng, W., Chen, Y., Han, Y. and Yu, P.S. (2018) Visual Domain Adaptation with Manifold Embedded Distribution Alignment. ACM In-ternational Conference on Multimedia, Seoul, 22-26 October 2018, 402-410.  
&lt;br&gt;https://doi.org/10.1145/3240508.3240512</mixed-citation></ref><ref id="hanspub.41958-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Jia, Z., Ke, G. and Cai, X. (2013) Convergence Analysis of Alter-nating Direction Method of Multipliers for a Class of Separable Convex Programming. Abstract and Applied Analysis, 2013, 205-215. &lt;br&gt;https://doi.org/10.1155/2013/680768</mixed-citation></ref><ref id="hanspub.41958-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Hong, M., Luo, Z.Q. and Razaviyayn, M. (2015) Con-vergence Analysis of Alternating Direction Method of Multipliers for a Family of Nonconvex Problems. SIAM Journal on Optimization, 26, 337-364.  
&lt;br&gt;https://doi.org/10.1109/ICASSP.2015.7178689</mixed-citation></ref><ref id="hanspub.41958-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Hong, M. and Luo, Z.Q. (2012) On the Linear Convergence of the Alternating Direction Method of Multipliers. Mathematical Programming, 162, 165-199. &lt;br&gt;https://doi.org/10.1007/s10107-016-1034-2</mixed-citation></ref></ref-list></back></article>