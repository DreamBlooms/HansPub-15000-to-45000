<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114101</article-id><article-id pub-id-type="publisher-id">CSA-41782</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_48463544.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于图神经网络的行人重识别方法
  Person Re-Identification Method Based on Graph Neural Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郝</surname><given-names>志峰</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>苏</surname><given-names>伟根</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>许</surname><given-names>柏炎</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>温</surname><given-names>雯</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>蔡</surname><given-names>瑞初</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>广东工业大学计算机学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff2"><addr-line>佛山科学技术学院数学与大数据学院，广东 佛山;广东工业大学计算机学院，广东 广州</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>983</fpage><lpage>993</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   基于深度学习的行人重识别方法面临的挑战在于如何解决由于行人姿态多变、背景混杂、摄像头视角差异大和部分遮挡等情况引起的行人不对齐的问题。如何提取细粒度的、具有强判别性的特征成为解决问题的关键，因此本文提出了一种新型的基于图神经网络的行人重识别方法，其中包括：1) 结合人体语义解析结果提取细粒度局部特征，构建部位关系图，并通过图神经网络学习到细粒度的图表示。2) 通过图度量学习方法联合优化学习网络，学习强判别性的特征表示。本文提出的方法在主流评估数据集上与行人重识别前沿方法进行实验比较，结果表明了方法的有效性。 The challenge for the deep learning-based person re-identification method is how to solve the problem of pedestrian misalignment caused by pedestrian posture change, mixed background, large camera viewing angle difference, and partial occlusion. How to extract fine-grained and highly discriminative features has become the key to solving the problem. Therefore, a new person re-identification method based on graph neural network is proposed, which includes: 1) Combine the human semantic parsing model to locate the position map and learn fine-grained local features as graph representation through the graph neural network. 2) Through the proposed graph metric learning method to jointly optimize the learning network to learn more discriminative feature representation. Experiments were performed on mainstream evaluation datasets, and the results showed the effectiveness of the method. 
  
 
</p></abstract><kwd-group><kwd>行人重识别，图度量学习，人体语义解析，局部对齐, Person Re-Identification</kwd><kwd> Graph Metric Learning</kwd><kwd> Human Semantic Parsing</kwd><kwd> Part Alignment</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>基于深度学习的行人重识别方法面临的挑战在于如何解决由于行人姿态多变、背景混杂、摄像头视角差异大和部分遮挡等情况引起的行人不对齐的问题。如何提取细粒度的、具有强判别性的特征成为解决问题的关键，因此本文提出了一种新型的基于图神经网络的行人重识别方法，其中包括：1) 结合人体语义解析结果提取细粒度局部特征，构建部位关系图，并通过图神经网络学习到细粒度的图表示。2) 通过图度量学习方法联合优化学习网络，学习强判别性的特征表示。本文提出的方法在主流评估数据集上与行人重识别前沿方法进行实验比较，结果表明了方法的有效性。</p></sec><sec id="s2"><title>关键词</title><p>行人重识别，图度量学习，人体语义解析，局部对齐</p></sec><sec id="s3"><title>Person Re-Identification Method Based on Graph Neural Network</title><p>Zhifeng Hao<sup>1,2</sup>, Weigen Su<sup>2</sup>, Boyan Xu<sup>2</sup>, Wen Wen, Ruichu Cai<sup>2</sup></p><p><sup>1</sup>School of Mathematics and Big Data, Foshan University, Foshan Guangdong</p><p><sup>2</sup>School of Computer Science, Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/22-1542106x4_hanspub.png" /></p><p>Received: Mar. 22<sup>nd</sup>, 2021; accepted: Apr. 16<sup>th</sup>, 2021; published: Apr. 23<sup>rd</sup>, 2021</p><p><img src="//html.hanspub.org/file/22-1542106x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>The challenge for the deep learning-based person re-identification method is how to solve the problem of pedestrian misalignment caused by pedestrian posture change, mixed background, large camera viewing angle difference, and partial occlusion. How to extract fine-grained and highly discriminative features has become the key to solving the problem. Therefore, a new person re-identification method based on graph neural network is proposed, which includes: 1) Combine the human semantic parsing model to locate the position map and learn fine-grained local features as graph representation through the graph neural network. 2) Through the proposed graph metric learning method to jointly optimize the learning network to learn more discriminative feature representation. Experiments were performed on mainstream evaluation datasets, and the results showed the effectiveness of the method.</p><p>Keywords:Person Re-Identification, Graph Metric Learning, Human Semantic Parsing, Part Alignment</p><disp-formula id="hanspub.41782-formula18"><graphic xlink:href="//html.hanspub.org/file/22-1542106x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/22-1542106x7_hanspub.png" /> <img src="//html.hanspub.org/file/22-1542106x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>行人重识别(Person Re-Identification, Person Re-ID)，是指在跨摄像头的情况下，在多张行人图像中找出匹配目标人物的图像 [<xref ref-type="bibr" rid="hanspub.41782-ref1">1</xref>]，广泛应用于安防系统和智能商业等领域。行人重识别实现思路为计算查询图像与图库中所有图像的相似度，根据相似度从大到小排列得到重识别结果 [<xref ref-type="bibr" rid="hanspub.41782-ref2">2</xref>]。目前，行人重识别亟需解决的问题有：行人姿态多变、背景变化大、拍摄角度差异大、存在局部遮挡等。解决上述难题的关键在于特征提取和相似性度量。</p><p>传统行人重识别任务研究主要集中在人工设计特征提取器，比如文献 [<xref ref-type="bibr" rid="hanspub.41782-ref3">3</xref>] 提出的尺度不变特征变换(Scale-Invariant Feature Transform, SIFT)，文献 [<xref ref-type="bibr" rid="hanspub.41782-ref4">4</xref>] 提出的局部二值模式(Local Binary Patterns, LBP)以及文献 [<xref ref-type="bibr" rid="hanspub.41782-ref5">5</xref>] 提出的局部最大特征值(Local Maximal Occurrence, LOMO)。但这些方法提取到的特征描述能力有限，难以应用于复杂情况下大数据量的行人重识别任务。随着深度学习的兴起，卷积神经网络(Convolution Neural Network, CNN)凭借出色的特征提取能力，取代了传统的人工特征提取器。研究者利用卷积神经网络提取行人整图图像的全局特征，并通过不同的局部划分策略，如图像切块 [<xref ref-type="bibr" rid="hanspub.41782-ref6">6</xref>]、通过姿态语义信息分块 [<xref ref-type="bibr" rid="hanspub.41782-ref7">7</xref>] 通过注意力机制分块 [<xref ref-type="bibr" rid="hanspub.41782-ref8">8</xref>]、和通过人体语义解析信息分块 [<xref ref-type="bibr" rid="hanspub.41782-ref9">9</xref>] 等，进行局部特征提取，最后将全局特征和局部特征拼接得到行人的特征表示，并通过计算行人特征表示之间的欧式距离作为行人图像的相似度。</p><p>基于深度卷积网络的局部特征提取方法虽然可以提取到有判别性的特征，在行人重识别任务上取得了不错的效果，但仍存在两个问题导致局部特征的判别性被削弱。第一，由于局部划分的粒度比较大，获取到的局部区域包含行人信息和背景噪声信息。比如PCB [<xref ref-type="bibr" rid="hanspub.41782-ref6">6</xref>] 使用垂直方向均匀切块的方法，每个局部块都包含了背景噪声信息。SpindleNet [<xref ref-type="bibr" rid="hanspub.41782-ref7">7</xref>] 根据语义将人体姿态中身体部位的关节点分成五组，对应身体的五个部位，将每组关节点在行人图像上围起来的区域作为局部特征提取的区域。这种方式提取到的局部图像同样也包含了背景噪声信息。第二，忽视了提取到的局部特征之间的关联关系。文献 [<xref ref-type="bibr" rid="hanspub.41782-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.41782-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.41782-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.41782-ref9">9</xref>] 等局部特征提取方法，直接将提取到的局部特征拼接，然后计算拼接后的局部特征之间的相似度，这种方式导致了模型在辨别两个穿着相似的行人时，由于缺少了额外的位置辅助信息，从而很难进行精确匹配。</p><p>针对上述问题，本文提出了一种基于人体语义解析和图神经网络的行人重识别模型。本文的主要贡献分为三个部分：</p><p>1) 本文引入人体语义解析网络预先提取行人身体局部区域，用于提取细粒度的局部特征，获取到行人身份的敏感信息，减少背景噪声信息带来的负面影响。</p><p>2) 本文引入图卷积神经网络，用于提取局部特征之间的关联关系，增强局部特征的判别性。</p><p>3) 本文将行人局部特征构造成图，引入全局对级相似度损失函数进行模型训练，进一步增强局部特征的判别性。</p><p>本文的结构如下：第二章介绍行人重识别的相关工作，第三章介绍提出的方法，第四章展示实验，第五章总结全文。</p></sec><sec id="s6"><title>2. 相关工作</title><sec id="s6_1"><title>2.1. 行人重识别</title><p>行人重识别旨在实现跨摄像头检索目标人物，主要分为特征提取和度量学习两部分。</p><p>随着深度学习技术的发展，CNN已经广泛应用于计算机视觉任务中的图像特征提取。通过CNN直接提取整张图像特征的方式简单高效，但提取到的特征很容易忽略行人身份的敏感信息，从而导致全局特征无法很好的区分类间相似度高或类内差异大的行人样本。因此，研究者们提出了基于局部特征的行人重识别方法，通过定位行人的身体部位，提取对应部位的身份敏感信息。Sun [<xref ref-type="bibr" rid="hanspub.41782-ref6">6</xref>] 等人提出了PCB模型，将行人图像水平切割分块，通过骨干网络提取到每块的特征，拼接成局部特征。Zhao [<xref ref-type="bibr" rid="hanspub.41782-ref7">7</xref>] 等人提出的SpindleNet引入了行人姿态信息，通过姿态关键点提取到7个身体区域，并从身体区域中提取局部特征。Li [<xref ref-type="bibr" rid="hanspub.41782-ref8">8</xref>] 等人提出了HA-CNN模型，引入了多条局部分支，每条分支分别关注图片的一块局部区域的特征，最终将各条分支学到的特征进行融合得到行人的局部特征。文献 [<xref ref-type="bibr" rid="hanspub.41782-ref9">9</xref>] 中的SPReID模型引入行人人体语义解析网络提取行人的身体区域的掩膜图像，与原图经过骨干网络得到的特征图结合，得到行人身体区域的局部特征。众多基于局部特征的行人重识别方法证实了局部特征的有效性。</p><p>度量学习是广泛应用于图像检索领域的一种方法，旨在通过网络学习出两张图片的相似度。在行人重识别任务上表现为同一行人的不同图像的相似度要大于不同行人的图像的相似度。Varior [<xref ref-type="bibr" rid="hanspub.41782-ref10">10</xref>] 等人使用了孪生网络，同时输入两张行人图像，采用对比损失(Contrastive Loss)训练模型，使得相同行人图像(正样本)之间的距离逐步缩小，不同行人图像(负样本)之间的距离逐步增大。文献 [<xref ref-type="bibr" rid="hanspub.41782-ref11">11</xref>] 首先构造包括一对正样本和一对负样本的三元组，分别命名为目标图片(Anchor)，正样本图片(Positive)和负样本图片(Negative)。然后采用三元组损失函数训练网络，拉近了正样本之间的距离，增大了负样本之间的距离。但三元组损失没有考虑到正样本的绝对距离，因此文献 [<xref ref-type="bibr" rid="hanspub.41782-ref12">12</xref>] 提出改进三元组损失，在损失函数上增加正样本绝对距离公式项约束。Chen [<xref ref-type="bibr" rid="hanspub.41782-ref13">13</xref>] 等人进一步考虑了正负样本对的绝对距离，提出了四元组损失(Quadruplet Loss)。</p></sec><sec id="s6_2"><title>2.2. 图卷积神经网络</title><p>图卷积网络(Graph Convolutional Network, GCN)是一种能对图数据进行深度学习的方法。它是对卷积神经网络在图域的扩展，能同时对结点特征信息与结构信息进行端到端的学习。文献 [<xref ref-type="bibr" rid="hanspub.41782-ref14">14</xref>] 首次将图卷积用于半监督结点分类任务并取得了成功。之后图卷积神经网络广泛应用于计算机视觉领域，如动作识别 [<xref ref-type="bibr" rid="hanspub.41782-ref15">15</xref>]、多标签图像识别 [<xref ref-type="bibr" rid="hanspub.41782-ref16">16</xref>]、3D几何重构 [<xref ref-type="bibr" rid="hanspub.41782-ref17">17</xref>] 等。在行人重识别领域，很多方法将行人图像样本建图，使用GCN来更新行人图像的特征表示。比如，Shen [<xref ref-type="bibr" rid="hanspub.41782-ref18">18</xref>] 等人将查询集和候选集的图片两两组成对，并通过GCN来学习查询集图片与候选集图片之间的关联关系。Ye [<xref ref-type="bibr" rid="hanspub.41782-ref19">19</xref>] 等人为每个摄像机中的图像样本构造一个图，然后利用图匹配方案进行跨摄像机的标签关联等。</p></sec></sec><sec id="s7"><title>3. 本文方法</title><p>本节主要介绍基于图神经网络的行人重识别模型的具体细节。如图1所示，网络模型分为全局表示学习分支和图表示学习分支，其中全局分支使用交叉熵损失函数和三元组损失函数对模型进行训练，学习到行人图像的全局表示。图表示学习分支先通过人体语义解析的结果得到行人图像的局部特征，然后将每块局部特征作为图结点，将局部块语义连接作为边构成图，通过图卷积网络更新局部特征，再通过交叉熵损失函数和图度量学习模块共同训练网络，增强提取到的特征的表达能力。</p><p>图1. 人体语义解析与图度量学习融合模型</p><sec id="s7_1"><title>3.1. 全局表示学习</title><p>由于残差神经网络 [<xref ref-type="bibr" rid="hanspub.41782-ref20">20</xref>] 在行人重识别任务取得了很好的效果，因此本文选取Resnet-50骨干网络来提取输入图片相应的特征图。为了增大特征图的感受野，本文将Resnet-50网络最后一层卷积层的步长设置为1，使得大小为256 &#215; 128的原始图片经过骨干网络后输出的特征图大小为8 &#215; 4，然后对特征图进行全局平均池化操作(Global Average Pooling, GAP)得到形状为C&#215;H&#215;W的特征，其中C为通道数，H为高度，W为宽度。再依次经过一层全连接层(Fully Connected, FC)进行降维、一层批处理归一化层(Batch Normalization, BN)进行归一化，得到全局表示。全局分支使用交叉熵损失函数和困难样本三元组损失函数进行联合训练，本文参考了文献 [<xref ref-type="bibr" rid="hanspub.41782-ref21">21</xref>] 的做法，将经过BN层前的特征用于计算三元组损失，BN层后的特征用于计算交叉熵损失。</p></sec><sec id="s7_2"><title>3.2. 基于人体语义解析的图表示学习</title><sec id="s7_2_1"><title>3.2.1. 局部特征提取</title><p>现有的局部特征提取方法 [<xref ref-type="bibr" rid="hanspub.41782-ref6">6</xref>] 证明了局部特征的有效性，但这些方法提取到的局部区域是粗粒度的。为了精准定位行人身体部位的区域，本文使用了在人体语义解析数据集(Look into Person, LIP [<xref ref-type="bibr" rid="hanspub.41782-ref22">22</xref>] )上预训练好的SCHP [<xref ref-type="bibr" rid="hanspub.41782-ref23">23</xref>] 模型，通过该模型提取到行人图像的19个像素级人体部位标注，包括头部、手臂、左腿、上半身、下半身等，形成掩膜图像，如图2所示。将骨干网络提取到的特征图和二值化的掩膜图像对应位置相乘，再将所得结果进行全局池化，从而提取人体不同身体部位区域的特征作为局部特征。</p></sec><sec id="s7_2_2"><title>3.2.2. 局部结构图的构建</title><p>现有的行人重识别方法提取到局部特征后，往往将各块局部特征单独考虑，通过简单的拼接或者最大池化操作进行融合，忽略了局部特征之间的关系。本文将提取到的19个身体部位的特征作为图的结点，并以19个结点的自然连接作为边，构成局部特征关系图G，如图3所示，每个柱形表示该部位的特征。获取到关系图G后，通过两层图卷积组成的图卷积网络进行结点间信息的传播，从而对关系图中的结点进行更新，得到融入结构信息的局部特征 f p ′ ，最后通过最大池化融合局部特征得到整图的表示 f e 。图卷积函数的公式表达如式(1)所示，其中H<sup>L</sup>表示第L层图卷积后的特征， A ^ 表示归一化后的邻接矩阵，W<sup>L</sup><sup>-1</sup>是模型学习到的变换矩阵，h表示非线性激活函数。</p><p>H L = h ( A ^ H L − 1 W L − 1 ) (1)</p><p>图2. Market-1501上部分检索图像的人体语义解析图</p><p>图3. 局部特征关系图</p></sec><sec id="s7_2_3"><title>3.2.3. 图度量模块</title><p>图度量学习旨在训练模型学习到满足正类图在特征空间距离更近，负类图在特征表示空间距离更远的图表示。常用的图度量有图嵌入(Graph Embedding)、模式计数(Motif Counting)和图编辑距离(Graph Edit Distance)等。本文的身体局部连接关系是固定的，因此不适用模式计数。而图编辑距离的方式需要预先计算编辑代价，不够高效。图嵌入的方式只考虑整图全局的相似度，容易丢失图结构信息，因此本文引入了一种图度量学习方法，通过对比两个图对应结点之间的相似性，并赋予不同的权重来获得整图的相似性。具体实现模块见图1 Graph Metric Learning Module。首先通过一层点积层计算两个图的对应结点的相似性，再经过一层全连接层学习各个结点的重要程度，得到整图的相似性，并通过全局成对相似度损失函数对正负样本的相似性分布做约束。</p><p>全局成对相似度损失函数由文献 [<xref ref-type="bibr" rid="hanspub.41782-ref24">24</xref>] 提出，旨在避免过度采样和欠采样，基于相同类别和不同类别的特征相似性呈独立分布的假设，最小化正类相似度和负类相似度分布的方差，最小化正类相似度分布的均值，并最大化负类相似度分布的均值，在学习局部图片特征的任务上效果远超传统的损失函数。该损失函数的表达式如下：</p><p>J = ( σ 2 + + σ 2 − ) + λ max ( 0 , m − ( μ + − μ − ) ) (2)</p><p>μ + = ∑ i = 1 N g ( f p i , f p i + ) / N (3)</p><p>μ − = ∑ i = 1 N g ( f p i , f p i − ) / N (4)</p><p>σ 2 + = ∑ i = 1 N ( g ( f p i , f p i + ) − μ + ) 2 / N (5)</p><p>σ 2 − = ∑ i = 1 N ( g ( f p i , f p i − ) − μ − ) 2 / N (6)</p></sec></sec><sec id="s7_3"><title>3.3. 损失函数</title><sec id="s7_3_1"><title>3.3.1. 标签平滑交叉熵损失函数</title><p>行人重识别通常被视作分类任务，并用交叉熵损失函数训练行人重识别网络。由于行人重识别中训练集和测试集中的行人是不同的，用传统的交叉熵损失函数训练得到的网络容易对训练集的行人标签过拟合，为了避免这种情况，我们借鉴了文献 [<xref ref-type="bibr" rid="hanspub.41782-ref25">25</xref>] 的做法，对标签进行了平滑处理，其中K表示行人标签数， ε 表示阈值，实验中设置为0.1，如式(7)，(8)所示。</p><p>Y c = { 1 − K − 1 K ε ,     c = Y ε / K ,   其 他 (7)</p><p>L xcent = − ∑ c = 1 K Y c log ( p c ) (8)</p></sec><sec id="s7_3_2"><title>3.3.2. 困难样本三元组损失函数</title><p>行人重识别任务也被视为聚类任务，目标是直接将相同行人的若干图片映射到高维空间，形成聚类效应。行人重识别任务中常用三元组损失函数进行模训练。考虑到大多数三元组是简单样本，不利于模型的训练，因此借鉴了Alex [<xref ref-type="bibr" rid="hanspub.41782-ref26">26</xref>] 等人提出的困难样本三元组损失函数，对于每个目标图片anchor在一个训练批次内选择欧式距离最远的正样本图片positive和欧式距离最近的负样本图片negative来训练网络，增强网络的泛化能力，从而使网络学习到更好的表征。难样本三元组损失函数如式(9)所示，其中 α 表示目标样本与正负样本的间隔参数，设置为0.3，d<sub>a</sub><sub>,p</sub>，d<sub>a</sub><sub>,n</sub>分别表示目标样本和正样本的欧氏距离，目标样本与负样本的欧氏距离。</p><p>L tri = ( max d a , p − min d a , n + α , 0 ) + (9)</p></sec><sec id="s7_3_3"><title>3.3.3. 联合损失函数</title><p>本文提出的行人重识别网络通过标签平滑交叉熵损失函数、三元组损失函数和全局成对相似度损失函数进行训练，共同约束特征，其中全局分支使用标签平滑交叉熵损失函数L<sub>xcent_g</sub>和三元组损失函数L<sub>tri_g</sub>进行训练，局部分支使用标签平滑交叉熵损失函数L<sub>xcent_l</sub>和全局成对相似度损失函数J进行训练。关于全局成对相似度损失函数已在3.2.3节中详细阐述。因此，本文的损失函数如式(10)所示。</p><p>L total = L xcent_g + L tri_g + L xcent_l + J (10)</p></sec></sec></sec><sec id="s8"><title>4. 实验与分析</title><sec id="s8_1"><title>4.1. 数据集</title><p>本文所提出的算法在行人重识别领域的三大主流数据集(Market1501、DukeMTMC-reID、CUHK03)进行了广泛实验，并通过mAP、Rank-1、Rank-5、Rank-10评价指标作出对比评估，我们的实验结果并未进行重排序操作。</p><p>Market1501数据集中，训练集图片数为12936，包括751个人；测试集为19732张图像，包括查询集3368张图像。DukeMTMC-reID数据集在2017年公开，训练集中702人，16522张图像；测试集中702人，19889张图像，包括查询集2228张图像。CUHK03数据集在2014年公开，本文实验中采用了新版的测试协议，将数据集分为包含767个行人的训练集和包含700个行人的测试集。表1列出了每个数据集的详细信息。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Details of the dataset</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >行人标签数</th><th align="center" valign="middle" >摄像头数</th><th align="center" valign="middle" >图片数</th><th align="center" valign="middle" >采集场景</th></tr></thead><tr><td align="center" valign="middle" >Market-1501</td><td align="center" valign="middle" >1501</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >32668</td><td align="center" valign="middle" >清华大学</td></tr><tr><td align="center" valign="middle" >DukeMTMC-reID</td><td align="center" valign="middle" >1404</td><td align="center" valign="middle" >8</td><td align="center" valign="middle" >36411</td><td align="center" valign="middle" >杜克大学</td></tr><tr><td align="center" valign="middle" >CUHK03</td><td align="center" valign="middle" >1467</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >14097</td><td align="center" valign="middle" >香港中英文大学</td></tr></tbody></table></table-wrap><p>表1. 数据集的细节</p></sec><sec id="s8_2"><title>4.2. 评估标准</title><p>评估行人重识别模型的性能时，需要获取相关实验的模型测评指标。针对不同的研究任务，评价指标也不尽相同。在行人重识别任务中，常用首位匹配率(Rank-1)和均值平均精度(mAP)这两个评价指标来描述模型性能的优劣。对应两个指标越高，说明模型对行人重识别的效果就越好。</p><p>首位匹配率(Rank-1)是在对应的查询集(Query Set)中的每张行人图像，通过行人重识别算法在候选集中选出候选序列，对应行人的图像排在候选序列第一位的数量占整个查询集图像数量的比例。假定查询集 P = { p 1 , p 2 , ⋯ , p m } ，候选集 G = { g 1 , g 2 , ⋯ , g n } ，m，n分别表示图像张数。P和G之间计算相似度得到相似度矩阵S，将S中的每行按数值从大到小的顺序排列，S中的数值越大，表示两张图像的相似度越高，定义s<sub>1</sub>为S中第一列中对应的来自查询集和候选集组成的图像对是同一行人身份的数量，则首位匹配率可表示成：Rank1 = s<sub>1</sub>/m。Rank-5，Rank-10表示候选序列中前5张图片和前十张图片中目标任务的命中率，可由同样的计算方法得出。</p><p>均值平均精度(mAP)是衡量模型总体好坏的指标，由模型的召回率和精确率进行计算得出。对于查询集的中的每个查询图像，定义精确度P为候选列表中前k个图像中正确匹配的图像数目n与k的比值，可表示成：P = n/k。而图像样本对应的召回率R为在候选列表的前k个图像正确匹配的数量n与行人在候选集中的图像数量m的比值，即：R = k/m。</p><p>当k取不同值时，可以在坐标系上得到多个P值的点和R值的点，用曲线分别依次连接各个P值点和各个R值点可以得到PR图。该样本的平均精度(AP)即为PR图中曲线围成的面积，而mAP为所有查询图像的AP值的均值。mAP作为常用的评估指标，可以更加综合和客观的评价模型性能。</p></sec><sec id="s8_3"><title>4.3. 消融实验</title><p>为了进一步验证提取到的局部特征的有效性，本文以全局分支组成的模型Model_G作为Baseline，并采用标签平滑交叉熵损失函数和三元组损失函数进行训练。基于相同Baseline在Market1501和DukeMTMC-reID数据集做了多组消融实验，实验结果如表2所示。</p><p>从结果中我们发现，同时考虑全局特征和局部特征的Model_G_L模型，在Market1501数据集上的表现为rank-1，mAP比Baseline提升了0.3%和1.3%，在DukeMTMC-reID数据集表现为rank-1提升了1.3%，mAP提升了0.6%。证明了局部特征能够补足全局特征遗漏的行人细节信息。</p><p>在进一步考虑局部特征关系，引入图卷积网络后，模型Model_G_L_GCN在两个数据集上rank-1，mAP分别再提升0.4%，0.5%和0.2%，1.8%。证明了引入局部关系信息，使用图卷积得到行人的图表示，可以更好的实现行人重识别任务。</p><p>为了验证图度量学习模块的有效性，训练局部分支时，去掉了triplet loss，引入图度量学习模块组成Model_G_L_GCN_Jloss模型，在rank-1和mAP上进一步提升了1.0%，0.4%和0.5%，1.0%。证明了图度量学习模块中的全局成对相似度损失函数在图表示学习任务上的效果优于三元组损失函数。图度量学习模块考虑了各个结点之间的相似性，并通过全连接层学习各个结点的重要性，比三元组损失单独考虑整体相似度更能学到鲁棒性的特征。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Person re-ID evaluation value on Market-1501 and DukeMTMC-reI</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >模型</th><th align="center" valign="middle"  colspan="4"  >Market-1501</th><th align="center" valign="middle"  colspan="4"  >DukeMTMC-reID</th></tr></thead><tr><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >Rank-5</td><td align="center" valign="middle" >Rank10</td><td align="center" valign="middle" >mAP</td><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >Rank-5</td><td align="center" valign="middle" >Rank-10</td><td align="center" valign="middle" >mAP</td></tr><tr><td align="center" valign="middle" >Model_G</td><td align="center" valign="middle" >93.8</td><td align="center" valign="middle" >97.8</td><td align="center" valign="middle" >98.3</td><td align="center" valign="middle" >84.5</td><td align="center" valign="middle" >85.5</td><td align="center" valign="middle" >92.9</td><td align="center" valign="middle" >94.7</td><td align="center" valign="middle" >72.7</td></tr><tr><td align="center" valign="middle" >Model_G_L</td><td align="center" valign="middle" >94.1</td><td align="center" valign="middle" >98.0</td><td align="center" valign="middle" >98.6</td><td align="center" valign="middle" >85.8</td><td align="center" valign="middle" >86.8</td><td align="center" valign="middle" >93.1</td><td align="center" valign="middle" >94.8</td><td align="center" valign="middle" >73.3</td></tr><tr><td align="center" valign="middle" >Model_G_L_GCN</td><td align="center" valign="middle" >794.5</td><td align="center" valign="middle" >98.1</td><td align="center" valign="middle" >98.7</td><td align="center" valign="middle" >86.3</td><td align="center" valign="middle" >87.0</td><td align="center" valign="middle" >93.5</td><td align="center" valign="middle" >95.2</td><td align="center" valign="middle" >75.1</td></tr><tr><td align="center" valign="middle" >Model_G_L_GCN_Jloss</td><td align="center" valign="middle" >95.5</td><td align="center" valign="middle" >98.3</td><td align="center" valign="middle" >98.9</td><td align="center" valign="middle" >86.7</td><td align="center" valign="middle" >87.5</td><td align="center" valign="middle" >93.7</td><td align="center" valign="middle" >95.4</td><td align="center" valign="middle" >76.1</td></tr></tbody></table></table-wrap><p>表2. 在Market-1501和DukeMTMC-ReID数据集上的实验结果</p></sec><sec id="s8_4"><title>4.4. 与当前主流方法对比</title><p>为了验证本文中所提算法的优越性，本文与近几年的行人重识别主流算法分别在Market1501、DukeMTMC-reID、CUHK03-NP三个数据集上进行了比较。由表3可知，在Market1501、DukeMTMC-reID数据集上，本文提出的算法评价指标Rank-1、Rank-5、Rank-10、mAP与众多对比模型相比均达到了较高水平。对比MGN和PSRL模型，本文在Market1501和DukeMTMC的rank-1和mAP指标略低，这是因为MGN利用了多层粒度的特征图提取局部特征，能适应背景环境变化大的情况。PSRL模型使用了特征拼接模块，进一步融合局部特征，并训练模型学习得到局部结构之间的连接关系。从表4可知，MGN和PSRL在行人存在严重遮挡的情况下提取到的特征不够鲁棒。CUHK03数据集的行人姿态变化大，遮挡问题严重，行人重识别难度比其他两个数据集要大。而本文提出的算法在该数据集上的效果远远超过上述两个模型，证明了本文通过图度量学习训练获取到的图表示更具鲁棒性。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Comparison with mainstream algorithm evaluation indicators on Market-1501 and DukeMTMC-reI</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >模型</th><th align="center" valign="middle"  colspan="4"  >Market-1501</th><th align="center" valign="middle"  colspan="4"  >DukeMTMC-reID</th></tr></thead><tr><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >Rank-5</td><td align="center" valign="middle" >Rank10</td><td align="center" valign="middle" >mAP</td><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >Rank-5</td><td align="center" valign="middle" >Rank-10</td><td align="center" valign="middle" >mAP</td></tr><tr><td align="center" valign="middle" >SpindleNet [<xref ref-type="bibr" rid="hanspub.41782-ref7">7</xref>]</td><td align="center" valign="middle" >76.9</td><td align="center" valign="middle" >98.1</td><td align="center" valign="middle" >94.6</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td></tr><tr><td align="center" valign="middle" >PSE [<xref ref-type="bibr" rid="hanspub.41782-ref27">27</xref>]</td><td align="center" valign="middle" >87.7</td><td align="center" valign="middle" >91.5</td><td align="center" valign="middle" >96.8</td><td align="center" valign="middle" >69.0</td><td align="center" valign="middle" >79.8</td><td align="center" valign="middle" >89.7</td><td align="center" valign="middle" >92.2</td><td align="center" valign="middle" >62.0</td></tr><tr><td align="center" valign="middle" >PGR [<xref ref-type="bibr" rid="hanspub.41782-ref28">28</xref>]</td><td align="center" valign="middle" >93.8</td><td align="center" valign="middle" >90.8</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >77.2</td><td align="center" valign="middle" >83.6</td><td align="center" valign="middle" >91.7</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >65.9</td></tr><tr><td align="center" valign="middle" >MaskReID [<xref ref-type="bibr" rid="hanspub.41782-ref29">29</xref>]</td><td align="center" valign="middle" >90.0</td><td align="center" valign="middle" >97.7</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >75.3</td><td align="center" valign="middle" >78.8</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >61.9</td></tr><tr><td align="center" valign="middle" >AlignedReID [<xref ref-type="bibr" rid="hanspub.41782-ref30">30</xref>]</td><td align="center" valign="middle" >91.8</td><td align="center" valign="middle" >97.6</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >79.3</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td></tr><tr><td align="center" valign="middle" >PCB [<xref ref-type="bibr" rid="hanspub.41782-ref6">6</xref>]</td><td align="center" valign="middle" >93.8</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >98.5</td><td align="center" valign="middle" >81.6</td><td align="center" valign="middle" >83.3</td><td align="center" valign="middle" >90.5</td><td align="center" valign="middle" >92.5</td><td align="center" valign="middle" >69.2</td></tr><tr><td align="center" valign="middle" >BFE [<xref ref-type="bibr" rid="hanspub.41782-ref31">31</xref>]</td><td align="center" valign="middle" >94.2</td><td align="center" valign="middle" >97.1</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >84.3</td><td align="center" valign="middle" >86.8</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >72.1</td></tr><tr><td align="center" valign="middle" >MGN [<xref ref-type="bibr" rid="hanspub.41782-ref32">32</xref>]</td><td align="center" valign="middle" >95.7</td><td align="center" valign="middle" >97.5</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >86.9</td><td align="center" valign="middle" >88.7</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >78.4</td></tr><tr><td align="center" valign="middle" >DuATM [<xref ref-type="bibr" rid="hanspub.41782-ref33">33</xref>]</td><td align="center" valign="middle" >91.4</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >76.6</td><td align="center" valign="middle" >81.8</td><td align="center" valign="middle" >90.2</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >64.6</td></tr><tr><td align="center" valign="middle" >HA-CNN [<xref ref-type="bibr" rid="hanspub.41782-ref8">8</xref>]</td><td align="center" valign="middle" >91.2</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >75.7</td><td align="center" valign="middle" >80.5</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >63.8</td></tr><tr><td align="center" valign="middle" >Mancs [<xref ref-type="bibr" rid="hanspub.41782-ref34">34</xref>]</td><td align="center" valign="middle" >93.1</td><td align="center" valign="middle" >97.1</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >82.3</td><td align="center" valign="middle" >84.9</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >71.8</td></tr><tr><td align="center" valign="middle" >PSRL [<xref ref-type="bibr" rid="hanspub.41782-ref35">35</xref>]</td><td align="center" valign="middle" >95.9</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >87.2</td><td align="center" valign="middle" >88.1</td><td align="center" valign="middle" >93.9</td><td align="center" valign="middle" >95.7</td><td align="center" valign="middle" >76.7</td></tr><tr><td align="center" valign="middle" >Our approach</td><td align="center" valign="middle" >95.5</td><td align="center" valign="middle" >98.3</td><td align="center" valign="middle" >98.9</td><td align="center" valign="middle" >86.7</td><td align="center" valign="middle" >87.5</td><td align="center" valign="middle" >93.7</td><td align="center" valign="middle" >95.4</td><td align="center" valign="middle" >76.1</td></tr></tbody></table></table-wrap><p>表3. 在Market-1501和DukeMTMC-reID上与主流算评价指标比较</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Comparison with mainstream algorithm evaluation indicators on CUHK03-N</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >模型</th><th align="center" valign="middle"  colspan="3"  >Detected</th><th align="center" valign="middle"  colspan="2"  >Labeled</th></tr></thead><tr><td align="center" valign="middle"  colspan="2"  >Rank-1</td><td align="center" valign="middle" >mAP</td><td align="center" valign="middle" >Rank-1</td><td align="center" valign="middle" >mAP</td></tr><tr><td align="center" valign="middle" >HA-CNN [<xref ref-type="bibr" rid="hanspub.41782-ref8">8</xref>]</td><td align="center" valign="middle" >41.7</td><td align="center" valign="middle"  colspan="2"  >38.6</td><td align="center" valign="middle" >44.4</td><td align="center" valign="middle" >41.0</td></tr><tr><td align="center" valign="middle" >PCB [<xref ref-type="bibr" rid="hanspub.41782-ref6">6</xref>]</td><td align="center" valign="middle" >62.8</td><td align="center" valign="middle"  colspan="2"  >56.7</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td></tr><tr><td align="center" valign="middle" >Mancs [<xref ref-type="bibr" rid="hanspub.41782-ref34">34</xref>]</td><td align="center" valign="middle" >65.5</td><td align="center" valign="middle"  colspan="2"  >60.5</td><td align="center" valign="middle" >65.5</td><td align="center" valign="middle" >60.5</td></tr><tr><td align="center" valign="middle" >MGN [<xref ref-type="bibr" rid="hanspub.41782-ref32">32</xref>]</td><td align="center" valign="middle" >68.0</td><td align="center" valign="middle"  colspan="2"  >67.4</td><td align="center" valign="middle" >68.0</td><td align="center" valign="middle" >67.4</td></tr><tr><td align="center" valign="middle" >BFE [<xref ref-type="bibr" rid="hanspub.41782-ref31">31</xref>]</td><td align="center" valign="middle" >73.6</td><td align="center" valign="middle"  colspan="2"  >69.7</td><td align="center" valign="middle" >73.6</td><td align="center" valign="middle" >71.7</td></tr><tr><td align="center" valign="middle" >PSRL [<xref ref-type="bibr" rid="hanspub.41782-ref35">35</xref>]</td><td align="center" valign="middle" >67.6</td><td align="center" valign="middle"  colspan="2"  >65.8</td><td align="center" valign="middle" >68.7</td><td align="center" valign="middle" >68.2</td></tr><tr><td align="center" valign="middle" >Our approach</td><td align="center" valign="middle" >76.3</td><td align="center" valign="middle"  colspan="2"  >70.8</td><td align="center" valign="middle" >76.4</td><td align="center" valign="middle" >72.5</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr></tbody></table></table-wrap><p>表4. 在CUHK03-NP上与主流算评价指标比较</p></sec></sec><sec id="s9"><title>5. 结论</title><p>本文从由行人姿态多变、背景混杂、摄像头视角差异大和部分遮挡导致的行人不对齐这一问题着手，提出了一种基于图神经网络的行人重识别算法，使用人体语义解析得到行人身体部位标注，提取细粒度局部特征构建部位关系图，再通过图卷积网络融入局部结构信息，并用图度量学习得到更具鲁棒性的图表示。最后在3个数据集上的实验结果表明，本文所提模型相比目前的主流算法重识别率更优。未来的工作是简化网络模型复杂度的情况下，寻求效果更优的重识别率的方法，并研究更为有效的跨数据集识别方法，实现跨域行人重识别任务。</p></sec><sec id="s10"><title>基金项目</title><p>国家自然科学基金资助项目(61876043，61976052)。</p></sec><sec id="s11"><title>文章引用</title><p>郝志峰,苏伟根,许柏炎,温 雯,蔡瑞初. 基于图神经网络的行人重识别方法Person Re-Identification Method Based on Graph Neural Network[J]. 计算机科学与应用, 2021, 11(04): 983-993. https://doi.org/10.12677/CSA.2021.114101</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41782-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">罗浩, 姜伟, 范星, 等. 基于深度学习的行人重识别研究进展[J]. 自动化学报, 2019, 45(11): 2032-2049.</mixed-citation></ref><ref id="hanspub.41782-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Zheng, L., Shen, L., Tian, L., et al. (2015) Scalable Person Re-Identification: A Benchmark. Proceed-ings of the IEEE International Conference on Computer Vision, Santiago, 7-13 December 2015, 1116-1124.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2015.133</mixed-citation></ref><ref id="hanspub.41782-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Lindeberg, T. (2012) Scale Invariant Feature Transform. Scholarpedia, 7, 10491.  
&lt;br&gt;https://doi.org/10.4249/scholarpedia.10491</mixed-citation></ref><ref id="hanspub.41782-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Ahonen, T., Hadid, A. and Pietikäinen, M. (2004) Face Recogni-tion with Local Binary Patterns. In: Pajdla, T. and Matas, J., Eds., European Conference on Computer Vision, Springer, Berlin, Heidelberg, 469-481.  
&lt;br&gt;https://doi.org/10.1007/978-3-540-24670-1_36</mixed-citation></ref><ref id="hanspub.41782-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Liao, S., Hu, Y., Zhu, X. and Li, S.Z. (2015) Person Re-Identification by Local Maximal Occurrence Representation and Metric Learning. Proceedings of the IEEE Confer-ence on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 2197-2206. &lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298832</mixed-citation></ref><ref id="hanspub.41782-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Sun, Y., Zheng, L., Yang, Y., et al. (2018) Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline). Proceedings of the European Con-ference on Computer Vision (ECCV), 480-496.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01225-0_30</mixed-citation></ref><ref id="hanspub.41782-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Zhao, H., Tian, M., Sun, S., et al. (2017) Spindle Net: Person Re-Identification with Human Body Region Guided Feature Decomposition and Fusion. Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 1077-1085. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.103</mixed-citation></ref><ref id="hanspub.41782-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Li, W., Zhu, X. and Gong, S. (2018) Harmonious Attention Network for Person Re-Identification. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 2285-2294.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00243</mixed-citation></ref><ref id="hanspub.41782-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Kalayeh, M.M., Basaran, E., Gökmen, M., et al. (2018) Human Semantic Parsing for Person Re-Identification. Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, Salt Lake City, 18-23 June 2018, 1062-1071. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00117</mixed-citation></ref><ref id="hanspub.41782-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Varior, R.R., Haloi, M. and Wang, G. (2016) Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification. In: Leibe, B., Matas, J., Sebe, N. and Welling, M., Eds., European Conference on Computer Vision, Springer, Cham, 791-808. &lt;br&gt;https://doi.org/10.1007/978-3-319-46484-8_48</mixed-citation></ref><ref id="hanspub.41782-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Hermans, A., Beyer, L. and Leibe, B. (2017) In Defense of the Triplet Loss for Person Re-Identification. arXiv preprint arXiv:1703.07737.</mixed-citation></ref><ref id="hanspub.41782-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Cheng, D., Gong, Y., Zhou, S., et al. (2016) Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 1335-1344. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.149</mixed-citation></ref><ref id="hanspub.41782-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Chen, W., Chen, X., Zhang, J. Huang, K. (2017) Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-Identification. Proceedings of the IEEE Conference on Computer Vi-sion and Pattern Recognition, Honolulu, 21-26 July 2017, 403-412. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.145</mixed-citation></ref><ref id="hanspub.41782-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Kipf, T.N. and Welling, M. (2016) Semi-Supervised Classification with Graph Convolutional Networks. arXiv preprint arXiv:1609.02907.</mixed-citation></ref><ref id="hanspub.41782-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Caramalau, R., Bhattarai, B. and Kim, T.K. (2020) Sequential Graph Convolutional Network for Active Learning. arXiv preprint arXiv:2006.10219.</mixed-citation></ref><ref id="hanspub.41782-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Z.M., Wei, X.S., Wang, P. and Guo, Y. (2019) Multi-Label Image Recognition with Graph Convolutional Networks. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, 15-20 June 2019, 5177-5186. &lt;br&gt;https://doi.org/10.1109/CVPR.2019.00532</mixed-citation></ref><ref id="hanspub.41782-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Lei, H., Akhtar, N. and Mian, A. (2020) Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds. IEEE Transactions on Pattern Analysis and Machine Intel-ligence, 1. &lt;br&gt;https://doi.org/10.1109/TPAMI.2020.2983410</mixed-citation></ref><ref id="hanspub.41782-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Shen, Y., Li, H., Yi, S., et al. (2018) Person Re-Identification with Deep Similarity-Guided Graph Neural Network. Proceedings of the European Conference on Computer Vision (ECCV), 486-504.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01267-0_30</mixed-citation></ref><ref id="hanspub.41782-ref19"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Ye, M., Li, J., Ma, A.J., et al. (2019) Dynamic Graph Co-Matching for Unsupervised Video-Based Person Re-Identification. IEEE Transactions on Image Processing, 28, 2976-2990. &lt;br&gt;https://doi.org/10.1109/TIP.2019.2893066</mixed-citation></ref><ref id="hanspub.41782-ref20"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S. and Sun, J. (2016) Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-nition, Las Vegas, 27-30 June 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.41782-ref21"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Luo, H., Jiang, W., Gu, Y., Liu, F., et al. (2019) A Strong Baseline and Batch Normalization Neck for Deep Person Re-Identification. IEEE Transactions on Multimedia, 22, 2597-2609. &lt;br&gt;https://doi.org/10.1109/TMM.2019.2958756</mixed-citation></ref><ref id="hanspub.41782-ref22"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Gong, K., Liang, X., Zhang, D., et al. (2017) Look into Person: Self-Supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing. Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 932-940. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.715</mixed-citation></ref><ref id="hanspub.41782-ref23"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Li, P., Xu, Y., Wei, Y. and Yang, Y. (2020) Self-Correction for Human Parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1. &lt;br&gt;https://doi.org/10.1109/TPAMI.2020.3048039</mixed-citation></ref><ref id="hanspub.41782-ref24"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Kumar, B.G.V., Carneiro, G. and Reid, I. (2016) Learning Lo-cal Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 5385-5394.</mixed-citation></ref><ref id="hanspub.41782-ref25"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Vanhoucke, V., Ioffe, S., et al. (2016) Rethinking the Inception Architecture for Comput-er Vision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 2818-2826. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.308</mixed-citation></ref><ref id="hanspub.41782-ref26"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Alex, D., Sami, Z., Banerjee, S. and Panda, S. (2018) Cluster Loss for Person Re-Identification. Proceedings of the 11th Indian Conference on Computer Vision, Graphics and Image Processing, December 2018, Article No. 43.  
&lt;br&gt;https://doi.org/10.1145/3293353.3293396</mixed-citation></ref><ref id="hanspub.41782-ref27"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Saquib Sarfraz, M., Schumann, A., Eberle, A. and Stiefelhagen, R. (2018) A Pose-Sensitive Embedding for Person Re-Identification with Expanded cross Neighborhood Re-Ranking. Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 420-429.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2018.00051</mixed-citation></ref><ref id="hanspub.41782-ref28"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">Li, J., Zhang, S., Tian, Q., et al. (2019) Pose-Guided Representa-tion Learning for Person Re-Identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1. &lt;br&gt;https://doi.org/10.1109/TPAMI.2019.2929036</mixed-citation></ref><ref id="hanspub.41782-ref29"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">Song, C., Huang, Y., Ouyang, W., et al. (2018) Mask-Guided Contrastive Attention Model for Person Re-Identification. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 1179-1188. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00129</mixed-citation></ref><ref id="hanspub.41782-ref30"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, X., Luo, H., Fan, X., et al. (2017) Alignedreid: Surpassing Human-Level Performance in Person Re-Identification. arXiv preprint arXiv:1711.08184.</mixed-citation></ref><ref id="hanspub.41782-ref31"><label>31</label><mixed-citation publication-type="other" xlink:type="simple">Dai, Z., Chen, M., Gu, X., et al. (2019) Batch DropBlock Network for Person Re-Identification and Beyond. Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, Seoul, 27 October-2 November 2019, 3691-3701.  
&lt;br&gt;https://doi.org/10.1109/ICCV.2019.00379</mixed-citation></ref><ref id="hanspub.41782-ref32"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Wang, G., Yuan, Y., Chen, X., et al. (2018) Learning Discrimina-tive Features with Multiple Granularities for Person Re-Identification. Proceedings of the 26th ACM international con-ference on Multimedia, October 2018, 274-282.  
&lt;br&gt;https://doi.org/10.1145/3240508.3240552</mixed-citation></ref><ref id="hanspub.41782-ref33"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">Si, J., Zhang, H., Li, C.G., et al. (2018) Dual Attention Matching Network for Context-Aware Feature Sequence Based Person Re-Identification. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-23 June 2018, 5363-5372. &lt;br&gt;https://doi.org/10.1109/CVPR.2018.00562</mixed-citation></ref><ref id="hanspub.41782-ref34"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">Wang, C., Zhang, Q., Huang, C., et al. (2018) Mancs: A Mul-ti-Task Attentional Network with Curriculum Sampling for Person Re-Identification. In: Ferrari, V., Hebert, M., Sminchisescu, C. and Weiss, Y., Eds., Proceedings of the European Conference on Computer Vision (ECCV), Springer, Cham, 365-381.  
&lt;br&gt;https://doi.org/10.1007/978-3-030-01225-0_23</mixed-citation></ref><ref id="hanspub.41782-ref35"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Li, Y., Yao, H., Zhang, T. and Xu, C. (2020) Part-Based Structured Representation Learning for Person Re-Identification. ACM Transactions on Multimedia Computing, Com-munications, and Applications (TOMM), 16, 1-22.  
&lt;br&gt;https://doi.org/10.1145/3412384</mixed-citation></ref></ref-list></back></article>