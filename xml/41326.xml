<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.113074</article-id><article-id pub-id-type="publisher-id">CSA-41326</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210300000_15867721.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  使用TB-LSTM-CRF提高工业中文文本实体识别任务
  An Improved Chinese Named Entity Recognition Method with TB-LSTM-CRF
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>嘉正</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>周</surname><given-names>佳乐</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>程</surname><given-names>良伦</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学 自动化学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>03</month><year>2021</year></pub-date><volume>11</volume><issue>03</issue><fpage>720</fpage><lpage>728</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   由于缺乏自然定界符，中文命名实体识别(NER)比英文更具挑战性。虽然中文分词(CWS)通常被认为是中文NER的关键和开放问题，但其准确性对于下游模型训练至关重要，而且经常遭受语音不足(OOV)的困扰。本文提出了一种改进的中文NER模型，称为TB-LSTM-CRF，该模型在LSTM-CRF之上引入了一个变压器块。带有Transformer Block的拟议模型利用self-attention机制从相邻字符和句子上下文中捕获信息。同时本文使用了一个全新的工业场景数据集，在此数据集上，与使用LSTM-CRF的基线相比，实验结果表明，TB-LSTM-CRF方法具有竞争力，几乎不需要任何外部资源，例如参数迁移。 Owing to the lack of natural delimiters, Chinese Named Entity Recognition (NER) is more challenging than it in English. While Chinese Word Segmentation (CWS) is generally regarded as key and open problem for Chinese NER, its accuracy is critical for the downstream models trainings and it also often suffers from Out-of-Vocabulary (OOV). In this paper, we propose an improved Chinese NER model called TB-LSTM-CRF, which introduces a Transformer Block on top of LSTM- CRF. The proposed model with Transformer Block exploits the self-attention mechanism to capture the information from adjacent characters and sentence contexts. At the same time, this article uses a brand new industrial scene data set. On this data set, with the LSTM-CRF scale, the experimental results show that the TB-LSTM-CRF method is competitive and hardly requires any external resources, such as parameter migration. 
  
 
</p></abstract><kwd-group><kwd>中文自然语言处理，中文命名实体识别，知识图谱, Chinese Natural Language Processing</kwd><kwd> Chinese Named Entity Recognition</kwd><kwd> Knowledge Graph</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>由于缺乏自然定界符，中文命名实体识别(NER)比英文更具挑战性。虽然中文分词(CWS)通常被认为是中文NER的关键和开放问题，但其准确性对于下游模型训练至关重要，而且经常遭受语音不足(OOV)的困扰。本文提出了一种改进的中文NER模型，称为TB-LSTM-CRF，该模型在LSTM-CRF之上引入了一个变压器块。带有Transformer Block的拟议模型利用self-attention机制从相邻字符和句子上下文中捕获信息。同时本文使用了一个全新的工业场景数据集，在此数据集上，与使用LSTM-CRF的基线相比，实验结果表明，TB-LSTM-CRF方法具有竞争力，几乎不需要任何外部资源，例如参数迁移。</p></sec><sec id="s2"><title>关键词</title><p>中文自然语言处理，中文命名实体识别，知识图谱</p></sec><sec id="s3"><title>An Improved Chinese Named Entity Recognition Method with TB-LSTM-CRF</title><p>Jiazheng Li, Jiale Zhou, Lianglun Cheng</p><p>School of Automation, Guangdong University Of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/28-1542073x4_hanspub.png" /></p><p>Received: Feb. 28<sup>th</sup>, 2021; accepted: Mar. 22<sup>nd</sup>, 2021; published: Mar. 30<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/28-1542073x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Owing to the lack of natural delimiters, Chinese Named Entity Recognition (NER) is more challenging than it in English. While Chinese Word Segmentation (CWS) is generally regarded as key and open problem for Chinese NER, its accuracy is critical for the downstream models trainings and it also often suffers from Out-of-Vocabulary (OOV). In this paper, we propose an improved Chinese NER model called TB-LSTM-CRF, which introduces a Transformer Block on top of LSTM- CRF. The proposed model with Transformer Block exploits the self-attention mechanism to capture the information from adjacent characters and sentence contexts. At the same time, this article uses a brand new industrial scene data set. On this data set, with the LSTM-CRF scale, the experimental results show that the TB-LSTM-CRF method is competitive and hardly requires any external resources, such as parameter migration.</p><p>Keywords:Chinese Natural Language Processing, Chinese Named Entity Recognition, Knowledge Graph</p><disp-formula id="hanspub.41326-formula9"><graphic xlink:href="//html.hanspub.org/file/28-1542073x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/28-1542073x7_hanspub.png" /> <img src="//html.hanspub.org/file/28-1542073x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 介绍</title><p>工业知识自动化势在必行，工业知识图谱亟需大力推行，而命名实体识别(NER)既是自然语言处理(NLP)中的一项重要任务，也NER被广泛认为是知识图构建过程中的第一步，是知识图谱构建过程中的下游自然语言处理的重要基础例如实体链接，关系提取，事件提取和共引用解析。</p><p>就一般的NER而言，高性能方法主要包括递归神经网络(RNN)，支持向量机(SVM)或长短期记忆(LSTM)和门递归单元(GRU)，以获取单词级别的上下文信息， [<xref ref-type="bibr" rid="hanspub.41326-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.41326-ref2">2</xref>] 所示其在英文领域取得了巨大成功，而由于中文句子没有与英文句子相同的自然定界符，而且中文句子中不同的分词可能导致句子的含义完全不同。汉语分词的问题还没有得到很好的解决。 [<xref ref-type="bibr" rid="hanspub.41326-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.41326-ref4">4</xref>] 可以看出，如果第一次错误地检测到边界，那么使用基于单词的NER模型进行正确提取就不可能了。最近，大多数用于中文NER的神经网络模型都严重依赖于出色的词嵌入表示形式和额外的词典，基于字符的方法比基于词汇的方法更适合中文NER [<xref ref-type="bibr" rid="hanspub.41326-ref5">5</xref>]。</p><p>目前已有部分研究针对其他垂直邻域的实体抽取实验 [<xref ref-type="bibr" rid="hanspub.41326-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.41326-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.41326-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.41326-ref9">9</xref>]，可是前提都需要大量的工作和极度依靠预训练模型性能。就工业领域NER而言，存在数据低资源和知识高门槛两大问题。数据低资源不止体现在工业机器人领域文本数据量少，还有专业质量不一，例如工程师甲描述某件事故为“电机发热冒烟”，而工程师乙则描述为“电机转子铁芯硅钢片绝缘损坏，电机运行温度过高或冒烟”。工程师的描述不一致，同一件故障会有多种描述，甚至缺少实体，导致模型训练更难拟合，这种问题在缺少训练数据的情况下更为严重。数据集的质量差还体现在由于存在标准模糊，尺度难把握，认知水平存在差异等问题，甚至工程师撰写日志时图方便而自定义一套易记语言，或者时描述不清晰，缺少实体，都会导致数据的标注质量差。对于工业领域数据集的专业性，使用一般模型并不能很好地识别其中极具专业性的实体。例如“动圈铁芯”这样的实体一般很难出现在一般数据集上，而且模型很容易将其识别成“动圈”和“铁芯”两个实体，甚至其他，很明显这是错误的。</p><p>而在工业领域的NER研究少之又少，针对这一问题，在研究过程中利用工业生产场景下电动机工作日志作为我们本次实验的种种一个目标数据集。基于字符的NER无法充分利用显式单词和单词序列信息，这也会影响NER的性能。为了解决这个问题，我们提出了一种基于自我注意机制的变压器模块来表示字符并将潜在的词汇信息集成到基于字符的LSTM-CRF中。如图1所示，我们使用Transformer模块 [<xref ref-type="bibr" rid="hanspub.41326-ref10">10</xref>] 搜索与上下文相关的字符，并输出包含与字符相关的信息的向量表示。门单元用于在包含多个字符信息的向量之间动态路由信息。最后，使用条件随机场(CRF)来整合和标准化在NER数据集上训练的预测，我们的方法可以从上下文中自动有效地找到更多有用的单词，从而获得更好的性能。与基于字符和基于单词的NER方法相比，我们的模型可以自动获取上下文词汇信息，而无需使用其他外部资源。而且由于其加入了Transformer模块和LSTM，能对前后文的特征信息有更充分的学习，更有利于学习专业性强的长实体。在OntoNote [<xref ref-type="bibr" rid="hanspub.41326-ref11">11</xref>]、Weibo NER [<xref ref-type="bibr" rid="hanspub.41326-ref12">12</xref>]、Chinese Resume [<xref ref-type="bibr" rid="hanspub.41326-ref13">13</xref>] 和工业电机数据集上我们的方法在各项指标上都有具有竞争力的提升。</p><p>图1. TB-LSTM-CRF模型</p></sec><sec id="s6"><title>2. TB-LSTM-CRF模型</title><p>为了从相邻字符中捕获信息。我们采用LSTM-CRF作为主要网络结构，并研究了中文NER的Transformer模块，该模块由一个自我注意层组成。</p><p>在本文中，我们使用BIOES标记方案 [<xref ref-type="bibr" rid="hanspub.41326-ref14">14</xref>] 进行实验。我们将输入句子序列表示为 X = { x 1 , x 2 , x 3 , ⋯ , x m } ，其中 x i 表示句子 X 中第i个字符嵌入向量，而m表示最大句子长度。相应地，我们定义 Y = { y 1 , y 2 , y 3 , ⋯ , y m } 作为句子预测序列，其中 y i 表示句子Y中第i个字符嵌入向量。目的是通过模型函数 f θ : X ↦ Y 来获得实体类别序列。</p><sec id="s6_1"><title>2.1. Transformer模块</title><p>Transformer模块旨在对输入字符序列进行编码，并在上下文中隐式对含义相关的字符进行分类。</p><p>第i字符的d维嵌入向量用 x i ∈ ℝ d 表示。模块整体框架如图2所示。</p><p>图2. Transformer模块整体框架</p><sec id="s6_1_1"><title>2.1.1. 位置编码</title><p>Transformer模块按照位置编码来理解语言的前后文，并使用self-attention机制和完全连接。Transformer模型没有递归神经网络的迭代操作，因此我们需要将每个单词的位置信息提供给Transformer模块，以便该模型可以识别句子中字符的顺序关系。位置嵌入被初始化为一个向量，其维度为(字典大小，嵌入维度)，并按以下两个部分计算：</p><p>P E p o s , 2 i = sin ( p o s / 10000 2 i / d ) (1)</p><p>P E p o s , 2 i + 1 = cos ( p o s / 10000 2 i / d ) (2)</p><p>在上面公式里，pos表示字符在句子中的位置，取值范围是[0，最大句子长度]。i表示字符向量的第i个维度，取值范围是[0，最大嵌入维度]。d表示字符向量的维度数量。这里有一对正弦和余弦函数，它们是与嵌入向量维度相对应的一组奇数和偶数的维。</p><p>然后将字符嵌入和位置编码对应相加，最后获得了包含位置信息的新字符嵌入向量 Λ i ∈ ℝ embed .dim 。 P E p o s 表示第pos个字符的位置编码。</p><p>Λ i = x i + P E p o s (3)</p></sec><sec id="s6_1_2"><title>2.1.2. Self-Attention机制</title><p>为了有效地学习句子中单词的依存关系并捕获句子的内部结构 [<xref ref-type="bibr" rid="hanspub.41326-ref15">15</xref>]，我们使用多头注意力(multi-head attention)来处理先前的特征向量 Λ ∈ ℝ batchsize ∗ seq .len ∗ embed .dim 。我们分配三个权重矩阵W_Q，W_Q，W_Q ∈ R ^ (embed.dim * embed.dim)并根据以下公式线性映射后获得三个权值矩阵 W Q , W K , W V ∈ ℝ embed .dim ∗ embed .dim ，和通过下面线性映射公式得到三个特征矩阵 Q , K , V ：</p><p>Q = Λ W Q (4)</p><p>K = Λ W K (5)</p><p>V = Λ W V (6)</p><p>然后，我们可以通过以下公式得到注意力矩阵：</p><p>H = softmax ( Q K T d k ) V (7)</p><p>Q K T 表示注意力矩阵， d k 的目的是使注意力矩阵遵循标准正态分布，从而使softmax层的结果更稳定。</p></sec><sec id="s6_1_3"><title>2.1.3. Add &amp; Layer Normalization</title><p>上一小节获得了注意矩阵H。我们对其进行转置以使其与特征矩阵Λ的尺寸一致，然后由于元素的尺寸相同，直接将这两个矩阵相加。</p><p>H = Λ + H (8)</p><p>Layer Normalization的作用是将神经网络中的隐藏层归一化为标准正态分布，从而加快训练和收敛速度：</p><p>H = LayerNorm ( H ) = α ⊙ h i j − μ i σ i 2 + ε + β (9)</p><p>其中 d k 是注意力矩阵H的第i行和第j列， ε 是一个很小的数字，可以防止方程中被零除。引入两个可训练的参数 α 和 β 来弥补归一化期间丢失的信息。</p><p>注意，⊙表示元素乘法而不是点积。通常，我们将 α 初始化为1，将 β 初始化为0。 μ i 和 σ i 分别代表每行的均值和方差。计算公式如下，其中m表示矩阵H中的列数。</p><p>μ i = 1 m ∑ j = 1 m h i j (10)</p><p>σ i 2 = 1 m ∑ j = 1 m ( h i j − μ i ) 2 (11)</p></sec><sec id="s6_1_4"><title>2.1.4. Feedforward</title><p>在前馈网络部分，此处使用两级线性映射，并通过ReLU激活函数进行激活。在前馈网络之后，后续的剩余连接和层规范化与前一小节中的类似，不同之处在于前一个H由前馈网络的输出代替。</p></sec></sec><sec id="s6_2"><title>2.2. Long Short-Term Memory (LSTM)</title><p>LSTM的目的是进一步处理上下文序列关系，其输入是Transformer模块输出的特征向量。</p></sec><sec id="s6_3"><title>2.3. Conditional Random Field (CRF)</title><p>为了提高序列标签预测的准确性，我们需要参考相邻字符的预测结果。这是序列标记的问题，也是CRF可以完美适配的工作。在LSTM输出该句子中每个字符的隐藏状态 h t ， h 2 ， ⋯ ， h m 之后，我们利用标准的CRF层来接收它。</p></sec><sec id="s6_4"><title>2.4. Training and Inference</title><p>一阶维特比算法用于在基于字符的输入序列上找到得分最高的标签序列。我们使用句子级别的对数似然损失和 L 2 正则化来训练模型：</p><p>L = ∑ i = 1 N log ( P ( y i | s i ) ) + λ 2 ‖ θ ‖ 2 (10)</p><p>此处 λ 和 θ 分别是为 L 2 正则的自定义设置参数。</p></sec></sec><sec id="s7"><title>3. 实验和结果</title><p>为了验证我们提出的方法的有效性，我们对涉及不同领域的几个中文NER数据集进行了一些实验。我们采用以下评估指标，分别采用标准精度(P)，召回率(R)和F1得分(F1)。实验数据集的详细信息将在表1中显示。</p><p>我们的实验中使用了三个数据集，分别包含在OntoNote [<xref ref-type="bibr" rid="hanspub.41326-ref11">11</xref>]，Weibo NER [<xref ref-type="bibr" rid="hanspub.41326-ref12">12</xref>] 和Chinese Resume [<xref ref-type="bibr" rid="hanspub.41326-ref13">13</xref>]。OntoNote数据集和WeiboNER数据集均包含4个命名实体类别：PER (人)，ORG (组织)，LOC (位置)和GPE (地缘政治实体)。中文简历数据集使用八种类型的命名实体进行注释：CONT (国家/地区)，EDU (教育机构)，LOC，PER，ORG，PRO (专业)，RACE (种族背景)和TITLE (职位)。</p><p>使用以上三个数据集，我们将TB-LSTM-CRF方法的性能与最新技术水平进行了比较，并形成了便于参考的表格。LSTM + CRF是基线方法。</p><sec id="s7_1"><title>3.1. OntoNote数据集</title><p>从表2的显示中可以看出，在包含4个实体分类的OntoNote数据集上，我们的方法在准确性和F1得分指标上取得了最佳结果。我们的方法比最近的Zhang et al. (2018) [<xref ref-type="bibr" rid="hanspub.41326-ref13">13</xref>] 的准确性高2.43%，在F1分数上比最好的Zhang et al. (2018) [<xref ref-type="bibr" rid="hanspub.41326-ref13">13</xref>] 高0.7%，比Zhu et al. (2019) [<xref ref-type="bibr" rid="hanspub.41326-ref16">16</xref>] 低1.07%。与基线相比，我们提出的新方法已经取得了全面的改进。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Statistics of dataset</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >类型</th><th align="center" valign="middle" >训练集</th><th align="center" valign="middle" >验证集</th><th align="center" valign="middle" >测试集</th></tr></thead><tr><td align="center" valign="middle"  rowspan="3"  >OntoNotes</td><td align="center" valign="middle" >句子</td><td align="center" valign="middle" >15.7 k</td><td align="center" valign="middle" >4.3 k</td><td align="center" valign="middle" >4.3 k</td></tr><tr><td align="center" valign="middle" >字符</td><td align="center" valign="middle" >491.9 k</td><td align="center" valign="middle" >200.5 k</td><td align="center" valign="middle" >208.1 k</td></tr><tr><td align="center" valign="middle" >实体</td><td align="center" valign="middle" >13.4 k</td><td align="center" valign="middle" >6.95 k</td><td align="center" valign="middle" >7.7 k</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >Weibo</td><td align="center" valign="middle" >句子</td><td align="center" valign="middle" >1.4 k</td><td align="center" valign="middle" >0.27 k</td><td align="center" valign="middle" >0.27 k</td></tr><tr><td align="center" valign="middle" >字符</td><td align="center" valign="middle" >73.5 k</td><td align="center" valign="middle" >14.5 k</td><td align="center" valign="middle" >14.8 k</td></tr><tr><td align="center" valign="middle" >实体</td><td align="center" valign="middle" >1.89 k</td><td align="center" valign="middle" >0.39 k</td><td align="center" valign="middle" >0.42 k</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >Resume</td><td align="center" valign="middle" >句子</td><td align="center" valign="middle" >3.8 k</td><td align="center" valign="middle" >0.46 k</td><td align="center" valign="middle" >0.48 k</td></tr><tr><td align="center" valign="middle" >字符</td><td align="center" valign="middle" >124.1 k</td><td align="center" valign="middle" >13.9 k</td><td align="center" valign="middle" >15.1 k</td></tr><tr><td align="center" valign="middle" >实体</td><td align="center" valign="middle" >1.34 k</td><td align="center" valign="middle" >0.16 k</td><td align="center" valign="middle" >0.15 k</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >工业电机数据集</td><td align="center" valign="middle" >句子</td><td align="center" valign="middle" >0.54 k</td><td align="center" valign="middle" >0.18 k</td><td align="center" valign="middle" >0.18 k</td></tr><tr><td align="center" valign="middle" >字符</td><td align="center" valign="middle" >17.124 k</td><td align="center" valign="middle" >4.46 k</td><td align="center" valign="middle" >4.81 k</td></tr><tr><td align="center" valign="middle" >实体</td><td align="center" valign="middle" >4.109 k</td><td align="center" valign="middle" >0.87 k</td><td align="center" valign="middle" >0.881 k</td></tr></tbody></table></table-wrap><p>表1. 数据集信息</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Result on OntoNot</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >准确率</th><th align="center" valign="middle" >召回率</th><th align="center" valign="middle" >F1</th></tr></thead><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref17">17</xref>] Yang et al. (2016)</td><td align="center" valign="middle" >65.59</td><td align="center" valign="middle" >71.84</td><td align="center" valign="middle" >68.57</td></tr><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref13">13</xref>] Zhang et al. (2018)</td><td align="center" valign="middle" >76.35</td><td align="center" valign="middle" >71.56</td><td align="center" valign="middle" >73.88</td></tr><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref16">16</xref>] Zhu et al. (2019)</td><td align="center" valign="middle" >75.05</td><td align="center" valign="middle" >72.29</td><td align="center" valign="middle" >73.64</td></tr><tr><td align="center" valign="middle" >基线模型</td><td align="center" valign="middle" >67.49</td><td align="center" valign="middle" >70.13</td><td align="center" valign="middle" >67.47</td></tr><tr><td align="center" valign="middle" >TB-LSTM-CRF</td><td align="center" valign="middle" >78.78</td><td align="center" valign="middle" >71.22</td><td align="center" valign="middle" >74.58</td></tr></tbody></table></table-wrap><p>表2. OntoNote数据集上的结果</p></sec><sec id="s7_2"><title>3.2. WeiBo数据集</title><p>在WeiBo数据集上，与最近的高级方法相比，我们提出的方法仍在三个指标上进行了改进。与基线方法相比，F1得分甚至提高了8.48% (表3)。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Result on Weib</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >准确率</th><th align="center" valign="middle" >召回率</th><th align="center" valign="middle" >F1</th></tr></thead><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref18">18</xref>] Peng and Dredze (2016)</td><td align="center" valign="middle" >55.25</td><td align="center" valign="middle" >62.97</td><td align="center" valign="middle" >58.99</td></tr><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref13">13</xref>] Zhang et al. (2018)</td><td align="center" valign="middle" >53.04</td><td align="center" valign="middle" >62.25</td><td align="center" valign="middle" >58.79</td></tr><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref16">16</xref>] Zhu et al. (2019)</td><td align="center" valign="middle" >55.38</td><td align="center" valign="middle" >62.98</td><td align="center" valign="middle" >59.31</td></tr><tr><td align="center" valign="middle" >基线模型</td><td align="center" valign="middle" >51.71</td><td align="center" valign="middle" >57.29</td><td align="center" valign="middle" >52.53</td></tr><tr><td align="center" valign="middle" >TB-LSTM-CRF</td><td align="center" valign="middle" >57.34</td><td align="center" valign="middle" >64.18</td><td align="center" valign="middle" >61.01</td></tr></tbody></table></table-wrap><p>表3. WeiBo数据集上的结果</p></sec><sec id="s7_3"><title>3.3. Chinese Resume数据集</title><p>与之前的数据集相比，Chinese Resume数据集具有较少的命名实体和更多的命名实体类别。我们的方法在此数据集上的性能比前两种方法稍差。Zhu et al. (2019) [<xref ref-type="bibr" rid="hanspub.41326-ref16">16</xref>] 在该数据集上的准确性和召回率均达到了最佳性能。我们的模型在F1得分上得分最高(表4)。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Result on Chinese resume datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >准确率</th><th align="center" valign="middle" >召回率</th><th align="center" valign="middle" >F1</th></tr></thead><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref13">13</xref>] Zhang et al. (2018)</td><td align="center" valign="middle" >94.81</td><td align="center" valign="middle" >94.11</td><td align="center" valign="middle" >94.46</td></tr><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref16">16</xref>] Zhu et al. (2019)</td><td align="center" valign="middle" >95.05</td><td align="center" valign="middle" >94.82</td><td align="center" valign="middle" >94.94</td></tr><tr><td align="center" valign="middle" >基线模型</td><td align="center" valign="middle" >91.34</td><td align="center" valign="middle" >90.58</td><td align="center" valign="middle" >89.33</td></tr><tr><td align="center" valign="middle" >TB-LSTM-CRF</td><td align="center" valign="middle" >94.23</td><td align="center" valign="middle" >94.25</td><td align="center" valign="middle" >95.22</td></tr></tbody></table></table-wrap><p>表4. 中文Resume数据集上的验证结果</p></sec><sec id="s7_4"><title>3.4. 工业电机数据集</title><p>我们提供了一个全新的数据集，工业电机数据集。其中包含工业场景里电动机的运行状态及诊断信息，包含了大量富有专业性的长实体。由于网上极少公开工业相关的实体识别数据集，本次数据集合极有可能是第一个相关领域实体识别数据集。我们的电动机工业数据集的静态统计如表5。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Statistics of industrial motor data se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集名称</th><th align="center" valign="middle" >NER标签类别</th><th align="center" valign="middle" >实体数量</th></tr></thead><tr><td align="center" valign="middle"  rowspan="7"  >工业电机数据集</td><td align="center" valign="middle" >设备</td><td align="center" valign="middle" >1504</td></tr><tr><td align="center" valign="middle" >子设备</td><td align="center" valign="middle" >422</td></tr><tr><td align="center" valign="middle" >部件</td><td align="center" valign="middle" >863</td></tr><tr><td align="center" valign="middle" >零件</td><td align="center" valign="middle" >333</td></tr><tr><td align="center" valign="middle" >属性</td><td align="center" valign="middle" >653</td></tr><tr><td align="center" valign="middle" >属性值</td><td align="center" valign="middle" >646</td></tr><tr><td align="center" valign="middle" >状态值</td><td align="center" valign="middle" >1439</td></tr></tbody></table></table-wrap><p>表5. 工业电机数据集静态统计信息</p><p>在该数据集上，我们的方法在各项性能上都处于领跑地位(表6)。</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Result on industrial motor datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >准确率</th><th align="center" valign="middle" >召回率</th><th align="center" valign="middle" >F1</th></tr></thead><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref13">13</xref>] Zhang et al. (2018)</td><td align="center" valign="middle" >79.64</td><td align="center" valign="middle" >84.82</td><td align="center" valign="middle" >84.64</td></tr><tr><td align="center" valign="middle" >[<xref ref-type="bibr" rid="hanspub.41326-ref16">16</xref>] Zhu et al. (2019)</td><td align="center" valign="middle" >82.43</td><td align="center" valign="middle" >86.28</td><td align="center" valign="middle" >86.87</td></tr><tr><td align="center" valign="middle" >基线模型</td><td align="center" valign="middle" >80.67</td><td align="center" valign="middle" >81.27</td><td align="center" valign="middle" >84.77</td></tr><tr><td align="center" valign="middle" >TB-LSTM-CRF</td><td align="center" valign="middle" >88.36</td><td align="center" valign="middle" >86.94</td><td align="center" valign="middle" >91.03</td></tr></tbody></table></table-wrap><p>表6. 工业电机数据集上的验证结果</p></sec><sec id="s7_5"><title>3.5. 联合数据集</title><p>为了验证我们的模型是否需要大量的外部资源，我们除了在单个数据集(工业电机数据集)中进行训练如3.4节实验外，我们还将模型从其余三个数据集上先预训练，保留训练结果后模型参数，最后将各参数作为初始值，重做一遍实验。得出结果如表所示。其F1分数值随迭代次数增加折线图如图3。</p><p>图3. TB-LSTM-CRF分别在工业电机数据集和联合数据集上实验的F1分数值</p><p>结果表明，虽然我们的模型在F1分数提升速度上比具有丰富资源的联合数据集方法训练模型时增长的快，可是最终两者的F1分数最终都趋于统一。表明，我们的模型在缺少训练样本，或外部特征资源时仍然具有竞争力。</p></sec></sec><sec id="s8"><title>4. 结论</title><p>在本文中，我们使用了Transformer模块和LSTM结合来提高中文NER模型的性能，在3.4节实验中说明我们的模型几乎无需依靠其他额外资源也能快速得到优异的F1分数值，而且我们的模型更加微型和实用。在3.1到3.3节实验表明，在不同的域数据集中，我们的模型可以比其他方法效果更好。</p><p>对于将来的工作，我们将继续改进我们的模型，使其更适合具有更多标签类别的任务。我们甚至将其扩展到实体关系的联合抽取任务上。</p></sec><sec id="s9"><title>基金项目</title><p>《工业过程数据实时获取与知识自动化》，国家自然科学基金委员会资助项目，项目编号：U17012621006336。《广东省新兴海洋经济产业地图与大数据平台》，广东省促进经济高质量发展专项资金项目。</p></sec><sec id="s10"><title>文章引用</title><p>李嘉正,周佳乐,程良伦. 使用TB-LSTM-CRF提高工业中文文本实体识别任务An Improved Chinese Named Entity Recognition Method with TB-LSTM-CRF[J]. 计算机科学与应用, 2021, 11(03): 720-728. https://doi.org/10.12677/CSA.2021.113074</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41326-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Yadav, V. and Bethard, S. (2019) A Survey on Recent Advances in Named Entity Recognition from Deep Learning Models. arXiv:1910.11470.</mixed-citation></ref><ref id="hanspub.41326-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Liu, L., Shang, J., Ren, X., Xu, F.F., Gui, H., Peng, J. and Han, J. (2018) Empower Sequence Labeling with Task- Aware Neural Language Model. 32nd AAAI Conference on Artificial Intelligence, New Orleans, 2-7 February 2018, 5253-5260.</mixed-citation></ref><ref id="hanspub.41326-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Yao, L., Ge, J., Li, C., Yao, Y., Li, Z., Zeng, J., et al. (2019) Word Seg-mentation for Chinese Judicial Documents. International Conference of Pioneering Computer Scientists, Engineers and Educators, 20-23 September 2019, Guilin, 466-478. &lt;br&gt;https://doi.org/10.1007/978-981-15-0118-0_36</mixed-citation></ref><ref id="hanspub.41326-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Chen, X., Shi, Z., Qiu, X. and Huang, X. (2017) Adversarial Multi-Criteria Learning for Chinese Word Segmentation. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, 30 July-4 August, 1193-1203. &lt;br&gt;https://doi.org/10.18653/v1/P17-1110</mixed-citation></ref><ref id="hanspub.41326-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Li, H., Hagiwara, M., Li, Q. and Ji, H. (2014). Comparison of the Im-pact of Word Segmentation on Name Tagging for Chinese and Japanese. Proceedings of the 9th International Conference on Language Resources and Evaluation, Reykjavik, 26-31 May 2014, 2532-2536.</mixed-citation></ref><ref id="hanspub.41326-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Yang, Y. and Katiyar, A. (2020) Simple Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning. Proceedings of the 2020 Con-ference on Empirical Methods in Natural Language Processing (EMNLP), Online, November 2020, 6365-6375. &lt;br&gt;https://doi.org/10.18653/v1/2020.emnlp-main.516</mixed-citation></ref><ref id="hanspub.41326-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Al-Smadi, M., Al-Zboon, S., Jararweh, Y. and Juola, P. (2020) Transfer Learning for Arabic Named Entity Recognition with Deep Neural Networks. IEEE Access, 8, 37736-37745. &lt;br&gt;https://doi.org/10.1109/ACCESS.2020.2973319</mixed-citation></ref><ref id="hanspub.41326-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Popovski, G., Seljak, B.K. and Eftimov, T. (2020) A Survey of Named-Entity Recognition Methods for Food Information Extraction. IEEE Access, 8, 31586-31594. &lt;br&gt;https://doi.org/10.1109/ACCESS.2020.2973502</mixed-citation></ref><ref id="hanspub.41326-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Prakash, R. and Dubey, R.K. (2020) Transfer Learning and Domain Adaptation for Named-Entity Recognition. In: Johri, P., Verma, J. and Paul, S., Eds., Applications of Machine Learning, Springer, Singapore, 67-73.  
&lt;br&gt;https://doi.org/10.1007/978-981-15-3357-0_5</mixed-citation></ref><ref id="hanspub.41326-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., et al. (2017) Attention Is All You Need. 31st Conference on Neural Information Processing Systems, Long Beach, 4-9 December 2017, 5998-6008.</mixed-citation></ref><ref id="hanspub.41326-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Weischedel, R., Pradhan, S., Ramshaw, L., Palmer, M., Xue, N., Marcus, M., et al. (2011) OntoNotes Release 4.0. LDC2011T03. Linguistic Data Consortium, Philadelphia.</mixed-citation></ref><ref id="hanspub.41326-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Peng, N. and Dredze, M. (2015) Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings. Pro-ceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, September 2015 548-554. &lt;br&gt;https://doi.org/10.18653/v1/D15-1064</mixed-citation></ref><ref id="hanspub.41326-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, Y. and Yang, J. (2018) Chinese NER Using Lattice LSTM. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, July 2018, 1554-1564. &lt;br&gt;https://doi.org/10.18653/v1/P18-1144</mixed-citation></ref><ref id="hanspub.41326-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Ratinov, L. and Roth, D. (2009) Design Challenges and Misconceptions in Named Entity Recognition. Proceedings of the Thirteenth Conference on Computational Natural Language Learning, June 2009, 147-155. 
&lt;br&gt;https://doi.org/10.3115/1596374.1596399</mixed-citation></ref><ref id="hanspub.41326-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Geng, X., Wang, L., Wang, X., et al. (2020) How Does Selective Mechanism Improve Self-Attention Networks? Proceedings of the 58th Annual Meeting of the Association for Computa-tional Linguistics, July 2020, 2986-2995. 
&lt;br&gt;https://doi.org/10.18653/v1/2020.acl-main.269</mixed-citation></ref><ref id="hanspub.41326-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Zhu, Y. and Wang, G. (2019) CAN-NER: Convolutional At-tention Network for Chinese Named Entity Recognition. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol. 1 (Long and Short Pa-pers), Minneapolis, 2-7 June 2019, 3384-3393.</mixed-citation></ref><ref id="hanspub.41326-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Yang, J., Teng, Z., Zhang, M. and Zhang, Y. (2016) Combining Discrete and Neural Features for Sequence Labeling. In: Gelbukh, A., Ed., Computational Linguistics and Intelligent Text Processing, Springer, Cham, 140-154.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-75477-2_9</mixed-citation></ref><ref id="hanspub.41326-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Peng, N. and Dredze, M. (2016) Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Berlin, August 2016, 149-155. &lt;br&gt;https://doi.org/10.18653/v1/P16-2025</mixed-citation></ref></ref-list></back></article>