<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.111015</article-id><article-id pub-id-type="publisher-id">CSA-39988</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210100000_72228508.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  人像抠图无监督语义精修算法
  Unsupervised Semantic Human Matting Refinement
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>曾</surname><given-names>广荣</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>程</surname><given-names>良伦</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>卢</surname><given-names>增</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>08</day><month>01</month><year>2021</year></pub-date><volume>11</volume><issue>01</issue><fpage>133</fpage><lpage>142</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对当前不使用三分图作为先验知识的人像抠图算法在远景人像抠图任务中存在多余的干扰信息、人像边缘轮廓粗糙、人体携带物品易与背景混淆等问题，提出了人像抠图无监督语义精修算法。该算法由人像边框感知模块与无监督语义精修模块组成。人像边框感知模块首先使用了行人检测模型识别出所有人像，并结合边框感知算法来去除多余的干扰信息。无监督语义精修模块利用了无监督语义分割模型提取特征，然后使用语义精修算法进行人像轮廓的修复。实验表明，在自制的远景人像数据集中，使用主流的人像抠图算法作为基线，并加入人像抠图无监督语义精修算法后，效果得到了明显的提高，人体携带物品也能精准识别，人像轮廓也更加清晰。同时在半身人像数据集中，效果也有一定的提升，表明了该算法也具有泛用性。 Aiming at the problems of human matting algorithm, which does not use trimap as a prior knowledge, such as redundant interference information, rough portrait edge contour, and easy confusion between objects carried by human body and the background, an unsupervised semantic matting algorithm for human matting is proposed. The algorithm is composed of human border sensing module and unsupervised semantic refinement module. The portraits border sensing module firstly uses the pedestrian detection model to identify all the portraits, and combines the border sensing algorithm to remove the redundant interference information. Unsupervised semantic refinement module uses unsupervised semantic segmentation model to extract features, and then uses semantic refinement algorithm to repair the portrait contour. Experiments show that in the self-made longterm portrait data set, the mainstream portrait matting algorithm is used as the baseline, and the unsupervised semantic refinement algorithm for portrait matting is added. The effect is significantly improved, and the objects carried by the human body can also be accurately identified. The outline is also clearer. At the same time, in the portrait data set, the effect is also improved to some extent, indicating that the algorithm is also generalized. 
  
 
</p></abstract><kwd-group><kwd>人像抠图，深度学习，语义精修, Human Matting</kwd><kwd> Deep Learning</kwd><kwd> Semantic Refinement</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>针对当前不使用三分图作为先验知识的人像抠图算法在远景人像抠图任务中存在多余的干扰信息、人像边缘轮廓粗糙、人体携带物品易与背景混淆等问题，提出了人像抠图无监督语义精修算法。该算法由人像边框感知模块与无监督语义精修模块组成。人像边框感知模块首先使用了行人检测模型识别出所有人像，并结合边框感知算法来去除多余的干扰信息。无监督语义精修模块利用了无监督语义分割模型提取特征，然后使用语义精修算法进行人像轮廓的修复。实验表明，在自制的远景人像数据集中，使用主流的人像抠图算法作为基线，并加入人像抠图无监督语义精修算法后，效果得到了明显的提高，人体携带物品也能精准识别，人像轮廓也更加清晰。同时在半身人像数据集中，效果也有一定的提升，表明了该算法也具有泛用性。</p></sec><sec id="s2"><title>关键词</title><p>人像抠图，深度学习，语义精修</p></sec><sec id="s3"><title>Unsupervised Semantic Human Matting Refinement<sup> </sup></title><p>Guangrong Zeng, Lianglun Cheng, Zeng Lu</p><p>Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/15-1542013x4_hanspub.png" /></p><p>Received: Dec. 25<sup>th</sup>, 2020; accepted: Jan. 19<sup>th</sup>, 2021; published: Jan. 26<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/15-1542013x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Aiming at the problems of human matting algorithm, which does not use trimap as a prior knowledge, such as redundant interference information, rough portrait edge contour, and easy confusion between objects carried by human body and the background, an unsupervised semantic matting algorithm for human matting is proposed. The algorithm is composed of human border sensing module and unsupervised semantic refinement module. The portraits border sensing module firstly uses the pedestrian detection model to identify all the portraits, and combines the border sensing algorithm to remove the redundant interference information. Unsupervised semantic refinement module uses unsupervised semantic segmentation model to extract features, and then uses semantic refinement algorithm to repair the portrait contour. Experiments show that in the self-made long-term portrait data set, the mainstream portrait matting algorithm is used as the baseline, and the unsupervised semantic refinement algorithm for portrait matting is added. The effect is significantly improved, and the objects carried by the human body can also be accurately identified. The outline is also clearer. At the same time, in the portrait data set, the effect is also improved to some extent, indicating that the algorithm is also generalized.</p><p>Keywords:Human Matting, Deep Learning, Semantic Refinement</p><disp-formula id="hanspub.39988-formula13"><graphic xlink:href="//html.hanspub.org/file/15-1542013x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/15-1542013x7_hanspub.png" /> <img src="//html.hanspub.org/file/15-1542013x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>随着相机、手机等硬件设备的发展，需要后期处理的图像也越来越多。但由于图像具有多样性与复杂性，无论传统图像处理方法还是深度学习方法，都难以做到对任何输入完全鲁棒。所以，针对特定类别的图像抠图算法 [<xref ref-type="bibr" rid="hanspub.39988-ref1">1</xref>] 更具有实际应用意义。</p><p>人像抠图是语义分割 [<xref ref-type="bibr" rid="hanspub.39988-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.39988-ref3">3</xref>] 的一种，是从图像或者视频中提取出人像前景，使人像与背景准确分离的一种技术。该技术在现实中有着广泛的应用，比如虚拟现实、增强现实、电影制作和摄影合成，这些应用主要基于图像合成技术。在图像合成技术中，通过人像抠图技术可以得到所需要的mask，mask中每一个像素点的值表示了原始图像每个像素是否属于人像前景，其精度直接影响合成图像质量的优劣。</p><p>对于图像I，人像抠图旨在找到人像前景F与背景B的最佳线性组合。对于任意的像素点i，可以满足以下公式：</p><p>I i = α i F i + ( 1 − α i ) B i , α i ∈ [ 0 , 1 ] (1)</p><p>其中I<sub>i</sub>为图像I的像素点i，α<sub>i</sub>表示在像素点i的前景不透明度。对于RGB图像，有7个未知变量： F i ( r ) ， F i ( g ) ， F i ( b ) ， B i ( r ) ， B i ( g ) ， B i ( b ) ， α i ，而已知变量仅有3个： I i ( r ) ， I i ( g ) ， I i ( b ) 。由于已经变量远小于未知变量，因此需要用户提供额外的先验知识才能求解，所以多数图像抠图算法 [<xref ref-type="bibr" rid="hanspub.39988-ref4">4</xref>] [<xref ref-type="bibr" rid="hanspub.39988-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.39988-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.39988-ref7">7</xref>] 利用三分图(trimap)作为先验知识，再进行图像抠图任务。比如Cho [<xref ref-type="bibr" rid="hanspub.39988-ref8">8</xref>] 等人结合了CF抠图 [<xref ref-type="bibr" rid="hanspub.39988-ref9">9</xref>] 与KNN [<xref ref-type="bibr" rid="hanspub.39988-ref5">5</xref>] 抠图两种方法，以两种方法的输出和RGB图像作为CNN网络的输入来学习图像与mask的映射关系。Xu等人 [<xref ref-type="bibr" rid="hanspub.39988-ref10">10</xref>] 用RGB图像与三分图来输入并集成了编码解码器结构和细化网络来预测mask。Lutz [<xref ref-type="bibr" rid="hanspub.39988-ref11">11</xref>] 等人则使用生成式对抗网络进行图像抠图任务。三分图是用户指定一部分已知的前景区域与已知的背景区域，然后求解未知的区域α。然而这个方法需要用户有一定水平的专业知识而且可能需要很长的时间才能得到满意的效果。因此近年来有学者尝试舍弃三分图作为先验知识并以端到端的方式学习图像与对应的mask之间的映射关系。比如Chen [<xref ref-type="bibr" rid="hanspub.39988-ref12">12</xref>] 等人从图像中学习隐性语义约束而不是用三分图或者涂鸦等先验知识来生成mask。Liu等人 [<xref ref-type="bibr" rid="hanspub.39988-ref13">13</xref>] 利用粗糙注释数据与精细注释数据相结合的方法，在不使用三分图的情况下，实现端到端的mask生成。但是这些方法在远景人像抠图中并不能达到满意的效果，原因有三个，首先是远景容易出现多余的干扰信息、其次是人体边缘轮廓的细化，最后是人体的携带物体容易与背景进行混淆。</p><p>因此，本文自制了远景人像数据集来进行实验。并提出了人像抠图无监督语义精修算法对mask进行语义精修。为了验证算法的效果，本文以SHM算法作为基线，并加上人像抠图无监督语义精修算法进行对比实验分析，表明了本文算法的优越性能。本文贡献如下：1) 为了去除人像背景噪音，结合了行人检测模型并提出了人像边框感知算法。2) 把无监督语义分割网络应用于人像抠图，并提出了人像精修模块来细化人体边缘轮廓和分离背景与人体携带物。3) 自制了远景人像抠图数据集，并使用充足的实验验证了算法的有效性。</p></sec><sec id="s6"><title>2. 人像抠图无监督语义精修算法</title><p>由于使用当前主流的人像抠图模型对远景人像进行人像抠图得到的mask存在噪音以及语义缺失问题。针对这些问题，本文提出了人像抠图无监督语义精修算法，该算法可以对人像抠图后的mask进行图像去噪和语义精修。算法如图1所示，由人像边框感知模块与无监督语义精修模块组成。人像边框感知模块通过行人检测模型与人像边框感知算法对输入的mask进行去噪。无监督语义精修模块则通过无监督语义分割网络并结合人像精修算法对去噪后的mask进行语义精修，得到更为精细完整的人像抠图。</p><sec id="s6_1"><title>2.1. 人像边框感知模块</title><p>如图1中的人像边框感知模块所示，人像边框感知模块的输入为RGB图像和mask，首先使用行人检测模型 [<xref ref-type="bibr" rid="hanspub.39988-ref14">14</xref>] 对RGB图像进行行人检测，得到人像边框 B = ( x 1 , x 2 , y 1 , y 2 ) ，其中 ( x 1 , y 1 ) 表示人像边框左上角的坐标， ( x 2 , y 2 ) 表示人像边框右下角的坐标。随后，使用人像边框感知算法对边框B进行修正并同时对mask进行去噪。如图2所示，该算法主要分为两个部分：人像边框内的修正算法与人像边框外的噪音去除方法。人像边框内的修正算法用于边框B没有完全框住人像的场景，该算法可以对人像边框B进行修正以达到完全框住人像的目的。</p><p>如图2(a)所示，行人的乐器在行人检测模型没有检测出来的情况下，人像边框内的修正算法可以很好地进行边框修正。具体实现方法如算法1所示。在得到完整的人像边框后，本文使用人像边框外噪音去除方法进行去噪。如图2(b)所示，该方法只保留面积最大的人像边框内的前景信息，并把该边框之外的前景信息设为背景信息，以去除人像背景的噪音。</p><p>算法1. 人像边框内的修正算法</p><p>Input: B = ( x 1 , x 2 , y 1 , y 2 ) 表示人像边框，mask表示透明度遮罩。</p><p>Output: B'表示修正后的人像边框。</p><p>1) initialize set B ′ = 0</p><p>2) for i ← x 1 to x<sub>2</sub> do</p><p>3) while m a s k i , y 1 &gt; 0</p><p>4) do y 1 ← y 1 + 1 end</p><p>5) while m a s k i , y 2 &gt; 0</p><p>6) do y 2 ← y 2 − 1 end</p><p>7) end for</p><p>8) for i ← y 2 to y<sub>1</sub> do</p><p>9) while m a s k x 1 , i &gt; 0</p><p>10) do x 1 ← x 1 − 1 end</p><p>图1. 整体流程图</p><p>图2. 人像边框感知算法流程图</p><p>11) while m a s k x 2 , i &gt; 0</p><p>12)do x 2 ← x 2 + 1 end</p><p>13) end for</p><p>14) B ′ ← ( x 1 ， x 2 ， y 1 ， y 2 )</p></sec><sec id="s6_2"><title>2.2. 无监督语义精修模块</title><p>如图1中的无监督语义精修模块所示，该模块包括无监督人像语义分割网络与人像精修算法，它的输入为人像边框感知模块的输出和RGB图像，输出为精修后的mask。</p><p>无监督语义分割网络 [<xref ref-type="bibr" rid="hanspub.39988-ref15">15</xref>] 如图3所示，输入RGB三通道图像P，表示为 P = { x n ∈ ℝ 3 } n = 1 N ，其中N为像素点的个数。每个像素点先正则化到[0, 1]区间内，再使用无监督聚类算法f对图像P的像素点进行语义预分类，分成k类，即 y n = f ( x n ) ，y<sub>n</sub>是一个1 &#215; k的矩阵，该矩阵元素为1的下标即为该像素点n的类别。给每个像素分配标签后，卷积神经网络可以根据该标签实现无监督学习。该网络包括特征提取部分与特征分类部分。特征提取部分由M个特征提取块组成，特征提取块包括卷积层，批归一化层(BN)和线性激活层(ReLU)，其中在卷积层上使用的是3 &#215; 3的卷积核。</p><p>图3. 无监督人像语义分割网络</p><p>h i = Re LU ( BN ( c o n v 3 &#215; 3 ( h i − 1 ) ) ) (2)</p><p>其中 h i 为第i个特征提取块的输出。批归一化层可以使输入到ReLU的图片接近正态分布。特征分类部分由使用1 &#215; 1卷积核的卷积层，BN层和全连接层组成。</p><p>G = classification ( BN ( c o n v 1 &#215; 1 ( h M ) ) ) (3)</p><p>其中G为 k &#215; N 维的矩阵。最后网络的训练需要计算矩阵G与标签Y的损失，本文使用Softmax Loss作为损失函数。</p><p>Loss = 1 N ∑ i = 1 N ( − ∑ j = 1 k y i , j log e G i , j ∑ l = 1 k e G i , l ) (4)</p><p>RGB三通道图像经过无监督人像语义分割网络后每个像素点被分为q类。相同类别且连续的像素点构成区域P<sub>k</sub>，其中 k ∈ { 1 , 2 , ⋯ , q } 。人像精修算法的核心在于统计mask在每个区域P<sub>k</sub>中前景与背景的个数，本文把像素点为黑色设为背景，像素点为白色设为前景。如果mask在区域P<sub>k</sub>中前景的数量与背景的数量的比值大于θ，则把mask在区域P<sub>k</sub>中的值都设置为前景的值，如果前景的数量与背景的数量的比值小于 ( 1 − θ ) ，则把mask在区域P<sub>k</sub>中的值都设置为背景的值。其中θ为超参数。该算法可以修改mask中的属于背景信息的前景信息，同时可以细化人像及携带物品的边缘轮廓。</p></sec></sec><sec id="s7"><title>3. 实验设计及结果分析</title><sec id="s7_1"><title>3.1. 实验准备</title><p>由于目前人像抠图数据集大多为半身人像图片，因此本文自制了一个远景人像数据集来进行实验设计与分析。该数据集的图像均采集于互联网，而且大多选择人像前景与背景变化较小的并且有携带物体的图片。数据集共有2000张图片，分辨率为1920像素 &#215; 988像素，标注内容包括2703个行人目标框，2000张mask图。本文将该数据集按8:1:1分为训练集、验证机、测试集。因此随机抽取1600张图像所为训练集，200张图像作为验证集，200张图像作为测试集。</p><p>本文的实验环境：ubuntu16.04操作系统，Intel i7中央处理器，1 TB固态硬盘，32 GB内存，NVIDIA GTX2080Ti GPU &#215; 2，并使用Pytorch深度学习框架进行训练。</p></sec><sec id="s7_2"><title>3.2. 实验结果与分析</title><p>由于SHM算法是在人像抠图任务中率先舍弃用三分图作为先验知识的算法，并且是广泛使用的人像抠图算法，因此本文选择把SHM算法当作基线进行对比实验分析。</p><p>本文采用2个广泛使用的评价指标进行评测，分别为均方误差MSE (mean-square error)和绝对误差SAD (sum of absolute differences)。这2个指标的值越低表示效果越好。公式如下：</p><p>MSE = 1 n ∑ i = 1 n ( x i − y i ) 2 (5)</p><p>SAD = 1 n ∑ i = 1 n | x i − y i | (6)</p><p>其中x<sub>i</sub>为实验获得的图像；y<sub>i</sub>是ground true图像。</p><p>在人像边框感知模块的实验中，本文使用YOLOv3算法 [<xref ref-type="bibr" rid="hanspub.39988-ref14">14</xref>] 在自制数据集上训练行人检测模型。对输入图片进行行人检测得到人像边框后，使用人像边框感知算法对标签图片进行去噪。</p><p>在无监督语义精修模块的实验中，迭代次数为500，学习率设为0.0001，并选用Adam算法 [<xref ref-type="bibr" rid="hanspub.39988-ref16">16</xref>] 作为优化器进行优化。对于不同的超参数θ，在表1中展示了使用Felzenszwalb算法 [<xref ref-type="bibr" rid="hanspub.39988-ref17">17</xref>] 和SLIC算法 [<xref ref-type="bibr" rid="hanspub.39988-ref18">18</xref>] 作为无监督聚类算法的实验效果，评价指标为使用语义精修算法后与使用之前降低均方误差。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The results of two pre-classification algorithms in different hyperparameters </title></caption><table><tbody><thead><tr><th align="center" valign="middle" >预分类算法</th><th align="center" valign="middle" >θ取值</th><th align="center" valign="middle" >MSE (&#215;10<sup>−</sup><sup>3</sup>)</th></tr></thead><tr><td align="center" valign="middle" >SLIC</td><td align="center" valign="middle" >0.6</td><td align="center" valign="middle" >3.1</td></tr><tr><td align="center" valign="middle" >SLIC</td><td align="center" valign="middle" >0.7</td><td align="center" valign="middle" >5.6</td></tr><tr><td align="center" valign="middle" >SLIC</td><td align="center" valign="middle" >0.8</td><td align="center" valign="middle" >5.4</td></tr><tr><td align="center" valign="middle" >Felzenszwalb</td><td align="center" valign="middle" >0.6</td><td align="center" valign="middle" >4.2</td></tr><tr><td align="center" valign="middle" >Felzenszwalb</td><td align="center" valign="middle" >0.7</td><td align="center" valign="middle" >7.5</td></tr><tr><td align="center" valign="middle" >Felzenszwalb</td><td align="center" valign="middle" >0.8</td><td align="center" valign="middle" >6.8</td></tr></tbody></table></table-wrap><p>表1. 两种预分类算法在不同超参数θ的实验结果</p><p>该数据显示，使用Felzenszwalb算法在θ为0.7的时候，实验效果最好，MSE平均降低0.0075。因为Felzenszwalb算法相比于SLIC算法，对于变化幅度小的照片，能很好地处理细节；对于变化幅度大的照片，也能很好地忽略一些细节。同时发现当θ取值0.6时，MSE降低最少，因为在区域P<sub>k</sub>中前景与背景像素点数量的比值相差较小的话，改变前景或者背景的值可能会得到错误的结果。而当θ取值较大时，很有可能会忽略一些本应该改变的前景(背景)像素点。</p><p>为了验证人像抠图无监督语义精修算法(SHMR)在远景人像数据集的性能效果，在表2展示了具体的实验数据，该实验是预分类算法使用Felzenszwalb算法，θ取0.7时，在有无人像抠图无监督语义精修算法(SHMR)的情况下，用SHM算法的实验效果，可以看出在加上了SHMR算法后效果得到了明显的提升。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Comparison of experimental results in the perspective portrait datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >MSE</th><th align="center" valign="middle" >SAD</th></tr></thead><tr><td align="center" valign="middle" >SHM</td><td align="center" valign="middle" >0.0142</td><td align="center" valign="middle" >0.0203</td></tr><tr><td align="center" valign="middle" >SHM + SHMR</td><td align="center" valign="middle" >0.0067</td><td align="center" valign="middle" >0.0114</td></tr></tbody></table></table-wrap><p>表2. 在远景人像数据集中的对比实验结果</p><p>为了验证在远景人像数据集中人像边框感知模块以及无监督语义精修模块的作用，本文进行了消融实验。如表3所示，在只使用人像边框感知模块的情况下，效果也有一定的提升。但是只使用无监督语义精修模块，效果反而变差，原因是在没有完全去除环境噪音的情况下使用无监督语义精修模块，会让噪音放大，使实验效果变差。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Results of two module ablation experiment</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >MSE</th><th align="center" valign="middle" >SAD</th></tr></thead><tr><td align="center" valign="middle" >只使用人像边框感知模块</td><td align="center" valign="middle" >0.0106</td><td align="center" valign="middle" >0.0179</td></tr><tr><td align="center" valign="middle" >只使用无监督语义精修模块</td><td align="center" valign="middle" >0.0230</td><td align="center" valign="middle" >0.0338</td></tr><tr><td align="center" valign="middle" >都使用</td><td align="center" valign="middle" >0.0067</td><td align="center" valign="middle" >0.0114</td></tr></tbody></table></table-wrap><p>表3. 两个模块消融实验的实验结果</p><p>图4为在远景人像数据集中，使用SHM算法进行人像抠图以及SHM加上SHMR人像精修算法的实验效果图。可以看出，在有行人的环境中，SHM算法容易产生额外噪音，而本文提出算法的人像边框感知模块可以很好地去除噪音。在人像有携带物的情况下，人像整体也难以识别出来，在引入无监督语义精修模块后，人体携带物体也能准确识别，人像边缘轮廓也更加清晰。</p><p>图4. 在远景人像数据集的实验效果图</p><p>为了验证SHMR人像精修算法的泛用性能，本文同时在半身人像抠图数据集进行实验分析。使用的数据集为爱分割(aisegment.com)高质量标注并开源的人像抠图数据集。该数据集包含34,427张图像和对应的matting结果图，分辨率为600像素 &#215; 800像素，超参数的设置与远景人像数据集相同。表4展示了在半身人像数据集中的对比实验结果。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Comparison of experimental results in the data set of half-length portrait</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >MSE</th><th align="center" valign="middle" >SAD</th></tr></thead><tr><td align="center" valign="middle" >SHM</td><td align="center" valign="middle" >0.0086</td><td align="center" valign="middle" >0.0132</td></tr><tr><td align="center" valign="middle" >SHM + SHMR</td><td align="center" valign="middle" >0.0071</td><td align="center" valign="middle" >0.0119</td></tr></tbody></table></table-wrap><p>表4. 在半身人像数据集中的对比实验结果</p><p>由表4可以看出，在半身人像数据集中，加入SHMR算法后效果同样得到提升，但是MSE的降低并没在远景人像数据集中那么多，原因是：在远景人像数据集中存在很多环境噪音，由于SHMR有人像边框感知模块，环境噪音可以得到去除，最后再进行人像边缘的精修。然而在半身人像数据集中，环境噪音非常少，主要侧重了人像边缘的精修，所以MSE的降低并没有在远景人像数据集中那么多。</p><p>图5为在半身人像数据集中，使用SHM算法进行人像抠图以及SHM加上SHMR人像精修算法的实验效果图。对比远景人像数据集的实验可以看出，半身人像数据集中环境噪音明显大幅度减少，并且主要侧重于人像轮廓的修复。而加入SHMR人像精修算法后人像轮廓修复得更加完整清晰，这也验证了该算法的泛用性。</p><p>图5. 在半身人像数据集的实验效果图</p></sec></sec><sec id="s8"><title>4. 结束语</title><p>本文针对远景人像抠图存在噪音、人体边缘轮廓粗糙、人体携带物易与背景混淆等问题，提出了人像抠图无监督语义精修算法。通过人像边框感知模块进行去噪并使用无监督语义精修模块优化细节，以达到人像前景与背景完整分割的目标。实验表明，在远景人像数据集中，在主流的人像抠图算法中加入人像抠图无监督语义精修算法后，效果有明显的提高，并且在半身人像数据集中，效果也有一定的提高。但由于本文将去噪模块与精修模块分割开来，也会导致误差的传递，接下来将针对端到端的人像抠图精修算法展开研究。</p></sec><sec id="s9"><title>文章引用</title><p>曾广荣,程良伦,卢 增. 人像抠图无监督语义精修算法Unsupervised Semantic Human Matting Refinement[J]. 计算机科学与应用, 2021, 11(01): 133-142. https://doi.org/10.12677/CSA.2021.111015</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.39988-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Wu, X., Fang, X.N., Chen, T., et al. (2020) JMNet: A Joint Matting Network for Automatic Human Matting. Computa-tion Visual Media, 6, 215-224. &lt;br&gt;https://doi.org/10.1007/s41095-020-0168-6</mixed-citation></ref><ref id="hanspub.39988-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Badrinarayanan, V., Kendall, A. and Cipolla, R. (2019) SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 2481-2495.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2016.2644615</mixed-citation></ref><ref id="hanspub.39988-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Long, J., Shelhamer, E. and Darrell, T. (2015) Fully Convolu-tional Networks for Semantic Segmentation. Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 3431-3440.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298965</mixed-citation></ref><ref id="hanspub.39988-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Aksoy, Y., Aydın, T.O. and Pollefeys, M. (2017) Designing Effective Inter-Pixel Information Flow for Natural Image Matting. Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 29-37. &lt;br&gt;https://doi.org/10.1109/CVPR.2017.32</mixed-citation></ref><ref id="hanspub.39988-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Q.F., et al. (2013) KNN Matting. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 35, 2175-2188. &lt;br&gt;https://doi.org/10.1109/TPAMI.2013.18</mixed-citation></ref><ref id="hanspub.39988-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Cai, S., Zhang, X., Fan, H., et al. (2019) Disentangled Image Matting. The IEEE International Conference on Computer Vision, Seoul, 27-28 October 2019, 8818-8827. &lt;br&gt;https://doi.org/10.1109/ICCV.2019.00891</mixed-citation></ref><ref id="hanspub.39988-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Shen, X., Tao, X., Gao, H., et al. (2016) Deep Automatic Portrait Matting. European Conference on Computer Vision, Amsterdam, 8-16 October 2016, 92-107. &lt;br&gt;https://doi.org/10.1007/978-3-319-46448-0_6</mixed-citation></ref><ref id="hanspub.39988-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Cho, D., Tai, Y.W. and Kweon, I. (2016) Natural Im-age Matting Using Deep Convolutional Neural Networks. The European Conference on Computer Vision, Amsterdam, 8-16 October 2016, 626-643.  
&lt;br&gt;https://doi.org/10.1007/978-3-319-46475-6_39</mixed-citation></ref><ref id="hanspub.39988-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Levin, A. (2006) A Closed Form Solution to Natural Image Matting. Proceedings of 2006 IEEE Conference on Computer Vision and Pattern Recognition, New York, 17-22 June 2006, 228-242.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2007.1177</mixed-citation></ref><ref id="hanspub.39988-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Ning, X., Price, B., Cohen, S. and Huang, T. (2017) Deep Image Matting. Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, 21-26 July 2017, 2970-2979.</mixed-citation></ref><ref id="hanspub.39988-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Lutz, S., Amplianitis, K. and Smolic, A. (2018) AlphaGAN: Generative Adversarial Networks for Natural Image Matting. British Machine Vision Conference, Newcastle, 3-6 September 2018, 259.</mixed-citation></ref><ref id="hanspub.39988-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Q., Ge, T., Xu, Y., et al. (2018) Semantic Human Matting. 2018 ACM Multimedia Conference, Seoul, 22-26 October 2018, 618-626. &lt;br&gt;https://doi.org/10.1145/3240508.3240610</mixed-citation></ref><ref id="hanspub.39988-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Liu, J., Yao, Y., Hou, W., et al. (2020) Boosting Semantic Human Matting with Coarse Annotations. Proceedings of 2020 IEEE Conference on Computer Vision and Pattern Recognition, Seattle, 14-19 June 2020, 8560-8569.  
&lt;br&gt;https://doi.org/10.1109/CVPR42600.2020.00859</mixed-citation></ref><ref id="hanspub.39988-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Redmon, J. and Farhadi, A. (2018) YOLOv3: An Incre-mental Improvement.</mixed-citation></ref><ref id="hanspub.39988-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Kanezaki, A. (2018) Unsupervised Image Segmentation by Backpropagation. IEEE Interna-tional Conference on Acoustics, Calgary, 15-20 April 2018, 1543-1547. &lt;br&gt;https://doi.org/10.1109/ICASSP.2018.8462533</mixed-citation></ref><ref id="hanspub.39988-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Kingma, D.P. and Jimmy, B. (2014) Adam: A Method for Stochastic Optimization.</mixed-citation></ref><ref id="hanspub.39988-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Felzenszwalb, P.F. and Huttenlocher, D.P. (2004) Efficient Graph-Based Image Segmen-tation. International Journal of Computer Vision, 59, 167-181. &lt;br&gt;https://doi.org/10.1023/B:VISI.0000022288.19776.77</mixed-citation></ref><ref id="hanspub.39988-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Achanta, R., Shaji, A., Smith, K., et al. (2012) SLIC Superpixels Compared to State-of-the-Art Superpixel Methods. IEEE Transactions on Pattern Analysis &amp; Machine Intel-ligence, 34, 2274-2282.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2012.120</mixed-citation></ref></ref-list></back></article>