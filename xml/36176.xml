<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">JISP</journal-id><journal-title-group><journal-title>Journal of Image and Signal Processing</journal-title></journal-title-group><issn pub-type="epub">2325-6753</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/JISP.2020.93018</article-id><article-id pub-id-type="publisher-id">JISP-36176</article-id><article-categories><subj-group subj-group-type="heading"><subject>JISP20200300000_63065971.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于MobileNet网络多国人脸分类识别
  Multi-National Face Classification and Recognition Based on MobileNet Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郭</surname><given-names>奕君</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>阿里木江·</surname><given-names>阿布迪日依木</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>努尔毕亚·</surname><given-names>亚地卡尔</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>朱</surname><given-names>亚俐</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>库尔班·</surname><given-names>吾布力</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>新疆大学信息科学与工程学院，新疆 乌鲁木齐；新疆大学信号与信息处理重点实验室，新疆 乌鲁木齐</addr-line></aff><aff id="aff3"><addr-line>新疆维吾尔自治区科技项目服务中心项目服务部，新疆 乌鲁木齐</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>11</day><month>06</month><year>2020</year></pub-date><volume>09</volume><issue>03</issue><fpage>146</fpage><lpage>155</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    随着各国经济贸易、文化交流往来的日益频繁，快速、有效地区分各国人员身份是当前人脸识别领域的一项重要研究。本文特针对亚洲区域五个国家(中国、日本、韩国、泰国、印度)进行人脸分类识别的研究，本文基于MobileNet进行五国人脸分类识别，因为这五国人脸较为相似，为能有效降低冗余，本文将八度卷积插入该网络中减少冗余，提升精度；并提出使用中心损失函数和交叉熵损失函数相结合的方法来提升准确率。经过实验验证，本文提出的在网络中插入八度卷积和中心损失函数两种改进方法均可以提升准确率，其最高准确率可达87.84%，其Error top 1最低达到0.120%。
    With the increasingly frequent economic, trade and cultural exchanges between countries, quickly and effectively distinguishing the identity of people in various countries is an important research in the field of face recognition. This paper focuses on the research of face classification and recognition in five Asian countries (Chinese, Japanese, Korean, Thailand and Indian). In this paper, face classification and recognition in five Asian countries are based on MobileNet. Because the faces of these five countries are similar, to reduce redundancy in this paper, octave convolution is inserted into the network to reduce redundancy and improve accuracy; and a method using a combination of center loss function and cross-entropy loss function is proposed to improve accuracy. After experimental verification, both the octave convolution and the center loss function proposed in this paper can improve the accuracy rate, and the highest accuracy rate can reach 87.84%, its Error top 1 is at least 0.120%. 
  
 
</p></abstract><kwd-group><kwd>多国人脸分类，八度卷积，中心损失函数，宽度乘子，倒残差模块, Multi-National Face Classification</kwd><kwd> Octave Convolution</kwd><kwd> Center Loss</kwd><kwd> Width Multiplier</kwd><kwd> 
Inverted Residuals</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于MobileNet网络多国人脸分类识别<sup> </sup></title><p>郭奕君<sup>1,2</sup>，阿里木江&#183;阿布迪日依木<sup>3</sup>，努尔毕亚&#183;亚地卡尔<sup>1,2</sup>，朱亚俐<sup>1,2</sup>，库尔班&#183;吾布力<sup>1,2*</sup></p><p><sup>1</sup>新疆大学信息科学与工程学院，新疆 乌鲁木齐</p><p><sup>2</sup>新疆大学信号与信息处理重点实验室，新疆 乌鲁木齐<sup> </sup></p><p><sup>3</sup>新疆维吾尔自治区科技项目服务中心项目服务部，新疆 乌鲁木齐</p><disp-formula id="hanspub.36176-formula39"><graphic xlink:href="//html.hanspub.org/file/3-2670232x5_hanspub.png"  xlink:type="simple"/></disp-formula><p>收稿日期：2020年5月20日；录用日期：2020年6月12日；发布日期：2020年6月19日</p><disp-formula id="hanspub.36176-formula40"><graphic xlink:href="//html.hanspub.org/file/3-2670232x6_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>随着各国经济贸易、文化交流往来的日益频繁，快速、有效地区分各国人员身份是当前人脸识别领域的一项重要研究。本文特针对亚洲区域五个国家(中国、日本、韩国、泰国、印度)进行人脸分类识别的研究，本文基于MobileNet进行五国人脸分类识别，因为这五国人脸较为相似，为能有效降低冗余，本文将八度卷积插入该网络中减少冗余，提升精度；并提出使用中心损失函数和交叉熵损失函数相结合的方法来提升准确率。经过实验验证，本文提出的在网络中插入八度卷积和中心损失函数两种改进方法均可以提升准确率，其最高准确率可达87.84%，其Error top 1最低达到0.120%。</p><p>关键词 :多国人脸分类，八度卷积，中心损失函数，宽度乘子，倒残差模块</p><disp-formula id="hanspub.36176-formula41"><graphic xlink:href="//html.hanspub.org/file/3-2670232x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/3-2670232x8_hanspub.png" /> <img src="//html.hanspub.org/file/3-2670232x9_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>随着时代变迁与人类生活领域的交互，为推动我国现代化体制建设进程，促进世界经济全球化，从改革开放时期以来，我国始终坚持对外开放力度，逐步扩大对外开放范围，尤其是外贸方面，使我国在对外贸易发展中取得长足的进步，让我国同世界各国人民共同走向人类命运共同体。在国家间往来日益频繁的今天，各国之间有经济、文化、繁育以及习俗之间的交融，种族特征在随着时间的推移与人类间互相演变出现新的特征，结合当前大数据知识应用体系留存不同国家人脸特征数据，以逐步丰富人脸识别分析基础资源数据库。人脸识别就显得尤为重要，它能有效对各个国家人员进行识别，以便我国边检口岸等一系列安全系统能更有效、快速地识别各个国家人员，构建更加有序的公共环境。也可以逐步丰富人脸识别分析基础资源数据库，这对未来相关领域的研究将会是一份宝贵的财富。</p><p>近年来国内外也有不少研究者们对人脸种族识别进行了一定的研究。Sheerko Hma Salah [<xref ref-type="bibr" rid="hanspub.36176-ref1">1</xref>] 等人使用颜色和纹理的多层融合方案进行人脸种族特征的提取，其样本数据是由多个数据库集合构成，然后使用KNN分类器区分高加索人、东亚人、非洲人，其平均准确率达96.5%。A. Rehman，G. Khan [<xref ref-type="bibr" rid="hanspub.36176-ref2">2</xref>] 等人使用Haralick + GLCM特征融合提取特征，并使用预测随机数作为分类区分亚洲人、欧洲人和非洲人，其准确率达到85.39%。黄惠 [<xref ref-type="bibr" rid="hanspub.36176-ref3">3</xref>] 将PCA和2DPCA进行特征提取，用多种不同的SVM作为分类器应用于自建中国少数民族(维吾尔族、柯尔克孜族、哈萨克族、回族、蒙古族)数据库进行民族识别，实验表明，使用RBF的SVM效果可达85%。王雅丽 [<xref ref-type="bibr" rid="hanspub.36176-ref4">4</xref>] 使用CNN提取深度虹膜纹理特征作为底层向量，并结合Fisher向量相融合，使用SVM分类器区分亚洲人和非亚洲人，最高准确率可达99.93%。邱盛 [<xref ref-type="bibr" rid="hanspub.36176-ref5">5</xref>] 在深度学习的研究中，利用了几种不同网络框架对自建的中国部分少数民族(汉族、蒙古族、藏族、维吾尔族)数据集进行民族识别分类，其中包括卷积神经网络和集成神经网络，并利用卷积神经网络搭建了一个基于这五个民族的人脸识别系统。</p><p>从当前国内外研究现状来看，当前研究中基本都在区分人种，而对同一区域不同国家的人脸面部研究几乎没有。因此本文提出针对同一区域(亚洲)上的五个国家(中国、日本、韩国、泰国、印度)进行人脸识别分类的研究。本文通过基于MobileNet网络上对本文自建的五国人脸数据集进行识别分类，通过对原始网络、插入八度卷积以及加入中心损失函数后的实验数据对比，验证本文提出方法的有效性。</p></sec><sec id="s4"><title>2. 自建五国人脸数据集</title><sec id="s4_1"><title>2.1. 人脸数据集的采集</title><p>由于当前没有公开的这五国的人脸数据集，且考虑到实地采集的困难性和限制性，本文通过网络爬虫获取这五国(中国、日本、韩国、泰国、印度)的人脸数据样本。由于网络爬虫图像样本数据存在大量由光照、姿势、遮挡各种因素引起的大量差异，本文通过使用，Multi-task convolutional neural network (MTCNN，多任务卷积神经网络)对本文通过网络爬虫采集的人脸数据样本进行“清洗”，即对人脸定位和对齐，本文将原始数据集裁剪成了128 &#215; 128尺寸大小。本文经过“清洗”后所建立的原始数据集样本共10,266张(中国2881张、日本1747张、韩国2381张、泰国1971张、印度1286张)，由于网络爬虫的限制导致得到的初始样本数据量不一致，得到的部分样本数据如图1所示，其样本顺序由左到右五列依次为中国、日本、韩国、泰国、印度。</p><p>图1. 部分五国人脸样本</p></sec><sec id="s4_2"><title>2.2. 人脸数据集的扩充</title><p>由于使用深度学习，需要大量人脸图像进行学习。故本文采取数据增强方法对样本数量进行扩充，通过对原始照片旋转一定的小角度(角度旋转范围在15度以内)、改变饱和度、对比度、锐度以及亮度来进行样本数据的扩充。</p></sec></sec><sec id="s5"><title>3. 基于Mobilenet网络模型核心</title><sec id="s5_1"><title>3.1. MobileNet网络</title><p>MobileNetV1网络结构模型是2017年谷歌团队提出的 [<xref ref-type="bibr" rid="hanspub.36176-ref6">6</xref>]，该网络主要针对便捷式或嵌入式设备使用，属于轻量级卷积神经网络，在2018~2019年谷歌团队在该网络基础上又先后提出了MobileNetV2网络 [<xref ref-type="bibr" rid="hanspub.36176-ref7">7</xref>]、MobileNetV3网络 [<xref ref-type="bibr" rid="hanspub.36176-ref8">8</xref>]。本文的所有实验都是基于MobileNetV1和MobileNetV2两个模型上进行的。该网络核心思想是深度可分离卷积，其原理是把普通卷积拆分成一个深度卷积和一个点卷积的组合以减少运算量。其原理如图2所示。</p><p>图2. 深度可分离卷积原理</p><p>MobileNet网络的第二个核心是提出了两个超参数：宽度因子α、分辨率因子β，这两个超参数的目的也是减少运算量。理论上来说，宽度因子α的取值越大，其训练精度越高，相应地训练时间也和参数量也增加。同理，对于分辨率因子β，其取值越大，训练精度越高，相应地训练时间也和参数量也增加。宽度因子α取值范围在[0, 1]，通常定义为四个值：0.25、0.5、0.75、1。分辨率因子β通常是128 &#215; 128、160 &#215; 160、196 &#215; 196、224 &#215; 224。</p><p>MobileNetV1网络共28层，其网络结构中，除了第一层和最后一层使用普通卷积，其余层均采用深度可分离卷积代替普通卷积。每一层卷积后面均需使用BN (Batch Normalization)和Relu6激活函数，目的是为了能有效调整每一层的特征图分布，加速网络收敛，增强了网络的泛化能力。</p><p>MobileNetV2网络共有54层，在MobileNetV2网络模型中，它是将MobileNetV1网络做了两处改进：1) 仿照ResNet网络的核心思想，加入了“倒残差模块”，即对原始特征图做“扩张–卷积–压缩”的操作；2) 去除最后层的Relu6激活函数，直接替换为线性输出。</p></sec><sec id="s5_2"><title>3.2. 八度卷积</title><p>八度卷积 [<xref ref-type="bibr" rid="hanspub.36176-ref9">9</xref>] 是2019年提出的一种新卷积，八度卷积的目的是为了在提高精度的同时，减少冗余。它的核心思想就是通过空间尺度变化将图像拆分为高频、低频两块，此操作可以大大降低参数量。完成此步骤是通过使用网络定义的超参数α将图像分解成高频低频两部分，通过高斯滤波器使得低频的空间是高频空间的一半，将原图像可以拆分成图3所示，通过该方法可以有效减少空间冗余。</p><p>八度卷积的另一个优点是可以很好的嵌入到任一神经网络中，即可以在不改变原网络结构及参数的前提下，通过对超参数α的设定即可用八度卷积代替普通卷积，较好的解决低频信息的冗余问题。八度卷积的使用中，除第一层和最后一层外对超参数设定为： α i n = α o u t = α ，第一层的输入和最后一层的输出均为0，即可完成对八度卷积的即插即用。</p><p>图3. 八度卷积原理图</p></sec></sec><sec id="s6"><title>4. 损失函数</title><p>由于亚洲这五国人脸面部极为相似，尤其是中日韩三国，为了能更好地将他们进行分类研究，本文提出将中心损失函数 [<xref ref-type="bibr" rid="hanspub.36176-ref10">10</xref>] 与交叉熵损失函数相结合的方法来改进实验精度。中心损失函数的作用是在分类识别中，减小类内的差异，也就是说使同一类的样本相似性更大，让同一类能尽可能向样本中心靠拢，而常用的交叉熵损失函数是为了增大各类别间的距离，减少类间的交叉问题。两者结合可以更好的区分开这五国人员。在两者结合后，整个网络的损失函数如公式(1)所示。</p><p>L = L s + λ L c = − ∑ i = 1 m log e ω y i T x i + b y i ∑ j − 1 n e ω y i T x i + b y i + λ 2 ∑ i = 1 m ‖ x i − c y i ‖ 2 2 (1)</p><p>其中， L S 、 L C 分别代表交叉熵损失函数和中心损失函数，λ表示两个损失函数之间的比重。 x i 表示在全连接层前的特征， C y i 代表第 y i 个类的特征中心，m代表的是每个batch的样本数量，n是类别数量，本文中的n是5。</p></sec><sec id="s7"><title>5. 基于MobileNet网络实验分析</title><sec id="s7_1"><title>5.1. 评价标准</title><p>本文实验中通过采用网络爬虫自建的五国人脸数据集，分别是中国、日本、韩国、泰国、印度五国。实验共用68,495张样本数据，各国分别13,699张，训练测试按照7:3比例随机抽取，即训练共47,945张，测试20,550张。在下面三个网络模型中，模型迭代次数均设置为1000次。本文中所有实验均在64位Windows 7的环境，内存为16 GB，CUDA 10.1，Python 3.6配置mxnet和python环境搭载GPU的主机上进行编程调试。</p><p>在MobileNetV1网络结构中，为了验证宽度因子α对实验结果的影响，分别选取了α = 0.75 (MobileNetV1_075)、α = 1 (MobileNetV1_100)两个模型进行实验，然后又对比了相同参数下，MobileNetV1_100和MobileNetV2_100两个模型，验证MobileNetV2网络相较于MobileNetV1网络在本实验中效果是否得到提升。在本文整个实验评价指标均采用Accuracy和Error top 1，其中Accuracy是针对训练集进行评价，Error top 1是针对测试集进行评价的。其公式如下：</p><p>A c c u r a c y = 正 确 预 测 样 本 数 总 样 本 数 (2)</p><p>E r r o r − t o p 1 = 正 确 标 记 与 输 出 不 同 的 样 本 数 总 样 本 数 (3)</p></sec><sec id="s7_2"><title>5.2. 实验结果分析</title><p>本文基于MobileNetV1和MobileNetV2两个网络上进行学习，首先是将五国人脸数据集应用在原网络上进行分类识别，其次是在两个网络模型上插入八度卷积后进行分类学习，最后是在此基础上改进损失函数的实验结果。图4是在MobileNetV1_075模型上的实验结果，图5是在MobileNetV1_100模型上的实验结果，图6是在MobileNetV2_100模型上的实验结果。</p><p>图4. (a) 基于MobilenetV1_075训练结果图；(b) 基于MobilenetV1_075测试结果图</p><p>由MobilenetV1_075两个实验结果图可以看出，在加入八度卷积后，其训练精度得到了一定的提升，而后在加入八度卷积基础上改进其损失函数，我们可以看到不管是在训练集上的Accuracy还是测试集上的Error top 1的得到了显著改善。</p><p>图5. (a) 基于MobilenetV1_100训练结果图；(b) 基于MobilenetV1_100测试结果图</p><p>由MobilenetV1_100两个实验结果图可以看出，在加入八度卷积后，其训练精度曲线与原模型基本保持吻合，只得到了较不明显的提升，而后在加入八度卷积基础上改进其损失函数，我们可以看到不管是在训练集上的Accuracy还是测试集上的Error top 1的得到了显著改善。</p><p>图6. (a) 基于MobilenetV2_100训练结果图；(b) 基于MobilenetV2_100测试结果图</p><p>由MobilenetV2_100两个实验结果图可以看出，在加入八度卷积后，其训练精度得到了较为显著的提升，而后在加入八度卷积基础上改进其损失函数，我们可以看到不管是在训练集上的Accuracy还是测试集上的Error top 1的得到了显著改善。具体实验中我们还要考虑训练时间等其他因素来判别三个网络模型的优异性。其实验结果统计如表1所示。表中p1、p2、p3分别代表上述三个实验，即p1是原网络上得到的结果，p2是加入八度卷积后的结果，p3是在p2实验基础上加入中心损失函数与交叉熵损失函数之后得到的实验结果。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Based on the results of facial classification experiments in the five countrie</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >网络模型</th><th align="center" valign="middle" >方法</th><th align="center" valign="middle" >Accuracy</th><th align="center" valign="middle" >Error-top 1</th><th align="center" valign="middle" >训练时间</th></tr></thead><tr><td align="center" valign="middle"  rowspan="3"  >MobileNetV1_075</td><td align="center" valign="middle" >p1</td><td align="center" valign="middle" >80.04%</td><td align="center" valign="middle" >1.051%</td><td align="center" valign="middle" >185,241 s</td></tr><tr><td align="center" valign="middle" >p2</td><td align="center" valign="middle" >81.96%</td><td align="center" valign="middle" >0.715%</td><td align="center" valign="middle" >184,491 s</td></tr><tr><td align="center" valign="middle" >p3</td><td align="center" valign="middle" >86.82%</td><td align="center" valign="middle" >0.185%</td><td align="center" valign="middle" >215,000 s</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >MobileNetV1_100</td><td align="center" valign="middle" >p1</td><td align="center" valign="middle" >83.21%</td><td align="center" valign="middle" >0.671%</td><td align="center" valign="middle" >208,791 s</td></tr><tr><td align="center" valign="middle" >p2</td><td align="center" valign="middle" >83.98%</td><td align="center" valign="middle" >0.613%</td><td align="center" valign="middle" >205,794 s</td></tr><tr><td align="center" valign="middle" >p3</td><td align="center" valign="middle" >87.84%</td><td align="center" valign="middle" >0.120%</td><td align="center" valign="middle" >210,789 s</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >MobileNetV2_100</td><td align="center" valign="middle" >p1</td><td align="center" valign="middle" >81.98%</td><td align="center" valign="middle" >0.725%</td><td align="center" valign="middle" >319,546 s</td></tr><tr><td align="center" valign="middle" >p2</td><td align="center" valign="middle" >83.14%</td><td align="center" valign="middle" >0.694%</td><td align="center" valign="middle" >320,456 s</td></tr><tr><td align="center" valign="middle" >p3</td><td align="center" valign="middle" >85.57%</td><td align="center" valign="middle" >0.212%</td><td align="center" valign="middle" >322,600 s</td></tr></tbody></table></table-wrap><p>表1. 基于五国人脸分类实验结果</p><p>由上述实验结果图和实验总结表可知，在原始模型上加入八度卷积后，MobileNetV1_075、MobileNetV1_100、MobileNetV2_100的三个模型上Accuracy分别提升了1.92%、0.77%、1.16%。Error-top 1分别下降了0.336%、0.058%、0.031%。在此基础上加入中心损失函数后，三个网络模型的精度均达到了最高，分别是86.82%、87.84%、85.57%，相较原网络分别提升了6.78%、4.63%、3.59%。Error-top 1分别下降了0.866%、0.551%、0.513%，均在该方法上达到了最低。对比MobileNetV1_075和MobileNetV1_100两组实验结果，我们可以看出在不改变其他参数仅改变宽度因子α时，其实验结果得到了提升，证明了宽度乘子对实验结果影响的有效性。对比MobileNetV1_100和MobileNetV2_100实验结果可以看出，在本实验中MobileNetV2网络效果低于MobileNetV1网络，且训练时间是MobileNetV1网络上的1.5倍。</p></sec></sec><sec id="s8"><title>6. 总结与展望</title><p>本文提出将八度卷积应用到五国人脸(中国、日本、韩国、泰国、印度)分类识别中，有效减少了冗余提升了精度，且在使用了中心损失函数和交叉熵损失函数相结合后，其精度达到了最高，整个实验结果中，精度最高可达到87.84%，其Error-top 1达到了0.120%，实验结果验证了本文提出改进方案的可行性，能有效提升准确率。本文的实验也说明了MobileNet网络中超参数α不同时实验效果的差异性，有效区分各国人脸信息在未来是一个重要的研究领域，为构建和谐安全幸福的良好社会环境有着不可替代的作用。在后续的学习研究中时，主要着力于解决以下问题：</p><p>1) 人脸数据样本对于区分各国人员起着至关重要的一步，因此如何能构建更好的各国人脸数据集应更一步的研究。且本文所构建的人脸数据集仅限于这五国的明星，并没有很好地包含各个国家不同年龄等因素，这需要在后续研究中不断改进。</p><p>2) 由于本文选取的网络模型是“轻便型”网络模型，因此可以针对此构建一个可适用于移动端口等的各国人员分类识别系统。</p></sec><sec id="s9"><title>基金项目</title><p>国家自然科学基金(61862061，61563052)。</p></sec><sec id="s10"><title>文章引用</title><p>郭奕君,阿里木江&#183;阿布迪日依木,努尔毕亚&#183;亚地卡尔,朱亚俐,库尔班&#183;吾布力. 基于MobileNet网络多国人脸分类识别Multi-National Face Classification and Recognition Based on MobileNet Network[J]. 图像与信号处理, 2020, 09(03): 146-155. https://doi.org/10.12677/JISP.2020.93018</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.36176-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Du, H.B., Salah, S.H. and Ahmed, H.O. (2014) A Color and Texture Based Multi-Level Fusion Scheme for Ethnicity Identification. Proceedings of SPIE—The International Society for Optical Engineering, Baltimore, 22 May 2014, 91200B. https://doi.org/10.1117/12.2057722</mixed-citation></ref><ref id="hanspub.36176-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Rehman, A., Khan, G., Siddiqi, A., et al. (2018) Modified Texture Features from Histogram and Gray Level Co-Occurence Matrix of Facial Data for Ethnicity Detection. 5th International Multi-Topic ICT Conference (IMTIC), Jamshoro, 25-27 April 2018, 1-6. https://doi.org/10.1109/IMTIC.2018.8467231</mixed-citation></ref><ref id="hanspub.36176-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">黄慧. 基于PCA与2DPCA的少数民族人脸识别比较[D]: [硕士学位论文]. 新疆: 伊犁师范学院电子与信息系, 2016.</mixed-citation></ref><ref id="hanspub.36176-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">王雅丽, 马静, 李海青, 等. 基于虹膜纹理深度特征和Fisher向量的人种分类[J]. 中国图象图形学报, 2018, 23(1): 28-38.</mixed-citation></ref><ref id="hanspub.36176-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">邱盛. 基于深度学习的人脸民族识别[D]: [硕士学位论文]. 广州: 华南理工大学计算机工程与科学系, 2016.</mixed-citation></ref><ref id="hanspub.36176-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., et al. (2017) Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 1-9.</mixed-citation></ref><ref id="hanspub.36176-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Sandler, M., Howard, A.G., Zhu, M., Chen, L.C., et al. (2018) MobileNetV2: Inverted Residuals and Linear Bottlenecks. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Salt Lake City, 18-23 June 2018, 4510-4520. https://doi.org/10.1109/CVPR.2018.00474</mixed-citation></ref><ref id="hanspub.36176-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Howard, A.G, Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., et al. (2019) Searching for MobilenetV3. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seoul, 27 October-2 November 2019, 1314-1324. https://doi.org/10.1109/ICCV.2019.00140</mixed-citation></ref><ref id="hanspub.36176-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Chen, Y.P., Fang, H.Q., Xu, B., et al. (2019). Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, 27 October-2 November 2019, 3434-3443. https://doi.org/10.1109/ICCV.2019.00353</mixed-citation></ref><ref id="hanspub.36176-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">魏哲, 王小华. 淋巴结转移检测的八度卷积方法[J]. 计算机应用, 2020, 40(3): 723-727.</mixed-citation></ref></ref-list></back></article>