<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2020.91004</article-id><article-id pub-id-type="publisher-id">AAM-33778</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20200100000_17752479.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于深度学习的小样本声纹识别研究
  Research on Small Sample Voiceprint Recognition Based on Deep Learning
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>韩</surname><given-names>侣</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>周</surname><given-names>林华</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>马</surname><given-names>文联</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郑</surname><given-names>伟杰</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>马</surname><given-names>涛</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>天星</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>长春理工大学理学院应用数学系，吉林 长春</addr-line></aff><aff id="aff3"><addr-line>数学实验省级教学示范中心(长春理工大学)，吉林 长春</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>30</day><month>12</month><year>2019</year></pub-date><volume>09</volume><issue>01</issue><fpage>30</fpage><lpage>37</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    本文研究了小样本声纹识别问题。实验中采用梅尔倒谱系数与其动态差分系数组成的39维特征作为基本声学特征，再将基本声学特征通过由三层受限玻尔兹曼机堆叠而成的深度置信网络提取128维深度声学特征，最后通过支持向量机和随机森林进行分声纹识别。训练深度置信网络时，每个说话人选用短时语音信号组成的小样本数据作为该网络的训练集，同时将训练好的深度置信网络模型作为深度声学特征提取器，用该特征提取器对非训练集中说话人语音信号提取深度声学特征，进一步验证了该深度声学特征提取器的泛化能力。实验结果表明，本文设计的声纹识别模型识别准确率高，且深度特征提取器有较好的泛化能力。
    This paper studies the problem of small sample voiceprint recognition. In the experiment, the 39-dimensional features composed of the Mel cepstral coefficient and its dynamic differential coefficients are used as the basic acoustic features, and the basic acoustic features are extracted from the 128-dimensional depth acoustics through a deep belief network stacked by a three-layer restricted Boltzmann machine. Finally through the support vector machine and random forest for voiceprint recognition. Training deep belief networks, each speaker to choose short speech signal of small sample data as the network training set, trained deep belief network model at the same time as the depth of the acoustic feature extraction, with the characteristics of the extractor on the depth of the training focus on the speaker voice signal extraction acoustic characteristics, the generalization ability of the depth acoustic feature extractor is further verified. The experimental results show that the soundprint recognition model designed in this paper has high recognition accuracy, and the depth feature extractor has better generalization ability. 
  
 
</p></abstract><kwd-group><kwd>声纹识别，深度置信网络，支持向量机，随机森林, Voiceprint Recognition</kwd><kwd> Deep Belief Networks</kwd><kwd> Support Vector Machine</kwd><kwd> Random Forest</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于深度学习的小样本声纹识别研究<sup> </sup></title><p>韩侣<sup>1</sup>，周林华<sup>2</sup>，马文联<sup>1*</sup>，郑伟杰<sup>2</sup>，马涛<sup>2</sup>，李天星<sup>2</sup></p><p><sup>1</sup>长春理工大学理学院应用数学系，吉林 长春</p><p><sup>2</sup>数学实验省级教学示范中心(长春理工大学)，吉林 长春</p><p>收稿日期：2019年12月12日；录用日期：2019年12月28日；发布日期：2020年1月3日</p><disp-formula id="hanspub.33778-formula44"><graphic xlink:href="//html.hanspub.org/file/4-2621100x6_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>本文研究了小样本声纹识别问题。实验中采用梅尔倒谱系数与其动态差分系数组成的39维特征作为基本声学特征，再将基本声学特征通过由三层受限玻尔兹曼机堆叠而成的深度置信网络提取128维深度声学特征，最后通过支持向量机和随机森林进行分声纹识别。训练深度置信网络时，每个说话人选用短时语音信号组成的小样本数据作为该网络的训练集，同时将训练好的深度置信网络模型作为深度声学特征提取器，用该特征提取器对非训练集中说话人语音信号提取深度声学特征，进一步验证了该深度声学特征提取器的泛化能力。实验结果表明，本文设计的声纹识别模型识别准确率高，且深度特征提取器有较好的泛化能力。</p><p>关键词 :声纹识别，深度置信网络，支持向量机，随机森林</p><disp-formula id="hanspub.33778-formula45"><graphic xlink:href="//html.hanspub.org/file/4-2621100x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/4-2621100x8_hanspub.png" /> <img src="//html.hanspub.org/file/4-2621100x9_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>声纹识别(Voiceprint Recognition, VPR)是通过分析每个说话人声纹特征之间的差异来达到对未知语音进行识别的目的，因此声纹识别又称为说话人识别。声纹识别研究的重点是声学特征提取和声学特征建模，在声学特征提取方面常用特征有线性预测倒谱系数 [<xref ref-type="bibr" rid="hanspub.33778-ref1">1</xref>] (Linear Predictive Cepstrum Coefficient, LPCC)、感知线性预测系数 [<xref ref-type="bibr" rid="hanspub.33778-ref2">2</xref>] (Perceptual Linear Predictive, PLP)以及梅尔倒谱系数 [<xref ref-type="bibr" rid="hanspub.33778-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.33778-ref4">4</xref>] (Mel-frequency Cepstrum Coefficient, MFCC)。在声学特征建模上相继采用了矢量量化法 [<xref ref-type="bibr" rid="hanspub.33778-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.33778-ref6">6</xref>] (Vector Quantization, VQ)、隐马尔科夫模型 [<xref ref-type="bibr" rid="hanspub.33778-ref7">7</xref>] (Hidden Markov Model, HMM)、人工神经网络 [<xref ref-type="bibr" rid="hanspub.33778-ref8">8</xref>] (Artificial Neural Network, ANN)以及高斯混合模型–通用背景模型 [<xref ref-type="bibr" rid="hanspub.33778-ref9">9</xref>] (Gaussian mixture model-universal background model, GMM-UBM)等技术。</p><p>在深度学习用于声纹识别之前，GMM-UBM是声纹识别广泛应用的技术之一，深度学习技术既可以作为一个深度声学特征提取器，同时也可以作为分类器 [<xref ref-type="bibr" rid="hanspub.33778-ref10">10</xref>] [<xref ref-type="bibr" rid="hanspub.33778-ref11">11</xref>]，如田垚等人采用深度信念网络在基本声学特征(MFCC)的基础上进行瓶颈特征提取并结合高低混合模型等模型进行识别 [<xref ref-type="bibr" rid="hanspub.33778-ref12">12</xref>]，并通过实验证明了较传统的声学特征而言，该方法具有一定的优势，闫河等人将信号频谱图通过卷积神经网络进行声纹识别 [<xref ref-type="bibr" rid="hanspub.33778-ref13">13</xref>]，约翰斯霍普金斯大学的Povey提出基于DNN的x-vector说话人确认系统，该系统将语音特征提取过程分为帧级(frame-level)和段级(segment-level)，并使用统计池化层连接两级特征 [<xref ref-type="bibr" rid="hanspub.33778-ref14">14</xref>]。虽然深度学习在特征提取是信号匹配上都有较好的表现，但模型的训练过程中需要大量的训练数据，而且在实际应用中不可能每增加新的说话人就从新训练一个深度网络模型，非训练集中的说话人通过这个深度网络所提取特征的表征能力也需进一步实验，即模型的泛化能力也需进一步验证。</p><p>对于上述问题，本文针对短时语音信号，采用小样本进行深度模型训练，并针对非训练集中的说话人的语音信号，也通过这个深度网络模型进行深度声学特征提取，最后通过支持向量机和随机森林进行分类验证，实验结果表明，在本文构建的深度网络结构提取的深度声学特征能在有监督分类器上得到较好的识别率。</p></sec><sec id="s4"><title>2. 深度学习方法概述</title><p>在声纹识别中常用的深度学习方法有多层感知器、卷积神经网络以及深度置信网络，其深度体现在隐含层的层数上，深度神经网络使用更多的隐含层的目的是希望更深层的学习后能对数据有更强的表征能力。深度信念网络DBNs是深度神经网络DNN的一种，在语音识别领域取得很大的成功，该网络体现了无监督学习在各层训练的有效性，指出各层可在前一层训练结果输出的基础上，再次进行无监督训练，在预训练的基础上模型获得了较好的初始参数，最后在通过反向传播算法对模型的参数进行反向调优。</p><sec id="s4_1"><title>2.1. 基于深度置信网络的参数预训练</title><p>受限玻尔兹曼机(restricted Boltzmann machines, RBM)是一类具有两层结构、对称连接且无反馈的神经网络模型，层间全连接，层内无连接。假设一个RBM有n个可见单元和m个隐单元，用向量v和h表示可见单元和隐单元的状态，其中 v i 表示第i个可见单元的状态， h j 表示第j个可见单元状态，那么，对于一组给定的状态 ( v , h ) ，RBM作为一个系统所必备的能量定义为 [<xref ref-type="bibr" rid="hanspub.33778-ref15">15</xref>] ：</p><p>E ( v , h | θ ) = − ∑ i = 1 n α i v i − ∑ j = 1 m b j h j − ∑ i = 1 n ∑ j = 1 m v i W i j h j 。</p><p>式中： W i j 为权重，表示连接了可见单元i与隐单元j， α i 与 b j 分别表示可见单元i与隐单元j的偏置，当参数确定时，基于能量函数，可以得到 ( v , h ) 的联合概率分布</p><p>P ( v , h | θ ) = e − E ( v , h | θ ) Z ( θ ) ,   Z ( θ ) = ∑ v , h e − E ( v , h | θ ) 。</p><p>式中 Z ( θ ) 为归一化因子，由RBM所定义的关于观测数据v的分布，即联合概率分布的边界分布，也称似然函数(likelihood function)，RBM的结构为层间连接，层内无连接，当给定可见单元的状态时，各隐单元与可见单元之间的激活状态是条件独立的，此时，可见单元与隐单元的激活概率为：</p><p>p ( h j = 1 | v , θ ) = σ ( b j + ∑ j v i W i j ) 。</p><p>p ( v i = 1 | h , θ ) = σ ( a i + ∑ i W i j h j ) 。</p><p>式中 σ ( x ) = 1 1 + exp ( − x ) 为sigmoid激活函数。</p><p>RBM的优化目标是要最大化可见层节点概率分布，在训练过程中可以通过对比散度算法(contrastive divergence, CD)来得到模型中的参数。对比散度算法的输入是一个训练样本 x 0 ，设隐层单元个数m，学习率 є ，最大训练周期T，输出是连接权重矩阵W，可见层偏置a，隐藏层偏置向量b，算法描述如下：</p><p>令可见层单元的初始状态 v 1 = x 0 ， W , a , b 为0~1之间随机数。</p><p>For t = 1 , 2 , ⋯ , T ；</p><p>For j = 1 , 2 , ⋯ , m (对所有隐单元)；</p><p>计算 P ( h 1 j = 1 | v 1 ) ，即 P ( h 1 j = 1 | v 1 ) = σ ( b i + ∑ i v 1 i W i j ) ；</p><p>从条件分布 P ( h 1 j | v 1 ) 中抽取 h 1 j ∈ { 0 , 1 } 。</p><p>End For</p><p>For i = 1 , 2 , ⋯ , n (对所有可见单元)；</p><p>计算 P ( v 2 i = 1 | h 1 ) ，即 P ( v 2 i = 1 | h 1 ) = σ ( a i + ∑ j W i j h 1 j ) ；</p><p>从条件分布 P ( v 2 i | h 1 ) 中抽取 v 2 i ∈ { 0 , 1 } 。</p><p>End For</p><p>For j = 1 , 2 , ⋯ , m (对所有隐单元)；</p><p>计算 P ( h 2 j = 1 | v 2 ) ，即 P ( h 2 j = 1 | v 2 ) = σ ( b i + ∑ i v 2 i W i j ) 。</p><p>End For</p><p>最后参数的更新计算为：</p><p>W ← W + є ( P ( h 1 = 1 | v 1 ) v 1 T − P ( h 2 = 1 | v 2 ) v 2 T ) a ← a + є ( v 1 − v 2 ) b ← b + є ( P ( h 1 = 1 | v 1 ) − P ( h 2 = 1 | v 2 ) ) 。</p></sec><sec id="s4_2"><title>2.2. 基于反向传播算法的参数调优</title><p>当DBN完成预训练之后，需要经过基于反向传播算法的参数调优。首先将DBN各层的网络参数做为DNN的参数的初值，最后加上一层softmax函数层得到完整的DNN结构。Softmax层的每个输出节点对应一个类别的概率，从而实现分类的目的。</p><p>假设最终的DBN结构有N个RBM构成，则这个结构共有 N + 1 层，该网络结构中第1层表示输入层，第 N + 1 层表示输出层，隐藏层k的取值为 k = 2 , 3 , ⋯ , N 。设 ω k 和 b k 表示隐藏层的权重和偏置， v k + 1 为上一层输入的加权，激活函数 σ 通常可以选取sigmoid、tanh和ReLu等函数。其隐藏层节点的输出可以表示为：</p><p>v k + 1 = ω k h k + b k h k = σ ( v k ) 。</p><p>对于输出层通常采用softmax函数：</p><p>p s = exp ( v s N + 1 ) ∑ j exp ( v s N + 1 ) 。</p><p>其中：j为输出类别索引， p s 表示第 p s 类输出类别的概率分值。在使用BP算法反向传播的过程中，通常使用交叉熵作为损失函数，通过最小化代价函数来修正DBN结构中的参数， d s 是一个维的向量，s代表正确的类别值，当s为训练数据所属类别时 d s = 1 ，当s为其它类别时 d s = 0 ，交叉熵函数为：</p><p>L = − ∑ s d s log p s 。</p></sec></sec><sec id="s5"><title>3. 有监督分类器</title><sec id="s5_1"><title>3.1. 支持向量机</title><p>支持向量机(Support Vector Machine, SVM)是在统计学习理论和结构风险最小原理基础上发展起来的一种学习方法，其机理可以简单地描述为：寻找一个满足分类要求的分割超平面，使训练集中的点距离该分割超平面尽可能地远，即寻找一个最优分割超平面，使其两侧的空白区域最大。SVM在解决小样本、非线性和高维模式识别问题中表现出特有的优势，并在很大程度上克服了“维数灾难”和“过学习”等问题，并有泛化能力强等优点。</p><p>对于给定样本集 { ( x 1 , y 1 ) , ( x 2 , y 2 ) , ⋯ , ( x n , y n ) } ，其中 x i ∈ R d ， y i ∈ { &#177; 1 } ，支持向量机的目标是构造出</p><p>一个超平面 ω ⋅ x + b = 0 将两类不同样本分割，使得两类间隔最大。相应的分类决策函数为 [<xref ref-type="bibr" rid="hanspub.33778-ref16">16</xref>] ：</p><p>f ( x ) = ω x + b 。</p><p>若 f ( x ) &gt; 0 ，则待测样本分为一类；若 f ( x ) &lt; 0 ，待测样本分为另一类。</p><p>假设给定的样本是线性可分的，则求最优分类面即求解二次规划问题：</p><p>Minimize Φ ( ω , b ) = 1 2 ‖ ω ‖ 2 + C ∑ i = 1 n ξ i s .t . y i ( ω ⋅ x + b ) − 1 + ξ i ≥ 0 , i = 1 , 2 , ⋯ , n 。</p><p>使用Lagrange优化算法可将上述最优分类面问题转化为对偶问题，并得到最优分类决策函数：</p><p>f ( x ) = sgn { ( w ⋅ x ) + b } = sgn { ∑ i = 1 m α i ∗ y i ( x i ⋅ x ) + b ∗ } 。</p><p>其中， α ∗ = ( α 1 ∗ , α 2 ∗ , ⋅ ⋅ ⋅ , α n ∗ ) 为拉格朗日乘子， b ∗ = 1 2 [ ( w ∗ ⋅ x ( 1 ) ) + ( w ∗ ⋅ x ( − 1 ) ) ] 是分类阈值， x ( 1 ) 表示两类</p><p>中属于第一类的任意支持向量， x ( − 1 ) 表示属于第二类的任意支持向量， w ∗ 为权值。</p><p>对于非线性问题，通常是通过核函数 K ( x , x ′ ) 将原始数据映射到高维的特征空间中，转化为高维空间中的线性问题，在变换后的空间中求最优分类超平面，即分类函数变为：</p><p>f ( x ) = sign ( ∑ i = 1 n α i * y i K ( x i ⋅ x ) + y j − ∑ i = 1 n α i * y i K ( x i ⋅ x j ) ) 。</p><p>常用的核函数主要有线性核函数、多项式核函数以及高斯核函等。</p></sec><sec id="s5_2"><title>3.2. 随机森林</title><p>随机森林是一个包含多个决策树的分类器，是一种重要的基于Bagging的集成学习方法，可以用作分类和回归。由于采用有放回的采样来构造不同数据集训练模型，模型的泛化能力通常比单一模型强。随机森林训练多个决策树弱分类器，然后组合这些弱分类器形成一个强分类器，通过投票的方式来得到最终的分类结果。和普通的决策树不同，随机森林在训练决策树时并不是选择一个全局最优的特征来分裂节点，而是先随机选择部分样本特征，然后再到里面选择一个局部最优的特征来为决策树划分左右子树，进一步提升了模型的泛化能力。</p><p>传统决策树在选择最优属性时是对所有可用特征进行选择，而在随机森林(Random Forest, RF)，对每一个基决策树，先从所有特征中随机抽样出一个包含k个特征的子集，然后基于该子集进行属性划分 [<xref ref-type="bibr" rid="hanspub.33778-ref17">17</xref>]。具体算法流程如下：</p><p>输入：训练数据集D，弱学习器算法G，弱学习器迭代次数T；</p><p>输出：强学习器 H ( x ) 。</p><p>Step 1：对于 t = 1 , 2 , ⋯ , T ：</p><p>1) 对训练数据集D采用自助法采样，得到包含m个样本的采样集 D t 。</p><p>2) 使用采样集 D t 训练第t个决策树模型 G t ( x ) ，从所有特征中随机抽样出一个包含k个特征的子集，使用CART算法构建决策树。</p><p>Step 2：若为分类问题，则使用T个模型的类别标签进行投票预测；若为回归问题，则用简单平均法进行预测。</p><p>随机森林算法结构简单、易于实现且计算开销较小，在很多现实任务中展现出强大的性能。相比于Bagging，随机森林不仅继承了通过样本扰动带来的样本多样性，还引入属性扰动进一步提升了模型的泛化能力。尽管随机森林中个体学习器的性能往往有所下降，但随着个体学习器数量的增多，模型的整体性能会获得更大的提升。同时，随机森林的训练效率往往优于Bagging，因为随机森林的个体学习器在进行属性划分时需要计算的特征个数更少。</p></sec></sec><sec id="s6"><title>4. 实验设置与结果分析</title><sec id="s6_1"><title>4.1. 声学特征提取</title><sec id="s6_1_1"><title>4.1.1. 实验数据设置及MFCC提取</title><p>本文采用AISHELL-ASR0009-OS1语音数据库，音频降采样为16 kHz数据库中包含400名来自中国不同口音区域的发言人。在实验设置中，根据不同的实验条件将划分不同的训练集与测试集。</p><p>梅尔到谱系数的提取包含预滤波、预加重、分帧、加窗、快速傅立叶变换、三角窗滤波、求对数、离散余弦变换、倒谱均值减、差分等步骤，在本实验中，设置帧长为25 ms、帧移为10 ms，滤波器组的滤波器数量为26，预加重过滤器的系数为0.97，最后得到的倒频谱数量为13，即13维MFCC，最后将13维MFCC特征与其一阶差分与二阶差分组合，得到39维的基本声学特征。若假设某人某句话共有n</p><p>帧，每帧提取的MFCC记为 X k ( k = 1 , 2 , ⋯ , n ) ，则这句话的MFCC重新计算为： 1 n ∑ k = 1 n x k 。</p></sec><sec id="s6_1_2"><title>4.1.2. 深度声学特征d-vector</title><p>深度置信网络由RBM堆叠组成，每个RBM的权值利用吉布斯采样进行估计，本实验的DBN采用三层RBM (网络节点为：39-128-128-128)堆叠，如下图1所示，最终以最后一层128个节点的输出值作为由39维MFCC经过DBN进行特征再提取得到深度声学特征。</p></sec></sec><sec id="s6_2"><title>4.2. 基于深度学习的声纹识别结果</title><p>本文实验数据来自于AISHELL-ASR0009-OS1语音数据库，从中选择一定的数据并设置了两个数据集，分别为每人5条语音信号和每人10条语音数据作为深度神经网络的训练集，每个数据集中共200人，具体设置如下表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Experimental data set setting</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >实验人数</th><th align="center" valign="middle" >训练集语料数</th><th align="center" valign="middle" >测试集语料数</th></tr></thead><tr><td align="center" valign="middle" >数据集1</td><td align="center" valign="middle" >200</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >50</td></tr><tr><td align="center" valign="middle" >数据集2</td><td align="center" valign="middle" >200</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >50</td></tr></tbody></table></table-wrap><p>表1. 实验数据集设置</p><p>图1. DBN结构</p><p>在本实验中首先采用支持向量机和随机森林模型对基本声学特征MFCC进行识别，然后在通过以上模型对深度声学特征d-vector (128 dim)进行对比。</p><p>对以上两种声纹特征识别的模型中，模型采用相同的参数，如在随机森林和决策树中，树的深度均采用15，支持向量机中的核函数均采用线性核函数，得到的模型的准确率，召回率如下表2与表3所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Recognition rate of MFCC (39 dim) in the mode</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >数据集1准确率</th><th align="center" valign="middle" >数据集2准确率</th></tr></thead><tr><td align="center" valign="middle" >随机森林</td><td align="center" valign="middle" >46.78%</td><td align="center" valign="middle" >60.08%</td></tr><tr><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >68%</td><td align="center" valign="middle" >71.22%</td></tr></tbody></table></table-wrap><p>表2. MFCC (39 dim)在模型中的识别率</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Recognition rate of d-vector (128 dim) in the mode</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >模型</th><th align="center" valign="middle" >数据集1准确率</th><th align="center" valign="middle" >数据集2准确率</th></tr></thead><tr><td align="center" valign="middle" >随机森林</td><td align="center" valign="middle" >75.21%</td><td align="center" valign="middle" >90.19%</td></tr><tr><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >86.8%</td><td align="center" valign="middle" >95.6%</td></tr></tbody></table></table-wrap><p>表3. d-vector (128 dim)在模型中的识别率</p></sec><sec id="s6_3"><title>4.3. 深度神经网络模型泛化验证</title><p>在本文所设置的实验中，主要分为基本声学特征MFCC提取、深度声学特征d-vector提取和有监督分类器的训练三个步骤，其中在深度特征提取这一阶段主要是通过训练一个深度神经网络，最终选用该网络中某一层的节点作为深度声学特征。</p><p>在模型的训练时分为深度神经网络训练和分类器的模型训练，深度特征的提取主要在于将低维基本声学特征变为高维的深度特征，此深度神经网络是采用了深度信念网络结构，该网络的特点是先进行无监督进行预训练，再进行有监督的微调，在这个过程中将语音信号中包括信道信息、语义信息、情绪以及说话人身份信息进行放大，在微调过程中是通过身份信息进行反向传播，这使得说话人身份信息相较于其它信息得以放大，因此深度特征在支持向量机等模型上进行分类时表现更好。</p><p>为了验证深度特征提取器的泛化能力，选取一定的语料设置如下的训练集与测试集，训练集中人数设置为150人，每人10条语料参与深度网络训练，其中网络结构以及分类器参数设置均与前实验相同，参与训练与未参与训练测试集每人均50条语料，实验结果如下表4所示。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Model generalization capability verification result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >语料集</th><th align="center" valign="middle" >人数</th><th align="center" valign="middle" >训练集每人语料数</th><th align="center" valign="middle" >测试每人集语料数</th><th align="center" valign="middle" >SVM Accuracy</th><th align="center" valign="middle" >DF Accuracy</th></tr></thead><tr><td align="center" valign="middle" >参与训练的话者</td><td align="center" valign="middle" >150</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >95.92%</td><td align="center" valign="middle" >92.01%</td></tr><tr><td align="center" valign="middle" >未参与训练话者</td><td align="center" valign="middle" >50</td><td align="center" valign="middle" ></td><td align="center" valign="middle" >50</td><td align="center" valign="middle" >91.5%</td><td align="center" valign="middle" >90.2%</td></tr></tbody></table></table-wrap><p>表4. 模型泛化能力验证结果</p><p>从表4可知，在参与深度信念网络训练的数据集上，支持向量机与随机森林的准确率分别为95.92%和92.01%，而在未参与训练的数据集上支持向量机与随机森林的准确率分别为91.5%和90.2%。从实验结果可知，虽然这50人没有参与神经网络模型的训练，但是通过该深度声学特征提取器提取的深度声学特征在有监督分类其上任有较好的表征能力，故深度声学特征提取器拥有较好的泛化能力，因此实际应用中，在一个集合中增加说话人的情况下，运用该网络结构不需从新训练深度网络模型。</p></sec></sec><sec id="s7"><title>5. 结论</title><p>本文运用了深度学习的方法构建了一个基于声纹识别的深度声学特征特征提取器，并将该特征结合支持向量机和随机森林等分类器进行声纹识别研究。实验中对比了基本声学特征MFCC与高维深度声学特征d-vector在支持向量机和树模型上的识别结果，实验结果表明支持向量机在两种特征上的分类性能更好，同时也对深度声学特征提取器的泛化能力进行了验证，并取得了较好的分类结果</p></sec><sec id="s8"><title>文章引用</title><p>韩 侣,周林华,马文联,郑伟杰,马 涛,李天星. 基于深度学习的小样本声纹识别研究Research on Small Sample Voiceprint Recognition Based on Deep Learning[J]. 应用数学进展, 2020, 09(01): 30-37. https://doi.org/10.12677/AAM.2020.91004</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.33778-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Atal, B.S. (1976) Automatic Recognition of Speakers from Their Voices. Proceedings of the IEEE, 64, 460-475.  
https://doi.org/10.1109/PROC.1976.10155</mixed-citation></ref><ref id="hanspub.33778-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Hermansky, H. (1990) Perceptual Linear Predictive (PLP) Analysis of Speech. The Journal of the Acoustical Society of America, 87, 1738-1752. https://doi.org/10.1121/1.399423</mixed-citation></ref><ref id="hanspub.33778-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Davis, S.B. (1980) Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences. IEEE Transactions on Acoustics, Speech, and Signal Processing, 28, 65-74.  
https://doi.org/10.1109/TASSP.1980.1163420</mixed-citation></ref><ref id="hanspub.33778-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Furui, S. (1981) Cepstral Analysis Technique for Automatic Speaker Verification. IEEE Transactions on Acoustics Speech and Signal Processing, 29, 254-272. https://doi.org/10.1109/TASSP.1981.1163530</mixed-citation></ref><ref id="hanspub.33778-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Burton, D.K. (1987) Text-Dependent Speaker Verification Using Vector Quantization Source Coding. IEEE Transactions on Acoustics Speech and Signal Processing, 35, 133-143. https://doi.org/10.1109/TASSP.1987.1165110</mixed-citation></ref><ref id="hanspub.33778-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Soong, F.K. and Rosenberg, A.E. (1988) On the Use of Instantaneous and Transitional Spectral Information in Speaker Recognition. IEEE Transactions on Acoustics Speech &amp; Signal Processing, 36, 871-879.  
https://doi.org/10.1109/29.1598</mixed-citation></ref><ref id="hanspub.33778-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Naik, J.M., Netsch, L.P. and Doddington, G.R. (1989) Speaker Verification over Long Distance Telephone Lines. International Conference on Acoustics, Speech, and Signal Processing, Glasgow, 23-26 May 1989, 524-527.</mixed-citation></ref><ref id="hanspub.33778-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Yang, Y., Ren, W., Hui, Z., et al. (2012) The Research of Voiceprint Recognition Based on Genetic Optimized RBF Neural Networks. IEEE International Conference on Computer Science &amp; Automation Engineering, Zhangjiajie, 25-27 May 2012, 425-428. https://doi.org/10.1109/CSAE.2012.6272630</mixed-citation></ref><ref id="hanspub.33778-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Abu, M.A., Zakariya, Q., et al. (2018) New Transformed Features Generated by Deep Bottleneck Extractor and a GMM-UBM Classifier for Speaker Age and Gender Classification. Neural Computing &amp; Applications, 30, 2581-2593.  
https://doi.org/10.1007/s00521-017-2848-4</mixed-citation></ref><ref id="hanspub.33778-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Variani, E., Lei, X., McDermott, E., Lopez Moreno, I. and Gonzalez-Dominguez, J. (2014) Deep Neural Networks for Small Footprint Text-Dependent Speaker Verification. IEEE International Conference on Acoustics, Speech and Signal Processing, Florence, 4-9 May 2014, 4052-4056. https://doi.org/10.1109/ICASSP.2014.6854363</mixed-citation></ref><ref id="hanspub.33778-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Hinton, G., Deng, L., Yu, D., et al. (2012) Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. IEEE Signal Processing Magazine, 29, 82-97.  
https://doi.org/10.1109/MSP.2012.2205597</mixed-citation></ref><ref id="hanspub.33778-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">田垚, 蔡猛, 何亮, 刘加. 基于深度神经网络和Bottleneck特征的说话人识别系统[J]. 清华大学学报(自然科学版), 2016, 56(11): 1143-1148.</mixed-citation></ref><ref id="hanspub.33778-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">闫河, 董莺艳, 王鹏, 罗成, 李焕. 基于CNN-LSTM网络的声纹识别研究[J]. 计算机应用与软件, 2019, 36(4): 166-170.</mixed-citation></ref><ref id="hanspub.33778-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Snyder, D., et al. (2017) Deep Neural Network-Based Speaker Embeddings for End-to-End Speaker Verification. Spoken Language Technology Workshop, San Diego, 13-16 December 2016, 165-170.</mixed-citation></ref><ref id="hanspub.33778-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">张春霞, 姬楠楠, 王冠伟. 受限玻尔兹曼机[J]. 工程数学学报, 2015, 32(2): 161-175.</mixed-citation></ref><ref id="hanspub.33778-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Hearst, M.A. (1998) Support Vector Machines. IEEE Intelligent Systems &amp; Their Applications, 13, 18-28.  
https://doi.org/10.1109/5254.708428</mixed-citation></ref><ref id="hanspub.33778-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Ho, T.K. (1995) Random Decision Forests. International Conference on Document Analysis &amp; Recognition, Montreal, 14-16 August 1995, 278-282.</mixed-citation></ref></ref-list></back></article>