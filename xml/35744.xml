<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.105105</article-id><article-id pub-id-type="publisher-id">CSA-35744</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200500000_24564666.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于特征融合的粒子滤波目标跟踪
  Particle Filter Tracking Based on Feature Fusion
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>徐</surname><given-names>国寒</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>沙</surname><given-names>洁</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff2"><addr-line>中南民族大学电子信息工程学院，湖北 武汉</addr-line></aff><pub-date pub-type="epub"><day>29</day><month>04</month><year>2020</year></pub-date><volume>10</volume><issue>05</issue><fpage>1018</fpage><lpage>1025</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   针对复杂场景下单一视觉特征难以准确表征被跟踪目标问题，本文提出了一种自适应融合颜色特征与形状特征的粒子滤波跟踪方法。首先分别提取目标的颜色特征和形状特征，根据当前场景动态调整各特征权重，通过特征加权计算粒子的权值，然后预测目标状态，得到跟踪结果。实验结果表明，该算法能够克服背景噪声、短时间遮挡等干扰，比采用单一特征的跟踪方法具有更好的鲁棒性。 In complex scenes, it is difficult for a single kind of visual feature to accurately represent tracked targets. This paper proposes a particle filter tracking method based on adaptively fusing color feature and shape feature. Firstly, we extract the color and shape features. The weight of each feature is then adjusted according to the dynamics of the current scene. The weighted features are used to calculate the weight for particles, by which we predict the target state and obtain the tracking result. Experiments show that the proposed algorithm can overcome background noise and short-term occlusion, showing more robust than those tracking algorithms using only single kind of visual feature. 
  
 
</p></abstract><kwd-group><kwd>目标跟踪，粒子滤波，颜色特征，形状特征，信息融合, Target Tracking</kwd><kwd> Particle Filter</kwd><kwd> Color Feature</kwd><kwd> Shape Feature</kwd><kwd> Information Fusion</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于特征融合的粒子滤波目标跟踪<sup> </sup></title><p>徐国寒，沙洁</p><p>中南民族大学电子信息工程学院，湖北 武汉</p><p>收稿日期：2020年5月4日；录用日期：2020年5月18日；发布日期：2020年5月25日</p><disp-formula id="hanspub.35744-formula68"><graphic xlink:href="//html.hanspub.org/file/22-1541771x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>针对复杂场景下单一视觉特征难以准确表征被跟踪目标问题，本文提出了一种自适应融合颜色特征与形状特征的粒子滤波跟踪方法。首先分别提取目标的颜色特征和形状特征，根据当前场景动态调整各特征权重，通过特征加权计算粒子的权值，然后预测目标状态，得到跟踪结果。实验结果表明，该算法能够克服背景噪声、短时间遮挡等干扰，比采用单一特征的跟踪方法具有更好的鲁棒性。</p><p>关键词 :目标跟踪，粒子滤波，颜色特征，形状特征，信息融合</p><disp-formula id="hanspub.35744-formula69"><graphic xlink:href="//html.hanspub.org/file/22-1541771x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/22-1541771x7_hanspub.png" /> <img src="//html.hanspub.org/file/22-1541771x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>目标跟踪作为计算机视觉领域的重要研究方向，在智能监控 [<xref ref-type="bibr" rid="hanspub.35744-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.35744-ref2">2</xref>] 和人机交互 [<xref ref-type="bibr" rid="hanspub.35744-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.35744-ref4">4</xref>] 等领域有着广泛的应用。在这些场景中，目标的运动轨迹是非线性的，而且背景中常常存在大量的噪声。由于粒子滤波器可以应对非线性系统和非高斯噪声，因此提供了一种稳健的跟踪框架 [<xref ref-type="bibr" rid="hanspub.35744-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.35744-ref6">6</xref>]。在复杂的跟踪场景下，目标外观易受光照变化、遮挡、形变等因素的影响。因此，单一的视觉特征很难准确地表征目标。不同信息在不同场景下所起到的作用有所差别，许多学者利用信息之间的冗余性和互补性，提出多种信息融合的目标跟踪算法，以解决单一信息的不足 [<xref ref-type="bibr" rid="hanspub.35744-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.35744-ref8">8</xref>] [<xref ref-type="bibr" rid="hanspub.35744-ref9">9</xref>]。本文采用民主融合策略将形状信息和颜色信息自适应融合，该策略可以根据当前场景动态调整各特征信息的权重，从而更好地利用不同特征信息，改善动态场景的跟踪性能。本文主要研究行人头部跟踪。</p></sec><sec id="s4"><title>2. 粒子滤波算法</title><p>粒子滤波的核心思想是使用非参数化的蒙特卡罗近似实现递推的贝叶斯滤波 [<xref ref-type="bibr" rid="hanspub.35744-ref10">10</xref>]，即利用带权值的粒</p><p>子表示系统的条件后验概率密度 p ( x k | z k ) ，其中 z k 表示当前的观测信息。从重要性密度函数中采样N个独立同分布的粒子 { x k i } i = 1 N ，根据蒙特卡洛思想，可以通过加权逼近系统的条件后验概率密度，即：</p><p>p ( x k | z k ) ≈ ∑ i = 1 N w ˜ k i δ ( x k − x k i ) (1)</p><p>其中 w ˜ k i 为第i个粒子在k时刻的归一化重要性权值， x k i 为k时刻第i个粒子的状态， δ ( ⋅ ) 为狄拉克(Delta)函数。</p><p>粒子滤波提供了一种鲁棒的跟踪框架，它可以保持选择的多样性，同时考虑多个状态假设。在跟踪过程中，通过对多个候选粒子加权求和可以获得当前跟踪目标的状态(状态包含目标位置、宽高等信息)。</p></sec><sec id="s5"><title>3. 特征提取</title><sec id="s5_1"><title>3.1. 颜色特征</title><p>为了在跟踪场景中区分跟踪目标和背景，必须选择区分度显著的视觉特征来描述目标。颜色是一种最常用的特征，在跟踪过程中，目标易发生旋转，缩放等变化，在这些过程中颜色特征一般比较稳定。使用颜色特征构建似然模型可以使目标跟踪变得简单易行 [<xref ref-type="bibr" rid="hanspub.35744-ref11">11</xref>]。</p><p>本文采用常用的颜色直方图来描述颜色特征，直方图特征维数为 K = 8 &#215; 8 &#215; 8 ，分别代表R、G、B三个颜色通道的量化级数。为了降低背景干扰，增加可靠性，引入一个核函数，定义如下：</p><p>k ( r ) = { 1 − r 2 ,     r ≤ 1 0 ,               其 他 (2)</p><p>其中r表示像素点与区域中心点的归一化距离，离区域中心点越远核函数值较小，反之较大。</p><p>设y表示区域的中心点， x i 为区域内的像素点，那么该区域的颜色分布可表示为：</p><p>p y ( u ) = f ∑ i = 1 I k ( ‖ y − x i ‖ a ) ⋅ δ [ h ( x i ) − u ] (3)</p><p>f = 1 ∑ i = 1 I k ‖ y − x i ‖ a (4)</p><p>a = H x 2 + H y 2 (5)</p><p>上式中I表示区域内所有的像素点个数； δ 为单位冲击函数，判断像素 x i 是否在直方图的第u个单元上；f确保了 ∑ u = 1 m p y ( u ) = 1 ； H x 和 H y 分别为目标区域的半宽和半高。图1表示选中区域对应的颜色直方图。</p><p>图1. 颜色直方图</p><p>参考文献 [<xref ref-type="bibr" rid="hanspub.35744-ref12">12</xref>] 的方法，使用巴氏系数来衡量候选粒子与目标模板之间颜色上的相似度：</p><p>ρ [ p , q ] = ∑ u = 1 m p ( u ) q ( u ) (6)</p><p>其中， p = { p ( u ) } u = 1 , ⋯ , m 和 q = { q ( u ) } u = 1 , ⋯ , m 分别表示候选粒子和目标模板对应的颜色直方图。</p><p>上式中，相似度与巴氏系数成正比。为了方便计算，进一步将它转化为巴氏距离：</p><p>d = 1 − ρ [ p , q ] (7)</p><p>巴氏距离越小，两种分布的相似度越高，越大则相似度越低。</p><p>得到相似度后，对相似度做进一步处理，使相似度高的粒子有较大的权值，反之粒子权值较小。颜色相似度定义为：</p><p>w c = 1 2 π σ e − d 2 2 σ 2 = 1 2 π σ e − ( 1 − ρ [ p , q ] ) 2 σ 2 (8)</p><p>其中 σ 表示颜色信息相似度的协方差。</p></sec><sec id="s5_2"><title>3.2. 形状特征</title><p>颜色信息一个固有缺陷是没有充分考虑像素点之间存在的空间结构关系。当出现颜色特征相似的目标、或者目标的颜色信息发生一定改变时，就会对跟踪产生影响。例如跟踪一个人的头部，当他转身造成面部颜色特征消失时，就有可能导致跟踪失败。</p><p>本文引入目标形状信息。我们将人的头部轮廓近似看为一个椭圆，如果在图像中搜索到椭圆区域，则此区域很有可能就是目标区域，可以根据轮廓周边梯度变化较大的特点加以确定。</p><p>对于一个椭圆区域X，椭圆圆周上的梯度和表示为 [<xref ref-type="bibr" rid="hanspub.35744-ref13">13</xref>]：</p><p>G ( X ) = 1 N s ∑ j = 1 N s g ( x j , y j ) (9)</p><p>在椭圆圆周上， g ( x j , y j ) 为像素点j处的灰度梯度， N s 是总像素数。</p><p>人的头部轮廓不是真正的椭圆，为了提取到与实际相符的梯度，我们做如下改进。先求出像素点j的法线，以像素点j为中心、在法线上截取一段固定长度的线段 L n 。我们用Canny边缘检测器沿j的法线方向检测图像边缘，可以得到线段 L n 中属于边缘的点。求得 L n 的所有边缘点的梯度，将梯度最大的点对应的梯度作为像素点j的梯度：</p><p>g ( x j , y j ) = max ( x i , y i ) ∈ L n { g ( x i , y i ) } (10)</p><p>其中 ( x i , y i ) 表示 L n 上的像素点的坐标。形状相似度为：</p><p>w g = 1 2 π σ g e − 1 2 σ g 2 G 2 ( X ) (11)</p><p>其中 σ g 是对应的高斯分布方差。</p></sec><sec id="s5_3"><title>3.3. 民主特征融合策略</title><p>粒子滤波算法中最重要的一步是求解粒子的权重，本文采用民主融合策略根据当前场景下不同信息的可靠性，为不同的视觉信息赋予相应的权重，求取每个粒子的权重 [<xref ref-type="bibr" rid="hanspub.35744-ref13">13</xref>]。这种自适应的融合方式可以实现不同信息的互补，提高跟踪算法的鲁棒性。最终的权值公式为：</p><p>w = k c w c + k g w g (12)</p><p>其中 k c 和 k g 分别为颜色信息和形状信息的权重系数，表示信息的可靠性。且有 k c + k g = 1 。则第k帧第i个粒子最终的权值表示为：</p><p>w k i = k c w c i + k g w g i (13)</p><p>最终目标预测状态为：</p><p>E ^ [ x k ] = ∑ i = 1 N w k i ⋅ x k i (14)</p><p>因此，我们需要定义一个可靠性因子q来衡量由单个信息得到的预测结果与信息融合的目标模板之间的距离，q定义为：</p><p>q c = e − a D c (15)</p><p>q g = e − a D g (16)</p><p>其中 q c 和 q g 分别表示颜色信息、形状信息与融合信息之间的可靠性， D c 和 D g 分别表示颜色信息权重、形状信息与融合信息之间的巴氏距离。距离越小，q越大，说明该信息的可靠性越高。a为常数。</p><p>一般情况下，相邻的两帧图像之间目标状态不会产生很大的变化，因此我们引入计算信息权重系数的公式：</p><p>τ k c k − k c k − 1 Δ t = q k − 1 − k c k − 1 (17)</p><p>其中 τ 是常量，用来确定加权。 Δ t 是连续两帧之间的时间间隔。</p></sec></sec><sec id="s6"><title>4. 完整的粒子滤波跟踪算法流程</title><p>完整的粒子滤波跟踪算法步骤如下：</p><p>1) 初始化：人工标注第一帧中的跟踪目标，以它为中心，采样得到N个粒子 x 0 = { x 0 i } i = 1 N ，初始化赋予所有粒子相同权重，初始 w c = w g = 0.5 ；计算目标模板的颜色分布直方图。</p><p>2) 预测：利用状态转移方程得到当前时刻的粒子群 x k = { x k i } i = 1 N 。状态转移方程为：</p><p>x k = A x k − 1 + w k − 1 (18)</p><p>A为状态转移矩阵， w k − 1 为高斯噪声。</p><p>3) 计算粒子的权值：根据公式(8)计算 w c ，根据公式(11)计算 w g ，然后根据公式(12)计算粒子融合后的权值w。</p><p>4) 目标状态估计：由公式(14)计算出k时刻的估计结果；由公式(15)和(16)分别计算两种信息的可靠性因子 q c 和 q g ；根据公式(17)和条件 k c + k g = 1 计算出k + 1时刻两种信息的权重系数 k c 和 k g 。</p><p>5) 重采样获得一组新的粒子集 x ′ k = { x k i ′ } i = 1 N 。</p><p>6) 判断是否结束，不结束则 k = k + 1 ，返回步骤2)。</p></sec><sec id="s7"><title>5. 实验结果与分析</title><sec id="s7_1"><title>5.1. 男子头部跟踪实验</title><p>该实验视频拍摄场景在室内 [<xref ref-type="bibr" rid="hanspub.35744-ref13">13</xref>]，待跟踪目标是场景中男子的头部，该视频中没有其他人出现；跟踪难点在于目标移动是非线性的，并且在移动过程中还带有非平面旋转，存在部分甚至是全部遮挡。视频长度为500帧，每帧大小为 128 &#215; 96 ，帧率为30帧每秒。分别用基于颜色信息、基于形状信息、基于两种信息融合的粒子滤波跟踪算法进行对比试验。实验结果见图2，图3，图4。</p><p>图2. 基于颜色信息的粒子滤波算法</p><p>图3. 基于形状信息的粒子滤波算法</p><p>图4. 基于信息融合的粒子滤波算法</p><p>从83帧到85帧，男子头部发生非平面旋转。如果仅采用颜色信息，由于头部旋转时面向镜头的主要是黑色头发、与初始的颜色特征有很大差距，因而跟踪框在这三帧明显偏向和肤色很接近的脖子部分。图232帧到235帧男子用和肤色很接近的黄纸扫过面部，跟踪框定位到黄纸上面去了，当黄纸离开后，跟踪才恢复，重新跟踪到目标。</p><p>图156帧和图157帧男子晃动头部，轮廓是一个倾斜的椭圆。如果仅采用形状信息，由于我们设定人的脸部轮廓是一个竖直的椭圆，就会导致跟踪出现错误，基本丢失目标。</p><p>如果采用两种信息融合的方法进行跟踪，在整个视频中都能准确的跟踪目标。实验证明，基于两种信息融合的粒子滤波算法相对于其他两种更具有稳定性。</p></sec><sec id="s7_2"><title>5.2. 女子头部跟踪实验</title><p>视频中的场景是室内 [<xref ref-type="bibr" rid="hanspub.35744-ref13">13</xref>]，摄像头随女子头部移动。视频长度为500帧，每帧大小为 128 &#215; 96 ，帧率为30帧每秒。此视频中跟踪目标存在平移，非平面内旋转，场景的颜色特征与目标颜色特征相似，相似目标的遮挡等诸多难点。我们采用不同的跟踪算法进行对比实验。实验结果见图5，图6，图7。</p><p>图5. 基于颜色信息的粒子滤波算法</p><p>图6. 基于形状信息的粒子滤波算法</p><p>图7. 基于两种信息融合的粒子滤波算法</p><p>从95帧到120帧，女子头部发生非平面旋转。如果仅采用颜色信息，由于图片左边的纸盒颜色和人的面部肤色较接近，并且头部旋转时面向镜头的主要是黑色的头发、与初始的颜色特征有很大差距，因而跟踪框在95帧和105帧丢失目标，跟踪到纸盒上面。图111帧到120帧女子的脸部转过来，跟踪算法恢复，重新跟踪到目标，进一步证明了粒子滤波算法具有从短暂跟踪失败中恢复的能力。</p><p>从275帧到295帧，对于仅采用形状信息的粒子滤波算法，女子头部左侧是梯度变化明显的窗帘背景，由于窗帘处梯度变化明显，导致跟踪失败。在图349帧中人脸挡住了窗帘，跟踪恢复。而采用两种信息融合的方法在275帧、295帧均未出现跟踪失败。说明了基于两种信息融合的粒子滤波算法的鲁棒性。</p></sec></sec><sec id="s8"><title>6. 结语</title><p>本文针对复杂场境下单一特征难以准确描述目标的问题，研究了一种基于形状信息和颜色信息自适应融合的粒子滤波算法。实验证明，与单独采用颜色或者形状特征的方法相比，基于两种信息融合的粒子滤波算法具有更好的鲁棒性。</p></sec><sec id="s9"><title>文章引用</title><p>徐国寒,沙 洁. 基于特征融合的粒子滤波目标跟踪Particle Filter Tracking Based on Feature Fusion[J]. 计算机科学与应用, 2020, 10(05): 1018-1025. https://doi.org/10.12677/CSA.2020.105105</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.35744-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Collins, R.T., Lipton, A.J. and Kanade, T. (2000) Introduction to the Special Section on Video Surveillance. IEEE Transaction on Pattern Analysis &amp; Machine Intelligence, 22, 745-746. &lt;br&gt;https://doi.org/10.1109/tpami.2000.868676</mixed-citation></ref><ref id="hanspub.35744-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Haritaoglu, I., Harwood, D. and Davis, L.S. (2000) W4: Re-al-Time Surveillance of People and Their Activities. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 22, 809-830. &lt;br&gt;https://doi.org/10.1109/34.868683</mixed-citation></ref><ref id="hanspub.35744-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Erol, A., Bebis, G., Nicolescu, M., et al. (2007) Vision-Based Hand Pose Estimation: A Review. Computer Vision &amp; Image Understanding, 108, 52-73. &lt;br&gt;https://doi.org/10.1016/j.cviu.2006.10.012</mixed-citation></ref><ref id="hanspub.35744-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Ganapathi, V., Plagemann, C., Koller, D., et al. (2010) Real Time Motion Capture Using a Single Time-of-Flight Camera. 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, 13-18 June 2010, 755-762. &lt;br&gt;https://doi.org/10.1109/cvpr.2010.5540141</mixed-citation></ref><ref id="hanspub.35744-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Gordon, N., Ristic, B. and Arulampalam, S. (2004) Beyond the Kalman Filter: Particle Filters for Tracking Applications. Artech House, London, 1077-2626.</mixed-citation></ref><ref id="hanspub.35744-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Arulampalam, M.S., Maskell, S., Gordon, N. and Clapp, T. (2002) A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking. IEEE Transaction on Signal Processing, 50, 174-188.  
&lt;br&gt;https://doi.org/10.1109/78.978374</mixed-citation></ref><ref id="hanspub.35744-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">杨智雄, 余春超, 严敏, 等. 基于特征融合的粒子滤波红外目标跟踪算法[J]. 红外技术, 2016, 38(3): 211-217.</mixed-citation></ref><ref id="hanspub.35744-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Dai, Y. and Liu, B. (2016) Robust Video Object Tracking via Bayesian Model Averaging-Based Feature Fusion. Optical Engineering, 55, Article ID: 083102. &lt;br&gt;https://doi.org/10.1117/1.oe.55.8.083102</mixed-citation></ref><ref id="hanspub.35744-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Dou, J.F. and Li, J.X. (2014) Robust Visual Tracking Base on Adaptively Multi-Feature Fusion and Particle Filter. Optik, 125, 1680-1686. &lt;br&gt;https://doi.org/10.1016/j.ijleo.2013.10.007</mixed-citation></ref><ref id="hanspub.35744-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Carpenter, J., Clifford, P. and Fearnhead, P. (1999) Improved Par-ticle Filter for Nonlinear Problems. IEE Proceedings—Radar Sonar and Navigation, 146, 1-7. &lt;br&gt;https://doi.org/10.1049/ip-rsn:19990255</mixed-citation></ref><ref id="hanspub.35744-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Pérez, P., Hue, C., Vermaak, J., et al. (2002) Color-Based Probabilis-tic Tracking. Proceedings of European Conference on Computer Vision (ECCV), 1, 661-675. &lt;br&gt;https://doi.org/10.1007/3-540-47969-4_44</mixed-citation></ref><ref id="hanspub.35744-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Nummiaro, K., Koller-Meier, E. and Van Gool, L. (2003) An Adaptive Color-Based Particle Filter. Image and Vision Computing, 21, 99-110. &lt;br&gt;https://doi.org/10.1016/s0262-8856(02)00129-4</mixed-citation></ref><ref id="hanspub.35744-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Birchfield, S. (1998) Elliptical Head Tracking Using Intensity Gradients and Color Histograms. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1, 232-237.  
http://cecas.clemson.edu/~stb/research/headtracker/ 
&lt;br&gt;https://doi.org/10.1109/cvpr.1998.698614</mixed-citation></ref></ref-list></back></article>