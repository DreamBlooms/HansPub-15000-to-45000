<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">SA</journal-id><journal-title-group><journal-title>Statistics and Application</journal-title></journal-title-group><issn pub-type="epub">2325-2251</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/SA.2021.101014</article-id><article-id pub-id-type="publisher-id">SA-40731</article-id><article-categories><subj-group subj-group-type="heading"><subject>SA20210100000_97987372.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于SICA罚的变量选择及应用
  Variable Selection and Application Based on SICA Penalty
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>吕</surname><given-names>鹏飞</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>项</surname><given-names>超</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>延新</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>宁波工程学院，浙江 宁波</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>04</day><month>02</month><year>2021</year></pub-date><volume>10</volume><issue>01</issue><fpage>145</fpage><lpage>150</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    高维数据的变量选择一直是统计学领域的热门研究方向。本文研究SICA罚估计在线性模型变量选择中的应用，结合LLA (Local linear approximation)和坐标下降算法给出一种有效的迭代算法，并提出BIC准则选择正则化参数。实际数据的分析表明，与其他变量选择方法相比较，SICA方法在参数估计精度和变量选择方面具有较好的表现。
    Variable selection of high-dimensional data has always been a hot research direction in the field of statistics. In this paper, we study the application of SICA penalty estimation in variable selection of linear model, give an effective iterative algorithm combined with LLA (local linear approximation) and coordinate descent algorithm, and propose BIC criterion to select regularization parameters. The analysis of actual data shows that SICA method has better performance in parameter estimation accuracy and variable selection compared with other variable selection methods. 
  
 
</p></abstract><kwd-group><kwd>SICA罚，变量选择，参数估计，线性模型，BIC准则, SICA Penalty</kwd><kwd> Variable Selection</kwd><kwd> Parameter Estimation</kwd><kwd> Linear Model</kwd><kwd> BIC Criteria</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>高维数据的变量选择一直是统计学领域的热门研究方向。本文研究SICA罚估计在线性模型变量选择中的应用，结合LLA (Local linear approximation)和坐标下降算法给出一种有效的迭代算法，并提出BIC准则选择正则化参数。实际数据的分析表明，与其他变量选择方法相比较，SICA方法在参数估计精度和变量选择方面具有较好的表现。</p></sec><sec id="s2"><title>关键词</title><p>SICA罚，变量选择，参数估计，线性模型，BIC准则</p></sec><sec id="s3"><title>Variable Selection and Application Based on SICA Penalty<sup> </sup></title><p>Pengfei Lv, Chao Xiang, Yanxin Wang<sup>*</sup></p><p>Ningbo University of Technology, Ningbo Zhejiang</p><p><img src="//html.hanspub.org/file/14-2580707x4_hanspub.png" /></p><p>Received: Jan. 25<sup>th</sup>, 2021; accepted: Feb. 19<sup>th</sup>, 2021; published: Feb. 26<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/14-2580707x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Variable selection of high-dimensional data has always been a hot research direction in the field of statistics. In this paper, we study the application of SICA penalty estimation in variable selection of linear model, give an effective iterative algorithm combined with LLA (local linear approximation) and coordinate descent algorithm, and propose BIC criterion to select regularization parameters. The analysis of actual data shows that SICA method has better performance in parameter estimation accuracy and variable selection compared with other variable selection methods.</p><p>Keywords:SICA Penalty, Variable Selection, Parameter Estimation, Linear Model, BIC Criteria</p><disp-formula id="hanspub.40731-formula16"><graphic xlink:href="//html.hanspub.org/file/14-2580707x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/14-2580707x8_hanspub.png" /> <img src="//html.hanspub.org/file/14-2580707x9_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>在统计建模的过程中，我们总是希望筛选出对响应变量影响较强的变量，剔除没有影响或影响较弱的变量。但面对一些高维数据，往往存在维数灾难(curse of dimension)的现象，即数据的维数要远远多于样本量的大小。这对传统的模型选择方法提出了巨大的挑战。</p><p>早在1970年，Hoerl和Kennard提出岭回归 [<xref ref-type="bibr" rid="hanspub.40731-ref1">1</xref>]，是对最小二乘估计的改进。岭回归通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。但岭回归不具有变量选择的功能，近年来，统计学家门提出过一系列变量选择的方法 [<xref ref-type="bibr" rid="hanspub.40731-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.40731-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.40731-ref4">4</xref>]。1996年，受NG (Nonnegative Garrot) [<xref ref-type="bibr" rid="hanspub.40731-ref5">5</xref>] [<xref ref-type="bibr" rid="hanspub.40731-ref6">6</xref>] 方法的启发，Robert Tibshirani首次提出LASSO方法 [<xref ref-type="bibr" rid="hanspub.40731-ref7">7</xref>]。该方法是岭回归的一种特殊形式。它通过构造一个惩罚函数来压缩一些回归系数，即强制系数绝对值之和小于某个固定值；同时设定一些回归系数为零，以此达到变量选择的目的。但LASSO估计是有偏估计，2001年，Fan和Li [<xref ref-type="bibr" rid="hanspub.40731-ref8">8</xref>] 提出SCAD方法，并理论上证明了SCAD罚估计具有Oracle性质。此后，2010年，Zhang提出MCP估计 [<xref ref-type="bibr" rid="hanspub.40731-ref9">9</xref>]，也是一种近似无偏估计。</p><p>2009年，Lv and Fan提出了一种新的罚函数估计方法——SICA罚 [<xref ref-type="bibr" rid="hanspub.40731-ref10">10</xref>] 用于变量选择和参数估计。本文针对线性模型，研究基于SICA罚的变量选择，结合LLA和坐标下降提出一种迭代算法，并提出BIC准则选择正则化参数。最后通过对实际数据分析，与其他方法进行比较。</p></sec><sec id="s6"><title>2. SICA罚估计原理</title><p>考虑线性模型</p><p>y = X β + ε (1)</p><p>其中，y为 n &#215; 1 的响应变量；X为 n &#215; p 设计阵； β = ( β 1 , β 2 , ⋯ , β p ) ，为 p &#215; 1 维未知参数向量； ε 为 n &#215; 1 独立同分布随机误差向量，均值为0，方差为 δ 2 。</p><p>基于模型(1)的SICA惩罚最小二乘定义如下：</p><p>Q ( n ) ( β ) = 1 2 n ‖ y − X β ‖ 2 + ∑ j = 1 p p λ , τ ( | β j | ) (2)</p><p>其中</p><p>p λ , τ ( | β j | ) = λ ( τ + 1 ) | β j | | β j | + τ (3)</p><p>为SICA惩罚函数， λ , τ 为正则化参数。 λ 控制罚函数的惩罚力度， τ 控制罚函数的凹凸度。当取 λ = 1 , τ 分别取0.01，0.05，0.1，绘制SICA罚函数图像如图1所示。由图可见 τ 的取值越小，SICA罚越趋近于L0罚。</p><p>图1. SICA罚函数</p><p>对上述(2)式求极小值，得到SICA惩罚最小二乘估计 β ^ = arg min β Q n ( β ) 。记 β ⌢ = { β ⌢ j ; j = 1 , ⋯ , p } 。极小化 Q n ( β ) 的结果使一些系数为0，从而实现了保留对响应变量影响较大的变量，去除对响应变量影响较小的变量。所以我们可以通过对 Q n ( β ) 实施极小化同时达到系数估计和变量选择的效果。</p></sec><sec id="s7"><title>3. SICA罚估计的算法及参数选择</title><sec id="s7_1"><title>3.1. 算法</title><p>在本文中使用局部一次逼近(LLA)方法来求解模型。对于固定正则化参数 λ ，LLA方法采用局部线性近似的方法估计 ∑ j = 1 p P α ( | β j | ) 。对于给定的初值 β 0 = ( β 1 0 , ⋯ , β p 0 ) T ，LLA将 ∑ j = 1 p p α ( | β j | ) 进行一阶泰勒展开：</p><p>∑ j = 1 p [ p λ , τ ( | β j 0 | ) + p ′ λ , τ ( | β j 0 | ( | β j | − | β j 0 | ) ) ] (4)</p><p>将上式带入SICA罚函数得到：</p><p>1 2 ‖ y − X β ‖ 2 + ∑ j = 1 p [ p λ , τ ( | β j ( 0 ) | ) + p ′ λ , τ ( | β j ( 0 ) | ( | β j | − | β j 0 | ) ) ] (5)</p><p>其中 p λ , τ ( | β j | ) = λ ( τ + 1 ) | β j | | β j | + τ , j = 1 , 2 , ⋯ , p 。上式其实是一种加权LASSO模型，然后采用坐标下降算法求最小值即得到各参数的值，模型得以求解。</p></sec><sec id="s7_2"><title>3.2. 正则化参数选择</title><p>使用SICA方法时要确定参数 λ 和 τ 。对于参数 τ 可以直接取0.01，相当于SCAD中取a = 3.7 [<xref ref-type="bibr" rid="hanspub.40731-ref11">11</xref>]。所以我们只需要固定 τ = 0.01 ，选取适当的参数 λ 即可。AIC准则 [<xref ref-type="bibr" rid="hanspub.40731-ref12">12</xref>]，GCV准则 [<xref ref-type="bibr" rid="hanspub.40731-ref13">13</xref>] 是常用的参数选择方法，但其不具有一致性，易发生过拟合的现象。BIC准则没有这一缺点，所以本文提出基于BIC准则的 的选取：</p><p>BIC λ = log ‖ y − X β λ ‖ 2 2 n − k + d f λ n log ( n ) (6)</p><p>其中 d f λ 为广义自由度：</p><p>d f λ = t r { X ( X ′ X + n ∑ λ ) − 1 X }</p><p>其中 ∑ λ = d i a g ( p ′ λ ( | β ⌢ λ 1 | ) / | β ⌢ λ 1 | , ⋯ , p ′ λ ( | β ⌢ λ k | ) / | β ⌢ λ k | ) ，k为模型中非零参数个数，n为样本数量，取 λ ⌢ = arg min λ ( BIC λ ) 。</p></sec></sec><sec id="s8"><title>4. 实际数据应用</title>实际数据分析<p>本文使用Python的Keras库中的数据集Boston_House，该数据集包含美国人口普查局收集的美国马萨诸塞州波士顿住房价格的有关信息，共506个样本，13个变量。通过多种方法对房价与各自变量之间建立回归模型。对数据的具体描述如下：</p><p>X<sub>1</sub>代表犯罪率；X<sub>2</sub>代表住宅用地所占比例；X<sub>3</sub>代表非零售地区所占比例；X<sub>4</sub>代表是否在河边(0代表不是，1代表是)；X<sub>5</sub>代表一氧化氮浓度；X<sub>6</sub>代表平均每居民房数；X<sub>7</sub>代表建筑年龄；X<sub>8</sub>代表与市中心的距离；X<sub>9</sub>代表公路可达指数；X<sub>10</sub>代表物业税率；X<sub>11</sub>代表城镇师生比例；X<sub>12</sub>代表黑人比例；X<sub>13</sub>代表低收入人口所占比例。</p><p>首先将数据以7:3的比例划分为训练集与测试集。分别采用经典最小二乘法，岭回归，LASSO回归，SICA方法对测试集进行回归建模，将所得模型运用于测试集做预测。从变量选择的效果与预测准确性来比较不同方法的优劣，准确性以MSE作为判断标准。</p><p>MSE称为均方误差(Mean Square Error)，是真实值与预测值的差值的平方然后求平均数，计算公式如下：</p><p>MSE = 1 n ∑ i = 1 n ( y i − y ⌢ i ) 2 (7)</p><p>其中 y i 表示真实值， y ⌢ i 表示预测值。</p><p>SICA方法首先要确定参数 λ 的值，事先给定 λ 一个取值范围，查看不同 λ 下BIC的值，取使得BIC最小的 λ 为参数。BIC随着参数 λ 变化而变化的结果如图2所示：</p><p>图2. BIC变化曲线</p><p>可以看出BIC曲线呈现V型，说明已经找到使得BIC最小的 λ 值。模型拟合的系数和MSE如表1，表2所示：</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> MSE under different method</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >最小二乘</th><th align="center" valign="middle" >岭回归</th><th align="center" valign="middle" >LASSO</th><th align="center" valign="middle" >逐步回归</th><th align="center" valign="middle" >SICA</th></tr></thead><tr><td align="center" valign="middle" >MSE</td><td align="center" valign="middle" >45.0775</td><td align="center" valign="middle" >61.1200</td><td align="center" valign="middle" >96.5622</td><td align="center" valign="middle" >36.8327</td><td align="center" valign="middle" >44.9028</td></tr></tbody></table></table-wrap><p>表1. 不同方法的MSE</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Coefficient under different method</title></caption><table><tbody><thead><tr><th align="center" valign="middle" ></th><th align="center" valign="middle" >最小二乘</th><th align="center" valign="middle" >岭回归</th><th align="center" valign="middle" >LASSO</th><th align="center" valign="middle" >逐步回归</th><th align="center" valign="middle" >SICA</th></tr></thead><tr><td align="center" valign="middle" >X<sub>1</sub></td><td align="center" valign="middle" >0.4007</td><td align="center" valign="middle" >0.7135</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >X<sub>2</sub></td><td align="center" valign="middle" >2.6494</td><td align="center" valign="middle" >1.7956</td><td align="center" valign="middle" >0.8958</td><td align="center" valign="middle" >0.1033</td><td align="center" valign="middle" >2.5856</td></tr><tr><td align="center" valign="middle" >X<sub>3</sub></td><td align="center" valign="middle" >−2.2908</td><td align="center" valign="middle" >−1.2861</td><td align="center" valign="middle" >−0.3943</td><td align="center" valign="middle" >−0.2543</td><td align="center" valign="middle" >−2.4047</td></tr><tr><td align="center" valign="middle" >X<sub>4</sub></td><td align="center" valign="middle" >1.3066</td><td align="center" valign="middle" >1.1989</td><td align="center" valign="middle" >0.1809</td><td align="center" valign="middle" >3.6142</td><td align="center" valign="middle" >1.3564</td></tr><tr><td align="center" valign="middle" >X<sub>5</sub></td><td align="center" valign="middle" >2.2435</td><td align="center" valign="middle" >0.2506</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >X<sub>6</sub></td><td align="center" valign="middle" >5.7128</td><td align="center" valign="middle" >1.1778</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >6.4960</td></tr><tr><td align="center" valign="middle" >X<sub>7</sub></td><td align="center" valign="middle" >2.0795</td><td align="center" valign="middle" >0.3829</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >2.2103</td></tr><tr><td align="center" valign="middle" >X<sub>8</sub></td><td align="center" valign="middle" >−10.0440</td><td align="center" valign="middle" >−6.9809</td><td align="center" valign="middle" >−3.4737</td><td align="center" valign="middle" >−1.8660</td><td align="center" valign="middle" >−9.8784</td></tr><tr><td align="center" valign="middle" >X<sub>9</sub></td><td align="center" valign="middle" >−1.2864</td><td align="center" valign="middle" >0.7831</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0.0334</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >X<sub>1</sub><sub>0</sub></td><td align="center" valign="middle" >−0.9701</td><td align="center" valign="middle" >−1.6955</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >−0.0073</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >X<sub>1</sub><sub>1</sub></td><td align="center" valign="middle" >0.1385</td><td align="center" valign="middle" >0.0590</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td><td align="center" valign="middle" >0</td></tr><tr><td align="center" valign="middle" >X<sub>1</sub><sub>2</sub></td><td align="center" valign="middle" >10.2976</td><td align="center" valign="middle" >1.1828</td><td align="center" valign="middle" >2.6773</td><td align="center" valign="middle" >0.0099</td><td align="center" valign="middle" >9.8597</td></tr><tr><td align="center" valign="middle" >X<sub>1</sub><sub>3</sub></td><td align="center" valign="middle" >−8.7597</td><td align="center" valign="middle" >−4.3576</td><td align="center" valign="middle" >−9.4379</td><td align="center" valign="middle" >−0.7102</td><td align="center" valign="middle" >−8.6776</td></tr></tbody></table></table-wrap><p>表2. 不同方法的变量系数</p><p>SICA方法剔除了5个变量，并且所得模型预测结果的MSE较小。这说明相对于最小二乘回归和岭回归，SICA方法准确的剔除了对因变量影响较小的因素，保留了影响较大的因素。LASSO方法虽然剔除了更多的变量，但是MSE却是最大的，说明剔除了应该保留的变量，变量选择的能力上不如SICA方法。逐步回归方法剔除了5个变量，MSE也小于SICA方法，说明对于这份数据，逐步回归的变量选择与模型预测优于SICA方法。综上，SICA方法无论是变量的选择还是模型的预测精度都优于大部分的传统方法。</p></sec><sec id="s9"><title>5. 结论</title><p>本文讨论了SICA方法在线性模型的变量选择和参数估计中的应用。通过对实际数据的分析，可以发现相对于大部分的传统方法，本文提出的SCIA方法有更强的变量选择能力，对参数的估计具有更高的精度。</p></sec><sec id="s10"><title>基金项目</title><p>全国统计科学研究项目(2019LY06)；浙江省统计研究课题(20TJZZ18)；浙江省自然科学基金资助项目(LY18A010026)；浙江省大学生科技创新活动计划暨新苗人才计划资助项目(2020R475013)。</p></sec><sec id="s11"><title>文章引用</title><p>吕鹏飞,项 超,王延新. 基于SICA罚的变量选择及应用Variable Selection and Application Based on SICA Penalty[J]. 统计学与应用, 2021, 10(01): 145-150. https://doi.org/10.12677/SA.2021.101014</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.40731-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Vinod, H.D. (2020) What’s the Big Idea? Ridge Regression and Regularisation, 17, 41-41.  
https://doi.org/10.1111/1740-9713.01472</mixed-citation></ref><ref id="hanspub.40731-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">曾津, 周建军. 高维数据变量选择方法综述[J]. 数理统计与管理, 2017(4): 678-692.</mixed-citation></ref><ref id="hanspub.40731-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">王大荣, 张忠占. 线性回归模型中变量选择方法综述[J]. 数理统计与管理, 2010(4): 615-627.</mixed-citation></ref><ref id="hanspub.40731-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">李根, 邹国华, 张新雨.高维模型选择方法综述[J]. 数理统计与管理, 2012, 31(4): 640-658.</mixed-citation></ref><ref id="hanspub.40731-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Breiman, L. (1995) Better Subset Regression Using the Nonnegative Garrote. Technometrics, 37, 373-384.  
https://doi.org/10.1080/00401706.1995.10484371</mixed-citation></ref><ref id="hanspub.40731-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Yuan, M. and Lin, Y. (2007) On the Nonnegative Garrote Estimator. Journal of the Royal Statistical Society (Series B), 69, 143-161. https://doi.org/10.1111/j.1467-9868.2007.00581.x</mixed-citation></ref><ref id="hanspub.40731-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Tibshirani, R. (1996) Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society (Series B), 58, 267-288. https://doi.org/10.1111/j.2517-6161.1996.tb02080.x</mixed-citation></ref><ref id="hanspub.40731-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Fan, J.Q. and Li, R.Z. (2001) Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties. Journal of the American Statistical Association, 96, 1348-1360. https://doi.org/10.1198/016214501753382273</mixed-citation></ref><ref id="hanspub.40731-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, C.-H. (2010) Nearly Unbiased Variable Selection under Minimax Concave Penalty. Annals of Statistics, 38, 894-942.  
https://doi.org/10.1214/09-AOS729</mixed-citation></ref><ref id="hanspub.40731-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Lv, J.C. and Fan, Y.Y. (2009) A Unified Approach to Model Selection and Sparse Recovery Using Regularized Least Squares. Annals of Statistics, 37, 3498-3528. https://doi.org/10.1214/09-AOS683</mixed-citation></ref><ref id="hanspub.40731-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">严奇琪, 王延新. 高维部分线性小波模型中的变量选择[J]. 宁波工程学院学报, 2018(2): 13-18.</mixed-citation></ref><ref id="hanspub.40731-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Akaike, H. (1973) Information Theory and an Extension of the Maximum Likelihood Principle. In: Petrov, B.N. and Csaki, F., Eds., International Symposium on Information Theory, Budapest, 267-281.</mixed-citation></ref><ref id="hanspub.40731-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">张肖萍, 吴炜明, 王延新. 高维数据变量选择中MCP正则化参数选择研究[J]. 统计学与应用, 2019, 8(6): 852-858.</mixed-citation></ref></ref-list></back></article>