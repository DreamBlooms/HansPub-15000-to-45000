<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.102036</article-id><article-id pub-id-type="publisher-id">CSA-34301</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20200200000_64913430.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于相似度的两视角多示例图像分类方法研究
  Research on Two-View Multi-Instance Image Classification Based on Similarity
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>尹</surname><given-names>子健</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>肖</surname><given-names>燕珊</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>波</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学计算机学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><aff id="aff3"><addr-line>广东工业大学自动化学院，广东 广州</addr-line></aff><pub-date pub-type="epub"><day>31</day><month>01</month><year>2020</year></pub-date><volume>10</volume><issue>02</issue><fpage>350</fpage><lpage>360</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   在实际中，某些数据中包含许多特权信息，可用于训练分类器，从而提高分类性能。例如，在图像分类中，标签用于描述图像，这些标签可视为特权信息，特权信息与图像互补，可以用于学习以此提高图像分类性能。多示例学习和两视角学习的特性适用于带有特权信息的图像分类，因此提出了一种基于相似度的两视角多示例方法用于带有特权信息的图像分类。所提方法将一张图像视为一个示例，若干张图像的集合视为包，将特权信息视为示例。为解决实际中示例的标签是未知的问题，因而引入相似度模型。所提方法首先将图像和特权信息划分为两个不同的视角，然后使用聚类算法构造包，最后训练支持向量机分类器。在四个数据集上的实验结果表明，所提方法与其他相类似模型相比精确率更高，并比较了两种包的聚类算法，分析了各参数敏感度。 In practice, some data contains a lot of privileged information, which can be used to train the classifier to improve classification performance. For example, in image classification, labels are used to describe images. These labels can be regarded as privileged information. The privileged information is complementary to the image and can be used for learning to improve the performance of image classification. The characteristics of multi-instance learning and two-view learning are suitable for image classification with privileged information. Therefore, a two-view multi-instance method based on similarity is proposed for image classification with privileged information. The proposed method considers one image as an instance, a collection of several images as a package, and privileged information as an instance. In order to solve the problem that the labels in the instances are unknown in practice, a similarity model is introduced. The proposed method first divides the image and privilege information into two different perspectives, then uses a clustering algorithm to construct the package, and finally trains a support vector machine classifier. The experimental results on four data sets show that the proposed method is more accurate than other similar models, and the two packet clustering algorithms are compared, and the sensitivity of each parameter is analyzed. 
  
 
</p></abstract><kwd-group><kwd>多示例学习，两视角学习，图像分类，支持向量机，特权信息, Multi-Instance Learning</kwd><kwd> Two-View Learning</kwd><kwd> Image Classification</kwd><kwd> Support Vector Machine</kwd><kwd> Privileged Information</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于相似度的两视角多示例图像分类方法研究<sup> </sup></title><p>尹子健<sup>1</sup>，肖燕珊<sup>1</sup>，刘波<sup>2</sup></p><p><sup>1</sup>广东工业大学计算机学院，广东 广州</p><p><sup>2</sup>广东工业大学自动化学院，广东 广州</p><p>收稿日期：2020年2月3日；录用日期：2020年2月18日；发布日期：2020年2月25日</p><disp-formula id="hanspub.34301-formula33"><graphic xlink:href="//html.hanspub.org/file/18-1541666x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>在实际中，某些数据中包含许多特权信息，可用于训练分类器，从而提高分类性能。例如，在图像分类中，标签用于描述图像，这些标签可视为特权信息，特权信息与图像互补，可以用于学习以此提高图像分类性能。多示例学习和两视角学习的特性适用于带有特权信息的图像分类，因此提出了一种基于相似度的两视角多示例方法用于带有特权信息的图像分类。所提方法将一张图像视为一个示例，若干张图像的集合视为包，将特权信息视为示例。为解决实际中示例的标签是未知的问题，因而引入相似度模型。所提方法首先将图像和特权信息划分为两个不同的视角，然后使用聚类算法构造包，最后训练支持向量机分类器。在四个数据集上的实验结果表明，所提方法与其他相类似模型相比精确率更高，并比较了两种包的聚类算法，分析了各参数敏感度。</p><p>关键词 :多示例学习，两视角学习，图像分类，支持向量机，特权信息</p><disp-formula id="hanspub.34301-formula34"><graphic xlink:href="//html.hanspub.org/file/18-1541666x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/18-1541666x7_hanspub.png" /> <img src="//html.hanspub.org/file/18-1541666x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>多示例学习是从监督学习算法的基础上进化而来的，是为解决包(多个示例的集合)的分类 [<xref ref-type="bibr" rid="hanspub.34301-ref1">1</xref>]。在传统的多示例学习中，示例分为正示例和负示例，当包中至少有一个正示例时，该包的标签标记为正，称为正包；当包中的实例都是负示例时，该包的标签标记为负，称为负包。多示例学习的任务就是把已知标签的包进行训练，然后对未知标签的包的标签进行预测。</p><p>近年来，图像分类是计算机视觉领域的研究热点之一，也是其他图像应用领域的基础。因多示例学习二分类的特性，多示例学习也越来越多地应用在图像分类领域中，对于包与示例的定义有两种，一种定义是将一张图像视为一个包，对图像分割后的产生的每个区域视为一个示例 [<xref ref-type="bibr" rid="hanspub.34301-ref2">2</xref>]。例如，Rao等人 [<xref ref-type="bibr" rid="hanspub.34301-ref3">3</xref>] 提出一种基于多尺度块的情感分类方法，该方法首先使用不同的图像分割方法提取多个比例的图像块，然后用多示例学习对图像的主要情感类型进行分类。另一种定义是将一张图像视为一个示例，把多张图像的集合视为包 [<xref ref-type="bibr" rid="hanspub.34301-ref4">4</xref>]。例如，Duan等人 [<xref ref-type="bibr" rid="hanspub.34301-ref4">4</xref>] 提出GMI (generalized multi-instance)学习算法，该算法使用k-means聚类算法根据低级的视觉特征将相关的多张图像聚合成一个包，包中的图像视为示例，从而将图像分类问题转换成多示例学习问题。</p><p>尽管针对图像分类的多示例学习的研究有很多，但大多数都假设训练数据和测试数据具有相同数量的特征。但是，现实中可能会遇到这样的情况，即训练数据具有比测试数据更多的特征，而额外的特征通常称为特权信息 [<xref ref-type="bibr" rid="hanspub.34301-ref5">5</xref>]。在图像分类中，特权信息可以用作训练期间可用的附加特征，这些附加特征可以帮助训练处更准确地分类器并提高识别性能和泛化能力。目前，特权信息学习已在许多领域得到了广泛的研究。例如，Guo等人 [<xref ref-type="bibr" rid="hanspub.34301-ref6">6</xref>] 提出ESVDD-neg (extended support vector data description with negative examples)方法，该方法使用特权信息学习来解决雷达自动目标识别问题。Wu等人 [<xref ref-type="bibr" rid="hanspub.34301-ref7">7</xref>] 提出MT-PSVR (multi-target support vector regression)模型，该模型在训练每个目标模型时通过将其他目标视为特权信息来显示探索目标之间的相关性。</p><p>两视角学习的提出主要用于解决二维数据的学习问题 [<xref ref-type="bibr" rid="hanspub.34301-ref8">8</xref>]。这里的“视角”表示来自多个源或不同特征子集的数据，例如多媒体片段由视频信号和音频信号组成。Li等人 [<xref ref-type="bibr" rid="hanspub.34301-ref9">9</xref>] 提出两视角TSVM (Two-view Transductive SVM)，用于已标记数据和未标记数据的分类。特权信息具有很多维度和互补性，例如网页包含内容文本和图像，它们相互补充并能够完整地描述对象。</p><p>特权信息的概念最初由Vapnik等人提出 [<xref ref-type="bibr" rid="hanspub.34301-ref10">10</xref>]，现实世界中的数据通常与丰富的描述相关联，并且这些丰富的描述在学习任务中被视为特权信息。例如，网络图像通常包含辅助信息(例如，文本描述，注释等)，这些信息被视为特权信息。实际中，特权信息可以是标签、属性、关键字等。随着在线图像共享平台(如Flickr、Instagram等)的发展，这些平台允许用户上传图像，并为图像贴上标签，如图1所示。这些标签(称为“特权信息”)能够描述图像内容，能提高图像检索的性能。这些图像和特权信息构成数据的二维性，因此可以应用于两视角学习。例如，Tang等人 [<xref ref-type="bibr" rid="hanspub.34301-ref11">11</xref>] 提出了多视角特权SVM模型。该模型在两视角学习和特权信息的基础上进行扩展。该方法将两种互补的特权信息分成两个不同的视角，并在两个视角之间添加约束来补偿它们之间的差距。</p><p>图1. 带有特权信息的Web图像</p><p>包和示例的特性适用于带有特权信息的图像分类，因此采用多示例学习来表示数据，采用两视角学习来划分数据并同时训练分类器。所提方法将一张图像视为一个示例，将若干图像的集合视为包，将一个关键词(特权信息)视为一个示例，将若干个关键词的集合视为包。使用特权信息提高图像分类的精确度的前提是包中示例的标签是已知的，但在现实中示例的标签是模糊的，而现有的研究较少提及这方面的处理，因此所提模型引入相似度模型来描述数据。所提模型可用于在线图像共享平台提高Web图像检索性能。</p></sec><sec id="s4"><title>2. 基于相似度的两视角多示例模型</title><sec id="s4_1"><title>2.1. 相似度数据模型</title><p>在多示例学习中，每个正包至少包含一个正示例，负包里都是负实例。但是，正包中的示例标签是模糊的，即有可能是正的，有可能是负的。为此，引入相似度数据模型来描述多示例学习问题。</p><p>用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x10_hanspub.png" xlink:type="simple"/></inline-formula>来表示训练包的集合。其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x11_hanspub.png" xlink:type="simple"/></inline-formula>表示一个正包，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x12_hanspub.png" xlink:type="simple"/></inline-formula>表示正包<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x13_hanspub.png" xlink:type="simple"/></inline-formula>的标签，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x14_hanspub.png" xlink:type="simple"/></inline-formula>；<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x15_hanspub.png" xlink:type="simple"/></inline-formula>表示一个负包，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x16_hanspub.png" xlink:type="simple"/></inline-formula>表示正包<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x17_hanspub.png" xlink:type="simple"/></inline-formula>的标签，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x18_hanspub.png" xlink:type="simple"/></inline-formula>。<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x19_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x20_hanspub.png" xlink:type="simple"/></inline-formula>分别表示正包和负包的数量。该模型基于示例的相似度选择正候选，有以下定义：</p><p>定义1：(基于单集的相似度)：给出一个示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x21_hanspub.png" xlink:type="simple"/></inline-formula>和一个子集S，示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x22_hanspub.png" xlink:type="simple"/></inline-formula>和子集S的相似度可以定义如下 [<xref ref-type="bibr" rid="hanspub.34301-ref12">12</xref>] ：</p><disp-formula id="hanspub.34301-formula35"><label>(1)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/18-1541666x23_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x24_hanspub.png" xlink:type="simple"/></inline-formula>是一个非线性映射函数，将示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x25_hanspub.png" xlink:type="simple"/></inline-formula>或<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x26_hanspub.png" xlink:type="simple"/></inline-formula>映射到特征空间中。于是，两类成员计算如下：</p><disp-formula id="hanspub.34301-formula36"><label>(2)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/18-1541666x27_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.34301-formula37"><label>(3)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/18-1541666x28_hanspub.png"  xlink:type="simple"/></disp-formula><p>让多示例包里的示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x29_hanspub.png" xlink:type="simple"/></inline-formula>表示为<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x30_hanspub.png" xlink:type="simple"/></inline-formula>。分别<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x31_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x32_hanspub.png" xlink:type="simple"/></inline-formula>表示示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x33_hanspub.png" xlink:type="simple"/></inline-formula>的趋近于正类和负类的相似度，有<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x34_hanspub.png" xlink:type="simple"/></inline-formula>以及<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x35_hanspub.png" xlink:type="simple"/></inline-formula>。如果示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x36_hanspub.png" xlink:type="simple"/></inline-formula>是正示例，则<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x37_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x38_hanspub.png" xlink:type="simple"/></inline-formula>。如果示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x39_hanspub.png" xlink:type="simple"/></inline-formula>是负示例，则<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x40_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x41_hanspub.png" xlink:type="simple"/></inline-formula>。如果示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x42_hanspub.png" xlink:type="simple"/></inline-formula>是模糊示例，其标签未知，用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x43_hanspub.png" xlink:type="simple"/></inline-formula>表示，有<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x44_hanspub.png" xlink:type="simple"/></inline-formula>以及<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x45_hanspub.png" xlink:type="simple"/></inline-formula>。其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x46_hanspub.png" xlink:type="simple"/></inline-formula>存储正包中的正候选，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x47_hanspub.png" xlink:type="simple"/></inline-formula>存储正包中除<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x48_hanspub.png" xlink:type="simple"/></inline-formula>以外的示例，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x49_hanspub.png" xlink:type="simple"/></inline-formula>存储负包中的示例。对于<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x50_hanspub.png" xlink:type="simple"/></inline-formula>中的示例，有二成员<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x51_hanspub.png" xlink:type="simple"/></inline-formula>分别趋近于正类和负类。根据以上定义，设<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x52_hanspub.png" xlink:type="simple"/></inline-formula>表示趋近于含有特权信息的正类和负类数据，简称为A视角。进一步让<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x53_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x54_hanspub.png" xlink:type="simple"/></inline-formula>。类似的，设<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x55_hanspub.png" xlink:type="simple"/></inline-formula>表示趋近于剔除特权信息的正类和负类数据，简称为B视角，有<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x56_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x57_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec><sec id="s4_2"><title>2.2. 对偶问题</title><p>所提方法分别对于A视角<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x58_hanspub.png" xlink:type="simple"/></inline-formula>和B视角<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x59_hanspub.png" xlink:type="simple"/></inline-formula>训练支持向量机。假设<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x60_hanspub.png" xlink:type="simple"/></inline-formula>为上述两个视角的超平面。所提方法是基于多示例学习，引导出最小化问题，其目标方程：</p><disp-formula id="hanspub.34301-formula38"><label>(4)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/18-1541666x61_hanspub.png"  xlink:type="simple"/></disp-formula><p>约束条件：</p><disp-formula id="hanspub.34301-formula39"><graphic xlink:href="//html.hanspub.org/file/18-1541666x62_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.34301-formula40"><graphic xlink:href="//html.hanspub.org/file/18-1541666x63_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x64_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x65_hanspub.png" xlink:type="simple"/></inline-formula>是控制两个视角的正则项。如果<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x66_hanspub.png" xlink:type="simple"/></inline-formula>，则A视角优于B视角，反之，B视角优于A视角。参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x67_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x68_hanspub.png" xlink:type="simple"/></inline-formula>是平衡边距和误差的参数。<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x69_hanspub.png" xlink:type="simple"/></inline-formula>是松弛变量。在A视角中，对于<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x70_hanspub.png" xlink:type="simple"/></inline-formula>中的每个示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x71_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x72_hanspub.png" xlink:type="simple"/></inline-formula>趋近于正类，又因为有<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x73_hanspub.png" xlink:type="simple"/></inline-formula>，所以<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x74_hanspub.png" xlink:type="simple"/></inline-formula>有<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x75_hanspub.png" xlink:type="simple"/></inline-formula>趋近于负类。B视角中的<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x76_hanspub.png" xlink:type="simple"/></inline-formula>同理可得。约束<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x77_hanspub.png" xlink:type="simple"/></inline-formula>表示两视角间的约束。<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x78_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x79_hanspub.png" xlink:type="simple"/></inline-formula>分别表示A视角和B视角的决策函数。<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x80_hanspub.png" xlink:type="simple"/></inline-formula>是两视角间的共识变量。<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x81_hanspub.png" xlink:type="simple"/></inline-formula>是是允许某些实例违反约束的松弛变量。此外，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x82_hanspub.png" xlink:type="simple"/></inline-formula>是映射函数，它将数据从输入空间映射到特征空间，可引入核函数来计算特征空间中两个向量的内积，即<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x83_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>为解决公式(4)中的优化问题，用拉格朗日方法，引入拉格朗日乘子<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x84_hanspub.png" xlink:type="simple"/></inline-formula>并对<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x85_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x86_hanspub.png" xlink:type="simple"/></inline-formula>求偏导得：</p><disp-formula id="hanspub.34301-formula41"><label>(5)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/18-1541666x87_hanspub.png"  xlink:type="simple"/></disp-formula><disp-formula id="hanspub.34301-formula42"><label>(6)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/18-1541666x88_hanspub.png"  xlink:type="simple"/></disp-formula><p>根据Kuhn-Tucker Theorem定理得：</p><disp-formula id="hanspub.34301-formula43"><label>(7)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/18-1541666x89_hanspub.png"  xlink:type="simple"/></disp-formula><p>将公式(5)和(6)代入(7)得到对偶问题：</p><disp-formula id="hanspub.34301-formula44"><label>(8)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/18-1541666x90_hanspub.png"  xlink:type="simple"/></disp-formula><p>约束条件：</p><disp-formula id="hanspub.34301-formula45"><graphic xlink:href="//html.hanspub.org/file/18-1541666x91_hanspub.png"  xlink:type="simple"/></disp-formula><p>算法实现步骤如表1所示。</p><table-wrap-group id="1"><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Algorithm implementation step</title></caption><table-wrap id="1_1"><table><tbody><thead><tr><th align="center" valign="middle" >输入：带有特权信息的图像集 输出：<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x92_hanspub.png" xlink:type="simple"/></inline-formula></th></tr></thead><tr><td align="center" valign="middle" >1：根据公式(2)和(3)初始化包<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x93_hanspub.png" xlink:type="simple"/></inline-formula>的标签<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x94_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >2：将文本和图像划分成两个视角</td></tr><tr><td align="center" valign="middle" >3：初始化<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x95_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >4：让t = 0</td></tr><tr><td align="center" valign="middle" >5：重复步骤6至19</td></tr><tr><td align="center" valign="middle" >6：t = t + 1</td></tr><tr><td align="center" valign="middle" >7：对于A视角和B视角中的每个正包<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x96_hanspub.png" xlink:type="simple"/></inline-formula>，重复步骤8至18</td></tr></tbody></table></table-wrap><table-wrap id="1_2"><table><tbody><thead><tr><th align="center" valign="middle" >8：对于正包<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x97_hanspub.png" xlink:type="simple"/></inline-formula>中的每一个示例，重复步骤9至12</th></tr></thead><tr><td align="center" valign="middle" >9：令<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x98_hanspub.png" xlink:type="simple"/></inline-formula>为包<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x99_hanspub.png" xlink:type="simple"/></inline-formula>的正候选</td></tr><tr><td align="center" valign="middle" >10：用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x100_hanspub.png" xlink:type="simple"/></inline-formula>更新<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x101_hanspub.png" xlink:type="simple"/></inline-formula>，用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x102_hanspub.png" xlink:type="simple"/></inline-formula>更新<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x103_hanspub.png" xlink:type="simple"/></inline-formula>。</td></tr><tr><td align="center" valign="middle" >11：通过用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x104_hanspub.png" xlink:type="simple"/></inline-formula>替代<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x105_hanspub.png" xlink:type="simple"/></inline-formula>来更新<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x106_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >12：计算<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x107_hanspub.png" xlink:type="simple"/></inline-formula>表示<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x108_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >13：通过用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x109_hanspub.png" xlink:type="simple"/></inline-formula>替代<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x110_hanspub.png" xlink:type="simple"/></inline-formula>来更新<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x111_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >14：获得新的正候选并返回<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x112_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >15：用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x113_hanspub.png" xlink:type="simple"/></inline-formula>来更新<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x114_hanspub.png" xlink:type="simple"/></inline-formula>；用<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x115_hanspub.png" xlink:type="simple"/></inline-formula>来更新<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x116_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >16：根据公式(2)和(3)计算<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x117_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >17：基于<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x118_hanspub.png" xlink:type="simple"/></inline-formula>解决QP问题得到F</td></tr><tr><td align="center" valign="middle" >18：用F更新<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x119_hanspub.png" xlink:type="simple"/></inline-formula></td></tr><tr><td align="center" valign="middle" >19：直到<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x120_hanspub.png" xlink:type="simple"/></inline-formula></td></tr></tbody></table></table-wrap></table-wrap-group><p>表1. 算法实现步骤</p></sec></sec><sec id="s5"><title>3. 实验与结果分析</title><sec id="s5_1"><title>3.1. 数据集</title><p>实验数据来自NUS-WIDE、Flickr30k、WebQuery和AwA2 (Animals with Attributes 2)这四个数据集：</p><p>l NUS-WIDE：该数据集包括来自Flickr的269648张图像的5018个唯一标签和相关标签。</p><p>l Flickr30k：该数据集已成为基于句子的图像描述的基准，包含31,783张日常活动、事件和场景的照片以及158,915句文字描述。</p><p>l WebQuery：该数据集包含从353个文本查询中检索到的71,478个Web图像。并且数据集中的每个图像都与多种语言的文字描述相关联。</p><p>l AwA2：该数据集由50个动物类别的37,322张图像组成，每个图像具有预先提取的特征表示，并带有85个语义属性进行注释。</p><p>该实验将一张图像视为一个示例，将一个标签、注释、一句文字描述视为一个示例，都使用聚类算法构造多示例包。</p></sec><sec id="s5_2"><title>3.2. 实验设计</title><sec id="s5_2_1"><title>3.2.1. 两种多示例包的聚类算法</title><p>在机器学习领域，有很多聚类算法，如k-means [<xref ref-type="bibr" rid="hanspub.34301-ref15">15</xref>]，DBSCAN [<xref ref-type="bibr" rid="hanspub.34301-ref16">16</xref>] 算法等，这些方法可以用来构造多示例包。实验中使用k-means和DBSCAN算法对于包进行构造。</p><p>k-means聚类算法：由于简单和效率高的特性，k-means聚类算法成为聚类算法中使用最广泛的算法。根据前人的经验，该实验设参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x121_hanspub.png" xlink:type="simple"/></inline-formula>，其中<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x122_hanspub.png" xlink:type="simple"/></inline-formula>表示训练包的数量。该实验使用的k-means聚类算法是基于欧氏距离：<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x123_hanspub.png" xlink:type="simple"/></inline-formula>，其中，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x124_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x125_hanspub.png" xlink:type="simple"/></inline-formula>分别是第i张和第j张图像的视觉和文本特征。<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x126_hanspub.png" xlink:type="simple"/></inline-formula>是两张图像之间的距离。</p><p>DBSCAN聚类算法：DBSCAN是基于密度的聚类算法，可以应用于凸样本集和非凸样本集。 DBSCAN将聚类定义为按密度连接的最大点集，将具有足够高密度的区域划分为聚类，并可以在噪声的空间数据库中找到任意形状的聚类。</p></sec><sec id="s5_2_2"><title>3.2.2. 比较算法和参数设定</title><p>由于所提方法是多示例学习方法，因此将其性能与以下四种多示例学习方法进行比较：</p><p>l sMIL-PI：该方法将松散标记的Web数据合并到学习过程中，利用附加的文本特征来有效地处理相关训练图像的噪声标签。该方法基于最大平均差异准则添加了一个正则化器，以减少数据分布不匹配 [<xref ref-type="bibr" rid="hanspub.34301-ref7">7</xref>]。</p><p>l PSVM-2V：该方法基于两视角学习用于特权信息学习，着重于将特权信息纳入多示例学习和训练SVM作为分类器 [<xref ref-type="bibr" rid="hanspub.34301-ref11">11</xref>]。</p><p>l SVM+：该方法是一种基于特权信息的学习范式，用特权特征空间中定义的松弛函数替换标准SVM中的松弛变量 [<xref ref-type="bibr" rid="hanspub.34301-ref13">13</xref>]。</p><p>l SVM-2K：该方法是最早提出的两视角学习方法，它构造两个带有标记和未标记数据的视角以训练分类器 [<xref ref-type="bibr" rid="hanspub.34301-ref14">14</xref>]。</p><p>实验将所提方法与SVM+，SVM-2K，sMIL-PI和PSVM-2V比较，采用平均精确度(average precision)来衡量各方法的性能。在SVM-2K和PSVM-2V中，设其参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x127_hanspub.png" xlink:type="simple"/></inline-formula>；在sMIL-PI中，设其参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x128_hanspub.png" xlink:type="simple"/></inline-formula>。实验中将以上这些参数以及sMIL-PI、PSVM-2V、SVM+中的权衡参数γ的值均选自范围<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x129_hanspub.png" xlink:type="simple"/></inline-formula>。经验表明，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x130_hanspub.png" xlink:type="simple"/></inline-formula>越小，分类的性能通常越好，因此使参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x131_hanspub.png" xlink:type="simple"/></inline-formula>。对于所提方法，设参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x132_hanspub.png" xlink:type="simple"/></inline-formula>，并且这些参数选自范围<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x133_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>对于所有基于SVM的方法，实验中使用高斯径向核函数：<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x134_hanspub.png" xlink:type="simple"/></inline-formula>，核参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x135_hanspub.png" xlink:type="simple"/></inline-formula>的值选自范围<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x136_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec></sec><sec id="s5_3"><title>3.3. 实验结果与分析</title><sec id="s5_3_1"><title>3.3.1. 性能比较</title><p>实验使用NUS-WIDE、Flickr30k、WebQuery和AwA2数据集对SVM+，SVM-2K，sMIL-PI，PSVM-2V和所提方法进行测试与比较。首先将图像和特权信息划分成两个不同的视角，然后在每个视角中分别使用k-means和DBSCAN聚类算法构造多示例包，构造包的数量为300、600、900、1200和1500分别对各方法进行测试，结果如表2~6所示。</p><p>从表2~6可以看出，在NUS-WIDE、Flickr30k、WebQuery和AwA2数据集上，从整体上看所提方法略优于比其他多示例方法。随着包的数量从300到900的增加，SVM+、SVM-2K、sMIL-2V、PSVM-2V和所提方法的平均精确度都有所上升。其中PSVM-2V、SVM-2K和所提方法在包的数量为900时达到峰值，随后开始下降，SVM+和sMIL-2V在包的数量为1200时达到峰值，随后也开始下降。随着包的增加，噪声也有所增加，导致分类精确度也随之下降。由此可以得出结论：基于相似度的多示例学习方法分类精度更高。同时由结果可以看出，整体上看使用DBSCAN比k-means算法效果要更优。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Average precision when the bag numbers is 300 (unit: %</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle"  colspan="2"  >NUS-WIDE</th><th align="center" valign="middle"  colspan="2"  >Flickr30k</th><th align="center" valign="middle"  colspan="2"  >WebQuery</th><th align="center" valign="middle"  colspan="2"  >AwA2</th></tr></thead><tr><td align="center" valign="middle" >算法</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td></tr><tr><td align="center" valign="middle" >SVM+</td><td align="center" valign="middle" >72.83</td><td align="center" valign="middle" >74.46</td><td align="center" valign="middle" >71.02</td><td align="center" valign="middle" >72.97</td><td align="center" valign="middle" >69.48</td><td align="center" valign="middle" >72.51</td><td align="center" valign="middle" >75.34</td><td align="center" valign="middle" >77.38</td></tr><tr><td align="center" valign="middle" >SVM-2K</td><td align="center" valign="middle" >66.15</td><td align="center" valign="middle" >68.37</td><td align="center" valign="middle" >65.57</td><td align="center" valign="middle" >66.30</td><td align="center" valign="middle" >67.33</td><td align="center" valign="middle" >68.43</td><td align="center" valign="middle" >70.75</td><td align="center" valign="middle" >72.04</td></tr><tr><td align="center" valign="middle" >sMIL-PI</td><td align="center" valign="middle" >76.81</td><td align="center" valign="middle" >79.19</td><td align="center" valign="middle" >73.88</td><td align="center" valign="middle" >76.55</td><td align="center" valign="middle" >75.46</td><td align="center" valign="middle" >76.31</td><td align="center" valign="middle" >74.82</td><td align="center" valign="middle" >76.53</td></tr><tr><td align="center" valign="middle" >PSVM-2V</td><td align="center" valign="middle" >80.18</td><td align="center" valign="middle" >80.87</td><td align="center" valign="middle" >81.32</td><td align="center" valign="middle" >82.46</td><td align="center" valign="middle" >80.37</td><td align="center" valign="middle" >79.47</td><td align="center" valign="middle" >81.24</td><td align="center" valign="middle" >82.72</td></tr><tr><td align="center" valign="middle" >所提方法</td><td align="center" valign="middle" >81.24</td><td align="center" valign="middle" >82.60</td><td align="center" valign="middle" >80.28</td><td align="center" valign="middle" >81.04</td><td align="center" valign="middle" >82.61</td><td align="center" valign="middle" >83.53</td><td align="center" valign="middle" >81.73</td><td align="center" valign="middle" >83.59</td></tr></tbody></table></table-wrap><p>表2. 包的数量为300时的平均精确度(单位：%)</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Average precision when the bag numbers is 600 (unit: %</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle"  colspan="2"  >NUS-WIDE</th><th align="center" valign="middle"  colspan="2"  >Flickr30k</th><th align="center" valign="middle"  colspan="2"  >WebQuery</th><th align="center" valign="middle"  colspan="2"  >AwA2</th></tr></thead><tr><td align="center" valign="middle" >算法</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td></tr><tr><td align="center" valign="middle" >SVM+</td><td align="center" valign="middle" >76.64</td><td align="center" valign="middle" >75.09</td><td align="center" valign="middle" >75.52</td><td align="center" valign="middle" >75.03</td><td align="center" valign="middle" >74.64</td><td align="center" valign="middle" >72.57</td><td align="center" valign="middle" >78.57</td><td align="center" valign="middle" >78.79</td></tr><tr><td align="center" valign="middle" >SVM-2K</td><td align="center" valign="middle" >70.48</td><td align="center" valign="middle" >71.53</td><td align="center" valign="middle" >73.19</td><td align="center" valign="middle" >73.06</td><td align="center" valign="middle" >72.29</td><td align="center" valign="middle" >74.15</td><td align="center" valign="middle" >71.08</td><td align="center" valign="middle" >72.42</td></tr><tr><td align="center" valign="middle" >sMIL-PI</td><td align="center" valign="middle" >76.86</td><td align="center" valign="middle" >75.92</td><td align="center" valign="middle" >78.01</td><td align="center" valign="middle" >80.55</td><td align="center" valign="middle" >79.36</td><td align="center" valign="middle" >82.23</td><td align="center" valign="middle" >76.28</td><td align="center" valign="middle" >79.36</td></tr><tr><td align="center" valign="middle" >PSVM-2V</td><td align="center" valign="middle" >82.82</td><td align="center" valign="middle" >82.96</td><td align="center" valign="middle" >84.47</td><td align="center" valign="middle" >86.63</td><td align="center" valign="middle" >83.90</td><td align="center" valign="middle" >82.14</td><td align="center" valign="middle" >83.11</td><td align="center" valign="middle" >83.69</td></tr><tr><td align="center" valign="middle" >所提方法</td><td align="center" valign="middle" >83.49</td><td align="center" valign="middle" >83.02</td><td align="center" valign="middle" >84.93</td><td align="center" valign="middle" >85.95</td><td align="center" valign="middle" >85.81</td><td align="center" valign="middle" >85.60</td><td align="center" valign="middle" >84.03</td><td align="center" valign="middle" >85.76</td></tr></tbody></table></table-wrap><p>表3. 包的数量为600时的平均精确度(单位：%)</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Average precision when the bag numbers is 900 (unit: %</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle"  colspan="2"  >NUS-WIDE</th><th align="center" valign="middle"  colspan="2"  >Flickr30k</th><th align="center" valign="middle"  colspan="2"  >WebQuery</th><th align="center" valign="middle"  colspan="2"  >AwA2</th></tr></thead><tr><td align="center" valign="middle" >算法</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td></tr><tr><td align="center" valign="middle" >SVM+</td><td align="center" valign="middle" >77.22</td><td align="center" valign="middle" >79.04</td><td align="center" valign="middle" >80.43</td><td align="center" valign="middle" >82.68</td><td align="center" valign="middle" >78.38</td><td align="center" valign="middle" >79.63</td><td align="center" valign="middle" >79.18</td><td align="center" valign="middle" >80.81</td></tr><tr><td align="center" valign="middle" >SVM-2K</td><td align="center" valign="middle" >75.05</td><td align="center" valign="middle" >76.23</td><td align="center" valign="middle" >75.03</td><td align="center" valign="middle" >78.30</td><td align="center" valign="middle" >76.26</td><td align="center" valign="middle" >79.31</td><td align="center" valign="middle" >74.34</td><td align="center" valign="middle" >77.42</td></tr><tr><td align="center" valign="middle" >sMIL-PI</td><td align="center" valign="middle" >80.05</td><td align="center" valign="middle" >81.54</td><td align="center" valign="middle" >83.22</td><td align="center" valign="middle" >83.61</td><td align="center" valign="middle" >80.39</td><td align="center" valign="middle" >82.09</td><td align="center" valign="middle" >80.98</td><td align="center" valign="middle" >81.47</td></tr><tr><td align="center" valign="middle" >PSVM-2V</td><td align="center" valign="middle" >83.64</td><td align="center" valign="middle" >85,39</td><td align="center" valign="middle" >85.07</td><td align="center" valign="middle" >86.94</td><td align="center" valign="middle" >83.37</td><td align="center" valign="middle" >82.08</td><td align="center" valign="middle" >85.46</td><td align="center" valign="middle" >87.63</td></tr><tr><td align="center" valign="middle" >所提方法</td><td align="center" valign="middle" >86.21</td><td align="center" valign="middle" >88.72</td><td align="center" valign="middle" >87.40</td><td align="center" valign="middle" >88.46</td><td align="center" valign="middle" >85.09</td><td align="center" valign="middle" >86.29</td><td align="center" valign="middle" >88.22</td><td align="center" valign="middle" >88.35</td></tr></tbody></table></table-wrap><p>表4. 包的数量为900时的平均精确度(单位：%)</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Average precision when the bag numbers is 1200 (unit: %</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle"  colspan="2"  >NUS-WIDE</th><th align="center" valign="middle"  colspan="2"  >Flickr30k</th><th align="center" valign="middle"  colspan="2"  >WebQuery</th><th align="center" valign="middle"  colspan="2"  >AwA2</th></tr></thead><tr><td align="center" valign="middle" >算法</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td></tr><tr><td align="center" valign="middle" >SVM+</td><td align="center" valign="middle" >79.80</td><td align="center" valign="middle" >81.31</td><td align="center" valign="middle" >80.84</td><td align="center" valign="middle" >84.52</td><td align="center" valign="middle" >79.41</td><td align="center" valign="middle" >81.06</td><td align="center" valign="middle" >82.07</td><td align="center" valign="middle" >81.19</td></tr><tr><td align="center" valign="middle" >SVM-2K</td><td align="center" valign="middle" >74.11</td><td align="center" valign="middle" >75.02</td><td align="center" valign="middle" >73.13</td><td align="center" valign="middle" >74.28</td><td align="center" valign="middle" >76.17</td><td align="center" valign="middle" >77.39</td><td align="center" valign="middle" >75.62</td><td align="center" valign="middle" >75.83</td></tr><tr><td align="center" valign="middle" >sMIL-PI</td><td align="center" valign="middle" >82.63</td><td align="center" valign="middle" >82.08</td><td align="center" valign="middle" >84.04</td><td align="center" valign="middle" >83.74</td><td align="center" valign="middle" >80.67</td><td align="center" valign="middle" >81.54</td><td align="center" valign="middle" >81.02</td><td align="center" valign="middle" >82.24</td></tr><tr><td align="center" valign="middle" >PSVM-2V</td><td align="center" valign="middle" >82.59</td><td align="center" valign="middle" >84.95</td><td align="center" valign="middle" >83.75</td><td align="center" valign="middle" >85.23</td><td align="center" valign="middle" >81.39</td><td align="center" valign="middle" >81.75</td><td align="center" valign="middle" >85.29</td><td align="center" valign="middle" >86.31</td></tr><tr><td align="center" valign="middle" >所提方法</td><td align="center" valign="middle" >84.62</td><td align="center" valign="middle" >84.54</td><td align="center" valign="middle" >86.02</td><td align="center" valign="middle" >86.36</td><td align="center" valign="middle" >83.70</td><td align="center" valign="middle" >84.01</td><td align="center" valign="middle" >87.25</td><td align="center" valign="middle" >86.49</td></tr></tbody></table></table-wrap><p>表5. 包的数量为1200时的平均精确度(单位：%)</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Average precision when the bag numbers is 1500 (unit: %</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle"  colspan="2"  >NUS-WIDE</th><th align="center" valign="middle"  colspan="2"  >Flickr30k</th><th align="center" valign="middle"  colspan="2"  >WebQuery</th><th align="center" valign="middle"  colspan="2"  >AwA2</th></tr></thead><tr><td align="center" valign="middle" >算法</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td><td align="center" valign="middle" >k-means</td><td align="center" valign="middle" >DBSCAN</td></tr><tr><td align="center" valign="middle" >SVM+</td><td align="center" valign="middle" >75.53</td><td align="center" valign="middle" >76.47</td><td align="center" valign="middle" >76.66</td><td align="center" valign="middle" >78.12</td><td align="center" valign="middle" >78.28</td><td align="center" valign="middle" >75.37</td><td align="center" valign="middle" >80.53</td><td align="center" valign="middle" >80.91</td></tr><tr><td align="center" valign="middle" >SVM-2K</td><td align="center" valign="middle" >72.02</td><td align="center" valign="middle" >73.64</td><td align="center" valign="middle" >70.15</td><td align="center" valign="middle" >69.58</td><td align="center" valign="middle" >69.82</td><td align="center" valign="middle" >73.26</td><td align="center" valign="middle" >71.47</td><td align="center" valign="middle" >72.24</td></tr><tr><td align="center" valign="middle" >sMIL-PI</td><td align="center" valign="middle" >78.35</td><td align="center" valign="middle" >79.74</td><td align="center" valign="middle" >76.21</td><td align="center" valign="middle" >77.49</td><td align="center" valign="middle" >78.37</td><td align="center" valign="middle" >80.36</td><td align="center" valign="middle" >74.51</td><td align="center" valign="middle" >75.83</td></tr><tr><td align="center" valign="middle" >PSVM-2V</td><td align="center" valign="middle" >80.33</td><td align="center" valign="middle" >81.07</td><td align="center" valign="middle" >82.96</td><td align="center" valign="middle" >81.47</td><td align="center" valign="middle" >79.04</td><td align="center" valign="middle" >80.73</td><td align="center" valign="middle" >82.29</td><td align="center" valign="middle" >83.65</td></tr><tr><td align="center" valign="middle" >所提方法</td><td align="center" valign="middle" >82.44</td><td align="center" valign="middle" >83.80</td><td align="center" valign="middle" >83.56</td><td align="center" valign="middle" >83.92</td><td align="center" valign="middle" >81.32</td><td align="center" valign="middle" >80.69</td><td align="center" valign="middle" >84.22</td><td align="center" valign="middle" >84.61</td></tr></tbody></table></table-wrap><p>表6. 包的数量为1500时的平均精确度(单位：%)</p></sec><sec id="s5_3_2"><title>3.3.2. 参数敏感度分析</title><p>此外，在所提方法中有一些参数，实验分析了不同参数值下的性能变化。实验使用NUS-WIDE数据集做测试，实验结果如图2和图3所示。首先固定<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x137_hanspub.png" xlink:type="simple"/></inline-formula>测试参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x138_hanspub.png" xlink:type="simple"/></inline-formula>。由实验结果可得，如图2所示，在范围<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x139_hanspub.png" xlink:type="simple"/></inline-formula>中，参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x140_hanspub.png" xlink:type="simple"/></inline-formula>在10<sup>−3</sup>到1时，性能上升，在1到10<sup>3</sup>时，性能下降，当参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x141_hanspub.png" xlink:type="simple"/></inline-formula>时，效果最优。接着固定<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x142_hanspub.png" xlink:type="simple"/></inline-formula>测试<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x143_hanspub.png" xlink:type="simple"/></inline-formula>，如图3所示，参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x144_hanspub.png" xlink:type="simple"/></inline-formula>在10<sup>−3</sup>到10时，性能上升，在10到10<sup>3</sup>时，性能下降，当参数<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x145_hanspub.png" xlink:type="simple"/></inline-formula>时，效果最优。</p><p>图2. 参数C<sub>1</sub>和C<sub>2</sub>的敏感度</p><p>图3. 参数D<sub>A</sub>和D<sub>B</sub>的敏感度</p></sec><sec id="s5_3_3"><title>3.3.3. 噪声敏感度测试</title><p>实验还测试算法性能对输入数据噪声的敏感程度，使用NUS-WIDE数据集做测试。将噪声添加到数据示例中方法的基本思想如图4所示。首先计算整个数据沿第i个维度的标准差<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x148_hanspub.png" xlink:type="simple"/></inline-formula>，然后获得高斯噪声的标准偏差<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x149_hanspub.png" xlink:type="simple"/></inline-formula>，其范围随机选自<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x150_hanspub.png" xlink:type="simple"/></inline-formula>。然后，对于第i维，我们将随机分布的噪声与标准差<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x151_hanspub.png" xlink:type="simple"/></inline-formula>相加。这样，在数据示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x152_hanspub.png" xlink:type="simple"/></inline-formula>中添加了噪声，可以将其表示为向量：<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x153_hanspub.png" xlink:type="simple"/></inline-formula>，其中<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x154_hanspub.png" xlink:type="simple"/></inline-formula>表示数据示例<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x155_hanspub.png" xlink:type="simple"/></inline-formula>的维数，<inline-formula><inline-graphic xlink:href="//html.hanspub.org/file/18-1541666x156_hanspub.png" xlink:type="simple"/></inline-formula>表示添加到数据示例第i维的噪声。</p><p>实验中使数据噪声的百分比在0%到30%之间变化。这里以向量加偏移常数方法生成的数据为例，并将噪声添加到每个包的示例中。不同比例的噪声对性能的影响如图5所示，随着噪声水平的提高，所有方法的性能都会下降。但是与其他方法相比，所提方法可以获得更高的精度，对噪声的敏感度较低。</p><p>图4. 将噪声添加到数据示例</p><p>图5. 噪声对算法性能的影响</p></sec><sec id="s5_3_4"><title>3.3.4. 算法运行时间</title><p>各算法的平均运行时间比重如图6所示，所提方法比其他所有方法的时间开销要大得多。这是因为所提方法考虑了示例的相似性，并且同时利用约束条件来拟合原始特征和特权信息。因此，所提方法比其他方法花费更多时间。</p><p>图6. 不同算法的平均运行时间</p></sec></sec></sec><sec id="s6"><title>4. 总结与展望</title><p>针对带有特权信息的图像提出一种基于相似度的两视角多示例图像分类方法，该方法考虑了示例的相似性以及具有特权信息特征和原始特征的约束，提出了一个迭代框架来解决使用特权信息进行多示例学习的问题，该方法可用于带有文本描述的图像分类。实验使用两种聚类算法(k-means和DBSCAN)来处理图像并比较其性能。实验结果表明，所提方法的性能明显优于其他方法。但是，所提方法的时间开销比其他方法都大，优化模型以降低时间开销和降低对噪声的敏感程度是未来的工作。</p></sec><sec id="s7"><title>基金项目</title><p>本文得到国家自然科学基金资助项目(No.61876044)的资助。</p></sec><sec id="s8"><title>文章引用</title><p>尹子健,肖燕珊,刘 波. 基于相似度的两视角多示例图像分类方法研究Research on Two-View Multi-Instance Image Classification Based on Similarity[J]. 计算机科学与应用, 2020, 10(02): 350-360. https://doi.org/10.12677/CSA.2020.102036</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.34301-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Dietterich, T.G., Lathrop, R.H. and Lozano-Pérez, T. (1997) Solving the Multiple Instance Problem with Axis-Parallel Rectangles. Artificial Intelligence, 89, 31-71. &lt;br&gt;https://doi.org/10.1016/S0004-3702(96)00034-3</mixed-citation></ref><ref id="hanspub.34301-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Lu, J. and Ma, S. (2009) Web Image Clustering Based-on Multiple Instance Learning. Computer Research and Development, 46, 1462-1470.</mixed-citation></ref><ref id="hanspub.34301-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Rao, T., Xu, M., Liu, H., Wang, J. and Burnett, I. (2016) Multi-Scale Blocks Based Image Emotion Classification Using Multiple Instance Learning. 2016 IEEE International Conference on Image Processing (ICIP), Phoenix, AZ, 25-28 September 2016, 634-638. &lt;br&gt;https://doi.org/10.1109/ICIP.2016.7532434</mixed-citation></ref><ref id="hanspub.34301-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Duan, L., Li, W., Tsang, I.W. and Xu, D. (2011) Improving Web Image Search by Bag-Based Reranking. IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 20, 3280-3290.&lt;br&gt;https://doi.org/10.1109/TIP.2011.2159227</mixed-citation></ref><ref id="hanspub.34301-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Yao, Y., Shen, F., Zhang, J., Liu, L., Tang, Z. and Shao, L. (2018) Extracting Privileged Information for Enhancing Classifier Learning. IEEE Transactions on Image Processing, 28, 436-450.</mixed-citation></ref><ref id="hanspub.34301-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Guo, Y., Xiao, H., Kan, Y. and Fu, Q. (2018) Learning Using Privileged Information for HRRP-Based Radar Target Recognition. IET Signal Processing, 12, 188-197. &lt;br&gt;https://doi.org/10.1109/MSP.2018.2841413</mixed-citation></ref><ref id="hanspub.34301-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Wu, G., Tian, Y. and Liu, D. (2018) Privileged Multi-Target Support Vector Regression. 2018 24th International Conference on Pattern Recognition, Beijing, China, August 2018. &lt;br&gt;https://doi.org/10.1109/ICPR.2018.8545479</mixed-citation></ref><ref id="hanspub.34301-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Wang, S., Lu, J., Gu, X., et al. (2015) Canonical Principal Angles Correlation Analysis for Two-View Data. Journal of Visual Communication and Image Representation, 35, 209-219. &lt;br&gt;https://doi.org/10.1016/j.jvcir.2015.12.001</mixed-citation></ref><ref id="hanspub.34301-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Guangxia, L.I., Hoi, S.C.H. and Chang, K. (2010) Two-View Transductive Support Vector Machines.</mixed-citation></ref><ref id="hanspub.34301-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Vapnik, V. and Vashist, A. (2009) A New Learning Paradigm: Learning Using Privileged Information. Neural Networks: The Official Journal of the International Neural Network Society, 22, 544-557. &lt;br&gt;https://doi.org/10.1016/j.neunet.2009.06.042</mixed-citation></ref><ref id="hanspub.34301-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Tang, J., Tian, Y., Zhang, P. and Liu, X. (2017) Multiview Privileged Support Vector Machines. IEEE Transactions on Neural Networks and Learning Systems, 29, 3463-3477.</mixed-citation></ref><ref id="hanspub.34301-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Xiao, Y.S., Liu, B., Hao, Z.F. and Cao, L.B. (2014) A Similarity-Based Classification Framework for Multiple-Instance Learning. IEEE Transactions on Cybernetics, 44, 500-515. &lt;br&gt;https://doi.org/10.1109/TCYB.2013.2257749</mixed-citation></ref><ref id="hanspub.34301-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Li, W., Dai, D.X., Tan, M.K., Dong, X. and Van Gool, L. (2016) Fast Algorithms for Linear and Kernel Svm+. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 27-30 June 2016, 2258-2266. &lt;br&gt;https://doi.org/10.1109/CVPR.2016.248</mixed-citation></ref><ref id="hanspub.34301-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Farquhar, J.D.R., Hardoon, D.R., Meng, H., Shawe-Taylor, J. and Szedmak, S. (2005) Two View Learning: SVM-2K, Theory and Practice. Advances in Neural Information Processing Systems 18, Neural Information Processing Systems, NIPS 2005, Vancouver, British Columbia, Canada, 5-8 December 2005.</mixed-citation></ref><ref id="hanspub.34301-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Hartigan, J.A. and Wong, M.A. (2013) A K-Means Clustering Algorithm. Applied Statistics, 28, 100-108.&lt;br&gt;https://doi.org/10.2307/2346830</mixed-citation></ref><ref id="hanspub.34301-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Louhichi, S., Gzara, M. and Ben Abdallah, H. (2014) A Density Based Algorithm for Discovering Clusters with Varied Density. 2014 World Congress on Computer Applications and Information Systems, Hammamet, Tunisia, 17-19 January 2014, 1-6. &lt;br&gt;https://doi.org/10.1109/WCCAIS.2014.6916622</mixed-citation></ref></ref-list></back></article>