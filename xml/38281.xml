<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">AAM</journal-id><journal-title-group><journal-title>Advances in Applied Mathematics</journal-title></journal-title-group><issn pub-type="epub">2324-7991</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/AAM.2020.910203</article-id><article-id pub-id-type="publisher-id">AAM-38281</article-id><article-categories><subj-group subj-group-type="heading"><subject>AAM20201000000_90680685.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>数学与物理</subject></subj-group></article-categories><title-group><article-title>
 
 
  一种支持向量机预处理方法的研究
  Research on a Preprocessing Method of Support Vector Machine
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>韩</surname><given-names>成志</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>李</surname><given-names>梦婷</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>郑</surname><given-names>恩涛</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>马</surname><given-names>国春</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>杭州师范大学理学院，浙江 杭州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>10</month><year>2020</year></pub-date><volume>09</volume><issue>10</issue><fpage>1757</fpage><lpage>1765</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    支持向量机(Support Vector Machine, SVM)在处理大规模数据集时，随着样本维度增高，样本数量增多会出现训练时间显著增多的问题。为了解决该问题，文章提出了一种基于主成分分析(Principal Component Analysis, PCA)和K边界近邻法(K Nearest Bound Neighbor, KNBN)的SVM预处理方法；先用PCA对训练数据降维消除训练数据中的冗余信息，然后利用KNBN预选取训练数据中的支持向量来减少训练数据量。数值实验结果表明，与PCA-SVM、KNBN-SVM和无数据预处理的SVM方法相比，采用本文提出的SVM预处理方法既保持了良好的分类预测精度，又缩短了大量训练时间。
    When the large-scale data set with higher dimension and larger number of samples is processed by the Support Vector Machine (SVM), the training time will increase significantly. In order to solve this problem, based on Principal Component Analysis (PCA) and K Nearest Bound Neighbor (KNBN), a preprocessing method of SVM is proposed. Firstly, PCA is used to reduce the dimension of the training data to eliminate the redundant information in the training data, and then KNBN is used to preselect the support vectors in the training data to reduce the amount of training data. The numerical experiment results show that SVM preprocessing method proposed in this paper, compared with PCA-SVM, KNBN-SVM and SVM without data preprocessing, can not only keep good classification prediction accuracy, but also save a lot of training time. 
  
 
</p></abstract><kwd-group><kwd>支持向量机，主成分分析法，K边界近邻法，预处理，训练时间, Support Vector Machine</kwd><kwd> Principal Component Analysis</kwd><kwd> K Nearest Bound Neighbor</kwd><kwd> Preprocessing</kwd><kwd> Training Time</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>支持向量机(Support Vector Machine, SVM)在处理大规模数据集时，随着样本维度增高，样本数量增多会出现训练时间显著增多的问题。为了解决该问题，文章提出了一种基于主成分分析(Principal Component Analysis, PCA)和K边界近邻法(K Nearest Bound Neighbor, KNBN)的SVM预处理方法；先用PCA对训练数据降维消除训练数据中的冗余信息，然后利用KNBN预选取训练数据中的支持向量来减少训练数据量。数值实验结果表明，与PCA-SVM、KNBN-SVM和无数据预处理的SVM方法相比，采用本文提出的SVM预处理方法既保持了良好的分类预测精度，又缩短了大量训练时间。</p></sec><sec id="s2"><title>关键词</title><p>支持向量机，主成分分析法，K边界近邻法，预处理，训练时间</p></sec><sec id="s3"><title>Research on a Preprocessing Method of Support Vector Machine<sup> </sup></title><p>Chengzhi Han<sup>*</sup>, Mengting Li, Entao Zheng, Guochun Ma<sup>#</sup></p><p>College of Science, Hangzhou Normal University, Hangzhou Zhejiang</p><p><img src="//html.hanspub.org/file/11-2621334x4_hanspub.png" /></p><p>Received: Oct. 7<sup>th</sup>, 2020; accepted: Oct. 20<sup>th</sup>, 2020; published: Oct. 27<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/11-2621334x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>When the large-scale data set with higher dimension and larger number of samples is processed by the Support Vector Machine (SVM), the training time will increase significantly. In order to solve this problem, based on Principal Component Analysis (PCA) and K Nearest Bound Neighbor (KNBN), a preprocessing method of SVM is proposed. Firstly, PCA is used to reduce the dimension of the training data to eliminate the redundant information in the training data, and then KNBN is used to preselect the support vectors in the training data to reduce the amount of training data. The numerical experiment results show that SVM preprocessing method proposed in this paper, compared with PCA-SVM, KNBN-SVM and SVM without data preprocessing, can not only keep good classification prediction accuracy, but also save a lot of training time.</p><p>Keywords:Support Vector Machine, Principal Component Analysis, K Nearest Bound Neighbor, Preprocessing, Training Time</p><disp-formula id="hanspub.38281-formula12"><graphic xlink:href="//html.hanspub.org/file/11-2621334x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/11-2621334x8_hanspub.png" /> <img src="//html.hanspub.org/file/11-2621334x9_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>支持向量机(Support Vector Machine, SVM)最早在1964年被Vapnik和Cortes等人提出的，它是一个有监督学习的二分类模型 [<xref ref-type="bibr" rid="hanspub.38281-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.38281-ref2">2</xref>] [<xref ref-type="bibr" rid="hanspub.38281-ref3">3</xref>]。在上世纪90年代SVM得到快速发展并衍生出一系列改进和扩展的算法。它被广泛应用在人像识别 [<xref ref-type="bibr" rid="hanspub.38281-ref4">4</xref>] 和文本分类 [<xref ref-type="bibr" rid="hanspub.38281-ref5">5</xref>] 等模式识别问题中。</p><p>SVM的学习策略是求解能够将训练数据集按照类别正确划分并且使几何间隔最大化的分离超平面，可转化为求解一个凸二次规划问题 [<xref ref-type="bibr" rid="hanspub.38281-ref3">3</xref>]。一般情况下，随着训练样本规模的扩大，即样本维度升高和样本数量增多，会导致SVM的训练时间延长，分类精度可能下降，存储空间也有所增加。如何对SVM数据进行预处理来提高SVM分类效率成为近年来的一个研究热点。</p><p>对SVM数据降维消除其中的冗余特征信息，可以起到提高SVM分类精度和减少SVM计算量的效果。PCA通过提取最能代表原始数据本质特征的特征因子来实现数据降维。目前有关PCA和SVM结合方面的研究有很多。2018年，余金澳 [<xref ref-type="bibr" rid="hanspub.38281-ref6">6</xref>] 等人提出一种面向方位敏感性的PCA-SVM分类识别方法，与传统的SVM分类方法相比，该方法对SAR图像的地面目标具有较高的分类识别率和运行效率。2018年，Anis Ben Aicha [<xref ref-type="bibr" rid="hanspub.38281-ref7">7</xref>] 等人利用PCA提取最关键、最相关的特征，采用SVM作为分类技术，实现正常肿瘤和癌前肿瘤的鉴别，此方法具有较好的敏感性、特异性、精密度和准确性。2019年，汪雯琦 [<xref ref-type="bibr" rid="hanspub.38281-ref8">8</xref>] 等人提出基于PCA和SVM分类的跨年龄人脸识别方法，该方法具有速度快和准确度高的优点。2019年，Wang [<xref ref-type="bibr" rid="hanspub.38281-ref9">9</xref>] 等人提出基于机器视觉的有色金属报废车辆分离系统的分类算法及运行参数优化研究，其中利用了PCA-SVM使得识别精度较高，计算速度足够快。</p><p>SVM的最优分离超平面由只占全体训练数据集一小部分的支持向量来确定 [<xref ref-type="bibr" rid="hanspub.38281-ref10">10</xref>]，通过预选取有可能成为支持向量的样本，舍弃非支持向量来减少训练样本数量，进而大幅度缩短SVM训练时间。从几何方面来看，线性可分情况下支持向量主要分布在两类样本的边界上且彼此之间靠的很近的那些样本 [<xref ref-type="bibr" rid="hanspub.38281-ref11">11</xref>]，因此可以提取那些异类样本之间离得很近的边界向量作为支持向量候选集，最后将其作为训练样本集进行SVM训练，可以减少计算量，提升训练速度。一些学者在支持向量预选取方面进行了研究。2008年，Zhang [<xref ref-type="bibr" rid="hanspub.38281-ref10">10</xref>] 提出基于KNN法的支持向量预选取方法，计算距离每个样本最近的k个样本，若k个样本中至少有一个异类样本，则这k个样本是边界向量。2009年，徐红敏 [<xref ref-type="bibr" rid="hanspub.38281-ref11">11</xref>] 等人提出一种支持向量机快速分类算法，在两类样本中选取与每类样本中每个样本最近的异类样本作为边界向量。2013年，胡志军 [<xref ref-type="bibr" rid="hanspub.38281-ref12">12</xref>] 等人提出基于距离排序的快速支持向量机分类算法，先计算每类样本与异类样本类中心的距离，再按照距离大小排序选取一定比例的小距离样本作为候选支持向量。2013年，李庆 [<xref ref-type="bibr" rid="hanspub.38281-ref13">13</xref>] 等人提出K边界近邻法支持向量预选取方法，在两类样本中选取与每类样本中每个样本最近的k个异类样本作为边界向量。</p><p>上述研究是从降低样本的维度方面或是从减少样本数量方面解决问题，比较片面性，不适用于同时维度高和数量多的大规模数据集预处理。为了克服这个问题，本文首先选用PCA对SVM大规模数据集降维，然后用KNBN在降维后的数据集上预选取支持向量得到一个约简集，称该方法为基于PCA和KNBN的SVM预处理方法。该SVM预处理方法能够结合PCA和KNBN的优点，拥有较高的分类预测精度，而且其训练时间大量缩短。</p><p>本文接下来第2节是主成分分析法的简介，第3节是K边界近邻法支持向量预选取方法的描述，第4节介绍基于PCA和KNBN的SVM预处理方法，第5节是数值实验结果及比较。</p></sec><sec id="s6"><title>2. 主成分分析法简介</title><p>主成分分析的基本原理是将原来变量重新组合成一组新的线性无关的几个变量，同时根据实际需要从中可以取出几个较少的变量尽可能多地反映原来变量的信息的统计方法，即在原有样本的M维空间内，用M个标准正交基进行重新映射，然后选取其中最重要的D个正交基进行保留，而在这D个正交基坐标轴上的坐标值就是原有样本映射到低维后的坐标。设原始样本矩阵为 X = [ x 1 x 2 ⋯ x N ] T ，其中 x i ∈ R M ，M是样本的维度， i = 1 , 2 , ⋯ , N ，矩阵X是一个N行M列矩阵。</p><p>下面是PCA降维的详细步骤：</p><p>1) 原始样本中心化，形成矩阵Y。Y中每个元素为：</p><p>y i j = x i j − x &#175; j ∈ Y ， (1)</p><p>其中 x &#175; j 是样本各维分量的平均值， j = 1 , 2 , ⋯ , M ，矩阵Y是N行M列矩阵。</p><p>2) 计算协方差矩阵R：</p><p>R = 1 N − 1 Y T Y ， (2)</p><p>其中 Y T 是Y的转置矩阵，矩阵R是M行M列矩阵。</p><p>3) 求协方差矩阵R的特征值和特征向量，并将特征值按大小进行排序得：</p><p>λ 1 ≥ λ 2 ≥ ⋯ ≥ λ M ≥ 0 ， (3)</p><p>排序之后的特征值对应的特征向量为：</p><p>e 1 , e 2 , ⋯ , e M ， (4)</p><p>其中 e j ∈ R M ， j = 1 , 2 , ⋯ , M 。</p><p>4) 计算特征值累计贡献率 η ，</p><p>η = ∑ i = 1 D λ i ∑ i = 1 M λ i &#215; 100 % ， (5)</p><p>其中 0 ≤ η ≤ 100 % ， 0 ≤ D ≤ M ，D为正整数。选取使得特征值累计贡献率大于 η 时的最小整数D，这时将前D个较大特征值对应的特征向量作为标准正交基矩阵E，</p><p>E = [ e 1 e 2 ⋯ e D ] ， (6)</p><p>矩阵E是M行D列矩阵。原始样本在标准正交基下的投影，即降维过后的样本矩阵 X * 为：</p><p>X * = X E ， (7)</p><p>矩阵 X * 是N行D列矩阵，原始样本便从M维降至D维。</p></sec><sec id="s7"><title>3. K边界近邻法支持向量预选取方法简介</title><p>K边界近邻法支持向量预选取的基本思想是通过选取距离每个样本最近的k个异类样本来构造支持向量候选集。已知训练样本集分为两类，正类样本 T 1 和负类样本 T 2 ，</p><p>T 1 = { x 1 + , x 2 + , ⋯ , x N 1 + } ， (8)</p><p>T 2 = { x 1 − , x 2 − , ⋯ , x N 2 − } ， (9)</p><p>其中 x i + , x j − ∈ R M ， i = 1 , 2 , ⋯ , N 1 ， j = 1 , 2 , ⋯ , N 2 ， N 1 是正类样本的个数， N 2 是负类样本的个数，M是样本的维度。</p><sec id="s7_1"><title>3.1. 样本距离</title><p>当两类样本线性可分时，两类样本之间距离可用欧氏距离来表示：</p><p>d ( x i + , x j − ) = ‖ x i + − x j − ‖ 2 ， (10)</p><p>当两类样本非线性可分时，通过映射函数 ϕ ( ⋅ ) 将原输入空间映射到高维的特征空间中，样本在高维空间中变得线性可分，这时的样本距离被称为非线性距离：</p><p>d ( x i + , x j − ) = K ( x i + , x i + ) − 2 K ( x i + , x j − ) + K ( x j − , x j − ) ， (11)</p><p>其中 K ( ⋅ , ⋅ ) 是核函数。高斯核函数是一种常用的核函数：</p><p>K ( x i + , x j − ) = exp ( − ‖ x i + − x j − ‖ 2 / 2 σ 2 ) ， (12)</p><p>其中 σ 是一个常数，采用高斯核函数时，非线性距离变为：</p><p>d ( x i + , x j − ) = 2 − 2 K ( x i + , x j − ) 。 (13)</p></sec><sec id="s7_2"><title>3.2. KNBN支持向量预选取方法</title><p>下面是KNBN方法的具体步骤：</p><p>1) 从正类样本中选择一个样本，求其与所有负类样本之间的距离，保留最近的k个负类样本，将他们放入边界向量集当中。</p><p>2) 返回步骤(1)，直至遍历所有的正类样本截止。</p><p>3) 将所有负类样本按照步骤(1)和步骤(2)操作，保留离每个负类样本最近的k个正类样本，将他们也放入支持向量候选集当中。</p><p>4) 把上面得到的边界向量集当中的相同样本删去，进行唯一化处理，最终得到支持向量候选集。</p><p>如图1所示，其中有两类样本，当 k = 4 时使用KNBN支持向量预选取方法得到边界向量集，适当选取k值，边界向量集一定能包含所有的支持向量，这样便构造出一个支持向量候选集。</p><p>图1. KNBN支持向量预选取示意图</p></sec></sec><sec id="s8"><title>4. 基于PCA和KNBN的SVM预处理方法</title><sec id="s8_1"><title>4.1. 基于PCA和KNBN的SVM预处理方法介绍</title><p>根据PCA降维的特征和KNBN支持向量预选取的特性，本文提出基于PCA和KNBN的SVM预处理方法。为了尽可能保存数据的原有结构信息，所以该数据预处理方法先将训练数据集进行PCA降维处理，再把降维过后的训练数据集进行KNBN支持向量预选取，最终得到一个SVM大规模数据集的约简集。本文所提算法的流程图如图2所示。</p><p>下面是基于PCA和KNBN的SVM预处理算法的具体描述。</p><p>算法：基于PCA和KNBN的SVM预处理算法。</p><p>输入：原始训练数据集 T = { ( x 1 , y 1 ) , ( x 2 , y 2 ) , ⋯ , ( x N , y N ) } ，其中 x i ∈ R M ， y i ∈ { − 1 , 1 } ， i = 1 , 2 , ⋯ , N ，PCA的特征值累计贡献率 η ，KNBN的参数值k。</p><p>输出：原始训练数据集经PCA和KNBN处理过后的约简集S。</p><p>Step 1. 将原始训练数据集T进行PCA降维。</p><p>1) 将T去掉类别特征，形成训练样本矩阵X。</p><p>2) 根据X构建中心化矩阵Y，Y中的每个元素如式(1)所示。</p><p>3) 按照式(2)构建协方差矩阵R。</p><p>4) 求协方差矩阵R的特征值及对应的特征项量，并对特征值按照从大到小的顺序进行排序。</p><p>5) 按照公式(5)计算特征值累计贡献率 η ，确定D值，将这D个最大特征值对应的特征向量构成新基矩阵E，将训练样本矩阵X乘以E，即可得到降维后训练样本样本矩阵 X * ，添上正负类别属性后形成降维后的训练数据集 T * 。</p><p>Step 2. 在降维后的训练数据集 T * 进行KNBN支持向量预选取。</p><p>1) 将数据集 T * 分为正类数据集 T 1 和负类数据集 T 2 。</p><p>2) 从 T 1 中选取一个样本，求其与所有负类样本间的距离，保留与其最近的k个负类样本作为在正类样本约束下的负类边界向量，直至遍历 T 1 ，形成负类边界向量集 S 2 。</p><p>3) 同样，从 T 2 中选取一个样本，求其与所有正类样本间的距离，保留与其最近的k个正类样本作为在负类样本约束下的正类边界向量，直至遍历 T 2 ，形成正类边界向量集 S 1 。</p><p>4) 将 S 1 和 S 2 合并， S * = S 1 ∪ S 2 ，并对 S * 进行唯一化处理，即删除相同的样本，可得到 T * 的支持向量候选集为S，即得到原始训练数据集T的约简集S。</p><p>图2. 基于PCA和KNBN的SVM预处理方法流程图</p></sec><sec id="s8_2"><title>4.2. 算法复杂度分析</title><p>设原始训练样本数量为N，样本维度为M， M &lt; N 。经PCA降维后样本维度为D， D &lt; M ，样本数量不变，其中正类样本数量为 N 1 ，负类样本数量为 N 2 ， N 1 + N 2 = N ；降维后的训练样本经KNBN支持向量预选取之后得到的约简集中样本数量为l，一般情况下， l ≪ N 。基于PCA和KNBN的SVM预处理算法复杂度是由PCA，KNBN和约简集进行SVM训练的时间复杂度共同决定的。</p><p>1) 该预处理方法Step 1的PCA降维过程中，去掉类别特征形成训练样本矩阵的运算次数可以忽略不计，构建中心化矩阵时需要计算2MN次，计算协方差矩阵时需要运算M<sup>2</sup>N次，协方差矩阵的特征值分解时需要M<sup>3</sup>次运算 [<xref ref-type="bibr" rid="hanspub.38281-ref14">14</xref>]，构建新基矩阵需要将特征值按大小进行排序，排序 [<xref ref-type="bibr" rid="hanspub.38281-ref15">15</xref>] 时需要 M log M 次运算，计算累计贡献率 η 和确定D值需要 ( M + 3 D − 2 ) 次运算，构造降维后的新矩阵需要进行 ( 2 D M N − D N ) 次运算。综合上述过程，PCA的总运算次数为：</p><p>M 2 N + 2 D M N + 2 M N − D N + M 3 + M log M + M + 3 D − 2 ，(14)</p><p>其中 D &lt; M ，则PCA的时间复杂度为 O ( M 2 N ) 。</p><p>2) 该预处理方法Step 2的KNBN支持向量预选取过程中，正负类数据集的分开需要进行N次运算，构造边界向量集这一过程中，对一个正类样本求其与所有负类样本之间距离，通常情况下需使用非线性距离，其中核函数采用高斯核函数，则需要 ( 3 D + 9 ) N 2 次运算，再将求得的 N 2 个距离进行排序时需要 N 2 log N 2 次运算，直至遍历所有正类样本后截止，则构造负类样本边界向量集需要运算 ( N 1 N 2 log N 2 + 3 D N 2 + 9 N 2 ) 次，按照上述过程在构造正类样本边界向量集时则需要运算 ( N 1 N 2 log N 1 + 3 D N 1 + 9 N 1 ) 次，下面唯一化处理过程的运算次数可忽略不计。KNBN过程总运算次数为：</p><p>N 1 N 2 log N 1 N 2 + ( 3 D + 9 ) ( N 1 + N 2 ) ， (15)</p><p>其中 N 1 + N 2 = N ， N 1 N 2 ≤ N 2 / 4 ，则KNBN的时间复杂度为 O ( N 2 log N ) 。</p><p>3) 一般情况下，标准SVM的时间复杂度 [<xref ref-type="bibr" rid="hanspub.38281-ref16">16</xref>] 是 O ( N 3 ) 。原始训练样本经PCA-KNBN预处理后的得到的约简集有l个样本，所以约简集进行SVM训练的时间复杂度为 O ( l 3 ) 。</p><p>综上所得，基于PCA和KNBN的SVM预处理算法复杂度为 O ( N 2 log N + M 2 N + l 3 ) ，所以在某种情况下，如原始训练数据集经过PCA-KNBN处理过后的约简集规模很小时，则SVM训练的时间是可以缩短的。</p></sec></sec><sec id="s9"><title>5. 与现有方法的数值实验</title><p>PCA-KNBN-SVM表示基于PCA和KNBN预处理的标准SVM，PCA-SVM [<xref ref-type="bibr" rid="hanspub.38281-ref8">8</xref>] 表示只进行PCA降维预处理的标准SVM，KNBN-SVM [<xref ref-type="bibr" rid="hanspub.38281-ref13">13</xref>] 表示只进行KNBN支持向量预选取预处理的标准SVM。为了验证基于PCA和KNBN的SVM预处理算法的有效性，将PCA-KNBN-SVM与PCA-SVM，KNBN-SVM和无数据预处理的标准SVM进行比较。实验采用Matlab R2018a，在2.3 GHz，Pentium，Dual CPU，4 GB内存的硬件平台上进行。SVM训练选用Libsvm-3.24函数包，其中核函数采用高斯核函数，取 σ = 1.3 。</p><sec id="s9_1"><title>5.1. 数据介绍</title><p>实验采用UCI数据库中的Polish companies bankruptcy data数据集 [<xref ref-type="bibr" rid="hanspub.38281-ref17">17</xref>]。该数据集有5个适合二分类的数据样本，分别是1 year，2 year，3 year，4 year和5 year数据，每个数据都有64个特征属性和1个类别属性。每个数据集的训练样本数量和测试样本数量具体情况如表1所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Polish company bankruptcy data se</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >训练样本/个</th><th align="center" valign="middle" >测试样本/个</th></tr></thead><tr><td align="center" valign="middle" >1 year</td><td align="center" valign="middle" >2800</td><td align="center" valign="middle" >4200</td></tr><tr><td align="center" valign="middle" >2 year</td><td align="center" valign="middle" >3500</td><td align="center" valign="middle" >6500</td></tr><tr><td align="center" valign="middle" >3 year</td><td align="center" valign="middle" >4000</td><td align="center" valign="middle" >6500</td></tr><tr><td align="center" valign="middle" >4 year</td><td align="center" valign="middle" >3000</td><td align="center" valign="middle" >6700</td></tr><tr><td align="center" valign="middle" >5 year</td><td align="center" valign="middle" >3000</td><td align="center" valign="middle" >2900</td></tr></tbody></table></table-wrap><p>表1. 波兰公司破产数据集</p></sec><sec id="s9_2"><title>5.2. 数值实验</title><p>本文分别采用无预处理的标准SVM，PCA-SVM，KNBN-SVM和PCA-KNBN-SVM对5个数据集进行训练和测试，其中KNBN算法中使用的样本距离为非线性距离式(12)所示，其中核函数参数值 σ = 1.3 。PCA降维过程中，特征值累计贡献率设置为 η = 99.5 % ，由此确定的5个数据集中的PCA参数值D如表2所示。KNBN支持向量预选取的参数值 [<xref ref-type="bibr" rid="hanspub.38281-ref13">13</xref>] 设置为 k = 4 。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Numerical experiment parameter value lis</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >1 year</th><th align="center" valign="middle" >2 year</th><th align="center" valign="middle" >3 year</th><th align="center" valign="middle" >4 year</th><th align="center" valign="middle" >5 year</th></tr></thead><tr><td align="center" valign="middle" >D</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >4</td></tr></tbody></table></table-wrap><p>表2. 数值实验参数值列表</p><p>实验结果记录的是各算法对应各数据集的训练时间和预测准确度。其中训练时间包括数据预处理时间和SVM训练的时间。数值实验结果如表3所示，其中数值结果是100次独立数值实验结果的平均值。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> List of numerical experiment result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >算法</th><th align="center" valign="middle" >训练时间/s</th><th align="center" valign="middle" >分类准确度/%</th></tr></thead><tr><td align="center" valign="middle"  rowspan="4"  >1 year</td><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >6.7661</td><td align="center" valign="middle" >96.2143</td></tr><tr><td align="center" valign="middle" >PCA-SVM</td><td align="center" valign="middle" >3.3170</td><td align="center" valign="middle" >96.2381</td></tr><tr><td align="center" valign="middle" >KNBN-SVM</td><td align="center" valign="middle" >3.6625</td><td align="center" valign="middle" >96.0810</td></tr><tr><td align="center" valign="middle" >PCA-KNBN-SVM</td><td align="center" valign="middle" >1.3604</td><td align="center" valign="middle" >96.1542</td></tr><tr><td align="center" valign="middle"  rowspan="4"  >2 year</td><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >11.3482</td><td align="center" valign="middle" >96.0831</td></tr><tr><td align="center" valign="middle" >PCA-SVM</td><td align="center" valign="middle" >5.3114</td><td align="center" valign="middle" >96.1538</td></tr><tr><td align="center" valign="middle" >KNBN-SVM</td><td align="center" valign="middle" >6.0901</td><td align="center" valign="middle" >96.0308</td></tr><tr><td align="center" valign="middle" >PCA-KNBN-SVM</td><td align="center" valign="middle" >1.9551</td><td align="center" valign="middle" >94.0554</td></tr><tr><td align="center" valign="middle"  rowspan="4"  >3 year</td><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >9.0499</td><td align="center" valign="middle" >95.2400</td></tr><tr><td align="center" valign="middle" >PCA-SVM</td><td align="center" valign="middle" >4.3659</td><td align="center" valign="middle" >95.2877</td></tr><tr><td align="center" valign="middle" >KNBN-SVM</td><td align="center" valign="middle" >4.9939</td><td align="center" valign="middle" >95.1385</td></tr><tr><td align="center" valign="middle" >PCA-KNBN-SVM</td><td align="center" valign="middle" >1.4137</td><td align="center" valign="middle" >95.2154</td></tr><tr><td align="center" valign="middle"  rowspan="4"  >4 year</td><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >6.6030</td><td align="center" valign="middle" >94.7493</td></tr><tr><td align="center" valign="middle" >PCA-SVM</td><td align="center" valign="middle" >3.2719</td><td align="center" valign="middle" >94.8358</td></tr><tr><td align="center" valign="middle" >KNBN-SVM</td><td align="center" valign="middle" >4.1297</td><td align="center" valign="middle" >94.7164</td></tr><tr><td align="center" valign="middle" >PCA-KNBN-SVM</td><td align="center" valign="middle" >1.2211</td><td align="center" valign="middle" >94.7910</td></tr><tr><td align="center" valign="middle"  rowspan="4"  >5 year</td><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >3.9666</td><td align="center" valign="middle" >93.1241</td></tr><tr><td align="center" valign="middle" >PCA-SVM</td><td align="center" valign="middle" >2.0605</td><td align="center" valign="middle" >93.1724</td></tr><tr><td align="center" valign="middle" >KNBN-SVM</td><td align="center" valign="middle" >2.9333</td><td align="center" valign="middle" >92.8345</td></tr><tr><td align="center" valign="middle" >PCA-KNBN-SVM</td><td align="center" valign="middle" >0.8076</td><td align="center" valign="middle" >93.1310</td></tr></tbody></table></table-wrap><p>表3. 数值实验结果列表</p><p>从表3中可以观察出，各算法在训练时间和分类准确度方面具有差异性，下面对它们进行分析比较。</p><p>在训练时间方面，使用PCA-SVM算法对每个数据集进行实验，其训练时间比无数据预处理的标准SVM要低很多，平均低了51%左右；KNBN-SVM算法的训练时间相对于无数据预处理的标准SVM也下降了很多，平均下降41%左右；本文提出的PCA-KNBN-SVM算法训练时间相对于无数据预处理的标准SVM平均下降了86%左右，相对于PCA-SVM平均下降了63%左右，相对于KNBN-SVM平均下降了69%左右。</p><p>在分类准确度方面，PCA-SVM算法要比无数据预处理的标准SVM稍高一些，平均高了0.06%左右；KNBN-SVM算法要比无数据预处理的标准SVM偏低一些，平均低了0.13%左右；PCA-KNBN-SVM算法相对于PCA-KNBN平均下降了0.15%左右，相对于KNBN-SVM平均升高了0.18%左右，而相对于无数据预处理的标准SVM在1 year，2 year和3 year数据中有些许降低，但在4 year和5 year数据有些许升高。</p><p>综合来看，PCA-KNBN-SVM算法相对于无预处理的SVM，PCA-SVM和KNBN-SVM，其训练时间缩短效果很明显；在分类准确度方面相对于PCA-SVM有所下降，相对于KNBN-SVM却有所提高，而相对于无预处理的SVM在某些数据中分类准确度稍高或稍低一些，总而言之，PCA-KNBN-SVM算法相对于其他算法在训练时间方面缩短效果很明显，而在分类准确度方面变化不大。</p></sec></sec><sec id="s10"><title>6. 结语</title><p>本文为了解决SVM在遇到大规模的数据集时出现训练时间增多和分类精度可能下降的问题，提出了一种基于PCA和KNBN的SVM预处理方法。通过选取UCI数据库中的Polish companies bankruptcy data 数据集进行数值实验，从数据实验结果观察得出在该预处理方法下的SVM相对于无预处理的SVM，只进行PCA预处理的SVM和只进行KNBN预处理的SVM有大幅度缩减训练时间的效果，在分类准确度方面相对于其他算法变化不大。结果表明基于PCA和KNBN的SVM预处理方法是可以在保持良好分类精度下提高训练速度的一种SVM预处理好方法。</p></sec><sec id="s11"><title>文章引用</title><p>韩成志,李梦婷,郑恩涛,马国春. 一种支持向量机预处理方法的研究Research on a Preprocessing Method of Support Vector Machine[J]. 应用数学进展, 2020, 09(10): 1757-1765. https://doi.org/10.12677/AAM.2020.910203</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.38281-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Vapnik, V. (1998) Statistical Learning Theory. Vol. 3, Chapter 10-11, Wiley, New York, 401-492.</mixed-citation></ref><ref id="hanspub.38281-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">周志华. 机器学习[M]. 北京: 清华大学出版社, 2016: 121-139, 298-300.</mixed-citation></ref><ref id="hanspub.38281-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012: 第7章, 95-135.</mixed-citation></ref><ref id="hanspub.38281-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Qin, J. and He, Z.S. (2005) A SVM Face Recognition Method Based on Gabor-Featured Key Points. Proceedings of 2005 International Conference on Machine Learning and Cybernetics, 8, 5144-5149.  
&lt;br&gt;https://doi.org/10.1109/ICMLC.2005.1527850</mixed-citation></ref><ref id="hanspub.38281-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Sun, A., Lim, E.P. and Ng, W.K. (2002) Web Classification Using Support Vector Machine. Proceedings of the 4th International Workshop on Web Information and Data Management, McLean, Virginia, November 2002, 96-99.  
&lt;br&gt;https://doi.org/10.1145/584931.584952</mixed-citation></ref><ref id="hanspub.38281-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">余金澳, 吴彦鸿. 一种面向方位敏感性的PCA-SVM分类识别方法[J]. 无线电工程, 2018, 48(2): 83-87.</mixed-citation></ref><ref id="hanspub.38281-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Aicha, A.B. (2018) Noninvasive Detection of Potentially Precancerous Lesions of Vocal Fold Based on Glottal Wave Signal and SVM Approaches. Procedia Computer Science, 126, 586-595. &lt;br&gt;https://doi.org/10.1016/j.procs.2018.07.293</mixed-citation></ref><ref id="hanspub.38281-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">汪雯琦, 高广阔. 基于PCA和SVM分类的跨年龄人脸识别[J]. 计算机时代, 2019(7): 1-4+8.</mixed-citation></ref><ref id="hanspub.38281-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Wang, C., Hu, Z.L., Pang, Q. and Hua, L. (2019) Research on the Classification Algorithm and Operation Parameters Optimization of the System for Separating Non-Ferrous Metals from End-of-Life Vehicles Based on Machine Vision. Waste Management, 100, 10-17. &lt;br&gt;https://doi.org/10.1016/j.wasman.2019.08.043</mixed-citation></ref><ref id="hanspub.38281-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, L., et al. (2008) Support Vectors Pre-Extracting for Support Vector Machine Based on K Nearest Neighbour Method. Proceedings of IEEE International Conference on Information and Automation, Zhangjiajie, 20-23 June 2008, 1353-1358.</mixed-citation></ref><ref id="hanspub.38281-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">徐红敏, 王若鹏, 张怀念. 支持向量机的快速分类算法[J]. 北京石油化工学院学报, 2009, 17(4): 55-58.</mixed-citation></ref><ref id="hanspub.38281-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">胡志军, 王鸿斌, 张惠斌. 基于距离排序的快速支持向量机分类算法[J]. 计算机应用与软件, 2013, 30(4): 85-87+100.</mixed-citation></ref><ref id="hanspub.38281-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">李庆, 胡捍英. 支持向量预选取的K边界近邻法[J]. 电路与系统学报, 2013, 18(2): 91-96.</mixed-citation></ref><ref id="hanspub.38281-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">万静, 吴凡, 何云斌, 李松. 新的降维标准下的高维数据聚类算法[J]. 计算机科学与探索, 2020, 14(1): 96-107.</mixed-citation></ref><ref id="hanspub.38281-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">陆微微, 刘晶. 一种提高K-近邻算法效率的新算法[J]. 计算机工程与应用, 2008, 44(4): 163-165+178.</mixed-citation></ref><ref id="hanspub.38281-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Tsang, I.W., Kwok, J.T. and Cheung, P.-M. (2005) Core Vector Machines: Fast SVM Training on Very Large Data Sets. The Journal of Machine Learning Research, 6, 363-392.</mixed-citation></ref><ref id="hanspub.38281-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Tomczak, S. Polish Companies Bankruptcy Data. Data Set. 
http://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data, 2020-09-25.</mixed-citation></ref></ref-list></back></article>