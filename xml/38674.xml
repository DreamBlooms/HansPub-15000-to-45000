<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2020.1011207</article-id><article-id pub-id-type="publisher-id">CSA-38674</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20201100000_89960228.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于异形卷积核的卷积神经网络图像分类方法
  Based on the Heterogeneous Convolution Kernel Image Classification of Convolutional Neural Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>马</surname><given-names>双</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>董</surname><given-names>安国</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>长鹏</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>长安大学理学院，陕西 西安</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>09</day><month>11</month><year>2020</year></pub-date><volume>10</volume><issue>11</issue><fpage>1962</fpage><lpage>1970</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   卷积神经网络具有强大的图像特征学习能力，在机器学习问题中取得了突破性的进展。针对目前卷积神经网络方法中的不足之处，创新性提出了异形卷积核的概念，在不改变卷积核参数数量的情况下通过改变卷积核的形状，扩大其感受野，提高网络提取图像特征的能力。通过图像移位的方法，解决了异形卷积核可行性问题。构建了一种融合异形卷积核和矩形卷积核的卷积神经网络，使得异形卷积核能够与传统卷积神经网络有效结合。实验结果表明，相较于传统的卷积神经网络，结合异形卷积核的卷积神经网络具有更高的分类精度。 With a strong ability to learn image features, convolution neural network has made a breakthrough in machine learning. Directing at the shortcomings of the current convolution neural network methods, the concept of heterogeneous convolution kernel is creatively proposed by changing the shape of the convolution kernel, expanding its receptive field and improving the ability of the network to extract image features, without changing the number of convolution kernel parameters. Through the method of image shift, the feasibility of heterogeneous convolution kernel is solved. The convolution neural network which combines the heterogeneous convolution kernel and the rectangular convolution kernel is constructed in order that the heterogeneous convolution kernel can be effectively combined with the traditional convolution neural network. The experimental results show that, compared with the traditional convolution neural network, the convolution neural network combined with special-shaped convolution kernel has higher classification accuracy. 
  
 
</p></abstract><kwd-group><kwd>卷积神经网络，卷积核，异形卷积核，图像分类，感受野, Convolutional Neural Network</kwd><kwd> Convolution Kernel</kwd><kwd> Heterogeneous Convolution Kernel</kwd><kwd> Image Classification</kwd><kwd> Receptive Field</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>卷积神经网络具有强大的图像特征学习能力，在机器学习问题中取得了突破性的进展。针对目前卷积神经网络方法中的不足之处，创新性提出了异形卷积核的概念，在不改变卷积核参数数量的情况下通过改变卷积核的形状，扩大其感受野，提高网络提取图像特征的能力。通过图像移位的方法，解决了异形卷积核可行性问题。构建了一种融合异形卷积核和矩形卷积核的卷积神经网络，使得异形卷积核能够与传统卷积神经网络有效结合。实验结果表明，相较于传统的卷积神经网络，结合异形卷积核的卷积神经网络具有更高的分类精度。</p></sec><sec id="s2"><title>关键词</title><p>卷积神经网络，卷积核，异形卷积核，图像分类，感受野</p></sec><sec id="s3"><title>Based on the Heterogeneous Convolution Kernel Image Classification of Convolutional Neural Network</title><p>Shaung Ma, Anguo Dong, Changpeng Wang</p><p>College of Faculty of Science, Chang’an University, Xi’an Shaanxi</p><p><img src="//html.hanspub.org/file/5-1541905x4_hanspub.png" /></p><p>Received: Oct. 28<sup>th</sup>, 2020; accepted: Nov. 12<sup>th</sup>, 2020; published: Nov. 19<sup>th</sup>, 2020</p><p><img src="//html.hanspub.org/file/5-1541905x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>With a strong ability to learn image features, convolution neural network has made a breakthrough in machine learning. Directing at the shortcomings of the current convolution neural network methods, the concept of heterogeneous convolution kernel is creatively proposed by changing the shape of the convolution kernel, expanding its receptive field and improving the ability of the network to extract image features, without changing the number of convolution kernel parameters. Through the method of image shift, the feasibility of heterogeneous convolution kernel is solved. The convolution neural network which combines the heterogeneous convolution kernel and the rectangular convolution kernel is constructed in order that the heterogeneous convolution kernel can be effectively combined with the traditional convolution neural network. The experimental results show that, compared with the traditional convolution neural network, the convolution neural network combined with special-shaped convolution kernel has higher classification accuracy.</p><p>Keywords:Convolutional Neural Network, Convolution Kernel, Heterogeneous Convolution Kernel, Image Classification, Receptive Field</p><disp-formula id="hanspub.38674-formula9"><graphic xlink:href="//html.hanspub.org/file/5-1541905x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2020 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/5-1541905x7_hanspub.png" /> <img src="//html.hanspub.org/file/5-1541905x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>如今图像的特征提取与分类已经成为计算机视觉中最重要的研究领域之一，卷积神经网络(Convolutional Neural Network, CNN)更是依赖其良好的特征提取能力和泛化能力，在其领域获得了巨大成功。卷积神经网络的快速发展一方面得益于计算机性能的大幅提升，使得构建并训练更大规模的网络不再受到硬件水平的限制；另一方面得益于大规模标注数据的增长，增强了网络的泛化能力Lecun等 [<xref ref-type="bibr" rid="hanspub.38674-ref1">1</xref>] 提出Le Net-5模型后更加积淀了卷积神经网络在图像处理领域的重要性。继Lecun之后，人们又对卷积神经网络的结构和内部方法做了许多改动和优化，比如Krizhevsky等 [<xref ref-type="bibr" rid="hanspub.38674-ref2">2</xref>] 提出的Alex Net中使用了Relu激活函数来代替Sidmorg函数，并首次引入Doupout来增强网络的泛化能力。在这之后又提出了VGGNet-16 [<xref ref-type="bibr" rid="hanspub.38674-ref3">3</xref>]、GoogLeNet [<xref ref-type="bibr" rid="hanspub.38674-ref4">4</xref>]、ResNet-50 [<xref ref-type="bibr" rid="hanspub.38674-ref5">5</xref>] 等优秀的网络架构，不仅增加了网络深度和准确率，还解决了网络过拟合等问题。2016年Huang [<xref ref-type="bibr" rid="hanspub.38674-ref6">6</xref>] 等提出了一种DenseNet模型缓解了消失梯度问题，加强了特征传播，鼓励特征重用，并大大减少了参数的数量。此外Hinton [<xref ref-type="bibr" rid="hanspub.38674-ref7">7</xref>] 等人在2017年提出胶囊网络(Capsule Network, CapsNet)更是改变了人们对卷积神经网络的认识，解决了CNN对物体之间的空间辨识度差及物体大幅度旋转之后识别能力低下的两个缺陷。</p><p>虽然以上网络对卷积神经网络做了诸多创新，但对卷积这一过程几乎没有改变，基本只是停留在对卷积核大小的改变以及卷积核与前一层的链接方式的改变上。卷积层作为卷积神经网络的主要部分，完成了对特征的提取从而达到更好的分类效果。通过研究发现卷积层的浅层用来提取边缘和背景等轮廓特征可以看作为边缘检测器，深层网络用来提取花纹等细节和更抽象特征 [<xref ref-type="bibr" rid="hanspub.38674-ref8">8</xref>]，但是现在几乎所有的卷积核都为矩形，若想使其更好地提取图像特征，从而提高网络精确度，只能通过改变卷积核的大小来实现，而卷积核大小的改变会使其参数迅速增长，不利于神经网络的计算与求解 [<xref ref-type="bibr" rid="hanspub.38674-ref9">9</xref>] [<xref ref-type="bibr" rid="hanspub.38674-ref10">10</xref>]。这使得卷积核不能更好地发挥其作用，那么如何使卷积核能够更好地提取目标特征便成为一个重要的问题。本文以3 * 3的卷积核为例，通过对卷积核形状的改变使得卷积核的感受野发生变化，从而使得其对于某种特定的特征具有更好的提取效果，使得卷积核更加高效地提取图片特征以至于增加卷积神经网络的特征提取能力以及分类正确率 [<xref ref-type="bibr" rid="hanspub.38674-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.38674-ref12">12</xref>]。</p></sec><sec id="s6"><title>2. 卷积神经网络(CNN)</title><p>首先给出卷积神经网络的结构以及卷积核的概念。</p><p>卷积神经网络通常包括卷积层、池化层、全连接层和输出层，其组合方式一般为卷积层和池化层的交替，最后是全连接层和输出层。如图1所视：</p><p>图1. CNN网络结构图</p><p>卷积神经网络的参数是通过最小化代价函数 C ( ω , b ) 来得到，即</p><p>min w , b C ( ω , b ) = 1 2 n ∑ x ∈ X ‖ y ( x ) − S ( w , b , x ) ‖ 2 ， (1)</p><p>其中X为所有样本的集合，n为样本容量， y ( x ) 为样本 x ∈ X 的监督值，K为卷积层个数，第k个卷积层的输入和输出单元数分别为 I k 和 J k ， w j k 为第k个卷积层中第j个输出单元所对应的卷积核， b j k 为第k个卷积层中第j个输出单元所对应的偏置量 k = 1 , 2 , ⋯ , K ， w = { w j k | j = 1 , 2 , ⋯ , I k ; k = 1 , 2 , ⋯ , K } ， b = { b j k | j = 1 , 2 , ⋯ , I k ; k = 1 , 2 , ⋯ , K } ，为各个卷积核的偏置量， S ( w , b , x ) 为CNN网络当输入为x时的输出。通过最小化代价函数来求解w，b从而得到完整的网络。由于卷积核中的参数与偏置量是优化模型(1)的最优解，所以，从理论层面上分析，卷积核越大，效果越好，解释如下：由于高阶卷积核可以通过低</p><p>阶卷积核外围补0得到，如： ( b 11 b 12 b 13 b 21 b 22 b 23 b 31 b 32 b 33 ) ⇒ ( 0 0 0 0 0 0 b 11 b 12 b 13 0 0 b 21 b 22 b 23 0 0 0 b 31 0 b 32 0 b 33 0 0 0 )</p><p>所以，最优的三阶卷积核置于五阶卷积核范畴中比较，其仅仅是个五阶可行解，或者可以理解为五阶卷积核当其周围全为零时的条件最优解。从而，CNN中卷积核阶数越高，网络精度越高。但是，由于模型(1)的求解是近似的，其精度受到参数个数及算法本身的影响 [<xref ref-type="bibr" rid="hanspub.38674-ref13">13</xref>]，高阶卷积核的参数维度以平方级递增，对求解的精度和效率有严重影响，也不利于模型深度的增加，为了避免高阶卷积核的这种负面影响，实际使用时常常放弃高阶卷积核在理论上的优势，选用低阶卷积核。</p></sec><sec id="s7"><title>3. 异形卷积核</title><p>基于以上的分析，我们提出异形卷积核的概念。在不提高参数维度的情况下，拓宽了卷积核的感受野，使得低阶卷积核能在一定程度上拥有高阶卷积核的优势，从而提高CNN的分类正确率。下面我们以3 * 3的卷积块为例，3 * 3的几种异形卷积核如图2所示：</p><p>图2. 异形卷积核及其感受野</p><p>如图2(e)所示将几种异形卷积核叠加在一起，可以扩展卷积核的感受野。从结构上看相比于矩形卷积核，异形卷积核也有利于对角特征的描述。</p><p>由上图2所示的卷积核的变化方式，那么还存在四种阶梯型的卷积核，若将这四种卷积核应用于卷积神经网络中只能改变其卷积的计算逻辑，无疑会增加网络的计算量。而提出异性卷积核的目的是为了在不增加网计算难度和参数数量的情况下改变其卷积核的感受野，所以这四种不适用与本文所提出的网络结构，所以不再讨论范围内。</p></sec><sec id="s8"><title>4. 融合异形卷积核和矩形卷积核的卷积神经网络</title><sec id="s8_1"><title>4.1. 异形卷积核的实现</title><p>以下任以3 * 3的卷积核为例。</p><p>假设 A = ( a i , j ) 3 ∗ 3 为3 * 3的标准卷积核， B = ( b i , j ) m ∗ n 为图像， C = ( c i , j ) m ∗ n 为A卷积B后的图像，则 c i , j = ∑ s = 1 3 ∑ t = 1 3 ( α s , t &#215; b i + s − 2 , j + t − 2 ) ，(2) 其中 i = 1 , 2 , ⋯ , m ， j = 1 , 2 , ⋯ , n ，(这里规定： i + s − 2 &lt; 1 ， i + s − 2 &gt; m 或 j + s − 2 &lt; 1 ， j + s − 2 &gt; n 时 b i + s − 2 , j + t − 2 = 0 )对于如图2(a)所示的异形卷积核(其元素如图中标记)记为 A &#175; ，令</p><p>c &#175; i , j = ∑ s = 1 3 ∑ t = 1 3 ( α s , t &#215; b &#175; i + s − 2 , j + t − 2 ) ， (3)</p><p>b &#175; i , j = { b i , j i = 2 k + 1 , k = 1 , 2 , ⋯ , m − 1 , j = 1 , 2 , ⋯ , n b i , j − 1 i = 2 k , k = 1 , 2 , ⋯ , m , j = 1 , 2 , ⋯ , n (4)</p><p>其中 i = 1 , 2 , ⋯ , m ， j = 1 , 2 , ⋯ , n 若记， B &#175; = ( b &#175; i , j ) m &#215; n ， C &#175; = ( c &#175; i , j ) m - 2 &#215; n - 2 ，则 C &#175; 的偶数行与 A &#175; 与图片B卷积后的图片偶数行相等。(我们把 B &#175; 称为矩阵B的“移位矩阵”)。</p><p>分析表明，仅看偶数行 A &#175; 与B卷等于A与 B &#175; 的卷积。</p><p>如果删除矩阵 B &#175; 的第一行像素记作 B &#175; 1 ，标准3 * 3卷积核A与矩阵 B &#175; 1 进行步长为2的卷积，就等于图2(b)卷积核与矩阵B做步长为2的卷积。设P为监督样本图像集合，P1 = {X|X是Y经公式(4)的变化所得图像，Y属于P}，对P2 = {X|X是Y删除第一行并在最后一行补0得到的图像，Y属于P1}。通过对于一般的图像，删除第一行像素后(最后一行补0)，其特征信息几乎不会变化，在获取图像特征时P1和P2几乎等价，通过深度学习得到的卷积核应该近似相同。基于上述的分析，以P1作为监督样本，利用CNN进行学习(卷积时变步长为1)，就相当于同时用到了图2(a)，图2(b)所示的卷积核。同理可以用相同的思路处理图2(c)，图2(d)的卷积核，也就是对图像做纵向的移位，然后使用矩形卷积核进行卷积。这样可以通过对于图片的变化使得在使用同样的卷积核的时候使其感受野增加，对于图像的改变也使得异形卷积核在CNN中更加容易实现。异形卷积核的实现如图3所示。</p><p>图3. 异形卷积核的实现</p></sec><sec id="s8_2"><title>4.2. 融合移位图片的卷积神经网络</title><p>基于以上的分析将移位的图片输入到CNN中进行学习，会使得卷积神经网络第一层的卷积核变为异形卷积核，从而提高网络第一层的感受野。但是每一种移位方式(纵向或横向)只能对应训练一个网络。且通过实验发现在使用不同的移位方式训练出来的网络，会使某些类的分类精度变高而另一些类的分类精度变小，这样使得总体分类精度并没有发生变化甚至降低，所以我们提出一种联合两种异形卷积核的卷积神经网络的分类模型。这个模型中我们需要建立3个独立的神经网络，分别将以不同方移位的图片放入相同的网络结构中进行学习，最后将softmax层的概率相加求和再进行分类。这样将两个网络的分类概率相加后可以使得其分到正确的类别。虽然计算量增大且网络参数变多但是可以大幅提高网络分类精度。网络结构如图4所示(注意网络训练的时候是分开训练的)：</p><p>图4. 融合网络结构</p><p>融合网络需要对所有的训练样本图片做移位操作(横向和纵向)。分别用正常图片和移位后的图片训练各自的网络，这样可以训练出3个独立的CNN网络，将训练好的3个网络的参数保留，在其输入层前加入对图片做移位操作的函数(原图训练的网络不加)，然后将其输出层舍去，把softmax分类概率相加求和构成新的输出层。这样构成一个新的网络，这个网络实际包含了三个相互独立的CNN，它们的输入是将同一张图片做不同的移位操作而得到的，所以每个独立的网络输入其实各不相同。</p></sec></sec><sec id="s9"><title>5. 实验</title><p>本文在MNIST、Fashion MNIST 和CIFAR-10三个数据集上进行仿真实验。MNIST数据集来自美国国家标准与技术研究所National Institute of Standards and Technology (NIST)。共70,000个手写数字图片大小为28 * 28，其中训练集60,000个，测试集10,000个。Fashion MNIST是MNIST数据集的升级版，两者格式大小相同，Fashion MNIST更具有挑战性。CIFAR-10数据集由10个类的60,000个32 * 32彩色图像组成，每个类有6000个图像，有50,000个训练图像和10,000个测试图像。使用MATLAB深度学习架构进行实验分析，为证明异形卷积核相对于传统卷积核的优势所在。构建了两种简单卷积神经网络。如图1和图5所示。</p><p>图5. 网络结构</p><p>分别用移位的图片(横向和纵向)和正常图片分别训练三个神经网络(使用图1所示的网络结构，卷积层第一层用8个3 * 3的卷积块，第二层用16个3 * 3的卷积块，第三层16个3 * 3的卷积块，池化层采用2 * 2步长为2的最大池化 [<xref ref-type="bibr" rid="hanspub.38674-ref14">14</xref>] )，训练过程使用具有动量的随机梯度下降法 [<xref ref-type="bibr" rid="hanspub.38674-ref11">11</xref>] [<xref ref-type="bibr" rid="hanspub.38674-ref12">12</xref>] 训练网络，初始学习率为0.1。我们先取MNIST每个数字的训练集的百分之一(小样本训练集可以更加清晰看到不同卷积核对于不同的特征敏感度)，在迭代到正确率基本不发生变化时停止，重复100次平均后得到以下结果：</p><p>如表1所示，在小样本训练集上可以看到正常的图片训练出的网络对数字1、4的识别正确率更高，横向拉伸图片对数字9、6识别率更高，纵向拉伸图片对7、3的识别率更高。虽然总的正确率差距不大，但是对不同的数字不同的卷积核表现出了不同正确率，若将不同的卷积核的优势融合起来便会提高网络总体的正确率。</p><p>所以使用第四章所说的方法，用水平移位、垂直移位、正常的三种图片分别训练三个网络，将各网络的输出层删除，再将softmax层的概率相加求和除以3然后进行分类(这里也可以使用加权求和的方式给每个方向不一样的权重，本文三个方向权重相同)。在训练网络前对每一张图片的最外层都补了一圈0，这样使得移位后的图片和原图片大小相同。CIFAR-10数据集使用图5网络结构，MNIST和Fashion MNIST数据集使用图1网络结构(图一所示网络卷积层第一层用16个5 * 5的卷积块，第二层用32个3 * 3的卷积块，第三层64个3 * 3的卷积块，池化层采用2 * 2步长为2的最大池化)，如图所示每一层卷积层和全连接层后会加一个归一化层加快训练速度，训练过程使用具有动量的随机梯度下降法训练网络，初始学习率为0.01。在迭代到正确率基本不发生变化时停止。结果如表2所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Experimental dat</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >训练图像类型</th><th align="center" valign="middle" >正常</th><th align="center" valign="middle" >横向拉伸</th><th align="center" valign="middle" >纵向拉伸</th></tr></thead><tr><td align="center" valign="middle" >总正确率</td><td align="center" valign="middle" >84.43%</td><td align="center" valign="middle" >85.16%</td><td align="center" valign="middle" >83.82%</td></tr><tr><td align="center" valign="middle" >数字0</td><td align="center" valign="middle" >93.99%</td><td align="center" valign="middle" >94.53%</td><td align="center" valign="middle" >92.54%</td></tr><tr><td align="center" valign="middle" >数字1</td><td align="center" valign="middle" >95.26%</td><td align="center" valign="middle" >93.90%</td><td align="center" valign="middle" >92.75%</td></tr><tr><td align="center" valign="middle" >数字2</td><td align="center" valign="middle" >83.29%</td><td align="center" valign="middle" >85.10%</td><td align="center" valign="middle" >86.35%</td></tr><tr><td align="center" valign="middle" >数字3</td><td align="center" valign="middle" >81.80%</td><td align="center" valign="middle" >82.82%</td><td align="center" valign="middle" >84.16%</td></tr><tr><td align="center" valign="middle" >数字4</td><td align="center" valign="middle" >88.19%</td><td align="center" valign="middle" >80.76%</td><td align="center" valign="middle" >86.87%</td></tr><tr><td align="center" valign="middle" >数字5</td><td align="center" valign="middle" >81.26%</td><td align="center" valign="middle" >84.80%</td><td align="center" valign="middle" >77.34%</td></tr><tr><td align="center" valign="middle" >数字6</td><td align="center" valign="middle" >91.45%</td><td align="center" valign="middle" >92.38%</td><td align="center" valign="middle" >87.58%</td></tr><tr><td align="center" valign="middle" >数字7</td><td align="center" valign="middle" >81.59%</td><td align="center" valign="middle" >80.45%</td><td align="center" valign="middle" >83.87%</td></tr><tr><td align="center" valign="middle" >数字8</td><td align="center" valign="middle" >74.17%</td><td align="center" valign="middle" >73.32%</td><td align="center" valign="middle" >67.97%</td></tr><tr><td align="center" valign="middle" >数字9</td><td align="center" valign="middle" >72.02%</td><td align="center" valign="middle" >82.59%</td><td align="center" valign="middle" >76.84%</td></tr></tbody></table></table-wrap><p>表1. 实验数据</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Experimental result</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  colspan="2"   rowspan="2"  >数据集</th><th align="center" valign="middle"  colspan="5"  >不同图片训练网络的正确率%</th></tr></thead><tr><td align="center" valign="middle"  colspan="2"  >正常</td><td align="center" valign="middle" >水平移位</td><td align="center" valign="middle" >垂直移位</td><td align="center" valign="middle" >融合网络</td></tr><tr><td align="center" valign="middle" >MNIST</td><td align="center" valign="middle"  colspan="2"  >99.33</td><td align="center" valign="middle"  colspan="2"  >99.27</td><td align="center" valign="middle" >99.30</td><td align="center" valign="middle" >99.41</td></tr><tr><td align="center" valign="middle" >Fashion-MNIST</td><td align="center" valign="middle"  colspan="2"  >88.31</td><td align="center" valign="middle"  colspan="2"  >88.07</td><td align="center" valign="middle" >87.67</td><td align="center" valign="middle" >89.63</td></tr><tr><td align="center" valign="middle" >CIFAR-10</td><td align="center" valign="middle"  colspan="2"  >76.90</td><td align="center" valign="middle"  colspan="2"  >75.62</td><td align="center" valign="middle" >75.39</td><td align="center" valign="middle" >80.30</td></tr><tr><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td><td align="center" valign="middle" ></td></tr></tbody></table></table-wrap><p>表2. 实验结果</p><p>如表2所示通过MNIST、Fashion MNIST和CIFAR-10数据集的仿真实验证明，联合了正常图像和拉伸图像训练出网络的softmax层，融合CNN网络的分类正确率有较为明显的提升。其次，经过对比发现，采用正常图片训练所得的网络精度越小，其融合网络对于其精度的提升就越大。所以融合网络对于在CNN表现不好的数据集中有着更大的使用前景。且通过表1所得结果，异形卷积核因对于曲线特征更加敏感，而对于直线特征的学习能力并没有矩形卷积块强，所以将两者结合提升了CNN的特征提取能力，从而提升了准确率。通过三个数据集的仿真实验，我们发现单独使用移位图片训练出的CNN网络没有正常图片训练出的CNN网络正确率高，那么单独使用异形卷积核并没有矩形卷积核的效果好。这应该是异性卷积核只对某些特征比较敏感的结果。</p><p>如表3所示融合网络在MNIST、Fashion MNIST和CIFAR-10三种数据集中的准确率都明显优于其它算法，且融合网络可以适用于大多数读者自己创建的CNN，只需要对训练数据集进行修改便可以将异形卷积核应用到网络中。这相对于对网络内部的修改有着更强可行性 [<xref ref-type="bibr" rid="hanspub.38674-ref15">15</xref>]。在实际问题中，可以针对问题中较为明显的特征设计异形卷积核，且对于更大的卷积核有着更多的变化方式。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Comparison of accuracy rates of various method</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >识别方法</th><th align="center" valign="middle" >准确率%</th></tr></thead><tr><td align="center" valign="middle"  rowspan="6"  >MNIST</td><td align="center" valign="middle" >线性分类器</td><td align="center" valign="middle" >91.90</td></tr><tr><td align="center" valign="middle" >K近邻</td><td align="center" valign="middle" >95.10</td></tr><tr><td align="center" valign="middle" >PCA</td><td align="center" valign="middle" >96.72</td></tr><tr><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >98.60</td></tr><tr><td align="center" valign="middle" >LeNet-5</td><td align="center" valign="middle" >99.20</td></tr><tr><td align="center" valign="middle" >融合网络</td><td align="center" valign="middle" >99.41</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >Fashion-MNIST</td><td align="center" valign="middle" >SVM</td><td align="center" valign="middle" >84.69</td></tr><tr><td align="center" valign="middle" >CNN</td><td align="center" valign="middle" >88.31</td></tr><tr><td align="center" valign="middle" >融合网络</td><td align="center" valign="middle" >89.63</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >CIFAR</td><td align="center" valign="middle" >CNN</td><td align="center" valign="middle" >76.9</td></tr><tr><td align="center" valign="middle" >K均值</td><td align="center" valign="middle" >79.2</td></tr><tr><td align="center" valign="middle" >融合网络</td><td align="center" valign="middle" >80.3</td></tr></tbody></table></table-wrap><p>表3. 各类方法准确率对比</p></sec><sec id="s10"><title>基金项目</title><p>陕西省自然科学基础研究计划资助项目(项目编号2018JQ1038)。</p></sec><sec id="s11"><title>文章引用</title><p>马 双,董安国,王长鹏. 基于异形卷积核的卷积神经网络图像分类方法Based on the Heterogeneous Convolution Kernel Image Classification of Convolutional Neural Network[J]. 计算机科学与应用, 2020, 10(11): 1962-1970. https://doi.org/10.12677/CSA.2020.1011207</p></sec><sec id="s12"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.38674-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Lecun, Y. and Bottou, L. (1998) Gradient-Based Learning App-Lied to Document Recognition. Proceedings of the IEEE, 86, 2278-2324. &lt;br&gt;https://doi.org/10.1109/5.726791</mixed-citation></ref><ref id="hanspub.38674-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky, A., Sutskever, I.I. and Hinton, G. (2012) Imagenet Classification with Deep Convolutional Neural Networks. Proceedings of the Advances in Neural Information Processing Systems, Lake Tahoe, 1097-1105.</mixed-citation></ref><ref id="hanspub.38674-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan, K. and Zisserman, A. (2014) Very Deep Convolutional Networks for Large-Scale Image Recognition. ICLR 2015, San Diego, 7-9 May 2015, 4898-4906.</mixed-citation></ref><ref id="hanspub.38674-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Liu, W., Jia, Y., et al. (2014) Going Deeper with Convolutions. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, 7-12 June 2015, 1-9. &lt;br&gt;https://doi.org/10.1109/CVPR.2015.7298594</mixed-citation></ref><ref id="hanspub.38674-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S., et al. (2016) Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.38674-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Huang, G., Liu, Z., Weinberger, K.Q., et al. (2016) Densely Connect-ed Convolutional Net Works. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 2261-2269.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2017.243</mixed-citation></ref><ref id="hanspub.38674-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Sabour, S., Frosst, N. and Hinton, G.E. (2017) Dynamic Routing between Capsules. Advances in Neural Information Processing Systems, Long Beach, 3856-3866.</mixed-citation></ref><ref id="hanspub.38674-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Gu, J., Wang, Z., Kuen, J., et al. (2015) Recent Advances in Convolutional Neural Networks. Computer Science.</mixed-citation></ref><ref id="hanspub.38674-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Zeiler, M.D. and Fergus, R. (2014) Visualizing and Understanding Convolutional Networks. In: European Conference on Computer Vision, Springer, Berlin, 818-833. &lt;br&gt;https://doi.org/10.1007/978-3-319-10590-1_53</mixed-citation></ref><ref id="hanspub.38674-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Cao, K. and Jain, A.K. (2015) Latent Orientation Field Estimation via Convolutional Neural Network. International Conference on Biometrics, Phuket, 19-22 May 2015, 349-356. &lt;br&gt;https://doi.org/10.1109/ICB.2015.7139060</mixed-citation></ref><ref id="hanspub.38674-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">李章维, 胡安顺, 王晓飞. 基于视觉的目标检测方法综述[J]. 计算机工程与应用, 2020, 56(8): 1-9.</mixed-citation></ref><ref id="hanspub.38674-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">陈超, 齐峰. 卷积神经网络的发展及其在计算机视觉领域中的应用综述[J]. 计算机科学, 2019, 46(3): 63-73.</mixed-citation></ref><ref id="hanspub.38674-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">唐贤伦, 杜一铭, 刘雨微, 李佳歆, 马艺玮. 基于条件深度卷积生成对抗网络的图像识别方法[J]. 自动化学报, 2018, 44(5): 855-864.</mixed-citation></ref><ref id="hanspub.38674-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Zhang, K., Guo, Y., Wang, X., et al. (2019) Multiple Feature Reweight DenseNet for Image Classification. IEEE Access, 7, 9872-9880. &lt;br&gt;https://doi.org/10.1109/ACCESS.2018.2890127</mixed-citation></ref><ref id="hanspub.38674-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">张顺, 龚怡宏, 王进军. 深度卷积神经网络的发展及其在计算机视觉领域的应用[J]. 计算机学报, 2019, 42(3): 453-482.</mixed-citation></ref></ref-list></back></article>