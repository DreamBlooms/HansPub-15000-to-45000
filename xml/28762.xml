<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2019.92029</article-id><article-id pub-id-type="publisher-id">CSA-28762</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20190200000_56661560.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  一种基于U型全卷积神经网络的深度估计模型
  Image Depth Estimation Model Based on Fully Convolutional U-Net
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>小康</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>付</surname><given-names>小宁</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>董</surname><given-names>悫</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff3"><addr-line>武汉高德红外股份有限公司，湖北 武汉</addr-line></aff><aff id="aff2"><addr-line>西安电子科技大学机电工程学院，陕西 西安</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>29</day><month>01</month><year>2019</year></pub-date><volume>09</volume><issue>02</issue><fpage>250</fpage><lpage>255</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   本文解决了从单张图像估计深度信息的问题。单张图像与深度图之间的映射是是模棱两可的，它需要全局信息和局部信息。本文部署了一个全卷积U型神经网络，它用预训练的ResNet-50网络提取图像特征，然后用残差上采样模块将特征图恢复到深度图的尺寸大小，并且使用了跳跃链接，整个网络呈现U型，从而对全局信息和局部信息进行融合。整个网络可以进行端到端的训练。 The problem of depth estimation from single image has been addressed. The mapping between a single image and the depth map is inherently ambiguous, and requires both global and local information. This paper presents a fully convolutional U-net whose encoder is pretrained ResNet50 without fully connected layer or pooling layer, and uses residual up-sampling layers to enlarge the feature maps. Besides, skip connection is introduced, making the model U-net, to fuse global and local information. The network can be end-to-end trained. 
  
 
</p></abstract><kwd-group><kwd>单目深度估计，全卷积神经网络，残差上采样，跳跃链接, Depth Estimation</kwd><kwd> Fully Convolutional Network</kwd><kwd> Residual Up-Sampling Layers</kwd><kwd> Skip Connection</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>一种基于U型全卷积神经网络的深度估计模型<sup> </sup></title><p>王小康<sup>1</sup>，付小宁<sup>1</sup>，董悫<sup>2</sup></p><p><sup>1</sup>西安电子科技大学机电工程学院，陕西 西安</p><p><sup>2</sup>武汉高德红外股份有限公司，湖北 武汉</p><p><img src="//html.hanspub.org/file/8-1541295x1_hanspub.png" /></p><p>收稿日期：2019年1月15日；录用日期：2019年1月25日；发布日期：2019年2月1日</p><disp-formula id="hanspub.28762-formula45"><graphic xlink:href="//html.hanspub.org/file/8-1541295x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>本文解决了从单张图像估计深度信息的问题。单张图像与深度图之间的映射是是模棱两可的，它需要全局信息和局部信息。本文部署了一个全卷积U型神经网络，它用预训练的ResNet-50网络提取图像特征，然后用残差上采样模块将特征图恢复到深度图的尺寸大小，并且使用了跳跃链接，整个网络呈现U型，从而对全局信息和局部信息进行融合。整个网络可以进行端到端的训练。</p><p>关键词 :单目深度估计，全卷积神经网络，残差上采样，跳跃链接</p><disp-formula id="hanspub.28762-formula46"><graphic xlink:href="//html.hanspub.org/file/8-1541295x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2019 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/8-1541295x7_hanspub.png" /> <img src="//html.hanspub.org/file/8-1541295x8_hanspub.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>深度图估计是计算机视觉的一个基础性问题，它可以展现一个场景的一些几何关系。近几年提出了不少的从立体图像中估计深度的成功方法。如果有精确的图像匹配，深度的确可以从立体图像中估计出来。这方面已经被广泛研究过了，但是在实践中常常会遇到单目图像的深度估计的问题，图片有的是室内的，有的是室外的。因此在缺乏深度传感器的情况下估计深度信息是必不可少的。再者，深度信息对计算机视觉的其它任务有很大帮助，比如图像识别 [<xref ref-type="bibr" rid="hanspub.28762-ref1">1</xref>] 与语义分割 [<xref ref-type="bibr" rid="hanspub.28762-ref2">2</xref>] 。</p><p>众所周知，从单目图像中估计深度信息是一个不适定问题，因为一张RGB图像可能对应无限多种现实世界的场景，并且获取不到可靠的视觉线索。有很多工作都尝试着解决这个问题。近来，卷积神经网络(Convolutional Neural Networks, CNNs)常常用来学习像素与深度之间的一种内含的关系。本文实现了一个端到端可训练的U型CNN架构，结合残差网络，可以学习图像像素灰度值与对应深度图之间的映射。</p></sec><sec id="s4"><title>2. 相关工作</title><sec id="s4_1"><title>2.1. 经典方法</title><p>经典方法依赖于对场景几何的强烈假设，依赖于手工制作的特征和概率图模型，而概率图模型利用图像的水平对齐几何信息或其他的几何信息。比如，Saxena et al. [<xref ref-type="bibr" rid="hanspub.28762-ref3">3</xref>] 利用线性回归和MPF从图像特征中预测出深度，之后又将该工作扩展到Make3D系统 [<xref ref-type="bibr" rid="hanspub.28762-ref4">4</xref>] 。然而，这个系统依赖于图像的水平对齐。</p></sec><sec id="s4_2"><title>2.2. 基于特征的映射方法</title><p>第二种相关工作是基于特征的，给定一张RGB图像，在RGB-D的数据集中找到最近邻的图相对，检索出来的深度图会被用来产生最后的深度图。Karsch et al. [<xref ref-type="bibr" rid="hanspub.28762-ref5">5</xref>] 利用SIFT流，之后用一种全局优化的方法，而Konrad et al. [<xref ref-type="bibr" rid="hanspub.28762-ref6">6</xref>] 计算检索出来的深度图的中值，之后用交叉双边滤波来平滑。Liu et al. [<xref ref-type="bibr" rid="hanspub.28762-ref7">7</xref>] 将优化问题建模为连续和离散的可变势能的条件随机场Conditional Random Field (CRF)。这些方法都基于这样一个假设：RGB图像中的区域之间的相似性也意味着相似的深度线索。</p></sec><sec id="s4_3"><title>2.3. 基于卷积神经网络的方法</title><p>近来，基于CNN的深度估计方法开始占据主流。由于这个任务跟语义分割很相近，所以大多数工作都基于The Image Net Large Scale Visual Recognition Challenge (ILSVRC) [<xref ref-type="bibr" rid="hanspub.28762-ref8">8</xref>] 中最成功的架构。Eigen et al. [<xref ref-type="bibr" rid="hanspub.28762-ref9">9</xref>] 是第一个运用CNN来预测单目图像深度的。他们用了两个深度网络模块，第一个做全局粗糙的速度估计，第二个在局部改善预测结果。这个想法之后被扩展 [<xref ref-type="bibr" rid="hanspub.28762-ref2">2</xref>] ，三个CNN网络栈被用来额外预测表面法线、类别以及深度。另外一个提高预测深度图质量的方向是将卷积神经网络与图模型结合。Liu et al. [<xref ref-type="bibr" rid="hanspub.28762-ref10">10</xref>] 提出用一种CRF loss的方式在CNN训练过程中学习一元的和成对势能，这种方法没有利用几何先验就达到了最好的结果.这个想法行得通是因为深度值是连续的 [<xref ref-type="bibr" rid="hanspub.28762-ref11">11</xref>] 。Li et al. [<xref ref-type="bibr" rid="hanspub.28762-ref12">12</xref>] 和Wang et al. [<xref ref-type="bibr" rid="hanspub.28762-ref13">13</xref>] 用层次CRFs来细化patch-wise的CNN预测结果，从超分辨到像素级别。</p></sec><sec id="s4_4"><title>2.4. 基于全卷积神经网络的方法</title><p>全卷积神经网络(Fully convolutional networks, FCN)在密集预测的问题中有令人满意的表现。 [<xref ref-type="bibr" rid="hanspub.28762-ref14">14</xref>] 使用FCN，并用CRF来进行后处理。除了传统的卷积层， [<xref ref-type="bibr" rid="hanspub.28762-ref15">15</xref>] 使用扩张卷积来有效增加神经元的感受野，而且不用增加模型的参数以及训练所需要的数据量； [<xref ref-type="bibr" rid="hanspub.28762-ref16">16</xref>] 在语义分割任务中使用转置卷积来上采样特征图和预测图，并提出U-net网络结构。Laina et al. [<xref ref-type="bibr" rid="hanspub.28762-ref17">17</xref>] 提出一种带有残差上采样块的全卷积残差神经网络。</p></sec></sec><sec id="s5"><title>3. 网络结构</title><p>本文的网络结构如图1所示，分别为特征提取部分和上采样部分。其中第一部分用ResNet50，删除了最后的全局池化层和全连接层，并以预训练模型的权值进行初始化；第二部分使用残差上采样 [<xref ref-type="bibr" rid="hanspub.28762-ref17">17</xref>] ，使得特征图逐步放大，最终输出预测的深度图。最后，使用跳跃连接，对第一部分的特征与第二部分的特征进行融合。</p><p>1) 残差上采样层</p><p>如图2所示，Unpooling层可以把特征图里的每个值都映射到一个2*2的矩阵的左上角处，而该矩阵的其他位置均填充数值0，后接一个5*5的卷积层。这样的结构可以把输入的特征图的尺寸增大，在本例中，则可以将尺寸加倍。然而，这样的结构会在数值0上花费过多的计算，并且使得输出的预测图造成棋盘效应(棋盘状伪影)。为了解决这个问题，本文引入残差上采样层(Fast up-projection layer [<xref ref-type="bibr" rid="hanspub.28762-ref17">17</xref>] )，用4个不同的小卷积，分别是3*3、3*2、2*3和2*2，代替5*5的卷积层。如图3所示，在残差上采样层的每个分支上特征图分别经过4个不同的卷积核，再做插入操作，使得特征图的尺寸加倍，同时通道数减半。</p><p>2) 跳跃连接</p><p>由于卷积神经网络是层级结构，也就是说，越靠近输出端的神经元拥有越大的感受野，并且可以产生越抽象的特征，而越靠近输入端的神经元其感受野越小但是包含更多的边缘信息。基于这个前提，本文将网络第一部分的特征与第二部分的特征进行级联，这种跳跃式的连接既让梯度更有效地反向传播，又保留更多的边缘信息。</p><p>图1. 本文提出的网络结构</p><p>图2. 上采样块</p><p>图3. 残差上采样块</p></sec><sec id="s6"><title>4. 实验</title><p>本文将提出的模型在NYU V2数据集 [<xref ref-type="bibr" rid="hanspub.28762-ref18">18</xref>] 上训练和测试。包含1449个室内场景RGB图像及其对应的深度图，其中795个训练样本对，654个测试样本对。在对训练集中拿出10%作为验证集，剩余的训练样本当做开发集，并对开发集离线做了随机旋转、改变尺寸以及剪切，最后数据扩增到了大约5000个样本对，并且保证RGB图与深度图是同步变换的。在训练的过程中，又对训练集中每一对样本进行同步的随机左右翻转和随机改变RGB图的颜色，这样的在线随机变换相当于进一步离线扩增数据集而且不用占用额外的磁盘容量，最后可以推算出数据扩增到了10,000个样本对。</p><p>本文模型实现基于PyTorch，电脑配置为Intel core i5处理器、NVIDIA GTX 1060显卡和8 GB内存。使用ADAM优化器，学习率初始化为0.003，并且每5个epoch将学习率乘于0.1，权重衰减系数为0.0001。</p><p>为了定量评估模型，本文使用如下指标。 d 为深度标签值， d ^ 为深度预测值， T 为图像中所有的像素的集合， δ 为准确率，threshold为准确率阈值(分别取1.25、1.25<sup>2</sup>和1.25<sup>3</sup>)。</p><p>δ = max ( d ^ i d i , d i d ^ i ) &lt; t h r e s h o l d</p><p>平均绝对误差(rel)； 1 | T | ∑ d ∈ T | d ^ − d | / d</p><p>平均 log 10 误差： 1 | T | ∑ d ∈ T | log 10 d ^ − log 10 d |</p><p>均方根误差： 1 | T | ∑ d ∈ T ‖ d ^ − d ‖ 2</p><p>实验结果：</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Comparison of evaluation index on NYU v2 datase</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >方法</th><th align="center" valign="middle"  colspan="3"  >准确率(越高越好)</th><th align="center" valign="middle"  colspan="3"  >误差(越低越好)</th></tr></thead><tr><td align="center" valign="middle" >δ &lt; 1.25</td><td align="center" valign="middle" >δ &lt; 1.25 2 <sup> </sup></td><td align="center" valign="middle" >δ &lt; 1.25 3</td><td align="center" valign="middle" >Rel</td><td align="center" valign="middle" >RMS</td><td align="center" valign="middle" >log10</td></tr><tr><td align="center" valign="middle" >Karsch [<xref ref-type="bibr" rid="hanspub.28762-ref5">5</xref>]</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >0.374</td><td align="center" valign="middle" >1.12</td><td align="center" valign="middle" >0.134</td></tr><tr><td align="center" valign="middle" >Liu [<xref ref-type="bibr" rid="hanspub.28762-ref7">7</xref>]</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >0.335</td><td align="center" valign="middle" >1.06</td><td align="center" valign="middle" >0.127</td></tr><tr><td align="center" valign="middle" >Eigen [<xref ref-type="bibr" rid="hanspub.28762-ref9">9</xref>]</td><td align="center" valign="middle" >0.611</td><td align="center" valign="middle" >0.887</td><td align="center" valign="middle" >0.971</td><td align="center" valign="middle" >0.215</td><td align="center" valign="middle" >0.907</td><td align="center" valign="middle" >-</td></tr><tr><td align="center" valign="middle" >Liu [<xref ref-type="bibr" rid="hanspub.28762-ref12">12</xref>]</td><td align="center" valign="middle" >0.614</td><td align="center" valign="middle" >0.883</td><td align="center" valign="middle" >0.971</td><td align="center" valign="middle" >0.230</td><td align="center" valign="middle" >0.824</td><td align="center" valign="middle" >0.095</td></tr><tr><td align="center" valign="middle" >本文方法</td><td align="center" valign="middle" >0.625</td><td align="center" valign="middle" >0.898</td><td align="center" valign="middle" >0.980</td><td align="center" valign="middle" >0.225</td><td align="center" valign="middle" >0.720</td><td align="center" valign="middle" >0.100</td></tr></tbody></table></table-wrap><p>表1. NYU v2数据集上实验评价指标对比</p><p>NYU V2数据集上的实验结果如表1所示，通过结果对比可以得知本文算法结果更优。与Liu [<xref ref-type="bibr" rid="hanspub.28762-ref12">12</xref>] 的结果相比，本文模型的准确率分别提高1.79%、1.7%和0.9%，同时平均相对误差和均方根误差分别降低了2.17%和12.6%。</p><p>分析：</p><p>在测试集中随机选取3张图片用于训练好的模型进行深度估计，本文方法的结果如图4所示。与Eigen等 [<xref ref-type="bibr" rid="hanspub.28762-ref9">9</xref>] 的方法比较，本文没有先粗略预测再细致调优的多尺度CNN结构，而是单尺度的网络；与Liu等 [<xref ref-type="bibr" rid="hanspub.28762-ref12">12</xref>] 提出的方法相比，本文在像素级别进行了密集预测，并且不需要用CRF作为后处理方式对预测图进行细节优化。</p><p>图4. NYU v2图像深度估计结果</p></sec><sec id="s7"><title>5. 结论</title><p>本文提出了一种新的深度估计模型，以解决单目图像的深度估计问题。该模型利用预训练的ResNet50作为编码器，利用残差上采样块构造出解码器，并且利用跳跃连接将底层特征与高层特征融合起来，从而加速网络收敛和保留更多的边缘信息，是一个端到端的全卷积网络，大大减少了参数的个数与训练所需的样本数量。与文献 [<xref ref-type="bibr" rid="hanspub.28762-ref5">5</xref>] 、 [<xref ref-type="bibr" rid="hanspub.28762-ref7">7</xref>] 、 [<xref ref-type="bibr" rid="hanspub.28762-ref9">9</xref>] 和 [<xref ref-type="bibr" rid="hanspub.28762-ref12">12</xref>] 相比，本文提出的模型误差更小、准确率更高。在未来的研究中，将会引入扩张卷积，利用这种卷积增加神经元的感受野，从而得到更优的局部特征。</p></sec><sec id="s8"><title>文章引用</title><p>王小康,付小宁,董 悫. 一种基于U型全卷积神经网络的深度估计模型Image Depth Estimation Model Based on Fully Convolutional U-Net[J]. 计算机科学与应用, 2019, 09(02): 250-255. https://doi.org/10.12677/CSA.2019.92029</p></sec><sec id="s9"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.28762-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Ren, X., Bo, L. and Fox, D. (2012) Rgb-(d) Scene Labeling: Features and Algorithms. 2012 IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), Providence, 16-21 June 2012, 2759-2766.</mixed-citation></ref><ref id="hanspub.28762-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Eigen, D. and Fergus, R. (2015) Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture. Proceedings of the IEEE International Conference on Computer Vision, Santiago, 7-13 December 2015, 2650-2658. &lt;br&gt;https://doi.org/10.1109/ICCV.2015.304</mixed-citation></ref><ref id="hanspub.28762-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Saxena, A., Chung, S.H. and Ng, A.Y. (2006) Learning Depth from Single Monocular Images. Advances in Neural Information Processing Systems, 18, 1161-1168.</mixed-citation></ref><ref id="hanspub.28762-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Saxena, A., Sun, M. and Ng, A.Y. (2009) Make3d: Learning 3d Scene Structure from a Single Still Image. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31, 824-840. &lt;br&gt;https://doi.org/10.1109/TPAMI.2008.132</mixed-citation></ref><ref id="hanspub.28762-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Karsch, K., Liu, C. and Kang, S. (2012) Depth Extraction from Video Using Non-Parametric Sampling. Proceedings of the 12th European Conference on Computer Vision—Volume Part V, Florence, 7-13 October 2012, 775-788.  
&lt;br&gt;https://doi.org/10.1007/978-3-642-33715-4_56</mixed-citation></ref><ref id="hanspub.28762-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Konrad, J., Wang, M. and Ishwar, P. (2012) 2D-to-3D Image Conversion by Learning Depth from Examples. 2012 IEEE Computer Society Conference on Computer Vision and Pat-tern Recognition Workshops (CVPRW), Providence, 16-21 June 2012, 16-22. &lt;br&gt;https://doi.org/10.1109/CVPRW.2012.6238903</mixed-citation></ref><ref id="hanspub.28762-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Liu, M., Salzmann, M. and He, X. (2014) Dis-crete-Continuous Depth Estimation from a Single Image. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Columbus, 23-28 June 2014, 716-723.  
&lt;br&gt;https://doi.org/10.1109/CVPR.2014.97</mixed-citation></ref><ref id="hanspub.28762-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015) Imagenet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115, 211-252. &lt;br&gt;https://doi.org/10.1007/s11263-015-0816-y</mixed-citation></ref><ref id="hanspub.28762-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Eigen, D., Puhrsch, C. and Fergus, R. (2014) Depth Map Prediction from a Single Image Using a Multi-Scale Deep Network. Advances in Neural Information Processing Systems, 2366-2374.</mixed-citation></ref><ref id="hanspub.28762-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Liu, F., Shen, C. and Lin, G. (2015) Deep Con-volutional Neural Fields for Depth Estimation from a Single Image. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 5162-5170. &lt;br&gt;https://doi.org/10.1109/CVPR.2015.7299152</mixed-citation></ref><ref id="hanspub.28762-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Liu, F., Shen, C., Lin, G. and Reid, I. (2016) Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 2024-2039.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2015.2505283</mixed-citation></ref><ref id="hanspub.28762-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Li, B., Shen, C., Dai, Y., van den Hengel, A. and He, M. (2015) Depth and Surface Normal Estimation from Monocular Images Using Regression on Deep Features and Hierar-chical CRFS. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, 7-12 June 2015, 1119-1127.</mixed-citation></ref><ref id="hanspub.28762-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Wang, P., Shen, X., Lin, Z., Cohen, S., Price, B. and Yuille, A.L. (2015) Towards Unified Depth and Semantic Prediction from a Single Image. Proceedings of the IEEE Conference on Computer Vision and Pat-tern Recognition, Boston, 7-12 June 2015, 2800-2809.</mixed-citation></ref><ref id="hanspub.28762-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Cao, Y., Wu, Z. and Shen, C. (2016) Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks. arXiv:1605.02305 [cs.CV]</mixed-citation></ref><ref id="hanspub.28762-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Li, B., Dai, Y., Chen, H. and He, M. (2017) Single Image Depth Estimation by Dilated Deep Residual Convolutional Neural Network and Soft-Weight-Sum Inference. arXiv:1705.00534 [cs.CV]</mixed-citation></ref><ref id="hanspub.28762-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Ronneberger, O., Fischer, P. and Brox, T. (2015) U-Net: Convolutional Networks for Biomedical Image Segmentation. International Con-ference on Medical Image Computing and Computer-Assisted Intervention, Munich, 5-9 October 2015, 234-241. &lt;br&gt;https://doi.org/10.1007/978-3-319-24574-4_28</mixed-citation></ref><ref id="hanspub.28762-ref17"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F. and Navab, N. (2016) Deeper Depth Prediction with Fully Convolutional Residual Networks. 2016 Fourth International Conference on 3D Vision (3DV), Stanford, 25-28 October 2016, 239-248.</mixed-citation></ref><ref id="hanspub.28762-ref18"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Silberman, N., Hoiem, D., Kohli, P. and Fergus, R. (2012) Indoor Segmentation and Support Inference from RGBD Images. Computer Vision— ECCV 2012, Florence, 7-13 October 2012, 746-760.  
&lt;br&gt;https://doi.org/10.1007/978-3-642-33715-4_54</mixed-citation></ref></ref-list></back></article>