<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114107</article-id><article-id pub-id-type="publisher-id">CSA-41845</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_29383996.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  一种无参数的局部线性判别分析方法
  A Parameter-Free Local Linear Discriminant Analysis Method
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>黄</surname><given-names>礼泊</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>凌</surname><given-names>永权</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>广东工业大学信息工程学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>1042</fpage><lpage>1052</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   为了解决因引入局部化思想的线性判别分析(Linear Discriminant Analysis, LDA)方法需要人工设置邻居个数而无法以自适应的方式挖掘数据的局部结构问题，提出了一种无参数的局部线性判别分析(Parameter-free Local Linear Discriminant Analysis, Pf-LLDA)方法。该方法首先建立了一个关于权重矩阵和变换矩阵的统一优化模型。然后，通过使用交替方向的方法迭代求解出了与数据局部结构相关的权重矩阵和与判别分析相关的变换矩阵。从而使得Pf-LLDA在无需人为设定邻居个数的情况下，自适应地挖掘出了数据的局部结构并最终实现了局部线性判别分析的能力。在仿真数据集和手写体真实数据集上的实验结果表明，Pf-LLDA挖掘出数据局部结构的同时实现了更优的判别分析结果。 For the Linear Discriminant Analysis (LDA) method which utilized the idea of localization and setting the number of neighbors by hand, it cannot mine the local structure embedded in the data adaptively. In order to solve this problem, this paper proposes a Parameter-free Local Linear Discriminant Analysis (Pf-LLDA) method. This method first establishes a unified optimization model about the weight matrix and the transformation matrix. Then, by using the alternating direction method, the solution of the weight matrix, which is associated with the local structure embedded in the data, and the transformation matrix, which is associated with the discriminant analysis are iteratively well found. Thus, Pf-LLDA adaptively mines the local structure of the data and finally achieves the capability of local linear discriminant analysis without artificially setting the number of neighbors. Experimental results on both synthetic and handwritten real datasets show that Pf-LLDA achieves better discriminant analysis results while mining the local structure of the data. 
  
 
</p></abstract><kwd-group><kwd>无参，近邻，线性判别分析，局部结构，降维, Parameter-Free</kwd><kwd> Nearest Neighbor</kwd><kwd> Linear Discriminant Analysis</kwd><kwd> Local Structure</kwd><kwd> Dimensionality Reduction</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>为了解决因引入局部化思想的线性判别分析(Linear Discriminant Analysis, LDA)方法需要人工设置邻居个数而无法以自适应的方式挖掘数据的局部结构问题，提出了一种无参数的局部线性判别分析(Parameter-free Local Linear Discriminant Analysis, Pf-LLDA)方法。该方法首先建立了一个关于权重矩阵和变换矩阵的统一优化模型。然后，通过使用交替方向的方法迭代求解出了与数据局部结构相关的权重矩阵和与判别分析相关的变换矩阵。从而使得Pf-LLDA在无需人为设定邻居个数的情况下，自适应地挖掘出了数据的局部结构并最终实现了局部线性判别分析的能力。在仿真数据集和手写体真实数据集上的实验结果表明，Pf-LLDA挖掘出数据局部结构的同时实现了更优的判别分析结果。</p></sec><sec id="s2"><title>关键词</title><p>无参，近邻，线性判别分析，局部结构，降维</p></sec><sec id="s3"><title>A Parameter-Free Local Linear Discriminant Analysis Method</title><p>Libo Huang, Wing-Kuen Ling</p><p>School of Information Engineering, Guangdong University of Technology, Guangzhou Guangdong</p><p><img src="//html.hanspub.org/file/28-1542126x4_hanspub.png" /></p><p>Received: Mar. 23<sup>rd</sup>, 2021; accepted: Apr. 19<sup>th</sup>, 2021; published: Apr. 26<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/28-1542126x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>For the Linear Discriminant Analysis (LDA) method which utilized the idea of localization and setting the number of neighbors by hand, it cannot mine the local structure embedded in the data adaptively. In order to solve this problem, this paper proposes a Parameter-free Local Linear Discriminant Analysis (Pf-LLDA) method. This method first establishes a unified optimization model about the weight matrix and the transformation matrix. Then, by using the alternating direction method, the solution of the weight matrix, which is associated with the local structure embedded in the data, and the transformation matrix, which is associated with the discriminant analysis are iteratively well found. Thus, Pf-LLDA adaptively mines the local structure of the data and finally achieves the capability of local linear discriminant analysis without artificially setting the number of neighbors. Experimental results on both synthetic and handwritten real datasets show that Pf-LLDA achieves better discriminant analysis results while mining the local structure of the data.</p><p>Keywords:Parameter-Free, Nearest Neighbor, Linear Discriminant Analysis, Local Structure, Dimensionality Reduction</p><disp-formula id="hanspub.41845-formula28"><graphic xlink:href="//html.hanspub.org/file/28-1542126x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/28-1542126x7_hanspub.png" /> <img src="//html.hanspub.org/file/28-1542126x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>维度约简在解决维度灾难问题上具有很重要的作用。降维作为维度约简中最为直观的一种方法在模式识别和机器学习领域具有很重要的研究意义 [<xref ref-type="bibr" rid="hanspub.41845-ref1">1</xref>]。</p><p>通常来讲，根据是否使用了数据标签信息，降维方法可以被简单的分为有监督的降维方法和无监督的降维方法 [<xref ref-type="bibr" rid="hanspub.41845-ref2">2</xref>]。相对于无监督的降维方法，有监督的降维方法因为利用了已知的数据标签信息而被更广泛的采用。线性判别分析(Linear Discriminant Analysis, LDA) [<xref ref-type="bibr" rid="hanspub.41845-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.41845-ref2">2</xref>] 便是一个典例。LDA通过最大化类间距离的同时并最小化类内距离的方式找到了具有判别性的线性变换。但由于LDA用样本均值代表整体样本的方式构造类内散度矩阵和整体散度矩阵，当同类样本不满足高斯分布假设(即多模态现象)时 [<xref ref-type="bibr" rid="hanspub.41845-ref3">3</xref>]，LDA将失去判别能力。为了使LDA能有效地应对多模态现象，国内外众多学者在传统LDA中引入了数据的局部结构。比较典型的有：非参数<sup>1</sup>判别分析(Nonparametric Discriminant Analysis, NDA) [<xref ref-type="bibr" rid="hanspub.41845-ref4">4</xref>]，局部Fisher判别分析(Local Fisher Discriminant Analysis, LFDA) [<xref ref-type="bibr" rid="hanspub.41845-ref3">3</xref>] 和局部敏感判别分析 [<xref ref-type="bibr" rid="hanspub.41845-ref5">5</xref>] (Locality Sensitive Discriminant Analysis, LSDA)等。更为具体地，NDA通过将每个样本替换表示成其近邻样本加权的形式引入了数据的局部结构。LFDA首先将LDA中的类内和整体散度矩阵重新表示成样本间距离平方和的形式，然后再引入局部保留投影的思想使样本在降维前后保持着一致的邻居关系，从而达到局部结构的引入。不同于LFDA，LSDA从谱图理论出发，通过将图表示成拉普拉斯矩阵的形式引入了数据的局部结构。此外，还有许多在LFDA和LSDA模型基础上的变体 [<xref ref-type="bibr" rid="hanspub.41845-ref6">6</xref>] [<xref ref-type="bibr" rid="hanspub.41845-ref7">7</xref>] [<xref ref-type="bibr" rid="hanspub.41845-ref8">8</xref>]，这些变体本质上都是通过改变样本间相似关系的权重值或者图节点间边值的大小得到对数据结构更准确的近邻表达。不幸的是，这些方法在引入数据局部结构时都必须人为设定样本的近邻个数。该近邻个数参数决定性地影响着数据降维后的可分性，且其具体值的确定需要科研人员对数据的先验知识具有很全面的认识，这很大程度上限制了局部线性判别分析方法的广泛应用 [<xref ref-type="bibr" rid="hanspub.41845-ref9">9</xref>]。因此，仅根据已有的数据样本，自动对数据进行局部线性判别分析是一个非常值得研究的问题 [<xref ref-type="bibr" rid="hanspub.41845-ref10">10</xref>]，同时在图像检索 [<xref ref-type="bibr" rid="hanspub.41845-ref4">4</xref>] 和人脸识别 [<xref ref-type="bibr" rid="hanspub.41845-ref11">11</xref>] 等方面具有很重要的应用价值。</p><p>受自权重自适应局部判别分析(Self-weighted Adaptive Locality Discriminant Analysis, SALDA) [<xref ref-type="bibr" rid="hanspub.41845-ref9">9</xref>] 的启发，本文提出了一种无参数的局部判别分析(Parameter-free Local Linear Discriminant Analysis, Pf-LLDA)方法。Pf-LLDA首先建立一个与LDA类似的模型。但不同于LDA模型只需要求解与降维相关的变换矩阵，Pf-LLDA不仅需要求解与降维相关的变换矩阵还需要求解与数据局部结构相关的权重矩阵。通过运用交替方向的优化求解思路 [<xref ref-type="bibr" rid="hanspub.41845-ref12">12</xref>]，Pf-LLDA模型在无需设置近邻个数的前提下得到了反映数据结构图的权重矩阵和考虑了数据局部结构性质的变换矩阵。最后，根据变换矩阵得到了测试样本的低维表示，并将其输入到k近邻分类器中进行验证。在仿真和真实数据集上的实验结果表明，Pf-LLDA不仅在多模态问题上实现了无参数的局部判别分析性能，而且在手写字体识别问题上相比其他方法得到了更优的结果。</p></sec><sec id="s6"><title>2. 算法描述</title><p>本章从已有的工作出发，引出了无参数的局部判别分析(Parameter-free Local Linear Discriminant Analysis, Pf-LLDA)模型，并给出了具体的求解方法与伪代码。更具体地，2.1节、2.2节和2.3节分别概述了线性判别分析(Linear Discriminant Analysis, LDA)、局部Fisher判别分析(Local Fisher Discriminant Analysis, LFDA)和自权重自适应局部判别分析(Self-weighted Adaptive Locality Discriminant Analysis, SALDA)模型；2.4节详细阐述了Pf-LLDA模型与求解方案。</p><sec id="s6_1"><title>2.1. 线性判别分析(LDA)</title><p>给定n个m维的数据样本 X = [ x 1 , ⋯ , x n ] ∈ ℝ m &#215; n ，并且已知这些样本来自c个不同的类别，LDA可以通过以下固定整体分散度的同时最小化类内分散度的方式得到具有判别性的变换矩阵 A ∈ ℝ m &#215; d 。</p><p>min A Tr ( A T S w A ) s . t . A T S t A = I d (1)</p><p>这里，d表示低维目标空间的维数； Tr ( X ) 表示对矩阵X的求迹运算； X T 表示对矩阵X转置； I d 表示 d &#215; d 维的单位阵。 S w 和 S t 分别表示数据的类内散度矩阵和整体散度矩阵，并分别由以下公式计算 [<xref ref-type="bibr" rid="hanspub.41845-ref3">3</xref>] [<xref ref-type="bibr" rid="hanspub.41845-ref7">7</xref>]：</p><p>S w = ∑ i = 1 c ∑ j = 1 n i ( x j i − μ i ) ( x j i − μ i ) T = 1 2 ∑ i = 1 c 1 n i ∑ j = 1 n i ∑ k = 1 n i ( x j i − x k i ) ( x j i − x k i ) T , (2)</p><p>S t = ∑ j = 1 n ( x i − μ ) ( x i − μ ) T = 1 2 n ∑ j = 1 n ∑ k = 1 n ( x j − x k ) ( x j − x k ) T . (3)</p><p>其中， x j i 表示属于类别i的样本 x j ； n i 表示属于类别i的样本个数； μ 和 μ i 分别表示所有样本的均值和属于类别i的样本均值。最终，LDA的变换矩阵A可以通过求解如下广义特征值问题的最小的特征值所对应的特征向量得到 [<xref ref-type="bibr" rid="hanspub.41845-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.41845-ref2">2</xref>]：</p><p>S w A = λ S t A . (4)</p></sec><sec id="s6_2"><title>2.2. 局部Fisher判别分析(LFDA)</title><p>局部Fisher判别分析通过将样本的近邻结构引入到构建类内散度矩阵和整体散度矩阵中，使得模型具有局部判别能力并能很好地对多模态分布下的数据进行有效降维。具体地，LFDA首先以如下方式重新定义了LDA的类内散度矩阵(2)和整体散度矩阵(3)：</p><p>S ^ w = 1 2 ∑ j = 1 n ∑ k = 1 n W j k ( w ) ( x j − x k ) ( x j − x k ) T = ∑ j = 1 n ( ∑ k = 1 n W j k ( w ) ) x j x k T − ∑ j = 1 n ∑ k = 1 n W j k ( w ) x j x k T = X D ( w ) X T − X W ( w ) X T = X L ( w ) X T , (5)</p><p>S ^ t = 1 2 ∑ j = 1 n ∑ k = 1 n W j k ( w ) ( x j − x k ) ( x j − x k ) T = X D ( t ) X T − X W ( t ) X T = X L ( t ) X T . (6)</p><p>其中， D ( i ) ∈ ℝ n &#215; n , i = { w , t } 表示以对称矩阵 W ( i ) ∈ ℝ n &#215; n , i = { w , t } 的行和为对角元素构造的对角矩阵；而权重矩阵 W ( i ) ∈ ℝ n &#215; n , i = { w , t } 以如下形式定义：</p><p>W j k ( w ) = { W i j / n i , if   y j = y k = i 0 , otherwise (7)</p><p>W j k ( t ) = { W i j / n ,         if   y j = y k 1 / n ,               otherwise (8)</p><p>这里， y j 表示样本 x j 的类别标签；W表示与样本近邻相关的仿射矩阵。一种简单的定义方式是：当样本 x j 与样本 x k 互为近邻时， W i j = 1 ，否则 W i j = 0 。更多关于W的定义方式可以参考文献 [<xref ref-type="bibr" rid="hanspub.41845-ref3">3</xref>]，需要强调的是，无论哪种定义方式，样本近邻点的个数都是需要人为指定的。最后LFDA使用改进的类内散度矩阵 S ^ w 和整体散度矩阵 S ^ t 对应地替换LDA中的 S w 和 S t ，得到与LDA类似的模型：</p><p>min A Tr ( A T S ^ w A ) s . t . A T S ^ t A = I d (9)</p><p>从而以类似于公式(4)的方式求得最终的变换矩阵。</p></sec><sec id="s6_3"><title>2.3. 自权重自适应局部判别分析(SALDA)</title><p>不同于LFDA使用人为指定样本近邻个数的方式来挖掘数据的局部结构，SALDA尝试根据数据自身信息自适应地挖掘数据的局部流型结构来达到局部判别分析的目的。具体而言，首先根据矩阵的迹运算和矩阵元素形式的F-范数等价的性质，即对于方阵 X ∈ ℝ m &#215; n ， Tr ( X T X ) = ‖ X ‖ F 2 = ∑ i = 1 m ∑ j = 1 n X i j 2 ，以及公式(2)，易得：</p><p>Tr ( A T S w A ) = 1 2 ∑ i = 1 c 1 n i ∑ j = 1 n i ∑ k = 1 n i ‖ A T ( x j i − x k i ) ‖ F 2 , (10)</p><p>这里 ‖ X ‖ F 表示矩阵X元素形式的F-范数。因此，单独将控制样本间差异的权重W提出来并控制其为正数，则由公式(10)与传统LDA模型(1)便可得到SALDA模型 [<xref ref-type="bibr" rid="hanspub.41845-ref9">9</xref>]：</p><p>min A ∑ i = 1 c ∑ j = 1 n i ∑ k = 1 n i W j k i ‖ A T ( x j i − x k i ) ‖ F 2 s . t . A T S t A = I d , W j k i ≥ 0 , (11)</p><p>需要注意的是，这里的权重W只是作为常数值单独进行表示而非模型的求解变量。参考LFDA中定义近邻结构的仿射矩阵发现，其内部的每一个元素表示样本相关性的大小。换句话说，当样本距离越近时，对应在仿射矩阵中的值就越大。因此，根据F-范数与欧拉距离的相似性定义，SALDA通过以下对同类别 内的样本对 x j 和 x k 的求欧拉距离的倒数形式定义W中的每一个元素值 [<xref ref-type="bibr" rid="hanspub.41845-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.41845-ref14">14</xref>]：</p><p>W j k i = 1 2 ‖ A T ( x j i − x k i ) ‖ F , (12)</p><p>当得到W时，模型就可以重新表示成：</p><p>min A Tr ( A T X L X T A ) s . t . A T S t A = I d (13)</p><p>这里，拉普拉斯矩阵 L = D − W ，D是由权重矩阵W得到的对角矩阵，其对角线上的元素为W对应行中所有元素值之和，即 D i i = ∑ j = 1 n W i j 。显然，若记 S ˜ w = X L X T ，模型(13)与模型(1)具有类似的结构，所以同样可以通过广义特征值分解的方法进行求解。</p><p>值得强调的是，SALDA为了运用近邻思想并避开近邻个数的设置来挖掘数据的局部结构，采用构建具有仿射矩阵性质的方式固定计算权重矩阵W。这种方式虽然在构建仿射矩阵方面似乎是有效的，但是在整体模型上却不是很合理。一方面，模型(11)中的待求解变量只有变换矩阵A，但模型却是以迭代计算(12)和求解子模型(13)的方式进行求解；另一方面，W并不是模型要求解的参数，但又参与模型的更新求解，这不仅模糊了模型的可解释性也导致模型的求解并不一定收敛。</p></sec><sec id="s6_4"><title>2.4. 无参数的局部判别分析(Pf-LLDA)</title><p>受SALDA启发，我们将权重矩阵 假定为一个需要求解的未知量，得到如下Pf-LLDA模型：</p><p>min ( A , W ) ∑ i = 1 c n i ∑ j = 1 n i ∑ k = 1 n i W i j i 2 ‖ A T ( x j i − x k i ) ‖ F 2 s . t . A T S t A = I d , W j k i ≥ 0 , ∑ k = 1 n i W j k i = n i n . (14)</p><p>相比于SALDA模型(11)，Pf-LLDA主要有以下几方面的不同：</p><p>• Pf-LLDA模型将变换矩阵A与权重矩阵W共同作为模型待求解的未知量进行建模，而SALDA只对变换矩阵A进行了建模；</p><p>• Pf-LLDA通过控制同类别样本的权重和为定值，理论地给出了W的自适应更新公式。相比于SALDA直接设定而无法保证收敛的方式，Pf-LLDA关于W的更新不仅具有可解释性而且保证模型收敛；</p><p>• Pf-LLDA通过设定不同类样本的权重和为 n i / n 的方式对不同类别进行了区分使得模型适用于类别失衡问题从而更好地提升降维性能。</p><p>模型(14)可以通过交替方向的优化求解思路 [<xref ref-type="bibr" rid="hanspub.41845-ref12">12</xref>] 迭代进行求解。下面具体给出了针对W和A的求解方式。</p><sec id="s6_4_1"><title>2.4.1. 固定W，更新A</title><p>当权重矩阵固定，即W为已知数时，模型(14)转换成：</p><p>min A ∑ i = 1 c n i ∑ j = 1 n i ∑ k = 1 n i W i j i 2 ‖ A T ( x j i − x k i ) ‖ F 2 s . t . A T S t A = I d (15)</p><p>与公式(10)的推导类似，模型(15)可进一步转换成模型(13)的形式。因此以上问题(15)可以通过广义特征值的方法进行求解。</p></sec><sec id="s6_4_2"><title>2.4.2. 固定A，更新W</title><p>当变换矩阵固定，即A为已知数时，模型(14)转换成：</p><p>min W ∑ i = 1 c n i ∑ j = 1 n i ∑ k = 1 n i W i j i 2 ‖ A T ( x j i − x k i ) ‖ F 2 s . t . W j k i ≥ 0 , ∑ k = 1 n i W j k i = n i n . (16)</p><p>模型(16)可以通过分别求解它的一般化子模型来得到最终的W。具体而言，对于任意确定的i和j，令向量 u = W j ⋅ i ∈ ℝ n i &#215; 1 ，其中 W j ⋅ i = [ W j 1 i , ⋯ , W j n i i ] T 。同时令对角矩阵 V = d i a g ( ‖ A T ( x j i − x k i ) ‖ F 2 ) , k = 1 , ⋯ , n i ，即V的对角线元素为 V k k = ‖ A T ( x j i − x k i ) ‖ F 2 。所以模型(16)在固定i和j时，可得到通用子模型：</p><p>min u u T V u s . t . u ≥ 0 , u T 1 = n i n . (17)</p><p>这里 1 ∈ ℝ n i &#215; 1 表示长度为 n i 的全1列向量。模型(17)可以通过拉格朗日乘子法 [<xref ref-type="bibr" rid="hanspub.41845-ref1">1</xref>] 计算得出u的每一个元素，具体如下：</p><p>u k = n i n ⋅ V k k ( ∑ t = 1 n i 1 V t t ) − 1 , k = 1 , ⋯ , n i . (18)</p><p>所以，模型(16)的最优解 W * 满足：</p><p>W j k i * = n i n ⋅ ‖ A T ( x j i − x k i ) ‖ F 2 ( ∑ t = 1 n i 1 ‖ A T ( x j i − x k i ) ‖ F 2 ) − 1 ,     i = 1 , ⋯ , c ; j = 1 , ⋯ , n i ; k = 1 , ⋯ , n i . (19)</p><p>有趣的是，Pf-LLDA通过设置W为模型变量的方式求解出来的结果与SALDA分析仿射矩阵性质的方式固定W的结果具有类似的结构。但由于SALDA缺乏理论的推导，同时更新公式(12)与(19)并非完全一样，导致SALDA模型求解时并不一定收敛。最终求解模型(14)的算法流程如算法1所示，其中初始化目标函数值 o 0 = + ∞ , o 1 = 1 保证了算法能正常进行循环迭代求解过程。当目标函数值在迭代更新前后的变化量小于精度误差 ε = 10 − 6 时，循环终止，并返回最终的变换矩阵A。</p><p>Algorithm 1. The algorithm of parameter-free local linear discriminant analysis</p><p>算法1. 无参数的局部判别分析算法</p></sec></sec></sec><sec id="s7"><title>3. 实验与分析</title><p>为了系统检验算法的有效性，我们分别对比了本文提出的算法与其它相关算法在两个仿真数据集与一个真实数据集上的表现。</p><sec id="s7_1"><title>3.1. 仿真数据集上的实验</title><p>如图1所示，每个数据集中有两个类，每类包含200个二维的数据点。图1a中两个类别的数据由均值为[−1, 0]和[1, 0]，方差同为[0.1, 0; 0, 1]的二维高斯分布函数生成。图1b中类别I的数据由均值为[−3, 0]和[3, 0]，方差同为[0.5, 0; 0, 0.5]的两个二维高斯分布函数生成；类别II的数据由均值为[0, 0]，方差为[0.1, 0; 0, 1]的二维高斯分布函数生成。显然，图1a中的不同类别的数据点只由一个高斯分布产生，即只存在单模态现象；而图1b中的类别I的数据集是由两个高斯分布的数据点构成，所以存在多模态现象。</p><p>图1. 不同方法在仿真数据集的投影方向</p><p>我们在这两个仿真数据上对比了线性判别分析(Linear Discriminant Analysis, LDA) [<xref ref-type="bibr" rid="hanspub.41845-ref4">4</xref>]、局部Fisher判别分析(Local Fisher Discriminant Analysis, LFDA) [<xref ref-type="bibr" rid="hanspub.41845-ref3">3</xref>]、自权重自适应局部判别分析(Self-weighted Adaptive Locality Discriminant Analysis, SALDA) [<xref ref-type="bibr" rid="hanspub.41845-ref9">9</xref>] 和我们提出的无参数的局部判别分析(Parameter-free Local Linear Discriminant Analysis, Pf-LLDA)方法。如图1所示，每种方法的一维投影线已经被描绘出来。不难发现，在单模态数据集上(图1a)，LDA、LFDA和Pf-LLDA都能找到合理的投影方向。但在多模态数据集上(图1b)，LDA却失败了。另一方面，SALDA无论在哪种模态的数据集上似乎都不具备判别能力。其主要的原因有：1) 由2.4节的分析和2.3节中权重更新公式(12)可知，SALDA通过简单对样本距离求倒数的方式很难保证模型收敛；2) 分析SALDA的代码<sup>2</sup>实现发现，SALDA主要是因为迭代了超过所设置的最大次数(1000次)而跳出模型求解。这就导致了SALDA得到的投影结果更倾向于陷入随机的一个局部最优值。相比之下，我们不仅通过理论证明了Pf-LLDA权重的更新方式实现了如LFDA的局部判别性分析，同时在这两个仿真数据集上以算法1的实现方式都能保证在100次迭代内完成收敛。</p></sec><sec id="s7_2"><title>3.2. 真实数据集上的实验</title><p><sup>2</sup>https://github.com/guomuhan/salda2/blob/master/ADLDA.m</p><p>USUP<sup>3</sup>是一个手写数字图像数据集。它是模式识别领域研究所广泛使用的数据集之一，扫描于美国邮局的邮件中，具有很好的普适性。我们随机取了其中5500张手写数字图像作为我们的实验数据，每张图像是由16 &#215; 16像素点的灰度图组成。图2显示了该数据集中的部分样本。</p><p>需要说明的是，为了避免降维算法中存在的过度降维 [<xref ref-type="bibr" rid="hanspub.41845-ref11">11</xref>] 和小样本 [<xref ref-type="bibr" rid="hanspub.41845-ref15">15</xref>] 问题，与文献 [<xref ref-type="bibr" rid="hanspub.41845-ref13">13</xref>] 类似，我们首先对所有数据进行主成分分析 [<xref ref-type="bibr" rid="hanspub.41845-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.41845-ref2">2</xref>] 处理，将每个样本表示成79维的向量。然后，将训练样本输入到LDA相关的算法中得到变换矩阵和低维表示的训练样本。接着，根据变换矩阵对测试样本进行降维，得到低维表示的测试样本。最后，将低维表示的测试样本和训练样本作为1近邻算法 [<xref ref-type="bibr" rid="hanspub.41845-ref1">1</xref>] [<xref ref-type="bibr" rid="hanspub.41845-ref9">9</xref>] 的输入，得到关于测试样本的分类准确率。需要说明的是，为了结果的准确性，每组实验都会独立重复30次并将最终准确率的平均值记录下来。</p><p>图2. USUP数据集中的部分样本</p><p>我们在真实数据集对比了LDA、NDA、FLDA、SALDA和局部敏感判别分析 [<xref ref-type="bibr" rid="hanspub.41845-ref5">5</xref>] (Locality Sensitive Discriminant Analysis, LSDA)方法。并用控制变量法从训练样本个数、所降的维度、近邻个数和类别个数四个角度进行实验验证。每个实验角度的参数由表1给出。具体而言，表1中关于“训练样本个数”角度的实验参数由第二列给出。此时，样本最终的维度为60；通过引入近邻个数实现的局部线性判别分析算法的近邻个数参数为5；数据的类别个数为5；而来自每类的训练样本个数为*：表示它是这组实验所要研究的变量。其他三个角度的实验参数设置同理。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> The detailed parameters for each experimen</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >实验的角度 固定的参数</th><th align="center" valign="middle" >训练样本个数</th><th align="center" valign="middle" >所降的维度</th><th align="center" valign="middle" >近邻个数</th><th align="center" valign="middle" >类别个数</th></tr></thead><tr><td align="center" valign="middle" >每类训练样本个数</td><td align="center" valign="middle" >*</td><td align="center" valign="middle" >20</td><td align="center" valign="middle" >20</td><td align="center" valign="middle" >20</td></tr><tr><td align="center" valign="middle" >所降的维度</td><td align="center" valign="middle" >60</td><td align="center" valign="middle" >*</td><td align="center" valign="middle" >60</td><td align="center" valign="middle" >60</td></tr><tr><td align="center" valign="middle" >近邻个数</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >*</td><td align="center" valign="middle" >5</td></tr><tr><td align="center" valign="middle" >类别个数</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >*</td></tr></tbody></table></table-wrap><p>表1. 各个实验的具体参数</p><p>表2对比了当每类训练样本为5、10、20、40、80、160和320时，各种方法在USUP数据集上的分类准确率。不难发现，当所降维度、近邻个数和类别个数相同时，所有方法的分类准确率会随着训练样本的增加而有所提升。相比之下，Pf-LLDA总体上得到了更好的分类效果。特别是在训练样本数较少的情况下，Pf-LLDA的优势更为明显。</p><table-wrap-group id="2"><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> The classification accuracy with different number of training samples (%</title></caption><table-wrap id="2_1"><table><tbody><thead><tr><th align="center" valign="middle" >方法 每类的 训练样本个数</th><th align="center" valign="middle" >LDA</th><th align="center" valign="middle" >NDA</th><th align="center" valign="middle" >FLDA</th><th align="center" valign="middle" >LSDA</th><th align="center" valign="middle" >SALDA</th><th align="center" valign="middle" >Pf-LLDA</th></tr></thead><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >27.36</td><td align="center" valign="middle" >59.55</td><td align="center" valign="middle" >22.62</td><td align="center" valign="middle" >54.82</td><td align="center" valign="middle" >51.11</td><td align="center" valign="middle" >61.73</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >29.85</td><td align="center" valign="middle" >64.82</td><td align="center" valign="middle" >21.96</td><td align="center" valign="middle" >53.30</td><td align="center" valign="middle" >70.60</td><td align="center" valign="middle" >76.21</td></tr></tbody></table></table-wrap><table-wrap id="2_2"><table><tbody><thead><tr><th align="center" valign="middle" >20</th><th align="center" valign="middle" >62.20</th><th align="center" valign="middle" >63.45</th><th align="center" valign="middle" >61.97</th><th align="center" valign="middle" >47.48</th><th align="center" valign="middle" >64.38</th><th align="center" valign="middle" >88.98</th></tr></thead><tr><td align="center" valign="middle" >40</td><td align="center" valign="middle" >85.40</td><td align="center" valign="middle" >86.73</td><td align="center" valign="middle" >86.28</td><td align="center" valign="middle" >73.89</td><td align="center" valign="middle" >63.56</td><td align="center" valign="middle" >90.52</td></tr><tr><td align="center" valign="middle" >80</td><td align="center" valign="middle" >90.29</td><td align="center" valign="middle" >91.94</td><td align="center" valign="middle" >90.90</td><td align="center" valign="middle" >87.65</td><td align="center" valign="middle" >73.94</td><td align="center" valign="middle" >92.74</td></tr><tr><td align="center" valign="middle" >160</td><td align="center" valign="middle" >91.91</td><td align="center" valign="middle" >94.66</td><td align="center" valign="middle" >93.52</td><td align="center" valign="middle" >93.95</td><td align="center" valign="middle" >81.58</td><td align="center" valign="middle" >94.27</td></tr><tr><td align="center" valign="middle" >320</td><td align="center" valign="middle" >92.89</td><td align="center" valign="middle" >95.67</td><td align="center" valign="middle" >95.52</td><td align="center" valign="middle" >95.58</td><td align="center" valign="middle" >86.69</td><td align="center" valign="middle" >95.80</td></tr></tbody></table></table-wrap></table-wrap-group><p>表2. 不同训练样本个数下的分类准确率(%)</p><p>表3对比了当训练样本数、近邻个数和类别个数相同时，不同方法将样本从高维降到不同低维度时的准确率。需要注意的是，每个样本经过主成分分析表示成79维的向量，所以所降的维度最高被设置为78。对比发现，在所降的维度较低时，Pf-LLDA相比于其他方法还是可以接受的。并且随着所降的维度的升高，Pf-LLDA实现了更高的准确率上限。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> The classification accuracy with different reduced dimensions (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法 所降的维度</th><th align="center" valign="middle" >LDA</th><th align="center" valign="middle" >NDA</th><th align="center" valign="middle" >FLDA</th><th align="center" valign="middle" >LSDA</th><th align="center" valign="middle" >SALDA</th><th align="center" valign="middle" >Pf-LLDA</th></tr></thead><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >85.62</td><td align="center" valign="middle" >77.28</td><td align="center" valign="middle" >84.26</td><td align="center" valign="middle" >32.63</td><td align="center" valign="middle" >67.96</td><td align="center" valign="middle" >71.60</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >85.57</td><td align="center" valign="middle" >84.06</td><td align="center" valign="middle" >87.39</td><td align="center" valign="middle" >50.00</td><td align="center" valign="middle" >62.53</td><td align="center" valign="middle" >77.05</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >85.07</td><td align="center" valign="middle" >86.59</td><td align="center" valign="middle" >86.86</td><td align="center" valign="middle" >74.82</td><td align="center" valign="middle" >66.94</td><td align="center" valign="middle" >83.45</td></tr><tr><td align="center" valign="middle" >32</td><td align="center" valign="middle" >85.69</td><td align="center" valign="middle" >87.76</td><td align="center" valign="middle" >87.27</td><td align="center" valign="middle" >77.92</td><td align="center" valign="middle" >70.81</td><td align="center" valign="middle" >88.95</td></tr><tr><td align="center" valign="middle" >64</td><td align="center" valign="middle" >85.92</td><td align="center" valign="middle" >86.93</td><td align="center" valign="middle" >86.17</td><td align="center" valign="middle" >72.80</td><td align="center" valign="middle" >64.40</td><td align="center" valign="middle" >90.47</td></tr><tr><td align="center" valign="middle" >70</td><td align="center" valign="middle" >85.47</td><td align="center" valign="middle" >86.43</td><td align="center" valign="middle" >86.06</td><td align="center" valign="middle" >70.36</td><td align="center" valign="middle" >63.03</td><td align="center" valign="middle" >90.28</td></tr><tr><td align="center" valign="middle" >78</td><td align="center" valign="middle" >85.46</td><td align="center" valign="middle" >85.94</td><td align="center" valign="middle" >85.47</td><td align="center" valign="middle" >69.28</td><td align="center" valign="middle" >66.02</td><td align="center" valign="middle" >90.40</td></tr></tbody></table></table-wrap><p>表3. 不同所降维度下的分类准确率(%)</p><p>图3描绘了对于不同的局部判别分析方法，近邻个数对分类准确率的影响。这里我们只展示了近邻个数从1到39时的结果。可以看到，不同的近邻个数对NDA、LFDA和LSDA的分类效果影响很大。特别地，LSDA的分类准确率由近邻个数为3时的73.73%降到近邻个数为37时的60.07%。相比之下，SALDA通过自权重的方式避开了对近邻个数的依赖从而实现了稳定的结果输出。但与2.4节中的分析一致，SALDA陷入随机的一个局部最优值导致其分类准确率并不理想。Pf-LLDA克服了SALDA的缺点，不仅实现了稳定的结果，同时也得到了较高的准确率。</p><p>图3. 近邻个数对分类准确率的影响</p><p>最后，我们验证了当类别个数从4增加到10时，不同方法的分类结果。如表4所示，相比于其他四种局部判别分析方法和作为基线的LDA方法，Pf-LLDA得到了显著的提升。这不仅依赖于Pf-LLDA利用了数据的局部结构，也依赖于我们所引入的无参方式自适应地挖掘出数据局部几何结构的差异性。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> The classification accuracy with different number of classes (%</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法 类别个数</th><th align="center" valign="middle" >LDA</th><th align="center" valign="middle" >NDA</th><th align="center" valign="middle" >FLDA</th><th align="center" valign="middle" >LSDA</th><th align="center" valign="middle" >SALDA</th><th align="center" valign="middle" >Pf-LLDA</th></tr></thead><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >43.24</td><td align="center" valign="middle" >53.39</td><td align="center" valign="middle" >43.17</td><td align="center" valign="middle" >34.82</td><td align="center" valign="middle" >75.43</td><td align="center" valign="middle" >89.45</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >60.86</td><td align="center" valign="middle" >64.38</td><td align="center" valign="middle" >61.08</td><td align="center" valign="middle" >47.87</td><td align="center" valign="middle" >65.99</td><td align="center" valign="middle" >88.86</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >68.86</td><td align="center" valign="middle" >72.90</td><td align="center" valign="middle" >70.44</td><td align="center" valign="middle" >53.47</td><td align="center" valign="middle" >65.01</td><td align="center" valign="middle" >85.81</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >74.21</td><td align="center" valign="middle" >76.92</td><td align="center" valign="middle" >75.44</td><td align="center" valign="middle" >56.13</td><td align="center" valign="middle" >64.76</td><td align="center" valign="middle" >85.95</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >72.85</td><td align="center" valign="middle" >75.13</td><td align="center" valign="middle" >74.27</td><td align="center" valign="middle" >54.90</td><td align="center" valign="middle" >60.09</td><td align="center" valign="middle" >83.15</td></tr><tr><td align="center" valign="middle" >9</td><td align="center" valign="middle" >72.94</td><td align="center" valign="middle" >73.88</td><td align="center" valign="middle" >73.84</td><td align="center" valign="middle" >54.23</td><td align="center" valign="middle" >58.81</td><td align="center" valign="middle" >80.93</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >75.33</td><td align="center" valign="middle" >75.21</td><td align="center" valign="middle" >75.89</td><td align="center" valign="middle" >56.95</td><td align="center" valign="middle" >60.52</td><td align="center" valign="middle" >81.07</td></tr></tbody></table></table-wrap><p>表4. 不同类别个数下的分类准确率(%)</p></sec></sec><sec id="s8"><title>4. 总结</title><p>本文提出了一种无参数的局部判别分析方法。该方法通过利用已知的数据信息，自适应地学习到了数据的低维局部结构。系统的实验结果表明，该方法在无需人为设置近邻个数的情况下，不仅挖掘出了数据的局部结构还有效的提高了分类准确率。未来将在模型中引入非线性或张量的思想，使模型适用于挖掘更复杂数据的数据结构。</p></sec><sec id="s9"><title>文章引用</title><p>黄礼泊,凌永权. 一种无参数的局部线性判别分析方法A Parameter-Free Local Linear Discriminant Analysis Method[J]. 计算机科学与应用, 2021, 11(04): 1042-1052. https://doi.org/10.12677/CSA.2021.114107</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.41845-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Fukunaga, K. (2013) Introduction to Statistical Pattern Recognition. Elsevier, New York.</mixed-citation></ref><ref id="hanspub.41845-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Farzana, A., Sadaoui, S. and Selim, B. (2021) Conceptual and Empirical Comparison of Dimensionality Reduction Algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE). Computer Science Review, 40, Article ID: 100378. &lt;br&gt;https://doi.org/10.1016/j.cosrev.2021.100378</mixed-citation></ref><ref id="hanspub.41845-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Sugiyama, M. (2007) Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis. Journal of Machine Learning Research, 8, 1027-1061.</mixed-citation></ref><ref id="hanspub.41845-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Cao, G.Q., Iosifidis, A. and Gabbouj, M. (2017) Multi-View Nonparametric Discriminant Analysis for Image Retrieval and Recognition. IEEE Signal Processing Letters, 24, 1537-1541. &lt;br&gt;https://doi.org/10.1109/LSP.2017.2748392</mixed-citation></ref><ref id="hanspub.41845-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Yu, H.Y., Gao, L.R., Li, W., et al. (2017) Locality Sensitive Discriminant Analysis for Group Sparse Representation-Based Hyperspectral Imagery Classification. IEEE Geoscience and Remote Sensing Letters, 14, 1358-1362.  
&lt;br&gt;https://doi.org/10.1109/LGRS.2017.2712200</mixed-citation></ref><ref id="hanspub.41845-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Yan, S.C., Xu, D., Zhang, B.Y., et al. (2006) Graph Embedding and Extensions: A General Framework for Dimensionality Reduction. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 29, 40-51.  
&lt;br&gt;https://doi.org/10.1109/TPAMI.2007.250598</mixed-citation></ref><ref id="hanspub.41845-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">谢钧, 刘剑. 一种新的局部判别投影方法[J]. 计算机学报, 2011, 34(11): 2243-2250.</mixed-citation></ref><ref id="hanspub.41845-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Nie, F.P., Xiang, S.M. and Zhang, C.S. (2007) Neighborhood Minmax Projections. In: International Joint Conferences on Artificial Intelligence, AAAI Press, Hyderabad, 993-998.</mixed-citation></ref><ref id="hanspub.41845-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Guo, M.H., Nie, F.P. and Li, X.L. (2018) Self-Weighted Adaptive Locality Discriminant Analysis. In: International Conference on Image Processing, IEEE Press, Athens, 3378-3382. &lt;br&gt;https://doi.org/10.1109/ICIP.2018.8451023</mixed-citation></ref><ref id="hanspub.41845-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Shi, Z.H., Wu, D.R., Huang, J., et al. (2020) Supervised Discriminative Sparse PCA with Adaptive Neighbors for Dimensionality Reduction. In: International Joint Conference on Neural Networks, IEEE Press, Glasgow, 1-8.  
&lt;br&gt;https://doi.org/10.1109/IJCNN48605.2020.9206927</mixed-citation></ref><ref id="hanspub.41845-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Wan, H., Guo, G.D., Wang, H., et al. (2015) A New Linear Discriminant Analysis Method to Address the Over-Reducing Problem. In: International Conference on Pattern Recognition and Machine Intelligence, Springer Press, Warsaw, 65-72. &lt;br&gt;https://doi.org/10.1007/978-3-319-19941-2_7</mixed-citation></ref><ref id="hanspub.41845-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">He, B.S. and Yuan, X.M. (2012) On the O(1/n) Convergence Rate of the Douglas-Rachford Alternating Direction Method. SIAM Journal on Numerical Analysis, 50, 700-709. &lt;br&gt;https://doi.org/10.1137/110836936</mixed-citation></ref><ref id="hanspub.41845-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Nie, F.P., Yuan, J.J. and Huang, H. (2014) Optimal Mean Robust Princi-pal Component Analysis. In: International Conference on Machine Learning, ACM Press, Beijing, 1062-1070.</mixed-citation></ref><ref id="hanspub.41845-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Jiang, W.H., Nie, F.P. and Huang, H. (2015) Robust Dictionary Learning with Capped l1-Norm. In: International Joint Conference on Artificial Intelligence, AAAI Press, Buenos Aires, 3590-3596.</mixed-citation></ref><ref id="hanspub.41845-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Alok, S. and Kuldip, P.K. (2015) Linear Discriminant Analysis for the Small Sample Size Problem: An Overview. International Journal of Machine Learning and Cybernetics, 6, 443-454. &lt;br&gt;https://doi.org/10.1007/s13042-013-0226-9</mixed-citation></ref></ref-list></back></article>