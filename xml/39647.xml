<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">OJLS</journal-id><journal-title-group><journal-title>Open Journal of Legal Science</journal-title></journal-title-group><issn pub-type="epub">2329-7360</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/OJLS.2021.91007</article-id><article-id pub-id-type="publisher-id">OJLS-39647</article-id><article-categories><subj-group subj-group-type="heading"><subject>OJLS20210100000_37626986.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>人文社科</subject></subj-group></article-categories><title-group><article-title>
 
 
  人工智能判断刑事证明标准的实践与问题
  Practice and Problems of Artificial Intelligence in Judging Criminal Proof Standards
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>文</surname><given-names>黎</given-names></name><xref ref-type="aff" rid="aff1"><sub>1</sub></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>重庆邮电大学网络空间安全与信息法学院，重庆</addr-line></aff><pub-date pub-type="epub"><day>29</day><month>12</month><year>2020</year></pub-date><volume>09</volume><issue>01</issue><fpage>48</fpage><lpage>53</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  人工智能在智慧法院建设中扮演重要角色，在刑事司法领域中也得到了有效的应用，其主要应用模式分为证据辅助判断模式、专家机器人模式和智能办案辅助模式。但司法实践中，人工智能对刑事证明标准的判断出现了诸多问题。如，人工智能本身技术局限带来判断方式僵硬化、算法黑箱导致的安全性问题、算法自带偏见和司法工作人员过渡依赖造成的自由裁量权受限问题以及产生判断事故后，责任如何归属。针对上述问题，实践中可以成立专门的算法监督机构解决算法黑箱问题，通过立法确立人工智能辅助地位，确立司法人员的主体地位，明确事故责任归属主体，加强司法人员关于人工智能的理论素养。做到这些，才能给人工智能在刑事司法领域中的发展，提供健康的生长土壤。
   Artificial intelligence plays an important role in the construction of intelligent court and has been effectively applied in the field of criminal justice. Its main application modes include evidence as-sisted judgment mode, expert robot and intelligent case handling assistant mode. But in judicial practice, there are many problems in AI’s judgment of criminal proof standard. For example, the technical limitations of ARTIFICIAL intelligence itself lead to the rigidity of judgment methods, se-curity problems caused by black boxes of algorithms, inherent bias of algorithms and limited dis-cretion caused by excessive dependence of judicial staff, as well as the attribution of responsibility after judgment accidents. In view of the above problems, a special algorithm supervision institution can be established to solve the algorithm black-box problem, establish the auxiliary status of ARTIFICIAL intelligence through legislation, establish the subject status of judicial personnel, de-fine the subject of responsibility attribution, and strengthen the theoretical literacy of judicial per-sonnel about ARTIFICIAL intelligence. Only by doing so can AI provide healthy soil for growth in the criminal justice field.
 
</p></abstract><kwd-group><kwd>人工智能系统，刑事证明标准，实践探索，问题解决, Artificial Intelligence System</kwd><kwd> Criminal Proof Standard</kwd><kwd> Practice Exploration</kwd><kwd> Problem Solving</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>人工智能在智慧法院建设中扮演重要角色，在刑事司法领域中也得到了有效的应用，其主要应用模式分为证据辅助判断模式、专家机器人模式和智能办案辅助模式。但司法实践中，人工智能对刑事证明标准的判断出现了诸多问题。如，人工智能本身技术局限带来判断方式僵硬化、算法黑箱导致的安全性问题、算法自带偏见和司法工作人员过渡依赖造成的自由裁量权受限问题以及产生判断事故后，责任如何归属。针对上述问题，实践中可以成立专门的算法监督机构解决算法黑箱问题，通过立法确立人工智能辅助地位，确立司法人员的主体地位，明确事故责任归属主体，加强司法人员关于人工智能的理论素养。做到这些，才能给人工智能在刑事司法领域中的发展，提供健康的生长土壤。</p></sec><sec id="s2"><title>关键词</title><p>人工智能系统，刑事证明标准，实践探索，问题解决</p></sec><sec id="s3"><title>Practice and Problems of Artificial Intelligence in Judging Criminal Proof Standards<sup> </sup></title><p>Li Wen</p><p>Law School of Cyberspace Security and Information, Chongqing University of Posts and Telecommunications, Chongqing</p><p><img src="//html.hanspub.org/file/7-2920349x4_hanspub.png" /></p><p>Received: Dec. 5<sup>th</sup>, 2020; accepted: Dec. 28<sup>th</sup>, 2020; published: Jan. 4<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/7-2920349x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Artificial intelligence plays an important role in the construction of intelligent court and has been effectively applied in the field of criminal justice. Its main application modes include evidence assisted judgment mode, expert robot and intelligent case handling assistant mode. But in judicial practice, there are many problems in AI’s judgment of criminal proof standard. For example, the technical limitations of ARTIFICIAL intelligence itself lead to the rigidity of judgment methods, security problems caused by black boxes of algorithms, inherent bias of algorithms and limited discretion caused by excessive dependence of judicial staff, as well as the attribution of responsibility after judgment accidents. In view of the above problems, a special algorithm supervision institution can be established to solve the algorithm black-box problem, establish the auxiliary status of ARTIFICIAL intelligence through legislation, establish the subject status of judicial personnel, define the subject of responsibility attribution, and strengthen the theoretical literacy of judicial personnel about ARTIFICIAL intelligence. Only by doing so can AI provide healthy soil for growth in the criminal justice field.</p><p>Keywords:Artificial Intelligence System, Criminal Proof Standard, Practice Exploration, Problem Solving</p><disp-formula id="hanspub.39647-formula24"><graphic xlink:href="//html.hanspub.org/file/7-2920349x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/7-2920349x7_hanspub.png" /> <img src="//html.hanspub.org/file/7-2920349x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 人工智能判断刑事证明标准的现状</title><p>从上世纪七十年代布坎南发表《关于人工智能和法律若干问题的考察》到我国大力推进智慧司法，纵观人工智能技术发展史，人工智能技术在司法领域的应用范围越来越深入，国内外相继出现了相应的辅助办案系统，如，英国皇家检察署证据判断辅助系统、美国“专家机器人”系统、上海“206”刑事案件智能办案辅助系统等。这些智能系统的出现，有效地促进了传统的刑事证明标准判断方式上的创新，与此同时，在刑事证明标准判断领域，前沿科研人员也一直在尝试将人工智能系统通过不同方式应用到司法工作人员判断刑事证明标准的过程之中。国外的智能系统，在司法实践中的应用可以分为两类：1) 证据辅助判断模式，指司法人员在进行刑事证明标准的判断时，借助人工智能技术对当前的案件证据进行分析，对决策行为应满足的条件及结果进行预测，并得出结论，帮助司法人员分析案件证据，减少决策时间。英国皇家检察署证据判断辅助系统 [<xref ref-type="bibr" rid="hanspub.39647-ref1">1</xref>] 正是这种证据辅助判断模式当中的典范。2) 专家机器人模式。“专家机器人” [<xref ref-type="bibr" rid="hanspub.39647-ref2">2</xref>] 是美国教授帕梅拉在《专家机器人：利用人工智能协助法官采纳科学性专家证言》提出的概念，在这篇文章中他提到用人工智能辅助法官对专家证言的可采性进行判断。“专家证言”指专家证人基于他们自身的知识和能力，根据相关科学理论对案件事实进行合理推测所的出的结论，其功能在于帮助法官和陪审团成员理解与案件事实相关的专业知识。但是在美国，专家证言的使用上，法官充当着看门人的作用，其任务是对专家证言的科学性基础进行评判，在有必要的情况下，也可以对专家证人的资历资质进行审查。那么，法官应怎样去评判专家证人的证言呢？答案毋庸置疑，法官怎么会有能力去评判另外一个不同领域的权威人士所给出的证言呢？法官不可能做到样样俱通，所以专家机器人应运而生。但是这两种模式的局限在于只能帮助司法人员对证据进行判断。反观国内的智能办案辅助模式，就能较好地辅助司法人员判断刑事证明标准，如上海“206”刑事案件智能办案辅助系统 [<xref ref-type="bibr" rid="hanspub.39647-ref3">3</xref>]，该系统主要利用互联网、大数据分析、云计算等现代科学先进技术，再结合图文识别、自然语言理解、司法实体识别、智能语音识别、实体关系分析、司法要素自动提取等人工智能技术，制定统一适用的证据规则指引和证据标准指引，再嵌入公检法司法机关刑事办案系统中，以达到司法办案人员用统一证据标准、规则，对证据进行检验、监督、把关的目的。截至目前为止，“206”系统已搭载了二十余项功能，并收集了相关案件信息三千余万条。在实际应用中，比如盗窃罪，“206”系统会在庭审过程中对涉案证据进行“7环13段”分类，把证据按放在证据链中不同的阶段，方便法官对全案证据是否达到排出合理怀疑做出判断，或者判断犯罪行为是否符合法条的各个要素，辅助法官对刑事证明标准进行判断。</p></sec><sec id="s6"><title>2. 人工智能判断刑事证明标准存在的主要问题</title><p>上述人工智能判断刑事证明标准的方式，虽然有其独到之处，发挥了人工智能相应的优势。但在实践中，还是存在了诸多问题和争议，下文将从人工智能系统本身、运用过程和运用结果三个角度对出现的主要问题进行分析。</p><sec id="s6_1"><title>2.1. 系统本身的技术限制</title><p>由于智能系统本身的技术缺陷，在进行刑事证明标准判断时，智能系统自带的算法会衍生出许多问题。如，判断方式框架化，算法黑箱，算法偏见等因素导致的判断方式僵硬，人工智能系统的安全性等问题。</p><sec id="s6_1_1"><title>2.1.1. 人工智能系统的判断方式过于形式化、僵硬化</title><p>在判断刑事证明标准过程中，司法人员理应采取主客观相结合的方式，去应对各种各样的案件。而人工智能办案系统由于设计者的初衷，对案件的进程进行了一步一步的规划，拟定了一个既存的框架对证据进行判断。无疑是想一蹴而就，制定一把通用的尺子去丈量纷繁复杂的案件。这种做法可不可取呢？试想，在司法实践中，科学技术的快速革新，犯罪分子犯罪的方式层出不穷，滋生大量的新型案件和疑难杂症的案件，人工智能办案系统能以固有框架去逐一对证据进行审查判断，提高司法效率吗？以“206”系统为例，该系统只是通过对大量案件进行整理，对案件中各个要素进行分类，对每一个要素的各个条件给予甄别判断，并且只有完成目前的步骤，才能开展下一阶段。有相关使用者说道，我在办一个盗窃案时，“206”系统识别出案件的卷宗里缺少鉴定意见，当时还十分庆幸这个系统非常实用。但在另外一个盗窃案中，失窃的是人民币，系统也提出缺少鉴定意见并要求进行相关的解释，我就对该系统弃之若履了，后面在也没有使用过。综上，目前的人工智能技术判断刑事证明标准过于形式化，僵硬化，需要技术人员进行不断的升级，有一定的滞后性，有可能影响司法的公正。法律源于生活，不能用一套固定的规则对生活中形形色色的人情关系进行简单的直接套用，这样做会把灵活多变的判断方式进行模板化，套路化，不利于司法的发展。</p></sec><sec id="s6_1_2"><title>2.1.2. 人工智能系统的安全性无法保障</title><p>人工智能技术的本质是算法，不断的数据输入到结果输出，计算机可以对数据经验的自动学习生成高级认知框架，但这种机器学习的过程是不透明的，这种不透明性导致了算法黑箱 [<xref ref-type="bibr" rid="hanspub.39647-ref4">4</xref>] 的存在。一方面，算法设计者一般也是非法学领域的高端人才，对有些专业的晦涩难懂的法律专业术语可能会产生认知误差，这也进一步导致了算法的不确定性，导致人工智能得出的结果出现偏差。另一方面，人工智能系统基本都是由第三方技术开发公司研发的，对于系统的定期维护、升级，一般也外包给这些公司。这种雇佣关系，很容易造成不法分子通过此渠道窃取相关的个人隐私和商业机密，更严重者造成涉密信息的泄露。由于卷宗的特性，里面会详细记载与案件相关的任何信息，一旦第三方技术公司掌握了这些信息，我们就很难控制信息不被泄露，毕竟“高价之下，必有勇夫”，所以这方面我们应引起极大的重视。还有一点，第三方技术公司可能会借助人工智能系统干预司法公正，降低司法公信力。如，腾讯和浙江高院联手打造的“微法院” [<xref ref-type="bibr" rid="hanspub.39647-ref5">5</xref>]，只要浙江法院审理跟腾讯公司相关的案件，公众都会心存疑虑，对判决结果抱有怀疑的态度。设计人工智能办案系统的初衷，本来是为了在保障司法公正的同时提高司法效率，但现在的情形却有点违背了设计的初衷，把司法的公正性推到了风口浪尖。</p></sec><sec id="s6_1_3"><title>2.1.3. 人工智能系统可能存在主观偏见</title><p>算法偏见 [<xref ref-type="bibr" rid="hanspub.39647-ref6">6</xref>]，近期受到社会广泛关注。机器怎么会带有偏见呢？前文提到，人工智能的本质是算法，人工智能系统是技术研发人员通过计算机语言进行编写的程序构成的。在算法的编写期间，技术人员把自己的价值判断通过算法灌输到了人工智能系统，所以在这个过程中就有可能把自己的偏见移植给了系统。在人工智能系统对刑事证明标准进行判断时，这种偏见会让判断结果产生差异。据相关研究显示，在美国刑事司法领域中运用的风险评估工具对被追诉一方的肤色存在种族偏见，对不同肤色的人给予了不一样的风险评估指数，所以人工智能系统可能存有主观偏见，并不是空穴来风。当然，就目前来看算法偏见在国内还没出现此类现象，其原因可能是因为国内人工智能技术还在起步阶段，主要还是在做一些简单数据分类处理，导致这种现象不明显。但我国应借鉴域外的经验教训，提前防范于未然，让此类可能影响对刑事证明标准进行判断的因素扼杀于摇篮之中。毕竟对刑事证明标准的判断关乎到无罪与有罪的界限，是刑事领域中打击犯罪和保障人权的关键所在，所以消除机器带来的偏见问题，我们仍应引起高度重视。</p></sec></sec><sec id="s6_2"><title>2.2. 过度依赖导致法官自由裁量权受限</title><p>不可否认，人工智能技术在刑事证明标准领域，为司法判断人员提供了很大的便利，提高了进行证据审查、判断时，司法资源的利用率。“人 + 机”的刑事证明标准判断方式，也符合当下国家大力建设科技强国，智慧法院的时代背景。但在此类智能系统的运用上，容易造成法官对智能系统的过度依赖，一方面，这种智能系统的运用可以间接的缩小法官对自己的自由裁量权的适用范围。近年来案件检索系统和裁判文书系统的逐渐完善，相应的智能系统也投入到司法实践的运用当中。比如前文所述的“206”智能办案系统，让案件资源共享在法官之间成为了便捷的桥梁，法官可以随时对类似案件进行检索，查看其他法官在针对类似刑事证明标准判断问题相应的经验和做法。但带来便利的同时，更需要引起我们关注的是，若法官不能独立进行对刑事证明标准的判断，过多依赖人工智能系统，容易造成大事小事一手抓的社会风气，造成法的滥用。人天生具有思想惰性，容易对既有的规则或模式过度套用，对法官的自由裁量权起到不良的作用。另一方面，虽然目前人工智能在刑事证明标准领域中的运用，主要涉及对刑事证据进行形式审查，停留在对证据能力进行判断的层面，致使大多数人工智能系统和司法实践相结合的模式还停留在初级司法人工智能阶段。但随着科技快速革新，人工智能或许可以帮助法官在进行刑事证明标准判断时，进行一些简单的逻辑上的推理和论证。所以如何解决过度依赖问题，把握人工智能和刑事证明标准判断之间的动态平衡，是保障法官自由裁量权在刑事证明标准的判断之中不被削弱的关键之处。</p></sec><sec id="s6_3"><title>2.3. 出现判断错误时责任主体如何确定</title><p>每一个新兴事物的前期发展，解决新老问题都是其出初期到成熟期的必经之路。科技本身的滞后性，必将导致人工智能在对新案或疑难案件中刑事证明标准的判断上出现问题。这是在智慧司法的建设过程中我们不得不解决的问题。目前国内也没有相应的法律法规对这一点进行明确的规定；反观域外，人工智能系统也主要是集中在律师事务所，其责任主体相对明朗，而在我国的法治大背景下，“人工智能 + 法院”的智能系统办案方式的处境显得十分严峻。在我国实行法院终审制度，意味着法院是定纷止争，惩罚犯罪分子，保障人权的最后手段，其中司法人员对刑事证明标准的判断起着决定性的作用。智慧法院的建设初衷，是为了实现同案同判，统一证据标准，严格规范司法工作人员在各个诉讼环节中的行为，提高民众对司法的公信力。所以，在出现判断事故时，责任主体如何确定显得极为重要。</p><p>纵观人工智能在刑事证明标准判断中的三种模式，智能办案系统本身具有很强的技术性，这导致了这些智能系统的研发一般交由第三方机构。那么人工智能在帮助司法人员进行刑事证明标准判断时出现司法事故，将由谁来承担责任？系统本身、第三方技术研发公司还是司法工作人员？每一个责任主体承担责任会导致不同的社会影响，比如归于智能办案系统本身的责任，这无疑是把司法判断失误当成是意外事故，这样做会导致公检法等司法机关怕用、不敢用甚至不能用。公检法作为国家公权力机关，本身应保证自己决策的正确性，才能提高社会的公信力，频繁的错误必将导致民众信赖被消耗殆尽，不符合法治精神。所以，人工智能系统判断刑事证明标准时出现事故，怎样合理划分事故责任，确定承担责任的主体，还值得商榷。</p></sec></sec><sec id="s7"><title>3. 人工智能判断刑事证明标准的未来路径</title><p>针对前文所述判断过程中存在的主要问题，为了人工智能系统更好地辅助判断证明标准，笔者将从以下四方面对人工智能判断刑事证明标准的完善路径提出建议：</p><sec id="s7_1"><title>3.1. 确立人工智能的辅助地位</title><p>通过立法确立人工智能辅助地位。当前的人工智能并没有跳出冯&#183;诺依曼型 [<xref ref-type="bibr" rid="hanspub.39647-ref7">7</xref>] 的范畴，其技术水平还停留在弱人工智能阶段。计算机还是主要通过算法对输入的信息进行处理加工，运用运算器、存储器进一步处理得出结论。如，前文所述，“206”智能办案系统，或域外的人工智能系统，都是采用的这种输出模型。这些系统大多搭载了数据分析功能、图文识别功能，可以帮助司法人员解决一些冗杂的技术含量低的任务。所以人工智能系统受技术的局限，目前只能用于辅助司法人员对刑事证明标准进行判断。退一步讲，就算人工智能在不久的将来迎来强人工智能时代，能够在某些方面进行拟人类的行为或思考方式，人工智能也仅仅只能起到工具性的作用。在我国，刑事证明标准的判断方式主要采取主客观相结合的模式。其主观性决定了人工智能不能独立地对刑事证明标准进行判断。一方面人类经验不可能全部转化为数据或算法，尤其是那部分只可意会不可言传的经验，可能根本都无法通过系统直接数据化。另一方面，目前人工智能技术中深度学习应用部分，主要是在语音、图像和文字领域方面有所发展，主要是点对点方面的研究。在进行刑事证明标准判断时，往往是将不同种类的经验糅合在一起进行最后的决策得出结论。所以其智能化程度并不影响其辅助定位。</p></sec><sec id="s7_2"><title>3.2. 明确司法人员主体地位和责任承担机制</title><p>当然，司法人员行刑事证明标准判断时，面对冗杂的证据材料，或需要专业领域的知识进行判断的证据，我们可以充分利用人工智能帮助提高工作效率。比如美国的专家机器人系统，可以对专家证言进行验证，解决法官的知识盲区。但如何把握好人工智能和法官裁量权之间的平衡，如何规范司法人员运用人工智能系统的行为，显得更为重要。对此，我们可以通过立法的方式规定司法工作者的最终决定权，确定司法工作人员的主体地位。可以从立法的角度对人工智能进行定位，明确其辅助地位，避免司法人员在进行刑事证明标准判断时过度依赖或滥用。通过立法的方式，对事故承担责任主体进行明确规定，确立司法人员责任承担优先原则。如前文所述，人工智能在参与刑事证明标准判断时只能起到辅助作用，若没有相应的规章制度，那就免不了一些司法人员抱着侥幸心理推卸责任，背离了智慧法院的建设初衷。同时可以建立配套的人员考核机制，规范司法人员在使用人工智能辅助判断刑事证明标准的行为，可以有效地预防司法工作人员在工作中怠于行使法定职责和义务，过度依赖人工智能系统。</p></sec><sec id="s7_3"><title>3.3. 加强对算法的监督管理</title><p>打造一支专门的算法监督团队。算法监督团队为人工智能在刑事证明判断中提供了道路方向。人工智能作为新兴产业，我们应该用正确的管理机制对人工智能进行长远的规划。由于算法黑箱的存在，为了杜绝算法的暗箱操作，国家应加大对算法监管型人才的培养。近年来，以人工智能为时代背景开展的现代法学教育，把人工智能加入了法学教育的大纲之中，为了响应社会的人才需求，某些大学成立了人工智能学院以及人工智能法学院等相类似的学院，培养了许多“智能 + 法律”的综合性人才，为算法监督团队的构造提供了契机。成立专门的公权力监督机关，建立完善的监督体制。随着科技强国，智慧法院的口号不断落实，人工智能的会越发变得智能化，在刑事司法领域中的运用范围会进一步增大。当然，其中所蕴含的算法会越来越复杂。所以，最高院应带头设立专门的算法监管部门，对人工智能的算法的合理性、合法性进行审查和检验。该委员会一方面可以对人工智能系统研发初期的算法进行初步的审查，审查合格后才能投入实践当中的运用。另一方面，如涉案人员对人工智能对刑事证明标准的判断的结果有质疑时，可以香该部门提出申请，让其对人工智能系统的算法进行审查，必要时可以请求研发公司公示其有问题的算法。</p></sec><sec id="s7_4"><title>3.4. 加强司法工作人员“人机混合智能”相关理论素养</title><p>人工智能在刑事证明标准判断中的应用，其核心要点就是如何处理好司法人员和智能办案系统之间的关系。如何在二者之间保持平衡。所以，加强司法工作人员本身的理论素养，有利于引导人工智能的应用向良性发展。近来，学界主流观点是推进“人机混合智能”的建设。这种人机混合智能有两种应用形态，其一是人作为一个计算机节点或决策节点与某个智能系统相混合形成的处理系统；另一种则是将人的相关领域的认知模式嵌入到智能系统中，形成的一种近乎为人的系统。目前，市面上的人工智能办案系统都采用了第一种模式。但在司法实务中，有许多司法人员思想极端片面，两极分化严重，要么用，要么不用。不可否认，人工智能系统还有诸多方面的不足，如前文所述“206”系统会在人民币失窃案件中出现对证物提供鉴定意见等类似错误，但我们应怀着包容的态度对待人工智能，遇到问题时，及时向相关部门反馈即可。为了能够正确引导类似现象，加强司法人员对人机混合智能的理论素养有着十分重要的价值，可以为人工智能技术在刑事司法领域快速生长提供良好的土壤，所以加强司法工作人员有关“人机混合智能”相关理论素养，势在必行。</p></sec></sec><sec id="s8"><title>致谢</title><p>首先，由衷感谢指导老师对我的悉心指导，带着我厘清文章部分逻辑问题和语句表达方面的用词不当；其次感谢同学们在我寻找资料时，对我提出的建议，让我能快速检索我需要的文献；再次感谢我引用的参考文献的作者，为我的论文提供了许多灵感；最后感谢学校提供的电子图书馆等资料共享平台，让我对论文相关问题进行了有效的针对性的解决。</p></sec><sec id="s9"><title>文章引用</title><p>文 黎. 人工智能判断刑事证明标准的实践与问题Practice and Problems of Artificial Intelligence in Judging Criminal Proof Standards[J]. 法学, 2021, 09(01): 48-53. https://doi.org/10.12677/OJLS.2021.91007</p></sec><sec id="s10"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.39647-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Greenfield, J. (1998) Decision Support within the Criminal Justice System. International Review of Law, Computers &amp; Technology, 12, 269-278. &lt;br&gt;https://doi.org/10.1080/13600869855423</mixed-citation></ref><ref id="hanspub.39647-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Pamela S. Katz, 邓桐, 刘鑫. 专家机器人: 利用人工智能协助法官采纳科学性专家证言[J]. 证据科学, 2017, 25(4): 487-513.</mixed-citation></ref><ref id="hanspub.39647-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">严剑漪. 揭秘“206”: 法院未来的人工智能图景——上海刑事案件智能辅助办案系统164天研发实录[J]. 人民法治, 2018(2): 38-43.</mixed-citation></ref><ref id="hanspub.39647-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">张淑玲. 破解黑箱: 智媒时代的算法权力规制与透明实现机制[J]. 中国出版, 2018(7): 49-53.</mixed-citation></ref><ref id="hanspub.39647-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">唐小民. 移动微法院开启诉讼新模式[J]. 人民论坛, 2020(15): 220-221.</mixed-citation></ref><ref id="hanspub.39647-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">刘友华. 算法偏见及其规制路径研究[J]. 法学杂志, 2019, 40(6): 55-66.</mixed-citation></ref><ref id="hanspub.39647-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">罗维鹏. 人工智能裁判的问题归纳与前瞻[J]. 国家检察官学院学报, 2018, 26(5): 16-31+172.</mixed-citation></ref></ref-list></back></article>