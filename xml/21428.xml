<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">HJDM</journal-id><journal-title-group><journal-title>Hans Journal of Data Mining</journal-title></journal-title-group><issn pub-type="epub">2163-145X</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/HJDM.2017.73008</article-id><article-id pub-id-type="publisher-id">HJDM-21428</article-id><article-categories><subj-group subj-group-type="heading"><subject>HJDM20170300000_24362888.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于随机游走的数据聚类
  Data Clustering Based on Random Walk
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>崔</surname><given-names>伟</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>夏</surname><given-names>汛</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>孙</surname><given-names>瑜鲁</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>泸州职业技术学院，四川 泸州 </addr-line></aff><aff id="aff3"><addr-line>四川大学电子信息学院，四川 成都</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>12</day><month>07</month><year>2017</year></pub-date><volume>07</volume><issue>03</issue><fpage>70</fpage><lpage>76</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    为了实现大数据量、复杂类型数据的聚类分析，本文运用随机游走算法是将数据集合映射为图，各个数据表示节点，用一个加权函数表示数据与数据之间的关系，该加权函数能根据相似性准则表示数据集中两个数据间的权重。在随机游走算法中，权重的大小代表了随机游走者从非种子点第一次到达某一种子点的偏好。最后根据最大转移概率实现聚类分析。结果表明随机游走算法在数值型数据的聚类分析中能够实现聚类。
    In order to realize the clustering analysis of large data volume and complex types of data, the random walk algorithm maps the data set into graphs, each data represents node, and uses a weighting function to represent the relationship between data and data. The similarity criterion indicates the weight between two data in the data set. In the random walk algorithm, the weight of the weight represents the random walker from the non-seed point for the first time to reach a seed point of preference. Finally, cluster analysis is realized according to the maximum transition probability. The results show that the random walk algorithm can achieve clustering in the clus-tering analysis of numerical data. 
  
 
</p></abstract><kwd-group><kwd>聚类分析，随机游走，权重函数, Clustering Analysis</kwd><kwd> Random Walk Algorithm</kwd><kwd> Weighting Function</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>基于随机游走的数据聚类<sup> </sup></title><p>崔伟<sup>1</sup>，夏汛<sup>1</sup>，孙瑜鲁<sup>2</sup><sup>*</sup></p><p><sup>1</sup>泸州职业技术学院，四川 泸州</p><p><sup>2</sup>四川大学电子信息学院，四川 成都</p><disp-formula id="hanspub.21428-formula75"><graphic xlink:href="http://html.hanspub.org/file/2-1760124x5_hanspub.png"  xlink:type="simple"/></disp-formula><p>收稿日期：2017年6月28日；录用日期：2017年7月17日；发布日期：2017年7月20日</p><disp-formula id="hanspub.21428-formula76"><graphic xlink:href="http://html.hanspub.org/file/2-1760124x6_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>为了实现大数据量、复杂类型数据的聚类分析，本文运用随机游走算法是将数据集合映射为图，各个数据表示节点，用一个加权函数表示数据与数据之间的关系，该加权函数能根据相似性准则表示数据集中两个数据间的权重。在随机游走算法中，权重的大小代表了随机游走者从非种子点第一次到达某一种子点的偏好。最后根据最大转移概率实现聚类分析。结果表明随机游走算法在数值型数据的聚类分析中能够实现聚类。</p><p>关键词 :聚类分析，随机游走，权重函数</p><disp-formula id="hanspub.21428-formula77"><graphic xlink:href="http://html.hanspub.org/file/2-1760124x7_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2017 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="http://image.hanspub.org:8080\Html/htmlimages\1-2890033x\e70a10f1-7c93-45ea-9603-062237856e4b.png" /><img src="http://image.hanspub.org:8080\Html\htmlimages\1-2890033x\e898c85e-ffc4-45c9-b817-14224a4d6960.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>聚类是按照某个特定准则把已知数据集分成不同的类，同类的数据对象间相似度尽可能大，不同类的数据对象间的相似度尽可能小。聚类分析作为数据挖掘技术中的重要组成部分，目前在许多领域都得到了广泛的研究和应用如模式识别 [<xref ref-type="bibr" rid="hanspub.21428-ref1">1</xref>] 、数据分析 [<xref ref-type="bibr" rid="hanspub.21428-ref2">2</xref>] 、图像处理 [<xref ref-type="bibr" rid="hanspub.21428-ref3">3</xref>] 、市场研究 [<xref ref-type="bibr" rid="hanspub.21428-ref4">4</xref>] 、Web文档分类 [<xref ref-type="bibr" rid="hanspub.21428-ref5">5</xref>] 等。聚类算法的选择取决于数据的类型及其聚类的目的。根据其基本思想可分为划分、层次、密度、基于网格的方法以及基于模型的方法。</p><p>基于划分的主要思想是：首先给定簇数目，然后对数据集采用迭代重定位方法实现划分，划分质量取决于初始种子和聚类标准。K-means算法 [<xref ref-type="bibr" rid="hanspub.21428-ref6">6</xref>] 从数据集中任意选择<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x10_hanspub.png" xlink:type="simple"/></inline-formula>个对象作为初始种子，以最短距离为准则将数据进行分类，该方法以均值表示类中心易受奇异数据的影响，为了抑制异常数据对聚类的影响，K-medoids算法 [<xref ref-type="bibr" rid="hanspub.21428-ref7">7</xref>] 以类个体表示聚类中心。上述算法因采用平方误差作为收敛条件，聚类结果为局部最优解，对此提出了调和均值(KHM)算法 [<xref ref-type="bibr" rid="hanspub.21428-ref8">8</xref>] 的评价函数。基于划分方法的聚类需要先验知识，即事先指定数据类别个数，为了避免个数对聚类结果的影响提出了层次聚类算法如AGNES算法 [<xref ref-type="bibr" rid="hanspub.21428-ref9">9</xref>] ，其主要思想是首先将每个数据对象作为初始簇，然后对这些单元类逐层根据距离最近原则进行聚合，使单元簇越来越大直至满足所要求的簇数目为止。AGNES算法比较简单，但可伸缩性较差。为此提出了DIANA算法 [<xref ref-type="bibr" rid="hanspub.21428-ref10">10</xref>] ，该算法将给定数据看作一个大的簇，在每一步迭代过程中将上层簇根据簇的直径或平均相异度分解为更小的簇，直至满足终止条件。传统层次聚类方法聚类过程中会遇到合并或分裂点选择的困难，因此Guha等人提出了改进的层次聚类算法CURE算法 [<xref ref-type="bibr" rid="hanspub.21428-ref11">11</xref>] 。该方法用具有代表性的若干点代表一个聚类，避免了用所有点或单个质心代表一个簇的传统方法，使其能够识别具有复杂形状和不同大小的聚类，从而对孤立点的处理更加健壮。大多数聚类算法在进行聚类时只估计点与点之间的相似度，这种局部算法很容易出现错误。因此ROCK算法 [<xref ref-type="bibr" rid="hanspub.21428-ref12">12</xref>] 在CURE算法基础上根据成对点的邻域情况进行聚类比只关注相似度的聚类方法更加鲁棒。基于层次和划分的聚类算法对凸形的聚类簇效果较好，而数字点阵图由于形状变化较大，聚类效果较差且运行时间较长。</p><p>为了弥补上述聚类算法的不足，本文利用随机游走算法进行分析。随机游走在聚类分析中的应用首先选择聚类中心，以初始聚类中心为中心，逐个输入样本；利用随机游走算法得到各个初始聚类中心到达输入样本的概率，以最大转移概率为原则将样本归入聚类中心所属的那一类。同时利用均值计算该类的重心，以该重心作为新的聚类中心再输入下一个样本，直到所有数据被分类。</p></sec><sec id="s4"><title>2. 随机游走</title><p>随机游走算法将给定数据集合看作固定数目的节点和边的离散对象，然后将数据聚类分析问题转化为无向加权图来进行求解。对数据集进行统一的定义，首先将数据集映射成一个无向加权图<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x11_hanspub.png" xlink:type="simple"/></inline-formula>，它由代表数据值的节点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x12_hanspub.png" xlink:type="simple"/></inline-formula>和表示数据与其相邻数据间关系的边界<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x13_hanspub.png" xlink:type="simple"/></inline-formula>组成。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x14_hanspub.png" xlink:type="simple"/></inline-formula>表示连接两个顶点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x15_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x16_hanspub.png" xlink:type="simple"/></inline-formula>的边，每条边被赋予一定的权值，记为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x17_hanspub.png" xlink:type="simple"/></inline-formula>，表示两个顶点之间的相似或差异程度。顶点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x18_hanspub.png" xlink:type="simple"/></inline-formula>的度定义为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x19_hanspub.png" xlink:type="simple"/></inline-formula>，它等于所有与结点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x20_hanspub.png" xlink:type="simple"/></inline-formula>相关联的边的权值的和；此外，假设<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x21_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x22_hanspub.png" xlink:type="simple"/></inline-formula>。由于随机游走算法是一种人工交互式的算法，因此用户需要预先根据数据性质设置<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x23_hanspub.png" xlink:type="simple"/></inline-formula>个种子点(标记点)，然后为每个未被标记的数据节点分配一个<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x24_hanspub.png" xlink:type="simple"/></inline-formula>维向量，来表示一个未被标记点到达所有种子点的随机游走过程，每一维向量表示从每个未标记点出发，第一次到达<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x25_hanspub.png" xlink:type="simple"/></inline-formula>个种子点的概率。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x26_hanspub.png" xlink:type="simple"/></inline-formula>个概率中最大的值为未标记点所属的类标签，通过该方法具有相似性的数据就可归为一类，从而根据不同类别之间的差异实现数据聚类。</p><p>利用径向基核函数定义数据间的相似度即：</p><disp-formula id="hanspub.21428-formula78"><label>(1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x27_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x28_hanspub.png" xlink:type="simple"/></inline-formula>表示聚类数目，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x29_hanspub.png" xlink:type="simple"/></inline-formula>表示数据集中任意两个数据。</p></sec><sec id="s5"><title>3. 随机游走求解</title><p>在一定的边界条件下，随机游走转移概率的求解问题与联合狄利克雷求解问题的解相似。因此，可以通过求解联合狄利克雷问题的解来实现随机游走算法求解。在给定区域<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x30_hanspub.png" xlink:type="simple"/></inline-formula>上狄利克雷积分形式为：</p><disp-formula id="hanspub.21428-formula79"><label>(2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x31_hanspub.png"  xlink:type="simple"/></disp-formula><p>随机游走从一个非标记点到标记点的概率等于该标记点在边界条件下的狄利克雷函数，求解的问题即在某个边界条件下求解拉普拉斯函数如：</p><disp-formula id="hanspub.21428-formula80"><label>(3)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x32_hanspub.png"  xlink:type="simple"/></disp-formula><p>组合拉普拉斯矩阵在映射图中定义如下所示：</p><disp-formula id="hanspub.21428-formula81"><label>(4)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x33_hanspub.png"  xlink:type="simple"/></disp-formula><p>拉普拉斯<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x34_hanspub.png" xlink:type="simple"/></inline-formula>的值由节点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x35_hanspub.png" xlink:type="simple"/></inline-formula>与<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x36_hanspub.png" xlink:type="simple"/></inline-formula>共同决定，该矩阵是满足边界条件下的对称正定矩阵。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x37_hanspub.png" xlink:type="simple"/></inline-formula>为节点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x38_hanspub.png" xlink:type="simple"/></inline-formula>的度，定义为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x39_hanspub.png" xlink:type="simple"/></inline-formula>表示<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x40_hanspub.png" xlink:type="simple"/></inline-formula>第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x41_hanspub.png" xlink:type="simple"/></inline-formula>行所有元素之和。</p><p>图<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x42_hanspub.png" xlink:type="simple"/></inline-formula>的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x43_hanspub.png" xlink:type="simple"/></inline-formula>条边即顶点间的关联矩阵为：</p><disp-formula id="hanspub.21428-formula82"><label>(5)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x44_hanspub.png"  xlink:type="simple"/></disp-formula><p>由上式可知，关联矩阵由边<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x45_hanspub.png" xlink:type="simple"/></inline-formula>和节点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x46_hanspub.png" xlink:type="simple"/></inline-formula>决定，图中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x47_hanspub.png" xlink:type="simple"/></inline-formula>为任意方向，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x48_hanspub.png" xlink:type="simple"/></inline-formula>为联合梯度算子，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x49_hanspub.png" xlink:type="simple"/></inline-formula>为联合散度算子。</p><p>我们构造一个大小为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x50_hanspub.png" xlink:type="simple"/></inline-formula>的对角阵<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x51_hanspub.png" xlink:type="simple"/></inline-formula>，其对角线上的值为映射图边上的权值即：</p><disp-formula id="hanspub.21428-formula83"><label>(6)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x52_hanspub.png"  xlink:type="simple"/></disp-formula><p>如果连续，联合梯度算子和联合散度算子之积可以表示各向同性的联合拉普拉斯矩阵即：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x53_hanspub.png" xlink:type="simple"/></inline-formula>。在映射图中，矩阵<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x54_hanspub.png" xlink:type="simple"/></inline-formula>可看作向量上一个加权内积大小的度量，此情况下，当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x55_hanspub.png" xlink:type="simple"/></inline-formula>时，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x56_hanspub.png" xlink:type="simple"/></inline-formula>可简化为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x57_hanspub.png" xlink:type="simple"/></inline-formula>。因此，调和函数求解问题可通过上述定义解决即：在固定标记点值已知情况下，非标记点到标记点的概率值可求。于是式(2)可转化为：</p><disp-formula id="hanspub.21428-formula84"><label>(7)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x58_hanspub.png"  xlink:type="simple"/></disp-formula><p>式中，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x59_hanspub.png" xlink:type="simple"/></inline-formula>为联合的拉普拉斯矩阵，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x60_hanspub.png" xlink:type="simple"/></inline-formula>为图中数据的概率值，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x61_hanspub.png" xlink:type="simple"/></inline-formula>的最小值可通过联合调和函数<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x62_hanspub.png" xlink:type="simple"/></inline-formula>求得，映射图中的所有节点可分为未标记点集合<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x63_hanspub.png" xlink:type="simple"/></inline-formula>和标记点集合集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x64_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x65_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x66_hanspub.png" xlink:type="simple"/></inline-formula>。将拉普拉斯矩阵按标记点和未标记点排列得：</p><disp-formula id="hanspub.21428-formula85"><label>(8)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x67_hanspub.png"  xlink:type="simple"/></disp-formula><p>拉普拉斯矩阵分解为：</p><disp-formula id="hanspub.21428-formula86"><label>(9)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x68_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x69_hanspub.png" xlink:type="simple"/></inline-formula>分别为标记点和非标记点的随机游走概率值，对<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x70_hanspub.png" xlink:type="simple"/></inline-formula>求<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x71_hanspub.png" xlink:type="simple"/></inline-formula>的微分得：</p><disp-formula id="hanspub.21428-formula87"><label>(10)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x72_hanspub.png"  xlink:type="simple"/></disp-formula><p>令<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x73_hanspub.png" xlink:type="simple"/></inline-formula>表示未标记点到达标记点为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x74_hanspub.png" xlink:type="simple"/></inline-formula>的概率，定义一个表示所有标记点集合的函数：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x75_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x76_hanspub.png" xlink:type="simple"/></inline-formula>，(<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x77_hanspub.png" xlink:type="simple"/></inline-formula>为种子点数目)。为所有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x78_hanspub.png" xlink:type="simple"/></inline-formula>的点定义一个矩阵：</p><disp-formula id="hanspub.21428-formula88"><label>(11)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x79_hanspub.png"  xlink:type="simple"/></disp-formula><p>因此，通过求解：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x80_hanspub.png" xlink:type="simple"/></inline-formula>得到到达单个标记点的概率；</p><p>通过<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x81_hanspub.png" xlink:type="simple"/></inline-formula>求得到所有种子点的概率，其中，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x82_hanspub.png" xlink:type="simple"/></inline-formula>个列矢量<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x83_hanspub.png" xlink:type="simple"/></inline-formula>组成X，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x84_hanspub.png" xlink:type="simple"/></inline-formula>个列矢量<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x85_hanspub.png" xlink:type="simple"/></inline-formula>组成M。因为对任意未被标记节点来说，它到所有种子点的概率之和为1，即：</p><disp-formula id="hanspub.21428-formula89"><label>(12)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/2-1760124x86_hanspub.png"  xlink:type="simple"/></disp-formula><p>因此，对于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x87_hanspub.png" xlink:type="simple"/></inline-formula>个标记种子点来说，计算<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x88_hanspub.png" xlink:type="simple"/></inline-formula>组方程，求可得出<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x89_hanspub.png" xlink:type="simple"/></inline-formula>个概率值。</p><p>在获得每个结点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x90_hanspub.png" xlink:type="simple"/></inline-formula>第一次到达<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x91_hanspub.png" xlink:type="simple"/></inline-formula>个种子点的概率后，逐个比较大小，以最大转移概率<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x92_hanspub.png" xlink:type="simple"/></inline-formula>实现聚类。</p></sec><sec id="s6"><title>4. 实验结果与分析</title><p>本文基于随机游走的数据聚类算法流程如图1所示。</p><p>为了验证本文算法，首先对来自UCI数字点阵图进行聚类分析，从中随机抽取数字值为0~9中的数据集，如图2所示，每个图形都是有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x93_hanspub.png" xlink:type="simple"/></inline-formula>的点阵构成，这样每个点阵图可以看作是<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/2-1760124x94_hanspub.png" xlink:type="simple"/></inline-formula>空间上的数据点，聚类数目和样本属性已知，聚类结果用信息检索领域的常用评价指标准确率P、召回率R和F测度衡量。使用K-means算法和模糊聚类算法和本文算法进行对比，得到的结果如表1所示。</p><p>图1. 算法流程图</p><p>图2. 数字点阵图</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Evaluation score of digital bitma</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >数字值</th><th align="center" valign="middle"  rowspan="2"  >聚类算法</th><th align="center" valign="middle"  colspan="3"  >测评分数</th><th align="center" valign="middle" >运行时间</th></tr></thead><tr><td align="center" valign="middle" >P (准确率)</td><td align="center" valign="middle" >R (召回率)</td><td align="center" valign="middle" >F测度</td><td align="center" valign="middle" >(s)</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >0</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.993</td><td align="center" valign="middle" >0.944</td><td align="center" valign="middle" >0.968</td><td align="center" valign="middle" >3.511</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.860</td><td align="center" valign="middle" >0.958</td><td align="center" valign="middle" >0.906</td><td align="center" valign="middle" >3.956</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.915</td><td align="center" valign="middle" >0.921</td><td align="center" valign="middle" >0.918</td><td align="center" valign="middle" >5.724</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >1</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.869</td><td align="center" valign="middle" >0.971</td><td align="center" valign="middle" >0.917</td><td align="center" valign="middle" >3.402</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.928</td><td align="center" valign="middle" >0.969</td><td align="center" valign="middle" >0.948</td><td align="center" valign="middle" >3.055</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.936</td><td align="center" valign="middle" >0.992</td><td align="center" valign="middle" >0.963</td><td align="center" valign="middle" >5.127</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >2</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.867</td><td align="center" valign="middle" >0.940</td><td align="center" valign="middle" >0.902</td><td align="center" valign="middle" >3.865</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.920</td><td align="center" valign="middle" >0.982</td><td align="center" valign="middle" >0.950</td><td align="center" valign="middle" >4.213</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.964</td><td align="center" valign="middle" >0.991</td><td align="center" valign="middle" >0.977</td><td align="center" valign="middle" >5.624</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >3</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.915</td><td align="center" valign="middle" >0.921</td><td align="center" valign="middle" >0.918</td><td align="center" valign="middle" >3.966</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.993</td><td align="center" valign="middle" >0.944</td><td align="center" valign="middle" >0.968</td><td align="center" valign="middle" >4.452</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.993</td><td align="center" valign="middle" >0.953</td><td align="center" valign="middle" >0.973</td><td align="center" valign="middle" >6.258</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >4</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.986</td><td align="center" valign="middle" >0.850</td><td align="center" valign="middle" >0.913</td><td align="center" valign="middle" >3.687</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.937</td><td align="center" valign="middle" >0.947</td><td align="center" valign="middle" >0.942</td><td align="center" valign="middle" >3.925</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.961</td><td align="center" valign="middle" >0.997</td><td align="center" valign="middle" >0.979</td><td align="center" valign="middle" >5.685</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >5</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.885</td><td align="center" valign="middle" >0.968</td><td align="center" valign="middle" >0.925</td><td align="center" valign="middle" >3.257</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.993</td><td align="center" valign="middle" >0.953</td><td align="center" valign="middle" >0.973</td><td align="center" valign="middle" >4.384</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.978</td><td align="center" valign="middle" >0.982</td><td align="center" valign="middle" >0.980</td><td align="center" valign="middle" >6.258</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >6</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.860</td><td align="center" valign="middle" >0.958</td><td align="center" valign="middle" >0.906</td><td align="center" valign="middle" >3.139</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.869</td><td align="center" valign="middle" >0.971</td><td align="center" valign="middle" >0.917</td><td align="center" valign="middle" >3.839</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.954</td><td align="center" valign="middle" >0.897</td><td align="center" valign="middle" >0.924</td><td align="center" valign="middle" >6.284</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >7</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.894</td><td align="center" valign="middle" >0.943</td><td align="center" valign="middle" >0.918</td><td align="center" valign="middle" >3.264</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.867</td><td align="center" valign="middle" >0.940</td><td align="center" valign="middle" >0.902</td><td align="center" valign="middle" >4.025</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.946</td><td align="center" valign="middle" >0.983</td><td align="center" valign="middle" >0.964</td><td align="center" valign="middle" >5.575</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >8</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.915</td><td align="center" valign="middle" >0.921</td><td align="center" valign="middle" >0.918</td><td align="center" valign="middle" >3.586</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.937</td><td align="center" valign="middle" >0.947</td><td align="center" valign="middle" >0.942</td><td align="center" valign="middle" >3.621</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.950</td><td align="center" valign="middle" >0.978</td><td align="center" valign="middle" >0.964</td><td align="center" valign="middle" >6.362</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >9</td><td align="center" valign="middle" >K-means</td><td align="center" valign="middle" >0.885</td><td align="center" valign="middle" >0.968</td><td align="center" valign="middle" >0.925</td><td align="center" valign="middle" >4.208</td></tr><tr><td align="center" valign="middle" >模糊聚类</td><td align="center" valign="middle" >0.894</td><td align="center" valign="middle" >0.943</td><td align="center" valign="middle" >0.918</td><td align="center" valign="middle" >4.803</td></tr><tr><td align="center" valign="middle" >本文算法</td><td align="center" valign="middle" >0.954</td><td align="center" valign="middle" >0.897</td><td align="center" valign="middle" >0.924</td><td align="center" valign="middle" >6.424</td></tr></tbody></table></table-wrap><p>表1. 数字点阵图的测评指标</p><p>从表1中可知，K-means算法、模糊聚类和本文算法对从数据集任意选取的0~9十个点阵图数字的准确率、召回率和F测度差异较小，其F测度均在0.9以上，但本文算法运行时间低于K-means算法、模糊聚类算法，由此可知随机游走算法能够应用于聚类分析且耗时较短。为了更直观的评价以上方法，根据表1得到求得K-means算法、模糊聚类和本文算法的F测度的均值分别为：0.921、0.934、0.957。由此可以看出利用随机游走对数字点阵图进行聚类分析，与K-means算法、模糊聚类相比具有一定的鲁棒性。</p></sec><sec id="s7"><title>5. 结语</title><p>本文根据随机游走算法原理，将数据集转换为矩阵图的形式然后利用随机游走算法求得样本到各个聚类中心的概率，根据最大概率原则划分样本所属类别，有效实现了随机游走算法在聚类分析中的应用。但由于本文只是针对数字点阵图进行了聚类分析，而没有对更为复杂的数据集进行分析，所以还有待进一步研究和改进。</p></sec><sec id="s8"><title>基金项目</title><p>川大-泸州战略合作科技项目(2015CDLZ-S12)。</p></sec><sec id="s9"><title>文章引用</title><p>崔 伟,夏 汛,孙瑜鲁. 基于随机游走的数据聚类Data Clustering Based on Random Walk[J]. 数据挖掘, 2017, 07(03): 70-76. http://dx.doi.org/10.12677/HJDM.2017.73008</p></sec><sec id="s10"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.21428-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">黄震华, 向阳, 张波, 等. 一种进行K-Means聚类的有效方法[J]. 模式识别与人工智能, 2010, 23(4): 516-521.</mixed-citation></ref><ref id="hanspub.21428-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">汤效琴, 戴汝源. 数据挖掘中聚类分析的技术方法[J]. 微计算机信息, 2003(1): 3-4.</mixed-citation></ref><ref id="hanspub.21428-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">张鑫, 赵丞. 层次聚类算法在图象处理中的应用[J]. 计算机光盘软件与应用, 2011(11): 23-23.</mixed-citation></ref><ref id="hanspub.21428-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">黄劲松, 赵平. 聚类分析在品牌市场定位研究中的应用[J]. 数理统计与管理, 2005, 24(1): 21-26.</mixed-citation></ref><ref id="hanspub.21428-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">王自强, 钱旭. 基于流形学习和SVM的Web文档分类4.681算法[J]. 计算机工程, 2009, 35(15): 38-40.</mixed-citation></ref><ref id="hanspub.21428-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">吴夙慧, 成颖, 郑彦宁, 等. K-Means 算法研究综述[J]. 现代图书情报技术, 2011, 27(5): 28-35.</mixed-citation></ref><ref id="hanspub.21428-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Park, H.S. and Jun, C.H. (2009) A Simple and Fast Algorithm for K-Medoids Clustering. Expert Systems with Applications, 36, 3336-3341. &lt;br&gt;https://doi.org/10.1016/j.eswa.2008.01.039</mixed-citation></ref><ref id="hanspub.21428-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Gungor, Z. and Unler, A. (2008) K-Harmonic Means Data Clustering with Tabu-Search Method. Applied Mathematical Modelling, 32, 1115-1125. &lt;br&gt;https://doi.org/10.1016/j.apm.2007.03.011</mixed-citation></ref><ref id="hanspub.21428-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Barbakh, W.A., Wu, Y. and Fyfe, C. (2009) Non-Standard Parameter Adaptation for Exploratory Data Analysis. Springer, Berlin Heidelberg, 7-28.</mixed-citation></ref><ref id="hanspub.21428-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">姚明海. 基于连续性原理的聚类算法研究[D]: [硕士学位论文]. 长春: 东北师范大学, 2010.</mixed-citation></ref><ref id="hanspub.21428-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">魏桂英, 郑玄轩. 层次聚类方法的CURE算法研究[J]. 科技和产业, 2005, 5(11): 22-24.</mixed-citation></ref><ref id="hanspub.21428-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">王荣, 王飞戈, 吴坤芳. 基于改进ROCK算法的个性化推荐系统研究[J]. 河南科学, 2011, 29(11): 1346-1349.</mixed-citation></ref></ref-list></back></article>