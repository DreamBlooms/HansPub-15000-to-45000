<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">CSA</journal-id><journal-title-group><journal-title>Computer Science and Application</journal-title></journal-title-group><issn pub-type="epub">2161-8801</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/CSA.2021.114121</article-id><article-id pub-id-type="publisher-id">CSA-42032</article-id><article-categories><subj-group subj-group-type="heading"><subject>CSA20210400000_92149040.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  基于轻量化网络的课堂学生学习状态判别
  Learning State Discrimination of Classroom Students Based on Lightweight Network
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>刘</surname><given-names>秋会</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>王</surname><given-names>林</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>梁</surname><given-names>明秀</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>贵州民族大学数据科学与信息工程学院，贵州 贵阳</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>13</day><month>04</month><year>2021</year></pub-date><volume>11</volume><issue>04</issue><fpage>1173</fpage><lpage>1185</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
   
   课堂学习状态判别是了解学生课堂学习情况及教师授课情况的关键步骤，轻量化网络能够提高学习状态的判别精度，轻量化网络是在神经网络算法的基础上进行优化的网络，在本文中，我们采用改进的轻量化网络Mobilnetv2对教室课堂学生学习状态进行判别，实验结果表明，通过提出的方法得到的课堂学生学习状态判别最高达到了99.00%的精度。 Classroom learning status discrimination is a key step to understand students’ classroom learning status and teachers’ teaching status. Lightweight networks can improve the accuracy of learning status. Lightweight networks are optimized networks based on neural network algorithms. In this article, the improved lightweight network Mobilnetv2 is used to discriminate the learning status of classroom students. The experimental results show that the accuracy of class student learning status discrimination obtained by the proposed method is up to 99.00%. 
  
 
</p></abstract><kwd-group><kwd>深度学习，课堂学生学习状态判别，Mobilenetv2，Swish, Deep Learning</kwd><kwd> Classroom Student Learning State Detection</kwd><kwd> Mobilenetv2</kwd><kwd> Swish</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>摘要</title><p>课堂学习状态判别是了解学生课堂学习情况及教师授课情况的关键步骤，轻量化网络能够提高学习状态的判别精度，轻量化网络是在神经网络算法的基础上进行优化的网络，在本文中，我们采用改进的轻量化网络Mobilnetv2对教室课堂学生学习状态进行判别，实验结果表明，通过提出的方法得到的课堂学生学习状态判别最高达到了99.00%的精度。</p></sec><sec id="s2"><title>关键词</title><p>深度学习，课堂学生学习状态判别，Mobilenetv2，Swish</p></sec><sec id="s3"><title>Learning State Discrimination of Classroom Students Based on Lightweight Network</title><p>Qiuhui Liu, Lin Wang, Mingxiu Liang</p><p>School of Data Science and Information Engineering, Guizhou Minzu University, Guiyang Guizhou</p><p><img src="//html.hanspub.org/file/42-1542133x4_hanspub.png" /></p><p>Received: Mar. 29<sup>th</sup>, 2021; accepted: Apr. 22<sup>nd</sup>, 2021; published: Apr. 29<sup>th</sup>, 2021</p><p><img src="//html.hanspub.org/file/42-1542133x5_hanspub.png" /></p></sec><sec id="s4"><title>ABSTRACT</title><p>Classroom learning status discrimination is a key step to understand students’ classroom learning status and teachers’ teaching status. Lightweight networks can improve the accuracy of learning status. Lightweight networks are optimized networks based on neural network algorithms. In this article, the improved lightweight network Mobilnetv2 is used to discriminate the learning status of classroom students. The experimental results show that the accuracy of class student learning status discrimination obtained by the proposed method is up to 99.00%.</p><p>Keywords:Deep Learning, Classroom Student Learning State Detection, Mobilenetv2, Swish</p><disp-formula id="hanspub.42032-formula187"><graphic xlink:href="//html.hanspub.org/file/42-1542133x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2021 by author(s) and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY 4.0).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="//html.hanspub.org/file/42-1542133x7_hanspub.png" /> <img src="//html.hanspub.org/file/42-1542133x8_hanspub.png" /></p></sec><sec id="s5"><title>1. 引言</title><p>学生课堂学习状态的研究与学生课堂学习效率及教师授课情况反馈息息相关，可作为学生课堂学习效率及教师授课情况的评价指标之一。目前课堂场景中对于学生课堂学习状态的研究主要从课堂学习行为 [<xref ref-type="bibr" rid="hanspub.42032-ref1">1</xref>]、课堂疲劳状态 [<xref ref-type="bibr" rid="hanspub.42032-ref2">2</xref>]、课堂人脸检测及关注度研究方面 [<xref ref-type="bibr" rid="hanspub.42032-ref3">3</xref>] 展开，本文主要的工作是在已经检测出学生头部的基础上，对课堂中学生头部状态进行分类判别。</p><p>在计算机视觉和模式识别中，图像分类常用基于卷积神经网络的分类模型进行研究，1998年，LeNet模型 [<xref ref-type="bibr" rid="hanspub.42032-ref4">4</xref>] 诞生，是最早的卷积神经网络之一，最早应用与MNIST [<xref ref-type="bibr" rid="hanspub.42032-ref5">5</xref>] 手写识别数字的识别；该模型参数少且结构单一，并不适用于复杂的图像分类任务。2012年，AlexNet模型 [<xref ref-type="bibr" rid="hanspub.42032-ref6">6</xref>] 作为ILSVR的冠军网络被人们认识，该模型使用ReLU [<xref ref-type="bibr" rid="hanspub.42032-ref7">7</xref>] 激活函数，大大提高了训练速度，同时使用了随机失活操作，避免了一定程度上的过拟合，但对图像特征的描述及提取仍然有限。GoogLeNet模型 [<xref ref-type="bibr" rid="hanspub.42032-ref8">8</xref>] 是ILSVR2014的冠军网络，最大特点在于引入Inception模块，该模块有四个分支且可以跨通道组织信息，大大提高了参数的利用效率，但同时也增加了计算量和出现过拟合。VGGNet模型 [<xref ref-type="bibr" rid="hanspub.42032-ref9">9</xref>] 是ILSVR2014的亚军网络，由AlexNet模型发展而来，在卷积层使用较小的卷积核并采用多尺度训练策略，但是模型深度增加导致训练速度缓慢。ILSVR2015的冠军网络是ResNet模型 [<xref ref-type="bibr" rid="hanspub.42032-ref10">10</xref>]，该模型旨在解决网络模型加深而错误率提高的问题，当模型复杂以后，随机梯度下降(SGD) [<xref ref-type="bibr" rid="hanspub.42032-ref11">11</xref>] 的优化变得更加困难，而模型达不到好的学习效果。Residual结构的提出，对卷积神经网路的发展产生了重要的意义。2016年，ResNeXt [<xref ref-type="bibr" rid="hanspub.42032-ref12">12</xref>] 通过重复一个构建块来构建，聚合了一组具有相同拓扑结构的转换，在保持复杂的限制条件下，增加基数也能很好地提高分类精度。SENet模型 [<xref ref-type="bibr" rid="hanspub.42032-ref13">13</xref>] 是ILSVR2017的冠军网络，该网络通过额外的分支来得到每个通道的权重，自适应地校正原各通道激活值响应。2017年Mobilenet模型 [<xref ref-type="bibr" rid="hanspub.42032-ref14">14</xref>] 提出，作为Mobilenet系列的第一个版本，该模型主要采取可分离卷积的方式，降低参数量却不降低精度。2018年Mobilenetv2模型 [<xref ref-type="bibr" rid="hanspub.42032-ref15">15</xref>] 在Mobilenetv1的基础上加入倒残差结构，从原来的残差结构进行降维到升维变成从升维到降维。保留了低维的信息特征。</p><p>图像分类虽然层出不穷，但是对于不同的研究目标有不同的适用网络，本文针对教室场景，以学生头部为目标，改进Mobilenetv2模型对学生头部图像进行分类，最后进行学习状态的判别。</p></sec><sec id="s6"><title>2. 基本原理</title><sec id="s6_1"><title>2.1. Mobilenetv2</title><p>MobileNetv2 [<xref ref-type="bibr" rid="hanspub.42032-ref15">15</xref>] 网络由google团队在2018年提出，在MobileNetv2网络中的重点是倒残差结构，如图1所示，(a)是Resnet网络中的残差结构，(b)是MobileNetv2中的倒残差结构。在残差结构中是1 &#215; 1卷积降维到3 &#215; 3卷积再到1 &#215; 1卷积升维，在倒残差结构中正好相反，是1 &#215; 1卷积升维到3 &#215; 3卷积再到1 &#215; 1卷积降维。这样做的原因在于高维信息通过ReLU激活函数后丢失的信息更少，而低维的运用ReLU激活函数信息丢失比较多，所以在最后一个1 &#215; 1的卷积层使用线性激活函数，来保留低维的信息特征。</p><p>图1. 残差块与倒残差块</p><p>表1是MobileNetv2网络的结构表，其中t代表的是扩展因子(倒残差结构中第一个1 &#215; 1卷积的扩展因子)，c代表输出特征矩阵的通道，n代表倒残差结构重复的次数，s代表步距(注意：这里的步距只是针对重复n次的第一层倒残差结构，后面的都默认为1)。步距不同的网络结构如图2所示。</p><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> MobileNet v2 network structur</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >Input</th><th align="center" valign="middle" >Operator</th><th align="center" valign="middle" >t</th><th align="center" valign="middle" >c</th><th align="center" valign="middle" >n</th><th align="center" valign="middle" >s</th></tr></thead><tr><td align="center" valign="middle" >224 2 &#215; 3</td><td align="center" valign="middle" >Conv2d</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2</td></tr><tr><td align="center" valign="middle" >112 2 &#215; 3 2</td><td align="center" valign="middle" >bottleneck</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >112 2 &#215; 16</td><td align="center" valign="middle" >bottleneck</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >24</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >2</td></tr><tr><td align="center" valign="middle" >56 2 &#215; 24</td><td align="center" valign="middle" >bottleneck</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >2</td></tr><tr><td align="center" valign="middle" >2 8 2 &#215; 3 2</td><td align="center" valign="middle" >bottleneck</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >64</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >2</td></tr><tr><td align="center" valign="middle" >1 4 2 &#215; 64</td><td align="center" valign="middle" >bottleneck</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >96</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >1 4 2 &#215; 96</td><td align="center" valign="middle" >bottleneck</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >160</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >2</td></tr><tr><td align="center" valign="middle" >7 2 &#215; 160</td><td align="center" valign="middle" >bottleneck</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >320</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >7 2 &#215; 320</td><td align="center" valign="middle" >Con2d 1 &#215; 1</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >1280</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1</td></tr><tr><td align="center" valign="middle" >7 2 &#215; 1280</td><td align="center" valign="middle" >Avgpool 7 &#215; 7</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >-</td></tr><tr><td align="center" valign="middle" >1 &#215; 1 &#215; 1280</td><td align="center" valign="middle" >Con2d 1 &#215; 1</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" >k</td><td align="center" valign="middle" >-</td><td align="center" valign="middle" ></td></tr></tbody></table></table-wrap><p>表1. MobileNet v2网络结构</p><p>图2. Stride不同的网络结构</p></sec><sec id="s6_2"><title>2.2. 激活函数</title><sec id="s6_2_1"><title>2.2.1. ReLU函数</title><p>ReLU函数是目前深度学习中用得较多且广受欢迎的非线性激活函数。在 x ≥ 0 时才有非零输出，在 x &lt; 0 时输出为0，将ReLU函数定义为：</p><p>f ( x ) = max ( 0 , x ) (1)</p><p>且函数曲线如图3所示。</p><p>图3. ReLU函数曲线</p><p>经过求导计算，可得(1)的导函数为：</p><p>f ′ ( x ) = max ( 0 , 1 ) (2)</p><p>由(2)可知，ReLU函数其实上是最大值函数，与Sigmoid及Tanh函数相比，当输入值为正数时，ReLU函数运算非常简单且快速，同时也避免了梯度消失。拥有更加好的网络性能。当输入为负数时，ReLU是完全不被激活的，ReLU函数就会和Sigmoid及Tanh函数存在一样的问题。</p></sec><sec id="s6_2_2"><title>2.2.2. Swish函数</title><p>Swish函数是x与sigmoid函数的结合，公式为</p><disp-formula id="hanspub.42032-formula188"><label>(3)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/42-1542133x28_hanspub.png"  xlink:type="simple"/></disp-formula><p>且函数曲线如图4。</p><p>图4. Swish函数曲线(β = 1)</p><p>对(3)求导可得：</p><disp-formula id="hanspub.42032-formula189"><label>(4)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/42-1542133x30_hanspub.png"  xlink:type="simple"/></disp-formula><p>由(4)可以看出，β是个常数且Swish激活函数具备有下界、平滑、非单调的特性。其也被看成是介于线性函数与ReLU函数之间的平滑函数。Swish激活函数在正值区域可以达到任何高度，避免了由于封顶而导致的梯度饱和。对负值区域，与ReLU相比，有轻微的梯度流。平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。</p></sec></sec></sec><sec id="s7"><title>3. 方法</title><sec id="s7_1"><title>3.1. 数据增强</title><p>在深度学习中，样本数量越多，训练出来的模型效果越好，模型的泛化能力越强。由于自建数据集的样本数量不是很多，为了提高模型的泛化能力，本文采用了三种方法对原有的数据进行增强，包括颜色扰动、高斯噪声滤波以及水平翻转。颜色扰动，指在某一个颜色空间通过增加或减少某些颜色分量，或者更改颜色通道的顺序。高斯噪声就是符合高斯正态分布的误差。有时我们需向标准数据中加入合适的高斯噪声来让数据变得有一定误差，从而让数据变得更具有实验价值。水平翻转是翻转中的一种，它是以坐标y轴为对称轴进行操作的过程，我们在这里使用水平翻转而没有使用全方位翻转的原因在于我们的数据具有一定程度的相似性，而且我们识别抬头低头的状态多数都是从正常的视觉出发，为了让数据增强具有合理性，我们使用了水平翻转。操作效果图如图5。</p><p>其中(a)表示原图，(b)表示经过颜色扰动后的图，(c)表示添加高斯噪声后的图，(d)表示水平翻转后的图。</p></sec><sec id="s7_2"><title>3.2. 更换激活函数</title><p>激活函数是在激活层起主要作用的非线性函数，Mobilenetv2模型用到的激活函数为ReLU6，ReLU6其实是ReLU函数的变形，它的定义如下式：</p><disp-formula id="hanspub.42032-formula190"><label>(5)</label><graphic position="anchor" xlink:href="//html.hanspub.org/file/42-1542133x31_hanspub.png"  xlink:type="simple"/></disp-formula><p>且函数曲线如图6。</p><p>ReLU6函数实际上是把ReLU函数取了上界，但当在负值区域，容易出现函数失活的问题，所以考虑一种更好的激活函数Swish来替代ReLU6。</p><p>图5. 数据增强效果图</p><p>图6. ReLU函数曲线图</p></sec></sec><sec id="s8"><title>4. 实验结果</title><sec id="s8_1"><title>4.1. 实验环境</title><p>本节实验软件为Python3.6计算机编程语言，框架Tensorflow2.2.0及Keras2.2.4环境，硬件为64位Windows操作系统，Intel(R)Core(TM)i7-9700 CPU和16GB RAM，独立显卡AMD Radeon Pro WX3100。</p></sec><sec id="s8_2"><title>4.2. 实验数据</title><p>本文实验用到的数据集由模型Yolov4与DropBlock结合算法检测到的学生头部结果图片制成，抬头低头图片数量各为1065张，总计2130张，命名为ClassUD。数据增强以后的数据集命名为ClassUD-A，每一种增强方法对每一种状态的数据都进行一次操作，抬头与低头状态的数据集扩增到了4260张，总计8520张图片。数据分布如表2。本文分类所需的数据皆是模仿监控视角的位置与高度拍摄所得，范围为一个教室的3至4排左右，且240张图片由40分钟视频以10秒一张图片的截取方式获得，对每张图片出现的学生头部进行裁剪且对不同的学生头部进行编号，此编号就代表这位学生，制成的数据集命名为ClassTest，依此类推，每一个编号的图片数应为240张，有些编号少于240张是由于在某些帧中头部存在被遮挡的情况，数据分布如表3所示。</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Training experiment data distributio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >数据集</th><th align="center" valign="middle" >抬头训练集</th><th align="center" valign="middle" >低头训练集</th></tr></thead><tr><td align="center" valign="middle" >ClassUD</td><td align="center" valign="middle" >1065</td><td align="center" valign="middle" >1065</td></tr><tr><td align="center" valign="middle" >ClassUD-A</td><td align="center" valign="middle" >4260</td><td align="center" valign="middle" >4260</td></tr></tbody></table></table-wrap><p>表2. 训练实验数据分布</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Test data distributio</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >编号 数据集</th><th align="center" valign="middle" >1</th><th align="center" valign="middle" >2</th><th align="center" valign="middle" >3</th><th align="center" valign="middle" >4</th><th align="center" valign="middle" >5</th><th align="center" valign="middle" >6</th><th align="center" valign="middle" >7</th><th align="center" valign="middle" >8</th><th align="center" valign="middle" >9</th><th align="center" valign="middle" >10</th><th align="center" valign="middle" >11</th><th align="center" valign="middle" >12</th><th align="center" valign="middle" >13</th><th align="center" valign="middle" >14</th></tr></thead><tr><td align="center" valign="middle" >ClassTest</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >219</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >215</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >226</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >52</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >234</td></tr></tbody></table></table-wrap><p>表3. 测试实验数据分布</p></sec><sec id="s8_3"><title>4.3. 实验对比</title><sec id="s8_3_1"><title>4.3.1. 数据增强实验</title><p>本节中的实验分别在五个深度学习分类模型上进行，迭代次数为80次，批量输入尺寸为32。第一个实验用Vgg16模型进行训练及测试，第二个实验用Googlenet模型进行训练及测试，第三个实验是在Resnet50模型下进行训练及测试，第四个实验在Mobilenev2模型下进行训练及测试。第五个实验用Alexnet模型进行训练及测试，所有实验均在数据集ClassUD及ClassUD-A上进行。学习率是深度学习中的一个重要的超参，如何设置学习率是训练出好模型的关键要素之一。为了在五个模型上对同一数据集做实验对比分析，需要在同一实验条件下进行实验，所以对于模型的学习率，根据经验分别设置了0.001及0.0001两个不同的值。</p><p>当学习率为0.001时，在数据集ClassUD及ClassUD-A上的实验结果如表4及图7所示。</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Experimental results when the learning rate is 0.00</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  colspan="2"  >方法 数据集</th><th align="center" valign="middle" >Vgg16</th><th align="center" valign="middle" >Googlenet</th><th align="center" valign="middle" >Resnet50</th><th align="center" valign="middle" >Mobilenetv2</th><th align="center" valign="middle" >Alexnet</th></tr></thead><tr><td align="center" valign="middle"  rowspan="3"  >ClassUD</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >11.50</td><td align="center" valign="middle" >2.4</td><td align="center" valign="middle" >1.23</td><td align="center" valign="middle" >4.33</td><td align="center" valign="middle" >0.42</td></tr><tr><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >49.32</td><td align="center" valign="middle" >91.88</td><td align="center" valign="middle" >81.92</td><td align="center" valign="middle" >98.33</td><td align="center" valign="middle" >77.65</td></tr><tr><td align="center" valign="middle" >平均验证精度(%)</td><td align="center" valign="middle" >50.03</td><td align="center" valign="middle" >81.28</td><td align="center" valign="middle" >77.96</td><td align="center" valign="middle" >91.33</td><td align="center" valign="middle" >70.83</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >ClassUD-A</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >49.22</td><td align="center" valign="middle" >10.06</td><td align="center" valign="middle" >5.29</td><td align="center" valign="middle" >18.57</td><td align="center" valign="middle" >1.41</td></tr><tr><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >49.54</td><td align="center" valign="middle" >96.38</td><td align="center" valign="middle" >79.05</td><td align="center" valign="middle" >99.03</td><td align="center" valign="middle" >96.38</td></tr><tr><td align="center" valign="middle" >平均验证精度(%)</td><td align="center" valign="middle" >50.02</td><td align="center" valign="middle" >95.85</td><td align="center" valign="middle" >81.82</td><td align="center" valign="middle" >98.77</td><td align="center" valign="middle" >95.72</td></tr></tbody></table></table-wrap><p>表4. 学习率为0.001时实验结果</p><p>由表4及图7可知，当学习率为0.001时，在数据集ClassUD上，训练耗时最少的模型为Alexnet，约为0.42小时，耗时最多的模型为Vgg16，约为11.50小时，训练与验证精度最高的模型为Mobilenetv2，分别为98.33%与91.33%，训练与验证精度最低的模型为Vgg16，约为49.32%与50.03%。在数据集ClassUD-A上，训练耗时最少的依然是Alexnet模型，约为1.41个小时，耗时最多的依然是模型Vgg16，约为49.22个小时，训练与验证精度最高的模型为Mobilnetv2，分别为99.03%及98.77%，训练及验证精度最低的模型为Vgg16，约为49.54%及50.02%。</p><p>图7. 五个模型的训练及验证图</p><p>数据增强后，对于Alexnet、Mobilenetv2及Googlenet模型来说，训练及验证精度都得到了不同程度上的提升；而对于Vgg16模型来说，数据增强几乎没有什么效果；对于Resnet50模型，训练精度反而还下降了；但是训练时间也同时在成倍的增长。Vgg16及Resnet50首先不是我们考虑的模型，其实按照训练时间来说，选择Alexnet模型最为合适，但是它较依赖于数据集的大小，而对于Googlenet及Mobilenetv2模型在ClassUD上表现得比较好，在ClassUD-A上更好，网络相对稳定且在不断的学习。</p><p>当学习率为0.0001时，在数据集ClassUD及ClassUD-A上的实验结果如表5及图8。</p><p>由表5及图8可知，当学习率为0.0001时，在数据集ClassUD上，训练耗时最少的模型为Alexnet，约为0.42小时，耗时最多的模型为Vgg16，约为11.49小时，训练与验证精度最高的模型为Mobilenetv2，分别为98.92%与87.76%，训练与验证精度最低的模型为Resnet50，约为82.41%与79.13%。在数据集ClassUD-A上，训练耗时最少的依然是Alexnet模型，约为1.47个小时，耗时最多的是模型Vgg16，约为49.22个小时，训练与验证精度最高的模型为Mobilnetv2，分别为99.37%及98.82%，训练及验证精度最低的模型为Resnet50，约为81.94%及85.61%。</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Experimental results when the learning rate is 0.000</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  colspan="2"  >方法 数据集</th><th align="center" valign="middle" >Vgg16</th><th align="center" valign="middle" >Googlenet</th><th align="center" valign="middle" >Resnet50</th><th align="center" valign="middle" >Mobilenetv2</th><th align="center" valign="middle" >Alexnet</th></tr></thead><tr><td align="center" valign="middle"  rowspan="3"  >ClassUD</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >11.49</td><td align="center" valign="middle" >2.6</td><td align="center" valign="middle" >1.29</td><td align="center" valign="middle" >4.45</td><td align="center" valign="middle" >0.42</td></tr><tr><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >94.23</td><td align="center" valign="middle" >93.95</td><td align="center" valign="middle" >82.41</td><td align="center" valign="middle" >98.92</td><td align="center" valign="middle" >95.13</td></tr><tr><td align="center" valign="middle" >平均验证精度(%)</td><td align="center" valign="middle" >83.28</td><td align="center" valign="middle" >85.91</td><td align="center" valign="middle" >79.13</td><td align="center" valign="middle" >87.76</td><td align="center" valign="middle" >85.76</td></tr><tr><td align="center" valign="middle"  rowspan="3"  >ClassUD-A</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >49.22</td><td align="center" valign="middle" >10.4</td><td align="center" valign="middle" >5.30</td><td align="center" valign="middle" >18.72</td><td align="center" valign="middle" >1.47</td></tr><tr><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >98.16</td><td align="center" valign="middle" >97.68</td><td align="center" valign="middle" >81.94</td><td align="center" valign="middle" >99.37</td><td align="center" valign="middle" >98.30</td></tr><tr><td align="center" valign="middle" >平均验证精度(%)</td><td align="center" valign="middle" >98.17</td><td align="center" valign="middle" >97.07</td><td align="center" valign="middle" >85.61</td><td align="center" valign="middle" >98.82</td><td align="center" valign="middle" >97.72</td></tr></tbody></table></table-wrap><p>表5. 学习率为0.0001时实验结果</p><p>图8. 五个模型的训练及验证图</p><p>数据增强以后，对于Alexnet、MobilenetV2、Googlenet及Vgg16模型来说，训练及验证精度都得到了不同程度上的提升；对于Resnet50模型，训练精度反而下降了一点；训练时间也同时在成倍的增长。Resnet50首先不是我们考虑的模型，同时Vgg16的训练时间太长，也不是我们考虑的模型。而Alexnet、Googlenet及Mobilenetv2模型不管是在ClassUD上还是在ClassUD-A上都表现得比较好，网络相对稳定且在不断的学习。</p><p>通过两个不同学习率及不同数据集下的实验，得出在数据集ClassUD-A上的实验效果比数据集ClassUD效果更好，在学习率为0.0001时比学习率为0.001时效果更好，所以之后的实验将在数据集ClassUD-A及学习率为0.0001的条件下进行。</p></sec><sec id="s8_3_2"><title>4.3.2. Swish激活函数实验</title><p>分别进行三个实验，把Alexnet、Googlenet及Mobilenetv2网络模型的激活函数替换为Swish后，称这些网络为Alexnet_S、Googlenet_S及Mobilnetv2_S，并在数据集ClassUD-A上进行了Alexnet、Alexnet_S、Googlenet、Googlenet_S、Mobilenetv2及Mobilenetv2_S实验的对比。实验结果见表6及图9。</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Experimental results of different model</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法 数据集</th><th align="center" valign="middle"  colspan="3"  >ClassUD-A</th></tr></thead><tr><td align="center" valign="middle"  rowspan="2"  >Alexnet</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >平均验证精度(%)</td></tr><tr><td align="center" valign="middle" >1.47</td><td align="center" valign="middle" >98.30</td><td align="center" valign="middle" >97.72</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >Alexnet_S</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >平均验证精度(%)</td></tr><tr><td align="center" valign="middle" >1.65</td><td align="center" valign="middle" >97.86</td><td align="center" valign="middle" >97.32</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >Googlenet</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >平均验证精度(%)</td></tr><tr><td align="center" valign="middle" >10.4</td><td align="center" valign="middle" >97.68</td><td align="center" valign="middle" >97.07</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >Googlenet_S</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >平均验证精度(%)</td></tr><tr><td align="center" valign="middle" >11.12</td><td align="center" valign="middle" >96.55</td><td align="center" valign="middle" >95.47</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >Mobilenetv2</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >平均验证精度(%)</td></tr><tr><td align="center" valign="middle" >18.72</td><td align="center" valign="middle" >99.37</td><td align="center" valign="middle" >98.82</td></tr><tr><td align="center" valign="middle"  rowspan="2"  >Mobilenetv2_S</td><td align="center" valign="middle" >训练时间(/小时)</td><td align="center" valign="middle" >平均训练精度(%)</td><td align="center" valign="middle" >平均验证精度(%)</td></tr><tr><td align="center" valign="middle" >18.83</td><td align="center" valign="middle" >99.55</td><td align="center" valign="middle" >99.00</td></tr></tbody></table></table-wrap><p>表6. 不同模型的实验结果</p><p>图9. 三个模型的训练及验证图</p><p>从表6及图9可知，在数据集ClassUD-A上，我们更改模型的激活函数为Swish后，在训练时间上，Alexnet_S比Alexnet多花了0.18 h、Googlenet_S比Googlenet多花了0.72 h、Mobilenetv2_S比Mobilenetv2多花了0.11 h；在平均训练精度上，Alexnet_S比Alexnet减少了0.44%，Googlenet_S比Googlenet减少了1.13%，Mobilenetv2_S比Mobilenetv2提高了0.18%；在平均验证精度上，Alexnet_S比Alexnet增加了0.01%，Googlenet_S比Googlenet减少了1.47%，Mobilenetv2_S比Mobilenetv2提高了0.18%。综合上述分析，在Alexnet_S、Googlenet_S及Mobilenetv2_S模型中，Mobilenetv2_S是训练时间增时最少，也是平均精度增加最多，更是训练精度及验证精度最高的模型。所以对于学生头部状态的判别选用Mobilenetv2_S模型进行实验。</p></sec><sec id="s8_3_3"><title>4.3.3. 头部状态分类判别实验</title><p>在进行分类判别之前，对学生头部状态数据集ClassTest进行了人工观察，观察结果见表7，进而用Mobilenetv2_S模型对学生头部状态数据集ClassTest进行分类判别，结果见表8。而人工观察与算法分类判别结果的平均值误差对比见表9。规定抬头状态比例达到60%及以上的学生为认真学习状态，低头状态比例达到60%以上的学生为非认真学习状态，若两种状态比例相近且不属于上面两类，判断学生为注意力不集中状态。</p><table-wrap id="table7" ><label><xref ref-type="table" rid="table7">Table 7</xref></label><caption><title> Manual observation result</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >学生</th><th align="center" valign="middle" >观察结果 数据集</th><th align="center" valign="middle" >ClassTest</th><th align="center" valign="middle" >抬头(%)</th><th align="center" valign="middle" >低头(%)</th></tr></thead><tr><td align="center" valign="middle"  colspan="2"  >1</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >85.83</td><td align="center" valign="middle" >14.17</td></tr><tr><td align="center" valign="middle"  colspan="2"  >2</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >80.83</td><td align="center" valign="middle" >19.17</td></tr><tr><td align="center" valign="middle"  colspan="2"  >3</td><td align="center" valign="middle" >219</td><td align="center" valign="middle" >33.33</td><td align="center" valign="middle" >66.67</td></tr><tr><td align="center" valign="middle"  colspan="2"  >4</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >62.92</td><td align="center" valign="middle" >37.08</td></tr><tr><td align="center" valign="middle"  colspan="2"  >5</td><td align="center" valign="middle" >215</td><td align="center" valign="middle" >40.93</td><td align="center" valign="middle" >59.07</td></tr><tr><td align="center" valign="middle"  colspan="2"  >6</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >28.75</td><td align="center" valign="middle" >71.25</td></tr><tr><td align="center" valign="middle"  colspan="2"  >7</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >67.50</td><td align="center" valign="middle" >32.50</td></tr><tr><td align="center" valign="middle"  colspan="2"  >8</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >72.92</td><td align="center" valign="middle" >27.08</td></tr><tr><td align="center" valign="middle"  colspan="2"  >9</td><td align="center" valign="middle" >226</td><td align="center" valign="middle" >73.45</td><td align="center" valign="middle" >26.55</td></tr><tr><td align="center" valign="middle"  colspan="2"  >10</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >94.58</td><td align="center" valign="middle" >5.42</td></tr><tr><td align="center" valign="middle"  colspan="2"  >11</td><td align="center" valign="middle" >52</td><td align="center" valign="middle" >80.77</td><td align="center" valign="middle" >19.23</td></tr><tr><td align="center" valign="middle"  colspan="2"  >12</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >68.33</td><td align="center" valign="middle" >31.67</td></tr><tr><td align="center" valign="middle"  colspan="2"  >13</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >71.67</td><td align="center" valign="middle" >28.33</td></tr><tr><td align="center" valign="middle"  colspan="2"  >14</td><td align="center" valign="middle" >234</td><td align="center" valign="middle" >84.19</td><td align="center" valign="middle" >15.81</td></tr></tbody></table></table-wrap><p>表7. 人工观察结果</p><table-wrap-group id="8"><label><xref ref-type="table" rid="table8">Table 8</xref></label><caption><title> Test data experimental result</title></caption><table-wrap id="8_1"><table><tbody><thead><tr><th align="center" valign="middle" >学生</th><th align="center" valign="middle" >分类精度 数据集</th><th align="center" valign="middle" >ClassTest</th><th align="center" valign="middle" >精度(%)</th><th align="center" valign="middle" >抬头(%)</th><th align="center" valign="middle" >低头(%)</th></tr></thead><tr><td align="center" valign="middle"  colspan="2"  >1</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >97.50</td><td align="center" valign="middle" >86.67</td><td align="center" valign="middle" >13.33</td></tr><tr><td align="center" valign="middle"  colspan="2"  >2</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >97.92</td><td align="center" valign="middle" >83.75</td><td align="center" valign="middle" >16.25</td></tr></tbody></table></table-wrap><table-wrap id="8_2"><table><tbody><thead><tr><th align="center" valign="middle" >3</th><th align="center" valign="middle" >219</th><th align="center" valign="middle" >87.67</th><th align="center" valign="middle" >26.48</th><th align="center" valign="middle" >73.52</th></tr></thead><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >96.67</td><td align="center" valign="middle" >65.42</td><td align="center" valign="middle" >34.58</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >215</td><td align="center" valign="middle" >95.81</td><td align="center" valign="middle" >45.12</td><td align="center" valign="middle" >54.89</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >89.58</td><td align="center" valign="middle" >32.08</td><td align="center" valign="middle" >67.92</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >95.83</td><td align="center" valign="middle" >70.83</td><td align="center" valign="middle" >29.17</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >96.25</td><td align="center" valign="middle" >69.58</td><td align="center" valign="middle" >30.42</td></tr><tr><td align="center" valign="middle" >9</td><td align="center" valign="middle" >226</td><td align="center" valign="middle" >88.94</td><td align="center" valign="middle" >60.17</td><td align="center" valign="middle" >39.82</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >88.75</td><td align="center" valign="middle" >84.17</td><td align="center" valign="middle" >15.83</td></tr><tr><td align="center" valign="middle" >11</td><td align="center" valign="middle" >52</td><td align="center" valign="middle" >92.31</td><td align="center" valign="middle" >73.08</td><td align="center" valign="middle" >26.92</td></tr><tr><td align="center" valign="middle" >12</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >95.42</td><td align="center" valign="middle" >71.67</td><td align="center" valign="middle" >28.33</td></tr><tr><td align="center" valign="middle" >13</td><td align="center" valign="middle" >240</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >71.67</td><td align="center" valign="middle" >28.33</td></tr><tr><td align="center" valign="middle" >14</td><td align="center" valign="middle" >234</td><td align="center" valign="middle" >99.14</td><td align="center" valign="middle" >83.33</td><td align="center" valign="middle" >16.67</td></tr></tbody></table></table-wrap></table-wrap-group><p>表8. 测试数据实验结果</p><table-wrap id="table9" ><label><xref ref-type="table" rid="table9">Table 9</xref></label><caption><title> Comparison of the results of the student’s head state error of the two different method</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >方法平均值 状态</th><th align="center" valign="middle" >人工观察</th><th align="center" valign="middle" >算法分类</th><th align="center" valign="middle" >误差</th></tr></thead><tr><td align="center" valign="middle" >抬头(%)</td><td align="center" valign="middle" >67.57</td><td align="center" valign="middle" >66.00</td><td align="center" valign="middle" >1.57</td></tr><tr><td align="center" valign="middle" >低头(%)</td><td align="center" valign="middle" >32.43</td><td align="center" valign="middle" >33.99</td><td align="center" valign="middle" >1.56</td></tr></tbody></table></table-wrap><p>表9. 两种不同方法的学生头部状态结果误差对比</p><p>从表7来看，学生1、2、4、7、8、9、10、11、12、13、14在时常40分钟的课堂里为认真学习状态，而学生3、6为非认真学习状态，学生5的抬头比例及低头比例相近，我们认为此学生在课堂里为注意力不集中状态。</p><p>从表8分类精度来看，对于14个学生目标，有10个学生头部的分类精度均在92.31%至100%之间，4个学生头部的分类精度均在87.67%至89.58%之间，基本达到检测要求，其中4个低于90%分类精度的原因是学生头部存在部分遮挡，导致检测结果不佳，从检测抬头低头比例来看，学生1、2、4、7、8、9、10、11、12、13、14在课堂中为认真学习状态，而学生3、6为非认真学习状态，学生5为注意力不集中状态。</p><p>从表9可知，我们的算法检测结果平均值与人工观察结果平均值相差不大，学生学习状态也大致相同。</p><p>综合以上分析，提出的Mobilenetv2_S算法适用于自建数据集并取得了良好的分类效果。</p></sec></sec></sec><sec id="s9"><title>基金项目</title><p>本文研究受贵州省教育厅创新群体重大研究项目(KY2018018)的支持。</p></sec><sec id="s10"><title>文章引用</title><p>刘秋会,王 林,梁明秀. 基于轻量化网络的课堂学生学习状态判别Learning State Discrimination of Classroom Students Based on Lightweight Network[J]. 计算机科学与应用, 2021, 11(04): 1173-1185. https://doi.org/10.12677/CSA.2021.114121</p></sec><sec id="s11"><title>参考文献</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.42032-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">左国才, 吴小平, 苏秀芝, 等. 基于CNN人脸识别模型的大学生课堂行为分析研究[J]. 智能计算机与应用, 2019, 9(6): 107-110.</mixed-citation></ref><ref id="hanspub.42032-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">李芙蓉. 基于MTCNN的多特征融合学生疲劳检测[J]. 信息技术, 2020, 44(6): 108-113, 120.</mixed-citation></ref><ref id="hanspub.42032-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">唐康, 先强, 李明勇. 基于人脸检测的大学课堂关注度研究[J]. 重庆师范大学学报(自然科学版), 2019, 36(5): 123.</mixed-citation></ref><ref id="hanspub.42032-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Lecun, Y., Bottou, L., Bengio, Y., et al. (1998) Gradient-Based Learning Applied to Document Recog-nition. Proceedings of the IEEE, 86, 2278-2324. &lt;br&gt;://doi.org/10.1109/5.726791</mixed-citation></ref><ref id="hanspub.42032-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">León, M.D., Moreno-Báez, A., Magallanes-Quintanar, R., et al. (2011) Assessment in Subsets of MNIST Handwritten Digits and Their Effect in the Recognition Rate. Journal of Pattern Recognition Research, 2, 244-252.  
&lt;br&gt;://doi.org/10.13176/11.348</mixed-citation></ref><ref id="hanspub.42032-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Krizhevsky, A., Sutskever, I. and Hinton, G. (2012) ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012, Lake Tahoe, 3-6 December 2012, 1097-1105.</mixed-citation></ref><ref id="hanspub.42032-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">蒋昂波, 王维维. ReLU激活函数优化研究[J]. 传感器与微系统, 2018, 37(2): 50-52.</mixed-citation></ref><ref id="hanspub.42032-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy, C., Liu, W., Jia, Y., et al. (2016) Going Deeper with Convolutions. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, 7-12 June 2015, 1-9. &lt;br&gt;://doi.org/10.1109/CVPR.2015.7298594</mixed-citation></ref><ref id="hanspub.42032-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan, K. and Zisserman, A. (2014) Very Deep Convolutional Networks for Large-Scale Image Recognition.</mixed-citation></ref><ref id="hanspub.42032-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">He, K., Zhang, X., Ren, S., et al. (2016) Deep Resid-ual Learning for Image Recognition. IEEE Conference on Computer Vision &amp; Pattern Recognition, Las Vegas, 27-30 June 2016, 770-778. &lt;br&gt;://doi.org/10.1109/CVPR.2016.90</mixed-citation></ref><ref id="hanspub.42032-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Bordes, A., Bottou, L. and Gallinari, P. (2009) SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent. Journal of Machine Learning Research, 10, 1737-1754.</mixed-citation></ref><ref id="hanspub.42032-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Xie, S., Girshick, R., Dollár, P., et al. (2017) Aggregated Residual Transformations for Deep Neural Networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, 21-26 July 2017, 5987-5995.  
&lt;br&gt;://doi.org/10.1109/CVPR.2017.634</mixed-citation></ref><ref id="hanspub.42032-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Hu, J., Shen, L. and Sun, G. (2018) Squeeze-and-Excitation Networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-22 June 2018, 7132-7141.  
&lt;br&gt;://doi.org/10.1109/CVPR.2018.00745</mixed-citation></ref><ref id="hanspub.42032-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Howard, A.G., Zhu, M., Chen, B., et al. (2017) MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.</mixed-citation></ref><ref id="hanspub.42032-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Sandler, M., Howard, A., Zhu, M., et al. (2018) MobileNetV2: Inverted Residuals and Linear Bottlenecks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, 18-22 June 2018, 4510-4520.  
&lt;br&gt;://doi.org/10.1109/CVPR.2018.00474</mixed-citation></ref></ref-list></back></article>