<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd"><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research article"><front><journal-meta><journal-id journal-id-type="publisher-id">HJDM</journal-id><journal-title-group><journal-title>Hans Journal of Data Mining</journal-title></journal-title-group><issn pub-type="epub">2163-145X</issn><publisher><publisher-name>Scientific Research Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.12677/HJDM.2017.72006</article-id><article-id pub-id-type="publisher-id">HJDM-20398</article-id><article-categories><subj-group subj-group-type="heading"><subject>HJDM20170200000_83766893.pdf</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>信息通讯</subject></subj-group></article-categories><title-group><article-title>
 
 
  考虑了特征协同作用的FAST特征选择算法的改进
  A Kind of FAST Feature Selection Algorithm Considering Feature Interaction
 
</article-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>陆</surname><given-names>碧云</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>张</surname><given-names>磊</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff2"><addr-line>中山大学数学学院，广东 广州</addr-line></aff><aff id="aff1"><addr-line>null</addr-line></aff><pub-date pub-type="epub"><day>04</day><month>05</month><year>2017</year></pub-date><volume>07</volume><issue>02</issue><fpage>51</fpage><lpage>63</lpage><permissions><copyright-statement>&#169; Copyright  2014 by authors and Scientific Research Publishing Inc. </copyright-statement><copyright-year>2014</copyright-year><license><license-p>This work is licensed under the Creative Commons Attribution International License (CC BY). http://creativecommons.org/licenses/by/4.0/</license-p></license></permissions><abstract><p>
 
 
  
    交互的特征是指那些分开考虑对目标集不相关或弱相关，但合在一起考虑却对目标集高度相关的特征。特征交互现象广泛存在，但找出有交互作用的特征却是一项具有挑战性的任务。本文旨在对基于聚类的FAST特征选择算法进行改进，在其基础上考虑特征的交互作用，首先去掉FAST的移除不相关特征的部分，接着加入交互权值变量，使得在移除不相关和冗余特征的同时，保留有交互作用的特征。为了对两个算法进行对比分析，我们选取了5个不同领域的16个公开数据集进行实证分析，并使用4种分类器对实验结果进行评估，包括C5.0、Bayes Net、Neural Net和Logistic，接着从选择的特征个数、算法运行时间和分类器的准确率3个方面对两个算法进行比较。实验结果表明，两者选择的特征个数相差不大，有时IWFAST甚至可以减少特征个数，同时IWFAST能提高分类器的准确率，尤其对于特征数量较多的情形，以及Game和Life领域。美中不足的是，IWFAST的运行时间较长，但仍在可接受的范围内。
    Interacting features are those appear to be irrelevant or weakly relevant with the class individually, but it may highly correlate to the class when it combined. Feature interaction is almost everywhere, but discovering feature interaction is a challenging task in feature selection. The purpose of this paper is to improve the FAST feature selection algorithm based on cluster by considering feature interaction. Firstly, deleted the irrelevant feature removal section, then brought in an interaction weight factor, so that we can retain interacted features when removed the irrelevant and redundant ones. In order to do the comparison between this two algorithms, we selected 16 public data sets which cover 5 different domains on the empirical analysis, and used 4 types of classifier to evaluate the results, namely, C5.0, Bayes Net, Neural Net and Logistic. Finally, we compared these two algorithms according to the number of selected features, running time of algorithm and the accuracy of classifier. The experimental result showed that it has little difference on the number of selected features, and sometimes IWFAST can produce smaller subsets of features. Meanwhile, IWFAST can improve the accuracy of the classifier, especially for the high- dimensional data set, or especially for the Game and Life area. The defect is that the running time of IWFAST is long, but is acceptable computational complexity. 
  
 
</p></abstract><kwd-group><kwd>特征选择，特征交互，交互权值变量，Filter，特征聚类，图论, Feature Selection</kwd><kwd> Feature Interaction</kwd><kwd> Interaction Weight Factor</kwd><kwd> Filter</kwd><kwd> Feature Clustering</kwd><kwd> Graph Theory</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>考虑了特征协同作用的FAST特征选择 算法的改进<sup> </sup></title><p>陆碧云，张磊</p><p>中山大学数学学院，广东 广州</p><p>收稿日期：2017年4月7日；录用日期：2017年4月27日；发布日期：2017年4月30日</p><disp-formula id="hanspub.20398-formula180"><graphic xlink:href="http://html.hanspub.org/file/3-1760114x5_hanspub.png"  xlink:type="simple"/></disp-formula></sec><sec id="s2"><title>摘 要</title><p>交互的特征是指那些分开考虑对目标集不相关或弱相关，但合在一起考虑却对目标集高度相关的特征。特征交互现象广泛存在，但找出有交互作用的特征却是一项具有挑战性的任务。本文旨在对基于聚类的FAST特征选择算法进行改进，在其基础上考虑特征的交互作用，首先去掉FAST的移除不相关特征的部分，接着加入交互权值变量，使得在移除不相关和冗余特征的同时，保留有交互作用的特征。为了对两个算法进行对比分析，我们选取了5个不同领域的16个公开数据集进行实证分析，并使用4种分类器对实验结果进行评估，包括C5.0、Bayes Net、Neural Net和Logistic，接着从选择的特征个数、算法运行时间和分类器的准确率3个方面对两个算法进行比较。实验结果表明，两者选择的特征个数相差不大，有时IWFAST甚至可以减少特征个数，同时IWFAST能提高分类器的准确率，尤其对于特征数量较多的情形，以及Game和Life领域。美中不足的是，IWFAST的运行时间较长，但仍在可接受的范围内。</p><p>关键词 :特征选择，特征交互，交互权值变量，Filter，特征聚类，图论</p><disp-formula id="hanspub.20398-formula181"><graphic xlink:href="http://html.hanspub.org/file/3-1760114x6_hanspub.png"  xlink:type="simple"/></disp-formula><p>Copyright &#169; 2017 by authors and Hans Publishers Inc.</p><p>This work is licensed under the Creative Commons Attribution International License (CC BY).</p><p>http://creativecommons.org/licenses/by/4.0/</p><p><img src="http://image.hanspub.org:8080\Html/htmlimages\1-2890033x\e70a10f1-7c93-45ea-9603-062237856e4b.png" /><img src="http://image.hanspub.org:8080\Html\htmlimages\1-2890033x\e898c85e-ffc4-45c9-b817-14224a4d6960.png" /></p></sec><sec id="s3"><title>1. 引言</title><p>特征选择是模式识别和机器学习中的数据预处理的一个步骤，在近些年来受到各个领域中诸多学者的关注。特征选择的目标，是从特征集合中选择一个尽可能小的特征子集，并且在表达原始特征时保留一个适当高的准确率。特征选择有很多优点，比如避免出现过拟合现象、提高数据的可视化、减少储存需求、减少训练时间、提高学习的准确率、提高结果的可理解性等 [<xref ref-type="bibr" rid="hanspub.20398-ref1">1</xref>] 。</p><p>特征选择的方法可以分为四大类：Filter、Wrapper、Embedded和Hybrid。Filter方法是选定一个指标来评估特征，根据特征指标值来对特征排序，同时设定阈值，将达不到该阈值的特征去掉。这类方法只考虑特征X和目标Y之间的关联，相对另两类特征选择方法Wrapper和Embedded计算开销最少。Wrapper方法和Filter不同，它不考虑特征X和目标Y直接的关联性，而是在添加一个特征后观察模型最终的表现来评估特征的好坏。它使用一个明确的分类器去评估特征子集，以不同的搜索策略下分类器的准确率来评价特征。这类方法的准确率一般较高，但是所选择的特征子集依赖于分类器的选择，计算复杂度较高，而且容易出现过拟合。Embedded方法将特征选择与算法本身紧密结合，在模型的训练过程中完成特征的选择，具体来说，先使用某些机器学习算法对模型进行训练，得到各个特征的权值系数，根据系数的大小选择特征。Hybrid方法是Filter和Wrapper的结合，它通过使用Filter来产出一个特征子集，再运用Wrapper来对该子集进行特征选择。在这四类方法中，Filter方法因其具有普遍性而受到广泛使用，特别是在处理大数据的情形下。</p><p>在Filter的特征选择算法中，基于聚类的特征选择算法被证实了比传统的算法更为有效，Pereira et al. [<xref ref-type="bibr" rid="hanspub.20398-ref2">2</xref>] 、Baker和McCallum [<xref ref-type="bibr" rid="hanspub.20398-ref3">3</xref>] 以及Dhillon et al. [<xref ref-type="bibr" rid="hanspub.20398-ref4">4</xref>] 利用了词的分布聚类来减少文本数据的维度。在聚类分析中，图论被广泛地运用于实践中。图论聚类的思路很简单：计算样本的邻近图，然后根据某一标准删除明显比邻边更大或更小的边，由此可以得到一个森林，森林中的每一颗树代表了一个簇。Qinbao Song, Jingjie Ni和Guangtao Wang [<xref ref-type="bibr" rid="hanspub.20398-ref5">5</xref>] 将图论聚类运用于特征选择中，采用了最小生成树(MST)理论，提出了一种基于快速聚类的特征选择算法(FAST)。FAST算法有两个步骤：首先，通过图论聚类方法将特征划分成几个类，然后，从每一个类中选出一个最具有代表性，与目标集最相关的特征，组成最终的特征子集。在这一算法中，不同类中的特征各自独立，因此该算法极可能产生有用而相互独立的特征子集。</p><p>然而，与大多数特征选择方法一样，FAST算法关注的是尽可能地去除不相关和冗余的特征。但除了鉴别不相关和冗余的特征之外，一个重要而又常常被忽视的概念就是特征间的信息交互 [<xref ref-type="bibr" rid="hanspub.20398-ref6">6</xref>] 。有交互作用的特征是指各自对目标集不相关，但一旦结合起来却对目标集高度相关的特征。XOR问题就是一个典型的例子。尽管最近有研究指出交互作用的存在，但是很少有文献给出交互作用的具体解决方案。Zilin Zeng，Hongjun Zhang，Rui Zhang和Chengxiang Yin [<xref ref-type="bibr" rid="hanspub.20398-ref7">7</xref>] 给出了信息交互作用的定义，提出了基于交互权值的特征选择算法，引入交互权值变量来衡量特征的冗余和交互作用。</p><p>本文拟对FAST算法进行改进，在其基础上考虑特征的交互作用，在算法中加入交互权值变量，用这一变量去修改邻近图的边的权重，并调整相应的特征筛选标准，构造一个新的特征选择算法，称为IWFAST。</p><p>为了证实IWFAST算法的有效性，我们用了5个领域的16个公开的数据集进行检验并与FAST算法进行对比分析。实证结果表明，与FAST算法相比，IWFAST算法在某些情况能够减少特征的个数，还能提高在不同分类器下算法的准确率。</p><p>本文接下来的内容按以下结构展开：第2章，介绍相关的工作。第3章，给出相关、冗余和交互的定义。第4章，分别介绍FAST算法和IWFAST算法的框架和具体流程。第5章，将两个算法分别运用于实际数据中，以做对比分析。最后，第6章，作出一个简要的结论并且给出进一步的工作计划。</p></sec><sec id="s4"><title>2. 相关工作</title><p>近些年来，层次聚类被用于文本分类的选词中。分布聚类可将词划分成不同的群组，在这类研究中，Pereira et al. [<xref ref-type="bibr" rid="hanspub.20398-ref2">2</xref>] 是基于词之间特殊的语法关系，而Baker和McCallum [<xref ref-type="bibr" rid="hanspub.20398-ref3">3</xref>] 则基于目标集的分布。由于词的分布聚类会自然地凝聚，从而导致次最优的词聚类以及较高的计算开销，Dhillon et al. [<xref ref-type="bibr" rid="hanspub.20398-ref4">4</xref>] 提出了一种新的信息理论划分算法用于词聚类，并且将其应用于文本分类中。Butterworth et al. [<xref ref-type="bibr" rid="hanspub.20398-ref8">8</xref>] 在特征的每一个类中使用一种特殊的测量标准，叫Barthelemy-Montjardet距离，然后在得到的聚类层级中使用系统树图来选择最相关的特征。不幸的是，这种基于Barthelemy-Montjardet距离的聚类评估标准不能识别出能够允许分类器提高原始准确率的特征子集，进一步说，与其他特征选择算法相比，这一方法得到的准确率较低。</p><p>层次聚类还能用于光谱数据的特征选择中。Van Dijck和Van Hulle [<xref ref-type="bibr" rid="hanspub.20398-ref9">9</xref>] 提出了一种Hybrid特征选择回归算法。Krier et al. [<xref ref-type="bibr" rid="hanspub.20398-ref10">10</xref>] 展示了一个结合了光谱变量的分层约束聚类和基于互信息的簇的选取的方法。这一特征聚类方法和Van Dijck和Van Hulle [<xref ref-type="bibr" rid="hanspub.20398-ref9">9</xref>] 的类似，唯一不同的是前者要求每个类中必须只包含连续的特征。这两种方法都使用了凝聚层次聚类来剔除冗余的特征。</p><p>与上面基于层次聚类的特征选择算法不同，Qinbao Song，Jingjie Ni和Guangtao Wang提出的FAST算法 [<xref ref-type="bibr" rid="hanspub.20398-ref5">5</xref>] 在聚类时运用了最小生成树的理论。这也是本文重点关注的内容。</p><p>在很多分类问题中，一个特征单独地看对目标集并不提供有用的信息，但当和另一个特征结合在一起时，会对分类的准确性产生明显的提升效果。如果我们只考虑相关性和冗余而忽视交互作用，我们就会损失一些有价值的特征。</p><p>于是，特征的交互作用逐渐引起了学者们的关注。Zhao and Liu [<xref ref-type="bibr" rid="hanspub.20398-ref11">11</xref>] 提出了一种基于数据一致性的特征评分标准，发展了一种Filter算法，叫INTERACT，以便在选择相关特征的同时探索特征的交互作用。最近，Wang et al. [<xref ref-type="bibr" rid="hanspub.20398-ref12">12</xref>] 提出了一个基于FOIL规则的算法FRFS，它不仅保留了相关的特征，剔除不相关和冗余的特征，还考虑了特征的交互作用。FRFS首先将FOIL规则中所有在前的特征合并，得到一个排除了冗余特征并保留了交互特征的候选特征子集。接着，使用一个新的CoverRatio标准去识别和剔除候选特征子集中不相关的特征，得到最终的特征子集。</p><p>Zilin Zeng，Hongjun Zhang, Rui Zhang和Chengxiang Yin [<xref ref-type="bibr" rid="hanspub.20398-ref7">7</xref>] 在信息理论框架中重新定义了相关性、冗余和交互作用，接着引入一个交互权值变量，该变量可以反映出特征是冗余的还是信息交互的，从而提出了基于交互权值的特征选择算法(IWFAST)。本文将利用IWFAST中的相关定义及交互权值变量，对FAST算法进行改进，使得在保留相关特征和剔除冗余特征的同时，仍能保留有交互作用的特征。</p></sec><sec id="s5"><title>3. 相关、冗余和交互的定义</title><p>在介绍相关、冗余和交互的定义之前，首先要引入一个概念，叫交互增益(即交互信息)。交互增益的引进利用了信息增益(即互信息)的思想。</p><p>信息增益是指期望信息或信息熵的有效减少量。设<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x9_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x10_hanspub.png" xlink:type="simple"/></inline-formula>是两个离散的随机变量，他们的联合概率分布是<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x11_hanspub.png" xlink:type="simple"/></inline-formula>，其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x12_hanspub.png" xlink:type="simple"/></inline-formula>。则信息增益<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x13_hanspub.png" xlink:type="simple"/></inline-formula>的定义为</p><disp-formula id="hanspub.20398-formula182"><label>(1)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-1760114x14_hanspub.png"  xlink:type="simple"/></disp-formula><p>信息增益<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x15_hanspub.png" xlink:type="simple"/></inline-formula>可被视为当两个随机变量有交互作用时对信息交互的衡量。Jakulin [<xref ref-type="bibr" rid="hanspub.20398-ref13">13</xref>] [<xref ref-type="bibr" rid="hanspub.20398-ref14">14</xref>] 概括了当有三个随机变量有交互作用时对信息交互的衡量，介绍了交互增益的定义：</p><disp-formula id="hanspub.20398-formula183"><label>(2)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-1760114x16_hanspub.png"  xlink:type="simple"/></disp-formula><p>该定义又可以变为</p><disp-formula id="hanspub.20398-formula184"><label>(3)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-1760114x17_hanspub.png"  xlink:type="simple"/></disp-formula><p>与互信息不同，交互信息可以为正、为0或为负。当两个随机变量合在一起可以提供一些在他们分开时不能提供的信息，则交互信息取正数；当两个随机变量可以提供一些相同的信息时，交互信息取负数；当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x18_hanspub.png" xlink:type="simple"/></inline-formula>(或<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x19_hanspub.png" xlink:type="simple"/></inline-formula>)的引入不能改变<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x20_hanspub.png" xlink:type="simple"/></inline-formula>(或<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x21_hanspub.png" xlink:type="simple"/></inline-formula>)与<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x22_hanspub.png" xlink:type="simple"/></inline-formula>的关系时，交互信息为0。</p><p>Zilin Zeng，Hongjun Zhang，Rui Zhang和Chengxiang Yin [<xref ref-type="bibr" rid="hanspub.20398-ref7">7</xref>] 利用上述交互信息的概念给出了相关、冗余和交互的定义：</p><p>定义1 (相关)设<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x23_hanspub.png" xlink:type="simple"/></inline-formula>是一个特征全集，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x24_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x25_hanspub.png" xlink:type="simple"/></inline-formula>。特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x26_hanspub.png" xlink:type="simple"/></inline-formula>对于目标集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x27_hanspub.png" xlink:type="simple"/></inline-formula>是相关的当且仅当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x28_hanspub.png" xlink:type="simple"/></inline-formula>，使得<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x29_hanspub.png" xlink:type="simple"/></inline-formula>。否则，特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x30_hanspub.png" xlink:type="simple"/></inline-formula>是不相关的。</p><p>依据定义1，相关性有条件地依赖于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x31_hanspub.png" xlink:type="simple"/></inline-formula>。一个特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x32_hanspub.png" xlink:type="simple"/></inline-formula>对<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x33_hanspub.png" xlink:type="simple"/></inline-formula>是相关的，只要存在<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x34_hanspub.png" xlink:type="simple"/></inline-formula>的一个子集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x35_hanspub.png" xlink:type="simple"/></inline-formula>，使得对于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x36_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x37_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x38_hanspub.png" xlink:type="simple"/></inline-formula>的条件互信息是非负的。这和传统的相关特征的定义 [<xref ref-type="bibr" rid="hanspub.20398-ref15">15</xref>] 是一致的，并且包括了强相关和弱相关两种情况。</p><p>定义2 (冗余)设<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x39_hanspub.png" xlink:type="simple"/></inline-formula>是一个拥有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x40_hanspub.png" xlink:type="simple"/></inline-formula>个特征的特征集，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x41_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x42_hanspub.png" xlink:type="simple"/></inline-formula>。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x43_hanspub.png" xlink:type="simple"/></inline-formula>中的特征是相互冗余的当且仅当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x44_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x45_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>依据定义2，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x46_hanspub.png" xlink:type="simple"/></inline-formula>表明<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x47_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x48_hanspub.png" xlink:type="simple"/></inline-formula>是相互冗余的，换句话说，他们都提供了部分关于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x49_hanspub.png" xlink:type="simple"/></inline-formula>的重叠的信息。因此，这一不等式暗示了<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x50_hanspub.png" xlink:type="simple"/></inline-formula>个特征中存在着冗余。</p><p>定义3 (交互)设<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x51_hanspub.png" xlink:type="simple"/></inline-formula>是一个拥有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x52_hanspub.png" xlink:type="simple"/></inline-formula>个特征的特征集，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x53_hanspub.png" xlink:type="simple"/></inline-formula>且<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x54_hanspub.png" xlink:type="simple"/></inline-formula>。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x55_hanspub.png" xlink:type="simple"/></inline-formula>中的特征是相互交互的当且仅当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x56_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x57_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>依据定义3，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x58_hanspub.png" xlink:type="simple"/></inline-formula>意味着特征子集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x59_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x60_hanspub.png" xlink:type="simple"/></inline-formula>间存在协同作用，即他们合在一起会比分开时提供更多的信息。换句话说，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x61_hanspub.png" xlink:type="simple"/></inline-formula>中任意一个特征的缺失都会降低预测<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x62_hanspub.png" xlink:type="simple"/></inline-formula>的能力。因此，交互的定义是合理的。特别地，当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x63_hanspub.png" xlink:type="simple"/></inline-formula>时，有，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x64_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec><sec id="s6"><title>4. 算法介绍</title><sec id="s6_1"><title>4.1. FAST特征选择算法</title><p>Qinbao Song，Jingjie Ni和Guangtao Wang [<xref ref-type="bibr" rid="hanspub.20398-ref5">5</xref>] 提出了一种新奇的FAST特征选择算法，它能有效地处理不相关和冗余的特征，得到一个好的特征子集。图1的左图展示了FAST算法的框架，其中包括了移除不相关特征和剔除冗余特征两个部分。前者通过移除不相关特征以得到对目标集相关的特征子集，后者通过从不同的类中选择代表性的特征来剔除冗余特征，从而得到最终的特征子集。</p><p>相关性准则一经确定，就可以直接移除不相关特征。而剔除冗余特征稍微复杂一些，它包括：在加权完成图中构造最小生成树；将最小生成树划分成森林，每一棵树代表一个类；从每个类中选择一个代表特征。</p><p>为了更详细地描述这一算法，需要介绍一些定义。</p><p>首先，从3节我们可以知道，相关的特征与目标集有很强的相关性，而冗余的特征相互之间是相关的。因此，特征的冗余和相关性分别对应了特征间的相关性和特征与目标集的相关性。</p><p>互信息衡量了特征与目标集的分布的统计独立性的不同程度。这是一个对特征之间或者特征与目标集的相关性的非线性估计。通过特征或目标集的熵将互信息标准化，得到对称不确定性(<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x65_hanspub.png" xlink:type="simple"/></inline-formula>)。FAST算法使用了对称不确定性(<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x66_hanspub.png" xlink:type="simple"/></inline-formula>)作为两个特征之间或者特征与目标集之间的相关性的度量，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x67_hanspub.png" xlink:type="simple"/></inline-formula>的定义如下：</p><disp-formula id="hanspub.20398-formula185"><label>(4)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-1760114x68_hanspub.png"  xlink:type="simple"/></disp-formula><p>其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x69_hanspub.png" xlink:type="simple"/></inline-formula>为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x70_hanspub.png" xlink:type="simple"/></inline-formula>的熵，定义为</p><disp-formula id="hanspub.20398-formula186"><label>(5)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-1760114x71_hanspub.png"  xlink:type="simple"/></disp-formula><p>对称不确定性将一对变量视为是对称的，它对变量的信息增益的偏倚进行了补偿，并将其标准化使得它的值落在<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x72_hanspub.png" xlink:type="simple"/></inline-formula>区间内。虽然该度量是对离散变量的定义，但他也可以用于连续变量中，只要将连续变量进行离散化即可。</p><p>在引入了对称不确定性的概念后，我们便可以用它来衡量相关性。特征间的相关性表示为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x73_hanspub.png" xlink:type="simple"/></inline-formula>，其中<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x74_hanspub.png" xlink:type="simple"/></inline-formula>；特征与目标集的相关性表示为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x75_hanspub.png" xlink:type="simple"/></inline-formula>，在FAST算法中，如果<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x76_hanspub.png" xlink:type="simple"/></inline-formula>的值大于某个预定的阈值<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x77_hanspub.png" xlink:type="simple"/></inline-formula>，则可以认为<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x78_hanspub.png" xlink:type="simple"/></inline-formula>对目标值是强相关的，小于该阈值的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x79_hanspub.png" xlink:type="simple"/></inline-formula>就可以移除了。</p><p>对称不确定性还可以衡量变量间的冗余 [<xref ref-type="bibr" rid="hanspub.20398-ref5">5</xref>] 。<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x80_hanspub.png" xlink:type="simple"/></inline-formula>是特征集的一个聚类，如果<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x81_hanspub.png" xlink:type="simple"/></inline-formula>，使得对每一个<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x82_hanspub.png" xlink:type="simple"/></inline-formula>，都有：</p><p><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x83_hanspub.png" xlink:type="simple"/></inline-formula>,</p><p>则<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x84_hanspub.png" xlink:type="simple"/></inline-formula>对于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x85_hanspub.png" xlink:type="simple"/></inline-formula>是冗余的。</p><p>当然，也可以通过对称不确定性筛选出每个类的代表特征，也就是说，对于一个类<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x86_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x87_hanspub.png" xlink:type="simple"/></inline-formula>，选择使得<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x88_hanspub.png" xlink:type="simple"/></inline-formula>最大的那一个<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x89_hanspub.png" xlink:type="simple"/></inline-formula>作为该类的代表特征。</p><p>图1. FAST算法和IWFAST算法的框架</p><p>FAST算法一共有三个步骤：1) 移除不相关的特征；2) 构造最小生成树；3) 分割最小生成树并选择每个类的代表特征。</p><p>移除不相关特征和选择代表特征上面已经解决，只剩下构造最小生成树和分割最小生成树。首先，为什么要构造最小生成树呢？对于一个含有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x91_hanspub.png" xlink:type="simple"/></inline-formula>个特征的数据集，每两个特征求<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x92_hanspub.png" xlink:type="simple"/></inline-formula>作为他们的边的权值，则可以得到一个无向的加权完成图<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x93_hanspub.png" xlink:type="simple"/></inline-formula>，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x94_hanspub.png" xlink:type="simple"/></inline-formula>反映了各个特征间的相关性。不幸的是，<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x95_hanspub.png" xlink:type="simple"/></inline-formula>含有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x96_hanspub.png" xlink:type="simple"/></inline-formula>个顶点<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x97_hanspub.png" xlink:type="simple"/></inline-formula>条边，对于高维数据集，则过于密集且各条边强烈地交织在一起。此外，分解这样的完成图被证实是一个NP-hard问题。因此，对<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x98_hanspub.png" xlink:type="simple"/></inline-formula>，首先要生成最小生成树，使得在包含所有顶点的情况下让边的权值的总和达到最小。在FAST算法中，运用Prim算法 [<xref ref-type="bibr" rid="hanspub.20398-ref16">16</xref>] 生成最小生成树。</p><p>接下来要解决如何分割最小生成树。利用上面的基于对称不确定性的冗余的定义，只要我们将最小生成树中所有权值小于<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x99_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x100_hanspub.png" xlink:type="simple"/></inline-formula>的边去掉，我们就能保证对于剩下的每一条边<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x101_hanspub.png" xlink:type="simple"/></inline-formula>，都有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x102_hanspub.png" xlink:type="simple"/></inline-formula>或<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x103_hanspub.png" xlink:type="simple"/></inline-formula>，从而可以知道同一类中的所有特征都是冗余的。经过上面的叙述，FAST算法的伪代码如表1：</p><p>在伪代码第9行中，让<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x104_hanspub.png" xlink:type="simple"/></inline-formula>加1是为了避免当<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x105_hanspub.png" xlink:type="simple"/></inline-formula>的值为零时相应的边的权值无效。</p></sec><sec id="s6_2"><title>4.2. IWFAST特征选择算法</title><p>首先，需要给出交互权值变量的定义 [<xref ref-type="bibr" rid="hanspub.20398-ref7">7</xref>] 。</p><p>从等式(2)我们可以看到，特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x106_hanspub.png" xlink:type="simple"/></inline-formula>的引入会影响特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x107_hanspub.png" xlink:type="simple"/></inline-formula>和目标集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x108_hanspub.png" xlink:type="simple"/></inline-formula>之间的独立性。正的交互增益意味着我们只有将两个特征合在一起考虑，才能描述他们的关系，同时会增加独立性的值。也就是说，特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x109_hanspub.png" xlink:type="simple"/></inline-formula>的引入对预测目标集有正的影响。相应地，我们需要增加特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x110_hanspub.png" xlink:type="simple"/></inline-formula>的权重。负的交互增益意味着新的特征的加入会抑制独立性的大小，也就是说，特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x111_hanspub.png" xlink:type="simple"/></inline-formula>的引入对预测目标集有负的影响。相应地，我们需要减少特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x112_hanspub.png" xlink:type="simple"/></inline-formula>的权重。因此，可以基于交互增益定义一个交互权值变量，它使得分析特征间的关系成为可能。</p><p>定义7 (交互权值变量)两个特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x113_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x114_hanspub.png" xlink:type="simple"/></inline-formula>之间的交互权值变量定义为：</p><disp-formula id="hanspub.20398-formula187"><label>(6)</label><graphic position="anchor" xlink:href="http://html.hanspub.org/file/3-1760114x115_hanspub.png"  xlink:type="simple"/></disp-formula><table-wrap id="table1" ><label><xref ref-type="table" rid="table1">Table 1</xref></label><caption><title> Pseudocode of FAST Algorith</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x116_hanspub.png" xlink:type="simple"/></inline-formula> 输入：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x117_hanspub.png" xlink:type="simple"/></inline-formula> -已知数据集；<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x118_hanspub.png" xlink:type="simple"/></inline-formula> -预定的阈值 输出：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x119_hanspub.png" xlink:type="simple"/></inline-formula> -最终的特征子集 //Part1：剔除不相关特征 1 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x124_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x123_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x122_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x121_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x120_hanspub.png" xlink:type="simple"/></inline-formula> 2 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x125_hanspub.png" xlink:type="simple"/></inline-formula> 3 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x128_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x127_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x126_hanspub.png" xlink:type="simple"/></inline-formula> 4 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x129_hanspub.png" xlink:type="simple"/></inline-formula> 5 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x130_hanspub.png" xlink:type="simple"/></inline-formula> 6 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x131_hanspub.png" xlink:type="simple"/></inline-formula> //Part2：最小生成树的构造 7 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x132_hanspub.png" xlink:type="simple"/></inline-formula>//<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x133_hanspub.png" xlink:type="simple"/></inline-formula>是一个完成图 8<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x134_hanspub.png" xlink:type="simple"/></inline-formula>每一对特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x136_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x135_hanspub.png" xlink:type="simple"/></inline-formula> 9 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x137_hanspub.png" xlink:type="simple"/></inline-formula> 10 将<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x138_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x139_hanspub.png" xlink:type="simple"/></inline-formula>加入<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x140_hanspub.png" xlink:type="simple"/></inline-formula>中并将<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x141_hanspub.png" xlink:type="simple"/></inline-formula>作为他们的边的权值; 11 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x142_hanspub.png" xlink:type="simple"/></inline-formula> 12 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x143_hanspub.png" xlink:type="simple"/></inline-formula>//使用<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x144_hanspub.png" xlink:type="simple"/></inline-formula>算法生成最小生成树 //Part3：树的分割 13 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x145_hanspub.png" xlink:type="simple"/></inline-formula> 14<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x146_hanspub.png" xlink:type="simple"/></inline-formula>每一条边<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x148_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x147_hanspub.png" xlink:type="simple"/></inline-formula> 15 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x151_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x150_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x149_hanspub.png" xlink:type="simple"/></inline-formula> 16 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x152_hanspub.png" xlink:type="simple"/></inline-formula>//剪枝 17 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x153_hanspub.png" xlink:type="simple"/></inline-formula> 18 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x154_hanspub.png" xlink:type="simple"/></inline-formula> //Part4：代表特征的选择 19 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x155_hanspub.png" xlink:type="simple"/></inline-formula> 20<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x156_hanspub.png" xlink:type="simple"/></inline-formula>每一棵树<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x158_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x157_hanspub.png" xlink:type="simple"/></inline-formula> 21 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x159_hanspub.png" xlink:type="simple"/></inline-formula> 22 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x160_hanspub.png" xlink:type="simple"/></inline-formula> 23<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x161_hanspub.png" xlink:type="simple"/></inline-formula> 24<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x163_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x162_hanspub.png" xlink:type="simple"/></inline-formula></th></tr></thead></tbody></table></table-wrap><p>表1. FAST算法的伪代码</p><p>对于交互权值变量，有以下三个定理 [<xref ref-type="bibr" rid="hanspub.20398-ref7">7</xref>] ：</p><p>定理1<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x164_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>定理2 如果特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x165_hanspub.png" xlink:type="simple"/></inline-formula>相对于特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x166_hanspub.png" xlink:type="simple"/></inline-formula>是冗余的，则有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x167_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>定理3 如果特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x168_hanspub.png" xlink:type="simple"/></inline-formula>相对于特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x169_hanspub.png" xlink:type="simple"/></inline-formula>是交互的，则有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x170_hanspub.png" xlink:type="simple"/></inline-formula>。</p><p>FAST算法在考虑特征的冗余时并没有考虑特征的交互作用，这会导致在特征选择过程中损失某些有用的特征变量。为了解决这一问题，我们使用交互权值变量(<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x171_hanspub.png" xlink:type="simple"/></inline-formula>)去修正特征之间或者特征与目标集之间的对称不确定性(<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x172_hanspub.png" xlink:type="simple"/></inline-formula>)。</p><p>有了交互权值变量的定义和定理之后，我们给出IWFAST算法的伪代码如表2所示：</p><p>在IWFAST算法中，我们去掉了剔除不相关特征这一步骤，将所有特征形成完全图，这是因为由于交互作用的影响，不相关特征跟其他特征一起也可能为预测目标集提供有用的信息，因此，在一开始就剔除不相关特征有可能会损失有价值的特征。所以IWFAST算法的框架如图1的右图所示：</p><table-wrap id="table2" ><label><xref ref-type="table" rid="table2">Table 2</xref></label><caption><title> Pseudocode of IWFAST Algorith</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >算法：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x173_hanspub.png" xlink:type="simple"/></inline-formula> 输入：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x174_hanspub.png" xlink:type="simple"/></inline-formula> -已知数据集；<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x175_hanspub.png" xlink:type="simple"/></inline-formula> -预定的阈值 输出：<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x176_hanspub.png" xlink:type="simple"/></inline-formula> -最终的特征子集 //Part1：最小生成树的构造 1 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x177_hanspub.png" xlink:type="simple"/></inline-formula>//<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x178_hanspub.png" xlink:type="simple"/></inline-formula>是一个完成图 2<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x179_hanspub.png" xlink:type="simple"/></inline-formula>每一对特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x181_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x180_hanspub.png" xlink:type="simple"/></inline-formula> 3 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x182_hanspub.png" xlink:type="simple"/></inline-formula> 4 将<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x183_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x184_hanspub.png" xlink:type="simple"/></inline-formula>加入<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x185_hanspub.png" xlink:type="simple"/></inline-formula>中并将<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x186_hanspub.png" xlink:type="simple"/></inline-formula>作为他们的边的权值; 5 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x187_hanspub.png" xlink:type="simple"/></inline-formula> 6 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x188_hanspub.png" xlink:type="simple"/></inline-formula>//使用<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x189_hanspub.png" xlink:type="simple"/></inline-formula>算法生成最小生成树 //Part2：树的分割 7 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x190_hanspub.png" xlink:type="simple"/></inline-formula> 8<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x191_hanspub.png" xlink:type="simple"/></inline-formula>每一条边<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x193_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x192_hanspub.png" xlink:type="simple"/></inline-formula> 9 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x196_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x195_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x194_hanspub.png" xlink:type="simple"/></inline-formula> 10 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x197_hanspub.png" xlink:type="simple"/></inline-formula>//剪枝 11 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x198_hanspub.png" xlink:type="simple"/></inline-formula> 12 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x199_hanspub.png" xlink:type="simple"/></inline-formula> //Part3：代表特征的选择 13 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x200_hanspub.png" xlink:type="simple"/></inline-formula> 14 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x205_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x204_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x203_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x202_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x201_hanspub.png" xlink:type="simple"/></inline-formula> 15 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x206_hanspub.png" xlink:type="simple"/></inline-formula> 16 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x207_hanspub.png" xlink:type="simple"/></inline-formula> 17 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x208_hanspub.png" xlink:type="simple"/></inline-formula> 18 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x211_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x210_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x209_hanspub.png" xlink:type="simple"/></inline-formula> 19 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x212_hanspub.png" xlink:type="simple"/></inline-formula> 20 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x213_hanspub.png" xlink:type="simple"/></inline-formula> 21 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x214_hanspub.png" xlink:type="simple"/></inline-formula> 22 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x217_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x216_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x215_hanspub.png" xlink:type="simple"/></inline-formula> 23 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x220_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x219_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x218_hanspub.png" xlink:type="simple"/></inline-formula> 24 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x221_hanspub.png" xlink:type="simple"/></inline-formula> 25 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x222_hanspub.png" xlink:type="simple"/></inline-formula> 26 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x223_hanspub.png" xlink:type="simple"/></inline-formula> 27 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x224_hanspub.png" xlink:type="simple"/></inline-formula> 28 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x225_hanspub.png" xlink:type="simple"/></inline-formula> 29 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x226_hanspub.png" xlink:type="simple"/></inline-formula> 30 给<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x227_hanspub.png" xlink:type="simple"/></inline-formula>去重得到<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x228_hanspub.png" xlink:type="simple"/></inline-formula>; 31 <inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x230_hanspub.png" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x229_hanspub.png" xlink:type="simple"/></inline-formula></th></tr></thead></tbody></table></table-wrap><p>表2. IWFAST算法的伪代码</p><p>在构造最小生成树这一部分，在确定边的权值时(即伪代码第3行)，我们在FAST算法的基础上乘以了<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x231_hanspub.png" xlink:type="simple"/></inline-formula>，因为如前所述，如果<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x232_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x233_hanspub.png" xlink:type="simple"/></inline-formula>相互冗余，则<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x234_hanspub.png" xlink:type="simple"/></inline-formula>，于是调整后的权值比FAST算法的权值小；若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x235_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x236_hanspub.png" xlink:type="simple"/></inline-formula>有交互作用，则<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x237_hanspub.png" xlink:type="simple"/></inline-formula>，那么调整后的权值比FAST算法的权值大。于是，在生成最小生成树时，交互特征的边就更可能被去掉，而更可能保留冗余特征的边，从而使得冗余的特征更可能被分在同一类中，交互的特征被分在不同的类中。</p><p>对于树的分割部分，跟FAST算法相同，以使得在同一类中的特征都是冗余的。而在代表特征的选择这一部分中，FAST算法是直接将每一类中使得<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x238_hanspub.png" xlink:type="simple"/></inline-formula>最大的特征作为代表特征，同样的，我们要对<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x239_hanspub.png" xlink:type="simple"/></inline-formula>进行调整，调整方法是：对于每一个特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x240_hanspub.png" xlink:type="simple"/></inline-formula>，找出使得<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x241_hanspub.png" xlink:type="simple"/></inline-formula>最大的特征<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x242_hanspub.png" xlink:type="simple"/></inline-formula>，从而用这个最大值乘以<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x243_hanspub.png" xlink:type="simple"/></inline-formula>得到调整后的标准，如伪代码14-17行所示。如果<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x244_hanspub.png" xlink:type="simple"/></inline-formula>的最大值足够大，表明<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x245_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x246_hanspub.png" xlink:type="simple"/></inline-formula>的交互作用值得被重视，就要将<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x247_hanspub.png" xlink:type="simple"/></inline-formula>和<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x248_hanspub.png" xlink:type="simple"/></inline-formula>一起放入特征子集中。</p><p>在本文中，经过多次实验，我们决定将<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x249_hanspub.png" xlink:type="simple"/></inline-formula>视为是否重视该交互作用的一个标准。如果未达到1.05，则忽略这两个特征的交互作用；如果满足条件，就要把两个特征捆绑在一起考虑。伪代码第18-21行就实现了这一想法。接着，我们已经对每个类中的特征都求得了一个调整后的对称不确定<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x250_hanspub.png" xlink:type="simple"/></inline-formula>，在每个类中取使<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x251_hanspub.png" xlink:type="simple"/></inline-formula>达到最大值的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x252_hanspub.png" xlink:type="simple"/></inline-formula>，但与FAST算法不同，在IWFAST算法中，只有最大的这个<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x253_hanspub.png" xlink:type="simple"/></inline-formula>超过了一个特定的阈值，我们才将相应的特征加入特征子集，而该特征是否有交互的另一特征，由<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x254_hanspub.png" xlink:type="simple"/></inline-formula>的值决定，这也就是上面22-29行的内容。最后，由于有交互作用，得到的特征子集可能会有重复的特征，因此再对该特征子集去重，得到最终的特征子集<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x255_hanspub.png" xlink:type="simple"/></inline-formula>。</p></sec></sec><sec id="s7"><title>5. 实证分析</title><sec id="s7_1"><title>5.1. 数据来源</title><p>为了很好地评估我们的IWFAST算法的性能和效力，以证实该算法是否能有效地用于实践中，并且能够使其他学者信服我们的结果，我们从UCI数据库中选择了16个数据集来进行实证对比分析。</p><p>这16个数据集的特征个数的范围从4到259，样本个数的范围从32到41188，目标集的范围从2到17。此外，这16个数据集囊括了5个领域，分别为商业领域(Business)、社会领域(Social)、游戏领域(Game)、生物领域(Life)和物理领域(Physical)。表3展示了这些数据集的相关信息。</p><p>其中F、I和T分别表示特征的个数，样本的个数和目标分类的个数。</p></sec><sec id="s7_2"><title>5.2. 实验准备</title><p>为了评估IWFAST算法的性能和效力以及将其与FAST算法进行比较，我们首先要进行以下实验准备与分析：数据离散化，缺失值的处理，阈值的选择。</p><p>FAST和IWFAST都涉及阈值的设定。对于FAST算法，将阈值设定为在所有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x256_hanspub.png" xlink:type="simple"/></inline-formula>的值中从大到小排名第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x257_hanspub.png" xlink:type="simple"/></inline-formula>的元素 [<xref ref-type="bibr" rid="hanspub.20398-ref5">5</xref>] 所对应的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x258_hanspub.png" xlink:type="simple"/></inline-formula>值，若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x259_hanspub.png" xlink:type="simple"/></inline-formula>为小数，对其四舍五入取整。对于IWFAST算法，我们将阈值设定为在所有<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x260_hanspub.png" xlink:type="simple"/></inline-formula>值中从大到小排名第<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x261_hanspub.png" xlink:type="simple"/></inline-formula>的元素所对应的<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x262_hanspub.png" xlink:type="simple"/></inline-formula>值，若<inline-formula><inline-graphic xlink:href="http://html.hanspub.org/file/3-1760114x263_hanspub.png" xlink:type="simple"/></inline-formula>为小数，同样对其取四舍五入取整。</p></sec><sec id="s7_3"><title>5.3. 结果与分析</title><p>为了更好地比较两个算法选择的特征子集的优劣，我们选择了4个分类器对其进行评估，最后再求出4个分类器所得结果的平均值。这4个分类器分别为决策树C5.0、贝叶斯网络(Bayes Net)、神经网络(Neural Net)和Logistic回归。评估过程依然使用软件Clementine 12.0。同时，所有数据集以70%作为训练集，30%作为测试集。此外，在决策树C5.0中，我们使用10折交叉验证法建立模型。我们将从三个方面分析两个算法的优劣：1) 选择的子集的特征个数；2) 获得特征子集所需要的运行时间；3) 分类器的准确率。特征个数与运行时间的比较统一记录于表4中。</p><p>从表4可以看出，有时FAST比IWFAST筛选的特征多，有时又少。而对于运行时间，IWFAST需要的时间总是比FAST多，这也与本文第4章对算法时间复杂度的理论分析结果一致。</p><p>4种分类器所得到的准确率及平均值则在表5中记录。</p><table-wrap id="table3" ><label><xref ref-type="table" rid="table3">Table 3</xref></label><caption><title> Summarize of sixteen dataset</title></caption><table><tbody><thead><tr><th align="center" valign="middle" >No.</th><th align="center" valign="middle" >Data Name</th><th align="center" valign="middle" >F</th><th align="center" valign="middle" >I</th><th align="center" valign="middle" >T</th><th align="center" valign="middle" >Area</th></tr></thead><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >Iris</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >150</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >Life</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Balance Scale</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >625</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >Social</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >Istanbul Stock Exchange</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >536</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >Business</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >Tic-Tac-Toe Endgame</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >958</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Game</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >Wine</td><td align="center" valign="middle" >13</td><td align="center" valign="middle" >178</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >Physical</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >Statlog (Australian Credit Approval)</td><td align="center" valign="middle" >14</td><td align="center" valign="middle" >690</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Business</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >Congressional Voting Records</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >435</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Social</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >Bank Marketing</td><td align="center" valign="middle" >20</td><td align="center" valign="middle" >41188</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Business</td></tr><tr><td align="center" valign="middle" >9</td><td align="center" valign="middle" >Ionosphere</td><td align="center" valign="middle" >33</td><td align="center" valign="middle" >351</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Physical</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >Chess (King-Rook vs. King-Pawn)</td><td align="center" valign="middle" >36</td><td align="center" valign="middle" >3196</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Game</td></tr><tr><td align="center" valign="middle" >11</td><td align="center" valign="middle" >Spectf Heart</td><td align="center" valign="middle" >44</td><td align="center" valign="middle" >267</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Life</td></tr><tr><td align="center" valign="middle" >12</td><td align="center" valign="middle" >Lung-cancer</td><td align="center" valign="middle" >56</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >Life</td></tr><tr><td align="center" valign="middle" >13</td><td align="center" valign="middle" >Audiology</td><td align="center" valign="middle" >68</td><td align="center" valign="middle" >226</td><td align="center" valign="middle" >17</td><td align="center" valign="middle" >Life</td></tr><tr><td align="center" valign="middle" >14</td><td align="center" valign="middle" >Ozone Level Detection</td><td align="center" valign="middle" >72</td><td align="center" valign="middle" >2158</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Physical</td></tr><tr><td align="center" valign="middle" >15</td><td align="center" valign="middle" >Musk1</td><td align="center" valign="middle" >166</td><td align="center" valign="middle" >476</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >Physical</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >Arrhythmia</td><td align="center" valign="middle" >259</td><td align="center" valign="middle" >452</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >Life</td></tr></tbody></table></table-wrap><p>表3. 16个数据集的概况</p><table-wrap id="table4" ><label><xref ref-type="table" rid="table4">Table 4</xref></label><caption><title> Number of features and running tim</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >No.</th><th align="center" valign="middle"  rowspan="2"  >Instances</th><th align="center" valign="middle"  rowspan="2"  >Features</th><th align="center" valign="middle"  colspan="2"  >Number of features selected</th><th align="center" valign="middle"  colspan="2"  >Running time (in s)</th></tr></thead><tr><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td></tr><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >150</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >0.989</td><td align="center" valign="middle" >0.768</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >625</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >1.184</td><td align="center" valign="middle" >0.908</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >536</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >1.498</td><td align="center" valign="middle" >0.977</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >958</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1.574</td><td align="center" valign="middle" >1.376</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >178</td><td align="center" valign="middle" >13</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >2.524</td><td align="center" valign="middle" >0.909</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >690</td><td align="center" valign="middle" >14</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >3.212</td><td align="center" valign="middle" >1.225</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >435</td><td align="center" valign="middle" >16</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >1.872</td><td align="center" valign="middle" >0.995</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >41188</td><td align="center" valign="middle" >20</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >2916.119</td><td align="center" valign="middle" >427.275</td></tr><tr><td align="center" valign="middle" >9</td><td align="center" valign="middle" >351</td><td align="center" valign="middle" >33</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >17.135</td><td align="center" valign="middle" >3.492</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >3196</td><td align="center" valign="middle" >36</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >11.277</td><td align="center" valign="middle" >3.061</td></tr><tr><td align="center" valign="middle" >11</td><td align="center" valign="middle" >267</td><td align="center" valign="middle" >44</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >3</td><td align="center" valign="middle" >21.384</td><td align="center" valign="middle" >3.441</td></tr><tr><td align="center" valign="middle" >12</td><td align="center" valign="middle" >32</td><td align="center" valign="middle" >56</td><td align="center" valign="middle" >10</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >36.294</td><td align="center" valign="middle" >4.071</td></tr><tr><td align="center" valign="middle" >13</td><td align="center" valign="middle" >226</td><td align="center" valign="middle" >68</td><td align="center" valign="middle" >7</td><td align="center" valign="middle" >12</td><td align="center" valign="middle" >171.739</td><td align="center" valign="middle" >5.309</td></tr><tr><td align="center" valign="middle" >14</td><td align="center" valign="middle" >2158</td><td align="center" valign="middle" >72</td><td align="center" valign="middle" >6</td><td align="center" valign="middle" >1</td><td align="center" valign="middle" >54.017</td><td align="center" valign="middle" >9.285</td></tr><tr><td align="center" valign="middle" >15</td><td align="center" valign="middle" >476</td><td align="center" valign="middle" >166</td><td align="center" valign="middle" >9</td><td align="center" valign="middle" >12</td><td align="center" valign="middle" >236.195</td><td align="center" valign="middle" >32.328</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >452</td><td align="center" valign="middle" >259</td><td align="center" valign="middle" >11</td><td align="center" valign="middle" >31</td><td align="center" valign="middle" >5119.651</td><td align="center" valign="middle" >123.520</td></tr></tbody></table></table-wrap><p>表4. 特征个数和运行时间</p><table-wrap id="table5" ><label><xref ref-type="table" rid="table5">Table 5</xref></label><caption><title> Accuracy of four classifiers and the average accurac</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >No.</th><th align="center" valign="middle"  colspan="2"  >C5.0</th><th align="center" valign="middle"  colspan="2"  >Bayes Net</th><th align="center" valign="middle"  colspan="2"  >Neural Net</th><th align="center" valign="middle"  colspan="2"  >Logistic</th><th align="center" valign="middle"  colspan="2"  >Average</th></tr></thead><tr><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td></tr><tr><td align="center" valign="middle" >1</td><td align="center" valign="middle" >95.74</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >95.74</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >95.74</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >96.81</td><td align="center" valign="middle" >100</td></tr><tr><td align="center" valign="middle" >2</td><td align="center" valign="middle" >82.92</td><td align="center" valign="middle" >82.92</td><td align="center" valign="middle" >85.94</td><td align="center" valign="middle" >85.94</td><td align="center" valign="middle" >91.15</td><td align="center" valign="middle" >91.15</td><td align="center" valign="middle" >88.54</td><td align="center" valign="middle" >88.54</td><td align="center" valign="middle" >87.14</td><td align="center" valign="middle" >87.14</td></tr><tr><td align="center" valign="middle" >3</td><td align="center" valign="middle" >65.62</td><td align="center" valign="middle" >66.62</td><td align="center" valign="middle" >68.12</td><td align="center" valign="middle" >66.62</td><td align="center" valign="middle" >70</td><td align="center" valign="middle" >66.62</td><td align="center" valign="middle" >71.25</td><td align="center" valign="middle" >66.62</td><td align="center" valign="middle" >68.75</td><td align="center" valign="middle" >66.62</td></tr><tr><td align="center" valign="middle" >4</td><td align="center" valign="middle" >81.4</td><td align="center" valign="middle" >73.33</td><td align="center" valign="middle" >75.44</td><td align="center" valign="middle" >70.88</td><td align="center" valign="middle" >68.07</td><td align="center" valign="middle" >66.67</td><td align="center" valign="middle" >60.35</td><td align="center" valign="middle" >61.4</td><td align="center" valign="middle" >71.32</td><td align="center" valign="middle" >68.07</td></tr><tr><td align="center" valign="middle" >5</td><td align="center" valign="middle" >92.98</td><td align="center" valign="middle" >98.25</td><td align="center" valign="middle" >94.74</td><td align="center" valign="middle" >94.74</td><td align="center" valign="middle" >96.49</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >94.74</td><td align="center" valign="middle" >100</td><td align="center" valign="middle" >94.74</td><td align="center" valign="middle" >98.25</td></tr><tr><td align="center" valign="middle" >6</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td><td align="center" valign="middle" >83.01</td></tr><tr><td align="center" valign="middle" >7</td><td align="center" valign="middle" >99.28</td><td align="center" valign="middle" >99.28</td><td align="center" valign="middle" >97.12</td><td align="center" valign="middle" >98.56</td><td align="center" valign="middle" >99.28</td><td align="center" valign="middle" >99.28</td><td align="center" valign="middle" >97.12</td><td align="center" valign="middle" >98.56</td><td align="center" valign="middle" >98.2</td><td align="center" valign="middle" >98.92</td></tr><tr><td align="center" valign="middle" >8</td><td align="center" valign="middle" >90.03</td><td align="center" valign="middle" >89.94</td><td align="center" valign="middle" >90.12</td><td align="center" valign="middle" >89.94</td><td align="center" valign="middle" >90.25</td><td align="center" valign="middle" >89.94</td><td align="center" valign="middle" >90.23</td><td align="center" valign="middle" >89.94</td><td align="center" valign="middle" >90.16</td><td align="center" valign="middle" >89.94</td></tr><tr><td align="center" valign="middle" >9</td><td align="center" valign="middle" >91.89</td><td align="center" valign="middle" >91.89</td><td align="center" valign="middle" >90.99</td><td align="center" valign="middle" >85.59</td><td align="center" valign="middle" >95.5</td><td align="center" valign="middle" >95.5</td><td align="center" valign="middle" >83.78</td><td align="center" valign="middle" >90.99</td><td align="center" valign="middle" >90.54</td><td align="center" valign="middle" >90.99</td></tr><tr><td align="center" valign="middle" >10</td><td align="center" valign="middle" >94.66</td><td align="center" valign="middle" >67.19</td><td align="center" valign="middle" >94.66</td><td align="center" valign="middle" >67.19</td><td align="center" valign="middle" >94.66</td><td align="center" valign="middle" >67.19</td><td align="center" valign="middle" >94.66</td><td align="center" valign="middle" >67.19</td><td align="center" valign="middle" >94.66</td><td align="center" valign="middle" >67.19</td></tr><tr><td align="center" valign="middle" >11</td><td align="center" valign="middle" >76.47</td><td align="center" valign="middle" >76.47</td><td align="center" valign="middle" >80</td><td align="center" valign="middle" >76.47</td><td align="center" valign="middle" >80</td><td align="center" valign="middle" >76.47</td><td align="center" valign="middle" >80</td><td align="center" valign="middle" >76.47</td><td align="center" valign="middle" >79.12</td><td align="center" valign="middle" >76.47</td></tr><tr><td align="center" valign="middle" >12</td><td align="center" valign="middle" >75</td><td align="center" valign="middle" >59.38</td><td align="center" valign="middle" >81.25</td><td align="center" valign="middle" >87.5</td><td align="center" valign="middle" >93.75</td><td align="center" valign="middle" >84.38</td><td align="center" valign="middle" >87.5</td><td align="center" valign="middle" >78.12</td><td align="center" valign="middle" >84.38</td><td align="center" valign="middle" >77.35</td></tr><tr><td align="center" valign="middle" >13</td><td align="center" valign="middle" >72.22</td><td align="center" valign="middle" >69.44</td><td align="center" valign="middle" >73.61</td><td align="center" valign="middle" >65.28</td><td align="center" valign="middle" >77.78</td><td align="center" valign="middle" >76.39</td><td align="center" valign="middle" >69.44</td><td align="center" valign="middle" >55.56</td><td align="center" valign="middle" >73.26</td><td align="center" valign="middle" >66.67</td></tr><tr><td align="center" valign="middle" >14</td><td align="center" valign="middle" >96.6</td><td align="center" valign="middle" >96.6</td><td align="center" valign="middle" >96.6</td><td align="center" valign="middle" >96.6</td><td align="center" valign="middle" >97.19</td><td align="center" valign="middle" >96.6</td><td align="center" valign="middle" >96.45</td><td align="center" valign="middle" >96.6</td><td align="center" valign="middle" >96.71</td><td align="center" valign="middle" >96.6</td></tr><tr><td align="center" valign="middle" >15</td><td align="center" valign="middle" >75</td><td align="center" valign="middle" >75</td><td align="center" valign="middle" >79.73</td><td align="center" valign="middle" >76.35</td><td align="center" valign="middle" >74.32</td><td align="center" valign="middle" >78.38</td><td align="center" valign="middle" >77.7</td><td align="center" valign="middle" >76.35</td><td align="center" valign="middle" >76.69</td><td align="center" valign="middle" >76.52</td></tr><tr><td align="center" valign="middle" >16</td><td align="center" valign="middle" >64.14</td><td align="center" valign="middle" >68.97</td><td align="center" valign="middle" >53.1</td><td align="center" valign="middle" >11.93</td><td align="center" valign="middle" >69.66</td><td align="center" valign="middle" >77.24</td><td align="center" valign="middle" >62.07</td><td align="center" valign="middle" >42.76</td><td align="center" valign="middle" >62.24</td><td align="center" valign="middle" >50.23</td></tr></tbody></table></table-wrap><p>表5. 4种分类器的准确率以其平均值</p><p>从平均值来分析，对于数据集1、5、7、9，FAST比IWFAST的准确率高；对于2、6，两者的准确率相同；对于数据集3、4、8、10、11、12、13、14、15、16，IWFAST比FAST的准确率高。可见对于特征个数较多的数据集，IWFAST更有优势，这也是因为在特征较多的情形下，FAST更有可能将一些有用的特征剔除，而IWFAST由于考虑了交互作用就更可能保留一些有用的特征。此外，就准确率而言，在FAST优于IWFAST的情形下，FAST与IWAFST的准确率之差并不大于4%，而IWFAST优于FAST的情形，IWFAST与FAST的准确率之差最大可达到27.47%。因此可以大致认为IWFAST分类能力优于FAST。</p><p>为了更好地比较两个算法的优劣，我们按领域分类取平均并求出了所有数据集总的平均值，三种评判标准的各领域平均值及总平均值记录如表6所示。</p><p>下面对表6进行分析。</p><p>首先是运行时间，IWFAST的运行时间都比FAST要长，但仍在可接受的范围内。</p><p>接着对特征个数和准确率进行讨论。对于Life和Physical领域，IWFAT能够减少FAST选择的特征的个数，并且提高预测的准确率，IWFAST分别使准确率提高了5.02%和0.11%。然而在Social领域中，IWFAST增加了特征的个数，但是却导致准确率下降0.36%，不过这一变化幅度较小，也就是说，两个算法相差不大。对于Business和Game领域，IWFAST通过增加特征个数使准确率分别提高了0.78%和13.3%。在各个领域中两个算法的特征个数相差不大，而准确率相差较大的是Life和Game领域，从这一角度，在Life和Game领域中，IWFAST的优势更为明显，尤其是Game领域。</p><p>对于总的平均值，IWFAST的特征个数小于FAST，运行时间长于FAST，而准确率比FAST算法高3.36%，因此，本文对FAST算法的改进是合理与可行的。</p><table-wrap id="table6" ><label><xref ref-type="table" rid="table6">Table 6</xref></label><caption><title> Average values for five kinds of item</title></caption><table><tbody><thead><tr><th align="center" valign="middle"  rowspan="2"  >Average</th><th align="center" valign="middle"  colspan="2"  >Number of features selected</th><th align="center" valign="middle"  colspan="2"  >Running time (in s)</th><th align="center" valign="middle"  colspan="2"  >Accuracy (%)</th></tr></thead><tr><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td><td align="center" valign="middle" >IWFAST</td><td align="center" valign="middle" >FAST</td></tr><tr><td align="center" valign="middle" >Life</td><td align="center" valign="middle" >7.2</td><td align="center" valign="middle" >10.4</td><td align="center" valign="middle" >1117.633</td><td align="center" valign="middle" >27.422</td><td align="center" valign="middle" >79.16</td><td align="center" valign="middle" >74.14</td></tr><tr><td align="center" valign="middle" >Social</td><td align="center" valign="middle" >4</td><td align="center" valign="middle" >3.5</td><td align="center" valign="middle" >1.508</td><td align="center" valign="middle" >0.952</td><td align="center" valign="middle" >92.67</td><td align="center" valign="middle" >93.03</td></tr><tr><td align="center" valign="middle" >Business</td><td align="center" valign="middle" >3.33</td><td align="center" valign="middle" >1.67</td><td align="center" valign="middle" >980.22</td><td align="center" valign="middle" >143.159</td><td align="center" valign="middle" >80.64</td><td align="center" valign="middle" >79.86</td></tr><tr><td align="center" valign="middle" >Game</td><td align="center" valign="middle" >5</td><td align="center" valign="middle" >2</td><td align="center" valign="middle" >6.427</td><td align="center" valign="middle" >2.219</td><td align="center" valign="middle" >80.93</td><td align="center" valign="middle" >67.63</td></tr><tr><td align="center" valign="middle" >Physical</td><td align="center" valign="middle" >5.75</td><td align="center" valign="middle" >6.5</td><td align="center" valign="middle" >81.092</td><td align="center" valign="middle" >11.504</td><td align="center" valign="middle" >90.7</td><td align="center" valign="middle" >90.59</td></tr><tr><td align="center" valign="middle" >Total average</td><td align="center" valign="middle" >5.44</td><td align="center" valign="middle" >5.88</td><td align="center" valign="middle" >537.29</td><td align="center" valign="middle" >38.684</td><td align="center" valign="middle" >84.23</td><td align="center" valign="middle" >80.87</td></tr></tbody></table></table-wrap><p>表6. 5个领域的各项平均值的比较</p></sec></sec><sec id="s8"><title>6. 结论</title><p>特征选择的主要目的是找出一个尽可能小的特征子集，使得该特征子集能够准确地预测目标集。特征交互存在于很多实际情形中，但找出有交互作用的特征是一项具有挑战性的任务。本文旨在对FAST算法进行改进，在FAST算法的基础上考虑特征的交互作用，使得在移除不相关和冗余特征的同时，保留有交互作用的特征。改进的IWFAST算法去掉了FAST中移除不相关特征这一步骤，对所有特征构造完成图，并在FAST的权值与相关度量的基础上加入交互权值变量，将调整后的度量作为完成图的权值，使得算法能够反映出特征的冗余或交互。接着，IWFAST算法仍需要生成最小生成树，分割最小生成树，选择代表特征，最终得到特征子集。</p><p>我们将FAST算法和IWFAST算法运用5个领域的16个公开数据集中，包括商业领域(Business)、社会领域(Social)、游戏领域(Game)、生物领域(Life)和物理领域(Physical)，并运用决策树C5.0、贝叶斯网络(Bayes Net)、神经网络(Neural Net)和Logistic回归这4个分类器对两个算法的结果进行评估，从特征个数、运行时间和准确率三个方面进行对比分析。实验结果表明，IWFAST算法的性能和效力要优于FAST算法，特别是在Game和Life领域，或者是对于特征数量较多的数据集。但是美中不足的是，IWFAST的运行时间较长，这是考虑交互作用所要付出的代价，不过也是在可接受的范围内。因此，IWFAST为基于聚类的特征选择算法提供了参考，也可以作为处理高维数据的一种有效方法。</p><p>当然，本文的算法还存在着一些不足，比如，IWFAST算法的运行时间偏长；算法只能运用于离散数据等，对连续数据离散化可能会损失部分信息。因此，我们将来的工作首先要重点关注IWFAST算法的优化，考虑是否有其他的编译环境或者编译语言能够缩减算法的运行时间。接着，要考虑是否可以将IWFAST推广到连续数据或混合型数据中。</p></sec><sec id="s9"><title>文章引用</title><p>陆碧云,张 磊. 考虑了特征协同作用的FAST特征选择算法的改进 A Kind of FAST Feature Selection Algorithm Considering Feature Interaction[J]. 数据挖掘, 2017, 07(02): 51-63. http://dx.doi.org/10.12677/HJDM.2017.72006</p></sec><sec id="s10"><title>参考文献 (References)</title></sec></body><back><ref-list><title>References</title><ref id="hanspub.20398-ref1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Guyon, I. and Elisseeff, A. (2003) An Introduction to Variable and Feature Selection. Journal of Machine Learning Research, 3, 1157-1182.</mixed-citation></ref><ref id="hanspub.20398-ref2"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Pereira, F. Tishby, N. and Lee, L. (2007) Distributional Clustering of English Words. Proceedings of Sixth International Conference on Advanced Language Processing and Web Information, Luoyang, 22-24 August 2007, 123-128.</mixed-citation></ref><ref id="hanspub.20398-ref3"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Baker, L.D. and McCallum, A.K. (1998) Distributional Clustering of Words for Text Classification. Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, 24-28 August 1998, 96-103. &lt;br&gt;https://doi.org/10.1145/290941.290970</mixed-citation></ref><ref id="hanspub.20398-ref4"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Dhillon, I.S., Mallela, S. and Kumar, R. (2003) A Divisive Infor-mation Theoretic Feature Clustering Algorithm for Text Classification. Machine Learning Research, 3, 1265-1287.</mixed-citation></ref><ref id="hanspub.20398-ref5"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Song, Q.B., Ni, J.J. and Wang, G.T. (2013) A Fast Clustering-Based Feature Subset Selection Algo-rithm for High-Dimensional Data. IEEE Transactions on Knowledge and Data Engineering, 25, 1.  
&lt;br&gt;https://doi.org/10.1109/TKDE.2011.181</mixed-citation></ref><ref id="hanspub.20398-ref6"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Jakulin, A. and Bratko, I. (2003) Analying Attribute Dependencies. Proceedings of Seventh European Conference on Principles and Practice of Knowledge Discovery in Databases, Cavtat-Dubrovnik, 22-26 September 2003, 229-240.</mixed-citation></ref><ref id="hanspub.20398-ref7"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Zeng, Z.L., Zhang, H.J., Zhang, R. and Yin, C.X. (2015) A Novel Feature Selection Method Considering Feature Interaction. Pattern Recognition, 48, 2656-2666. &lt;br&gt;https://doi.org/10.1016/j.patcog.2015.02.025</mixed-citation></ref><ref id="hanspub.20398-ref8"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Butterworth, R., Piatetcky-Shapiro, G. and Simovici, D.A. (2005) On Feature Selection through Clustering. Proceedings of the Fifth IEEE International Conference on Data Mining, Washington DC, 27-30 November 2005, 581-84. 
&lt;br&gt;https://doi.org/10.1109/ICDM.2005.106</mixed-citation></ref><ref id="hanspub.20398-ref9"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Van Dijck, G. and Van Hulle, M.M. (2006) Speeding Up the Wrapper Feature Subset Selection in Regression by Mutual Information Relevance and Redundancy Analysis. 16th In-ternational Conference: Artificial Neural Networks. Athens, 10-14 September 2006.</mixed-citation></ref><ref id="hanspub.20398-ref10"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Krier, C., Francois, D., Rossi, F. and Verleysen, M. (2007) Feature Clustering and Mutual Information for the Selection of Variables in Spectral Data. Proceedings of European Symposium on Artificial Neural Networks, Bruges, 25-27 April 2007, 157-162.</mixed-citation></ref><ref id="hanspub.20398-ref11"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Zhao, Z. and Liu, H. (2013) Searching for Interacting Features in Subset Selection. Intelligent Data Analysis, 13, 207- 228.</mixed-citation></ref><ref id="hanspub.20398-ref12"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Wang, G., Song, Q., Xu, B. and Zhou, Y. (2013) Selecting Feature Subset for High Dimension Data via the Propositional FOIL Rules. Pattern Recognition, 46, 199-214. &lt;br&gt;https://doi.org/10.1016/j.patcog.2012.07.028</mixed-citation></ref><ref id="hanspub.20398-ref13"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Jakulin, A. and Bratko, I. (2004) Testing the Significance of Attribute Interactions. Proceedings of the Twenty-First International Conference on Machine Learning, Banff, 4-8 July 2004, 409-416.  
&lt;br&gt;https://doi.org/10.1145/1015330.1015377</mixed-citation></ref><ref id="hanspub.20398-ref14"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Jakulin, A. (2003) Attribute Interactions in Machine Learning. Master Thesis, University of Liubljana, Kongresni.</mixed-citation></ref><ref id="hanspub.20398-ref15"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">John, G.H., Kohavi, R. and Fleger, K.P. (2012) Irrelevant Features and the Subset Selection Problem. Proceedings of the Eleventh International Conference on Machine Learning, Boca Raton, 12-15 December 2012, 121-129.</mixed-citation></ref><ref id="hanspub.20398-ref16"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Prim, R.C. (1957) Shortest Connection Networks and Some Generalizations. Bell System Technical Journal, 36, 1389- 1401. &lt;br&gt;https://doi.org/10.1002/j.1538-7305.1957.tb01515.x</mixed-citation></ref></ref-list></back></article>